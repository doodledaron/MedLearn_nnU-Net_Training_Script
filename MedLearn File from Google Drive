{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzfrYMubBu4G"
   },
   "source": [
    "# Lung Cancer Segmentation using nnU-Net\n",
    "Author: Tang Yu Xuan <br />\n",
    "InstitutiAS\n",
    "5. Evaluation\n",
    "6. Visualization\n",
    "7. Clinical Validation\n",
    "8. Results Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9EAQVeUuuQb"
   },
   "source": [
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodledaron\n"
     ]
    }
   ],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQqX-4fyE0AE"
   },
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 56991,
     "status": "ok",
     "timestamp": 1734495028037,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "h4a_6Pd6FFUv",
    "outputId": "60ab644b-0908-4169-bac9-db3cbea7bbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nnunetv2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torch>=2.1.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (2.5.1)\n",
      "Requirement already satisfied: acvl-utils<0.3,>=0.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (0.2.2)\n",
      "Requirement already satisfied: dynamic-network-architectures<0.4,>=0.3.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (0.3.1)\n",
      "Requirement already satisfied: tqdm in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (4.67.1)\n",
      "Requirement already satisfied: dicom2nifti in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (2.5.1)\n",
      "Requirement already satisfied: scipy in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (1.14.1)\n",
      "Requirement already satisfied: batchgenerators>=0.25 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (0.25.1)\n",
      "Requirement already satisfied: numpy in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (1.6.0)\n",
      "Requirement already satisfied: scikit-image>=0.19.3 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (0.25.0)\n",
      "Requirement already satisfied: SimpleITK>=2.2.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (2.4.0)\n",
      "Requirement already satisfied: pandas in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (2.2.3)\n",
      "Requirement already satisfied: graphviz in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (0.20.3)\n",
      "Requirement already satisfied: tifffile in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (2024.12.12)\n",
      "Requirement already satisfied: requests in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (2.32.3)\n",
      "Requirement already satisfied: nibabel in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (5.3.2)\n",
      "Requirement already satisfied: matplotlib in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (3.10.0)\n",
      "Requirement already satisfied: seaborn in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (0.13.2)\n",
      "Requirement already satisfied: imagecodecs in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (2024.9.22)\n",
      "Requirement already satisfied: yacs in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (0.1.8)\n",
      "Requirement already satisfied: batchgeneratorsv2>=0.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (0.2.1)\n",
      "Requirement already satisfied: einops in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nnunetv2) (0.8.0)\n",
      "Requirement already satisfied: connected-components-3d in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from acvl-utils<0.3,>=0.2->nnunetv2) (3.22.0)\n",
      "Requirement already satisfied: blosc2>=3.0.0b4 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from acvl-utils<0.3,>=0.2->nnunetv2) (3.0.0b4)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2) (11.0.0)\n",
      "Requirement already satisfied: future in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2) (1.0.0)\n",
      "Requirement already satisfied: unittest2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2) (3.5.0)\n",
      "Requirement already satisfied: fft-conv-pytorch in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from batchgeneratorsv2>=0.2->nnunetv2) (1.2.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2) (3.4.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2) (2.36.1)\n",
      "Requirement already satisfied: packaging>=21 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2) (0.4)\n",
      "Requirement already satisfied: filelock in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.1.2->nnunetv2) (1.3.0)\n",
      "Requirement already satisfied: pydicom>=2.2.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from dicom2nifti->nnunetv2) (3.0.1)\n",
      "Requirement already satisfied: python-gdcm in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from dicom2nifti->nnunetv2) (3.0.24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib->nnunetv2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib->nnunetv2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib->nnunetv2) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib->nnunetv2) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib->nnunetv2) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib->nnunetv2) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nibabel->nnunetv2) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pandas->nnunetv2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pandas->nnunetv2) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests->nnunetv2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests->nnunetv2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests->nnunetv2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests->nnunetv2) (2024.12.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-learn->nnunetv2) (1.4.2)\n",
      "Requirement already satisfied: PyYAML in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from yacs->nnunetv2) (6.0.2)\n",
      "Requirement already satisfied: ndindex in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.9.2)\n",
      "Requirement already satisfied: msgpack in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.1.0)\n",
      "Requirement already satisfied: numexpr in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (2.10.2)\n",
      "Requirement already satisfied: py-cpuinfo in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (9.0.0)\n",
      "Requirement already satisfied: httpx in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (0.28.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->nnunetv2) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from jinja2->torch>=2.1.2->nnunetv2) (3.0.2)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: traceback2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from unittest2->batchgenerators>=0.25->nnunetv2) (1.4.0)\n",
      "Requirement already satisfied: anyio in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from httpcore==1.*->httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (0.14.0)\n",
      "Requirement already satisfied: linecache2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from anyio->httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from anyio->httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.3.1)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n",
      "Requirement already satisfied: wandb in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (5.29.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (6.1.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (2.10.4)\n",
      "Requirement already satisfied: pyyaml in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (75.6.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\n",
      "Collecting datetime\n",
      "  Using cached DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting zope.interface (from datetime)\n",
      "  Using cached zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: pytz in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from datetime) (2024.2)\n",
      "Requirement already satisfied: setuptools in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from zope.interface->datetime) (75.6.0)\n",
      "Using cached DateTime-5.5-py3-none-any.whl (52 kB)\n",
      "Using cached zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
      "Installing collected packages: zope.interface, datetime\n",
      "Successfully installed datetime-5.5 zope.interface-7.2\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "Requirement already satisfied: matplotlib in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: nibabel in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (5.3.2)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nibabel) (6.4.5)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nibabel) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nibabel) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from nibabel) (4.12.2)\n",
      "Requirement already satisfied: numpy in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: torch in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting logging\n",
      "  Using cached logging-0.4.9.6.tar.gz (96 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[21 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 14, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/setuptools/__init__.py\", line 22, in <module>\n",
      "  \u001b[31m   \u001b[0m     import _distutils_hack.override  # noqa: F401\n",
      "  \u001b[31m   \u001b[0m   File \"/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/_distutils_hack/override.py\", line 1, in <module>\n",
      "  \u001b[31m   \u001b[0m     __import__('_distutils_hack').do_override()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/_distutils_hack/__init__.py\", line 89, in do_override\n",
      "  \u001b[31m   \u001b[0m     ensure_local_distutils()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/_distutils_hack/__init__.py\", line 75, in ensure_local_distutils\n",
      "  \u001b[31m   \u001b[0m     core = importlib.import_module('distutils.core')\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "  \u001b[31m   \u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 13, in <module>\n",
      "  \u001b[31m   \u001b[0m     from .cmd import Command\n",
      "  \u001b[31m   \u001b[0m   File \"/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 7, in <module>\n",
      "  \u001b[31m   \u001b[0m     import logging\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-qp96c81x/logging_110dcf3b3fb349a0ae974464383c0ddb/logging/__init__.py\", line 618\n",
      "  \u001b[31m   \u001b[0m     raise NotImplementedError, 'emit must be implemented '\\\n",
      "  \u001b[31m   \u001b[0m                              ^\n",
      "  \u001b[31m   \u001b[0m SyntaxError: invalid syntax\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Collecting pathlib\n",
      "  Using cached pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Using cached pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pathlib\n",
      "Successfully installed pathlib-1.0.1\n",
      "Collecting typing\n",
      "  Using cached typing-3.7.4.3-py3-none-any.whl\n",
      "Installing collected packages: typing\n",
      "Successfully installed typing-3.7.4.3\n",
      "Requirement already satisfied: seaborn in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Requirement already satisfied: plotly in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (5.24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: pandas in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pandas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: scikit-image in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.11.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image) (1.14.1)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: pillow>=10.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image) (11.0.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image) (2.36.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image) (2024.12.12)\n",
      "Requirement already satisfied: packaging>=21 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: scikit-learn in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: medpy in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: scipy>=1.10 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from medpy) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from medpy) (2.2.1)\n",
      "Requirement already satisfied: SimpleITK>=2.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from medpy) (2.4.0)\n",
      "Requirement already satisfied: wandb in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (5.29.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (6.1.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (2.10.4)\n",
      "Requirement already satisfied: pyyaml in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (75.6.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Import basic packages\n",
    "!pip install nnunetv2\n",
    "!pip install wandb\n",
    "!pip install shutil\n",
    "!pip install datetime\n",
    "!pip install collections\n",
    "!pip install json\n",
    "!pip install matplotlib\n",
    "!pip install nibabel\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install logging\n",
    "!pip install pathlib\n",
    "!pip install typing\n",
    "!pip install seaborn\n",
    "!pip install plotly\n",
    "!pip install pandas\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn\n",
    "!pip install medpy\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "error",
     "timestamp": 1734494966843,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "bO86sDwTFIq_",
    "outputId": "67e3f2c4-8fd0-499c-ce96-2c8a35725967"
   },
   "outputs": [],
   "source": [
    "# Check if nnunet can be imported\n",
    "import nnunetv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI1fltW7CTNl"
   },
   "source": [
    "## 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11126,
     "status": "ok",
     "timestamp": 1734495100023,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "is9GXRZ9uPsk"
   },
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# For visualization\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from skimage.measure import marching_cubes  # For 3D visualization\n",
    "\n",
    "# For evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from medpy.metric.binary import hd95\n",
    "\n",
    "# Experiment tracking\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqrPL6GnCuSb"
   },
   "source": [
    "## 1.2 Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1734495154834,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "2gi0Cz_cCx0Z",
    "outputId": "6b56155b-dcee-4f6f-c083-5937376c183b"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Centralized logging configuration for the project\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir: str, name: str = \"lung_segmentation\"):\n",
    "        self.log_dir = log_dir\n",
    "        self.name = name\n",
    "        self._initialize_logging()\n",
    "\n",
    "    def _initialize_logging(self):\n",
    "        \"\"\"Initialize logging configuration\"\"\"\n",
    "        # Create logs directory if it doesn't exist\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "        # Create timestamped log file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = os.path.join(self.log_dir, f\"{self.name}_{timestamp}.log\")\n",
    "\n",
    "        # Configure logging format\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "\n",
    "        # File handler\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        # Configure root logger\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "        # Log initial message\n",
    "        logging.info(f\"Logging initialized. Log file: {log_file}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logger(name: Optional[str] = None) -> logging.Logger:\n",
    "        \"\"\"Get logger instance\"\"\"\n",
    "        return logging.getLogger(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffk4jxxlC69a"
   },
   "source": [
    "## 1.3 GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1734495272957,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "4gSqxukcC9k1",
    "outputId": "cf1f2cee-aa01-4ed8-c64b-ca75ba468fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4060 Ti\n",
      "GPU Memory: 17.18 GB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu():\n",
    "    \"\"\"Verify GPU availability and print device information\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU available, using CPU\")\n",
    "    return device\n",
    "\n",
    "# this is the code from original workshop, just in case the check_gpu() is not working\n",
    "# check whether GPU accelerated computing is available\n",
    "# if there is an error here, enable GPU in the Runtime\n",
    "# assert torch.cuda.is_available()\n",
    "\n",
    "device = check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53AqZcnrDNbr"
   },
   "source": [
    "## 1.4 Initialize Weights & Biases - External library to track GPU Performance (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "executionInfo": {
     "elapsed": 71593,
     "status": "ok",
     "timestamp": 1734443148187,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "Xnlla_VjueYs",
    "outputId": "edcf10dc-fdb2-453c-9db5-5ef7e9493967"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/doodledaron/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/doodledaron/FYP/nnUnet/FYP-file/wandb/run-20241222_234751-fuxp8czd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net/runs/fuxp8czd' target=\"_blank\">flowing-dragon-9</a></strong> to <a href='https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net' target=\"_blank\">https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net/runs/fuxp8czd' target=\"_blank\">https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net/runs/fuxp8czd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Weights & Biases tracking\n"
     ]
    }
   ],
   "source": [
    "def init_wandb(project_name: str = \"FYP - nnU-Net\"):\n",
    "    \"\"\"Initialize Weights & Biases tracking\"\"\"\n",
    "    wandb.init(project=project_name)\n",
    "    print(\"Initialized Weights & Biases tracking\")\n",
    "\n",
    "init_wandb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1owWZTgcG6s4"
   },
   "source": [
    "## 2.1 Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "executionInfo": {
     "elapsed": 337087,
     "status": "ok",
     "timestamp": 1734443598114,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "1SQ71LlaG_AO",
    "outputId": "73bed25c-0319-4b85-f690-1dc502912f6d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABc8AAAHqCAYAAADSwLYsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABe5klEQVR4nO3dd5hU9dk/4GdpyyLs0luoNhBRVFRCNIKKIpbYEsuLEdSoUYpCNJEkithQkygWAjF5RSzERCOGmIgxKqgRiWDFgkAwoFJEhaUuZc/vj/yY1x36srsz6973dZ3rYk595rvLPHM+e+ZMTpIkSQAAAAAAACnVMl0AAAAAAABkG+E5AAAAAACkEZ4DAAAAAEAa4TkAAAAAAKQRngMAAAAAQBrhOQAAAAAApBGeAwAAAABAGuE5AAAAAACkEZ4DAAAAAEAa4TlsR7t27aJ///6ZLmO3XX/99ZGTk5PpMnbKRx99FDk5OfHAAw+U+7EeeOCByMnJiY8++ig1r127dnHyySeX+7EjIqZMmRI5OTkxZcqUCjkewNdVRfaO8tazZ8/o2bNnpsvYKVvro+Wlf//+0a5du9TjzT/zX/7yl+V+7IjK9V4KoDJwrl3xnGtD6QjPqRTOO++8qF27dnz44YdbLLv11lsjJycnnnrqqe3uo2fPnpGTkxM5OTlRrVq1yM/Pjw4dOsT3v//9ePbZZ8ur9Cpv85jn5OREjRo1omHDhtG1a9e44oor4r333iuz4/z617/O2tAkm2sDKA99+vSJBg0axJIlS7ZYtmLFimjRokV069YtiouLt7p9RfUO/s/mk8zNU25ubjRr1ix69uwZt9xyS3z22Wdlcpw1a9bE9ddfn5Uns9lcG0B5+epr/7am66+/frv7cK6dGc61s7s2vj5ykiRJMl0E7MjSpUujY8eOcdBBB8Xzzz+fmj9//vzYf//948QTT4zHH398u/vo2bNnzJs3L0aOHBkREatXr465c+fGE088Ef/+97/jrLPOiocffjhq1qyZ2qaoqCiqVatWYl5ltHHjxti4cWPUrl27wo+dk5MTxx13XJx//vmRJEmsWLEi3nrrrXjsscdi9erVcdttt8XQoUNT6ydJEkVFRVGzZs2oXr36Th+nc+fO0bhx41064d20aVNs2LAhcnNzU1cLtGvXLjp37rzDP8bsim3VVlxcHOvXr49atWpFtWr+lgl8fcyfPz86d+4cp556akyYMKHEsgEDBsR9990XM2bMiC5dumx1+4rqHdlo/fr1ERFRq1atCj3ulClT4uijj47BgwfHYYcdFps2bYrPPvssXnnllfjLX/4SBQUF8cc//jGOOeaY1DZb66M7smzZsmjSpEkMHz58h2HMV23YsCGKi4sjNzc3Iv579Vz79u3jF7/4RVx11VW79FxLU1sm30sBlKeHH354m8uuv/76mDdvXvztb3+LPn36bHM959rOtdM51+brpEamC4Cd0bRp07jtttvikksuifHjx0e/fv0iIuLyyy+PmjVrxl133bVT+ykoKIjzzjuvxLxbb701Bg8eHL/+9a+jXbt2cdttt6WWbT5Bq+xq1KgRNWpk7r/7vvvuu9VxP+WUU+JHP/pRdOzYMU488cSI+O8bgPJ+47F69erYY489onr16hkNWapVq+YkHPhaat++fQwfPjx+8pOfRP/+/eP444+PiIjXXnstxo4dG1ddddU2g/PNsq13VJSKDs3Tffvb347vfve7Jea99dZbcfzxx8eZZ54Z7733XrRo0SIiokL66OaenelwJdPvpQDKS3qv3ex3v/tdzJs3LwYNGrTd4Hwz59qZkW3vl5xr83Xkzy9UGj/4wQ/iiCOOiKuuuio+//zzePTRR2Py5Mlx0003xTe+8Y1S77d69epx9913R6dOneLee++NFStWpJal34dt8327Xn755Rg8eHA0adIk6tevH5deemmsX78+li9fHueff340aNAgGjRoED/+8Y8j/cMdxcXFMWrUqNh///2jdu3a0axZs7j00kvjyy+/LLHe5vuBvfzyy3H44YdH7dq1Y88994wHH3ywxHobNmyIESNGxD777BO1a9eORo0axZFHHlni43Fbuw/bxo0b48Ybb4y99torcnNzo127dvHTn/40ioqKSlXHrmrUqFE8+uijUaNGjbj55ptT87d2H7bFixfHBRdcEK1atYrc3Nxo0aJFnHrqqan7p7Vr1y7efffdmDp1aupja5vvF7v5ZzZ16tS4/PLLo2nTptGqVasSy7Z2r9a///3vcdBBB0Xt2rWjU6dO8cQTT5RYvq1726Xvc3u1bes+bI899lh07do18vLyonHjxnHeeefFJ598UmKd/v37R926deOTTz6J0047LerWrRtNmjSJq666KjZt2rSD0Qcof0OHDo0DDzwwLr/88li3bl1s2rQpfvjDH0bbtm1j+PDhpdrnrvSOza+TCxYsiJNPPjnq1q0b3/jGN2L06NEREfHOO+/EMcccE3vssUe0bdt2iyvkIyKWL18eV155ZbRu3Tpyc3Nj7733jttuu63E7Wa+eu/t++67L9VXDzvssHjttddK7G9H/Sxi6/c8X7p0aVx00UXRrFmzqF27dnTp0iXGjx9fYp1dqWNXdenSJUaNGhXLly+Pe++9NzV/a310xowZ0bt372jcuHHk5eVF+/bt48ILL0zV2KRJk4iIGDFixBa3A9j8M5s3b16ceOKJUa9evejbt29q2Vfvef5Vd955Z7Rt2zby8vKiR48eMWvWrBLLt3Uf+a/uc0e1ZeN7KYDy8u6778bgwYPj4IMPjl/84hel3o9zbefazrX5OhCeU2nk5OTEb37zm1ixYkVcdtllMWTIkDj00ENjwIABu73v6tWrx7nnnhtr1qyJl19+eYfrDxo0KObMmRMjRoyI73znO3HffffFtddeG6ecckps2rQpbrnlljjyyCPjF7/4RTz00EMltr300kvj6quvjiOOOCLuuuuuuOCCC+KRRx6J3r17x4YNG0qsO3fu3Pjud78bxx13XPzqV7+KBg0aRP/+/ePdd99NrXP99dfHiBEj4uijj4577703fvazn0WbNm3i9ddf3+5z+MEPfhDXXXddHHLIIXHnnXdGjx49YuTIkXHOOedsse7O1FEabdq0iR49esSrr74ahYWF21zvzDPPjIkTJ8YFF1wQv/71r2Pw4MGxcuXKWLBgQUREjBo1Klq1ahUdO3aMhx56KB566KH42c9+VmIfl19+ebz33ntx3XXXxTXXXLPduubMmRNnn3129OnTJ0aOHBk1atSI733ve6W6X9/O1PZVDzzwQJx11llRvXr1GDlyZFx88cXxxBNPxJFHHhnLly8vse6mTZuid+/e0ahRo/jlL38ZPXr0iF/96ldx33337XKdAGWtRo0acd9998X8+fPjxhtvjHvvvTdef/31GDNmTNSpU6fU+93Z3hHx39fJPn36ROvWreP222+Pdu3axcCBA+OBBx6IE044IQ499NC47bbbol69enH++efH/PnzU9uuWbMmevToEQ8//HCcf/75cffdd8cRRxwRw4YNK/ER6M0mTJgQv/jFL+LSSy+Nm266KT766KM444wzSvT2HfWzrVm7dm307NkzHnrooejbt2/84he/iIKCgujfv/9WP3m3M3WUxne/+93Iy8uLv//979tcZ+nSpXH88cfHRx99FNdcc03cc8890bdv33j11VcjIqJJkyYxZsyYiIg4/fTTU33xjDPOSO1j48aN0bt372jatGn88pe/jDPPPHO7dT344INx9913x4ABA2LYsGExa9asOOaYY7Z6v/3t2Zna0mXDeymAsrZmzZrU+cijjz6621eIO9d2rv1VzrWplBKoZIYNG5ZERFK9evVk5syZO71djx49kv3333+byydOnJhERHLXXXel5rVt2zbp169f6vG4ceOSiEh69+6dFBcXp+Z37949ycnJSX74wx+m5m3cuDFp1apV0qNHj9S8l156KYmI5JFHHilx7MmTJ28xv23btklEJC+++GJq3tKlS5Pc3NzkRz/6UWpely5dkpNOOmm7z3348OHJV/+7v/nmm0lEJD/4wQ9KrHfVVVclEZE8//zzu1zHtkREMmDAgG0uv+KKK5KISN56660kSZJk/vz5SUQk48aNS5IkSb788sskIpJf/OIX2z3O/vvvX2KsN9v8MzvyyCOTjRs3bnXZ/PnzU/M2P98//elPqXkrVqxIWrRokRx88MGpeeljur19bqu2F154IYmI5IUXXkiSJEnWr1+fNG3aNOncuXOydu3a1HpPPfVUEhHJddddl5rXr1+/JCKSG264ocQ+Dz744KRr165bHAsgUwYOHJjUrFkzqVu3bnLuuefu1Da72zuS5P9eJ2+55ZbUvC+//DLJy8tLcnJykkcffTQ1/4MPPkgiIhk+fHhq3o033pjsscceyYcfflji2Ndcc01SvXr1ZMGCBSWO3ahRo+SLL75IrffnP/85iYjkL3/5S+rYO9PPevToUaJnjBo1KomI5OGHH07NW79+fdK9e/ekbt26SWFh4S7VsS2be9Jjjz22zXW6dOmSNGjQIPU4vedtfi/12muvbXMfn3322RZjvdnmn9k111yz1WVt27ZNPd78fPPy8pKPP/44NX/69OlJRCRDhgxJzUsf023tc3u1ZfK9FEBFuvDCC5OISMaPH7/T2zjXdq7tXJuvM1eeU+k0btw4IiJatmwZnTt3LrP91q1bNyIiVq5cucN1L7roohIfI+rWrVskSRIXXXRRal716tXj0EMPjX//+9+peY899lgUFBTEcccdF8uWLUtNXbt2jbp168YLL7xQ4jidOnWKb3/726nHTZo0iQ4dOpTYZ/369ePdd9+NOXPm7PRz/dvf/hYRscWVcz/60Y8iIuKvf/3rLtdRWjsa97y8vKhVq1ZMmTJli4/b7YqLL754p++51rJlyzj99NNTj/Pz8+P888+PN954IxYvXlzqGnZkxowZsXTp0rj88stL3J/tpJNOio4dO27xc4mI+OEPf1ji8be//e0y+bkAlJWbb745GjVqFNWqVYs777yzTPa5Kz37Bz/4Qerf9evXjw4dOsQee+wRZ511Vmp+hw4don79+lv07G9/+9vRoEGDEj27V69esWnTpnjxxRdLHOfss8+OBg0apB5v7pub91nafva3v/0tmjdvHueee25qXs2aNWPw4MGxatWqmDp16i7VsTvq1q273TGvX79+REQ89dRTu3Wl+2WXXbbT65522mklbt93+OGHR7du3VLvdcpLNr2XAigrEyZMiPvvvz++//3vx/nnn19m+3Wu7Vx7M+faVEbCcyqVhQsXxvDhw6Nz586xcOHCuP3228ts36tWrYqIiHr16u1w3TZt2pR4XFBQEBERrVu33mL+V5vQnDlzYsWKFdG0adNo0qRJiWnVqlWxdOnS7R4nIqJBgwYl9nnDDTfE8uXLY999940DDjggrr766nj77be3W/9//vOfqFatWuy9994l5jdv3jzq168f//nPf3a5jtLa0bjn5ubGbbfdFk8//XQ0a9YsjjrqqLj99tt3ubG2b99+p9fde++9t7jH2r777hsRsdV7tpWVzePeoUOHLZZ17Nhxi59L7dq1U/dn3aysfi4AZSU/Pz86dOgQrVu3jmbNmpXJPne2Z2/tdbKgoCBatWq1xev81nr25MmTt+jXvXr1iojYYc/eHGBv3mdp+9l//vOf2GeffaJatZJv2/fbb7/U8l2pY3esWrVqu2Peo0ePOPPMM2PEiBHRuHHjOPXUU2PcuHFb3ON1e2rUqJG6X+rO2GeffbaYt++++5Zrv47IrvdSAGVhzpw58cMf/jD23Xff+PWvf12m+3au7Vx7M+faVEbCcyqVgQMHRkTE008/Hd/73vfi5ptvLrO//G3+cqn0Jrc12/qr6tbmJ1/5EpPi4uJo2rRpPPvss1udbrjhhp06zlf3edRRR8W8efPi/vvvj86dO8fvfve7OOSQQ+J3v/vdDp/H1r6EY2efV3odpTVr1qyoXr36dhvulVdeGR9++GGMHDkyateuHddee23st99+8cYbb+z0cfLy8na71q/a1thV5BeIZPLbywEyaWd6R8Su9euILXv2cccdt82enX4v7p3ZZ1n0sx0pr569YcOG+PDDD7f7PiknJycef/zxmDZtWgwcODA++eSTuPDCC6Nr166pE/gdyc3N3eIPBburPHt2NryXAthdRUVFcfbZZ8f69evj0UcfTV2xXFaca2+bc+0tOdcm2wjPqTQmTpwYkyZNihtvvDFatWoVo0aNilq1apXJF4Zu2rQpJkyYEHXq1IkjjzyyDKrdur322is+//zzOOKII6JXr15bTF26dCnVfhs2bBgXXHBB/P73v4+FCxfGgQceGNdff/0212/btm0UFxdv8fGzJUuWxPLly6Nt27alqmNXLViwIKZOnRrdu3ff4VUIe+21V/zoRz+Kv//97zFr1qxYv359/OpXv0ot39k3Jztj7ty5W7xZ+fDDDyPiv9/oHfF/V/Klf7FI+l+sd6W2zeM+e/bsLZbNnj27wn4uANlsV3rH7thrr71i1apVW+3XvXr12uqVYju73+31s3Rt27aNOXPmRHFxcYn5H3zwQWp5RXj88cdj7dq10bt37x2u+81vfjNuvvnmmDFjRjzyyCPx7rvvxqOPPhoRZduvI2KrH6X/8MMPU/064r89O71fR2zZs3eltmx5LwVQFq666qp444034vbbb4+DDz64TPftXPu/nGv/l3NtKiPhOZXCypUrY/DgwXHwwQfHoEGDIuK/98q68cYbY/LkyfHYY4+Vet+bNm2KwYMHx/vvvx+DBw+O/Pz8sip7C2eddVZs2rQpbrzxxi2Wbdy4casndjvy+eefl3hct27d2Hvvvbf7EekTTzwxIv777dRfdccdd0TEf+/7Vd6++OKLOPfcc2PTpk3b/UbsNWvWxLp160rM22uvvaJevXolnuMee+xRqvHbmk8//TQmTpyYelxYWBgPPvhgHHTQQdG8efNUDRFR4p63q1evjvHjx2+xv52t7dBDD42mTZvG2LFjSzy3p59+Ot5///0K+bkAZLOd7R1l4ayzzopp06bFM888s8Wy5cuXx8aNG3dpfzvbz9KdeOKJsXjx4vjDH/6Qmrdx48a45557om7dutGjR49dqqM03nrrrbjyyiujQYMG271o4csvv9zihPiggw6KiEg9xzp16kTElifEpfXkk0/GJ598knr8r3/9K6ZPnx59+vRJzdtrr73igw8+iM8++yw176233op//vOfJfa1K7Vlw3spgLIwceLEuPfee+M73/lODB48uEz37Vz7/zjX/i/n2lRGNTJdAOyMn//85/Hpp5/GE088UeLjMwMGDIjx48fHlVdeGSeccMIO/6K6YsWKePjhhyPiv41i7ty58cQTT8S8efPinHPO2WqjLUs9evSISy+9NEaOHBlvvvlmHH/88VGzZs2YM2dOPPbYY3HXXXfFd7/73V3aZ6dOnaJnz57RtWvXaNiwYcyYMSMef/zx1C1utqZLly7Rr1+/uO+++2L58uXRo0eP+Ne//hXjx4+P0047LY4++ujdfaolfPjhh/Hwww9HkiRRWFgYb731Vjz22GOxatWquOOOO+KEE07Y7rbHHntsnHXWWdGpU6eoUaNGTJw4MZYsWRLnnHNOar2uXbvGmDFj4qabboq99947mjZtGsccc0yp6t13333joosuitdeey2aNWsW999/fyxZsiTGjRuXWuf444+PNm3axEUXXRRXX311VK9ePe6///5o0qRJLFiwoMT+dra2mjVrxm233RYXXHBB9OjRI84999xYsmRJ3HXXXdGuXbsYMmRIqZ4PQGW0O72jLFx99dUxadKkOPnkk6N///7RtWvXWL16dbzzzjvx+OOPx0cffZT6EvOdsbP9LN0ll1wSv/nNb6J///4xc+bMaNeuXTz++OPxz3/+M0aNGlXmV9+/9NJLsW7duti0aVN8/vnn8c9//jMmTZoUBQUFMXHixNSJ7daMHz8+fv3rX8fpp58ee+21V6xcuTJ++9vfRn5+fipMyMvLi06dOsUf/vCH2HfffaNhw4bRuXPnUn8J/N577x1HHnlkXHbZZVFUVBSjRo2KRo0axY9//OPUOhdeeGHccccd0bt377joooti6dKlMXbs2Nh///2jsLAwtd6u1FbR76UAysOiRYvioosuiurVq8exxx6bOldOt9dee0X37t23uy/n2v/lXHv7nGtTGQnPyXozZ86M0aNHx+WXXx6HHXZYiWXVq1ePsWPHxje/+c34+c9/Hnfdddd29/Xxxx/H97///Yj471+NW7RoEd27d48xY8bEcccdV27P4avGjh0bXbt2jd/85jfx05/+NGrUqBHt2rWL8847L4444ohd3t/gwYNj0qRJ8fe//z2Kioqibdu2cdNNN8XVV1+93e1+97vfxZ577hkPPPBA6mR42LBhMXz48NI+tW3afJ+5atWqRX5+frRv3z769esXl1xySXTq1Gm727Zu3TrOPffceO655+Khhx6KGjVqRMeOHeOPf/xjifvNXnfddfGf//wnbr/99li5cmX06NGj1A19n332iXvuuSeuvvrqmD17drRv3z7+8Ic/lPioes2aNWPixIlx+eWXx7XXXhvNmzdPXZV3wQUXlNjfrtTWv3//qFOnTtx6663xk5/8JPbYY484/fTT47bbbov69euX6vkAVEa70zvKQp06dWLq1Klxyy23xGOPPRYPPvhg5Ofnx7777hsjRoxIfYHZztrZfpYuLy8vpkyZEtdcc02MHz8+CgsLo0OHDjFu3Ljo37//bj7LLd19990R8d8+V79+/dhvv/1ixIgRcfHFF2/xxVnpNgcEjz76aCxZsiQKCgri8MMPj0ceeaTE/VZ/97vfxaBBg2LIkCGxfv361JfBl8b5558f1apVi1GjRsXSpUvj8MMPj3vvvTdatGiRWme//faLBx98MK677roYOnRodOrUKR566KGYMGFCTJkypcT+dqW2inwvBVAeZs+enfoCxCuuuGKb6/Xr12+H4blz7f/jXHvbnGtTGeUkvqUGAAAAAABKcM9zAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANDUyXUB5Ky4ujk8//TTq1asXOTk5mS4HAHYoSZJYuXJltGzZMqpVqzp/59azAaiM9G19G4DKoTQ9+2sfnn/66afRunXrTJcBALts4cKF0apVq0yXUWH0bAAqM30bACqHXenZX/vwvF69ehHx30HJz8/PcDUAsGOFhYXRunXrVA+rKvRsACojfVvfBqByKE3P/tqH55s/Ppafn6+hA1CpVLWPQOvZAFRm+jYAVA670rOrzg3ZAAAAAABgJwnPAQAAAAAgjfAcAAAAAADSCM8BAAAAACCN8BwAAAAAANIIzwEAAAAAII3wHAAAAAAA0gjPAQAAAAAgjfAcAAAAAADSCM8BAAAAACCN8BwAAAAAANIIzwEAAAAAII3wHAAAAL5GXnzxxTjllFOiZcuWkZOTE08++WSJ5UmSxHXXXRctWrSIvLy86NWrV8yZMyczxQJAFhOeAwAAwNfI6tWro0uXLjF69OitLr/99tvj7rvvjrFjx8b06dNjjz32iN69e8e6desquFIAyG41Ml0AAAAAUHb69OkTffr02eqyJEli1KhR8fOf/zxOPfXUiIh48MEHo1mzZvHkk0/GOeecU5GlAkBWc+U5AAAAVBHz58+PxYsXR69evVLzCgoKolu3bjFt2rQMVgYA2ceV56WwYMGCWLZsWabLACDLNG7cONq0aZPpMvgKPRuAranKPXvx4sUREdGsWbMS85s1a5ZatjVFRUVRVFSUelxYWFg+BfK1ks3vxary6wCw84Tnu2jBggXRseN+sXbtmkyXAkCWycurEx988L434VlCzwZgW/TsXTdy5MgYMWJEpsugEsn292JeB4CdITzfRcuWLYu1a9dEtwuHR36LdpkuB4AsUbjoo5h+/4hYtmyZN+BZQs8GYGuqes9u3rx5REQsWbIkWrRokZq/ZMmSOOigg7a53bBhw2Lo0KGpx4WFhdG6detyq5PKL5vfi1X11wFg5wnPSym/Rbto2KZDpssAAHZAzwaA/9O+ffto3rx5PPfcc6mwvLCwMKZPnx6XXXbZNrfLzc2N3NzcCqqSrxPvxYDKTHgOAAAAXyOrVq2KuXPnph7Pnz8/3nzzzWjYsGG0adMmrrzyyrjppptin332ifbt28e1114bLVu2jNNOOy1zRQNAFhKeAwAAwNfIjBkz4uijj0493ny7lX79+sUDDzwQP/7xj2P16tVxySWXxPLly+PII4+MyZMnR+3atTNVMgBkJeE5AAAAfI307NkzkiTZ5vKcnJy44YYb4oYbbqjAqgCg8qmW6QIAAAAAACDbCM8BAAAAACCN8BwAAAAAANIIzwEAAAAAII3wHAAAAAAA0gjPAQAAAAAgjfAcAAAAAADSCM8BAAAAACCN8BwAAAAAANIIzwEAAAAAII3wHAAAAAAA0gjPAQAAAAAgjfAcAAAAAADSZDQ8HzNmTBx44IGRn58f+fn50b1793j66adTy9etWxcDBgyIRo0aRd26dePMM8+MJUuWZLBiAKia9GwAAACqmoyG561atYpbb701Zs6cGTNmzIhjjjkmTj311Hj33XcjImLIkCHxl7/8JR577LGYOnVqfPrpp3HGGWdksmQAqJL0bAAAAKqaGpk8+CmnnFLi8c033xxjxoyJV199NVq1ahX/+7//GxMmTIhjjjkmIiLGjRsX++23X7z66qvxzW9+MxMlA0CVpGcDAABQ1WTNPc83bdoUjz76aKxevTq6d+8eM2fOjA0bNkSvXr1S63Ts2DHatGkT06ZNy2ClAFC16dkAAABUBRm98jwi4p133onu3bvHunXrom7dujFx4sTo1KlTvPnmm1GrVq2oX79+ifWbNWsWixcv3ub+ioqKoqioKPW4sLCwvEoHgCpFzwYAAKAqyfiV5x06dIg333wzpk+fHpdddln069cv3nvvvVLvb+TIkVFQUJCaWrduXYbVAkDVpWcDAABQlWQ8PK9Vq1bsvffe0bVr1xg5cmR06dIl7rrrrmjevHmsX78+li9fXmL9JUuWRPPmzbe5v2HDhsWKFStS08KFC8v5GQBA1aBnAwAAUJVkPDxPV1xcHEVFRdG1a9eoWbNmPPfcc6lls2fPjgULFkT37t23uX1ubm7k5+eXmACAsqdnAwAA8HWW0XueDxs2LPr06RNt2rSJlStXxoQJE2LKlCnxzDPPREFBQVx00UUxdOjQaNiwYeTn58egQYOie/fu8c1vfjOTZQNAlaNnAwAAUNVkNDxfunRpnH/++bFo0aIoKCiIAw88MJ555pk47rjjIiLizjvvjGrVqsWZZ54ZRUVF0bt37/j1r3+dyZIBoErSswEAAKhqMhqe/+///u92l9euXTtGjx4do0ePrqCKAICt0bMBAACoarLunucAAAAAAJBpwnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAACAKmTTpk1x7bXXRvv27SMvLy/22muvuPHGGyNJkkyXBgBZpUamCwAAAAAqzm233RZjxoyJ8ePHx/777x8zZsyICy64IAoKCmLw4MGZLg8AsobwHAAAAKqQV155JU499dQ46aSTIiKiXbt28fvf/z7+9a9/ZbgyAMgubtsCAAAAVci3vvWteO655+LDDz+MiIi33norXn755ejTp0+GKwOA7OLKcwAAAKhCrrnmmigsLIyOHTtG9erVY9OmTXHzzTdH3759t7lNUVFRFBUVpR4XFhZWRKkAkFGuPAcAAIAq5I9//GM88sgjMWHChHj99ddj/Pjx8ctf/jLGjx+/zW1GjhwZBQUFqal169YVWDEAZIbwHAAAAKqQq6++Oq655po455xz4oADDojvf//7MWTIkBg5cuQ2txk2bFisWLEiNS1cuLACKwaAzMhoeD5y5Mg47LDDol69etG0adM47bTTYvbs2SXW6dmzZ+Tk5JSYfvjDH2aoYgComvRsAPj6WLNmTVSrVjIOqF69ehQXF29zm9zc3MjPzy8xAcDXXUbD86lTp8aAAQPi1VdfjWeffTY2bNgQxx9/fKxevbrEehdffHEsWrQoNd1+++0ZqhgAqiY9GwC+Pk455ZS4+eab469//Wt89NFHMXHixLjjjjvi9NNPz3RpAJBVMvqFoZMnTy7x+IEHHoimTZvGzJkz46ijjkrNr1OnTjRv3ryiywMA/j89GwC+Pu6555649tpr4/LLL4+lS5dGy5Yt49JLL43rrrsu06UBQFbJqnuer1ixIiIiGjZsWGL+I488Eo0bN47OnTvHsGHDYs2aNZkoDwD4//RsAKi86tWrF6NGjYr//Oc/sXbt2pg3b17cdNNNUatWrUyXBgBZJaNXnn9VcXFxXHnllXHEEUdE586dU/P/53/+J9q2bRstW7aMt99+O37yk5/E7Nmz44knntjqfoqKiqKoqCj1uLCwsNxrB4CqRM8GAACgKsia8HzAgAExa9asePnll0vMv+SSS1L/PuCAA6JFixZx7LHHxrx582KvvfbaYj8jR46MESNGlHu9AFBV6dkAAABUBVlx25aBAwfGU089FS+88EK0atVqu+t269YtIiLmzp271eXDhg2LFStWpKaFCxeWeb0AUFXp2QAAAFQVGb3yPEmSGDRoUEycODGmTJkS7du33+E2b775ZkREtGjRYqvLc3NzIzc3tyzLBIAqT88GAACgqsloeD5gwICYMGFC/PnPf4569erF4sWLIyKioKAg8vLyYt68eTFhwoQ48cQTo1GjRvH222/HkCFD4qijjooDDzwwk6UDQJWiZwMAAFDVZDQ8HzNmTERE9OzZs8T8cePGRf/+/aNWrVrxj3/8I0aNGhWrV6+O1q1bx5lnnhk///nPM1AtAFRdejYAAABVTcZv27I9rVu3jqlTp1ZQNQDAtujZAAAAVDVZ8YWhAAAAAACQTYTnAAAAAACQRngOAAAAAABphOcAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkEZ4DgAAAAAAaYTnAAAAAACQRngOAAAAAABphOcAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkEZ4DgAAAAAAaYTnAAAAAACQRngOAAAAAABphOcAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkEZ4DgAAAAAAaYTnAAAAAACQRngOAAAAAABphOcAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkKZGpgsAAAAAoHQWLFgQy5Yty3QZW3j//fczXQLAbhOeAwAAAFRCCxYsiI4d94u1a9dkupRt2lC0PtMlAJSa8BwAAACgElq2bFmsXbsmul04PPJbtMt0OSUsemdazJp0X2zcuDHTpQCUmvAcAAAAoBLLb9EuGrbpkOkySihc9FGmSwDYbb4wFAAAAAAA0gjPAQAAAAAgjfAcAAAAAADSCM8BAAAAACCN8BwAAAAAANIIzwEAAAAAII3wHAAAAAAA0gjPAQAAAAAgjfAcAAAAAADSCM8BAAAAACCN8BwAAAAAANIIzwEAAAAAII3wHAAAAAAA0gjPAQAAAAAgjfAcAAAAAADSCM8BAAAAACCN8BwAAAAAANIIzwEAAAAAII3wHAAAAAAA0gjPAQAAAAAgjfAcAAAAAADSCM8BAAAAACBNRsPzkSNHxmGHHRb16tWLpk2bxmmnnRazZ88usc66detiwIAB0ahRo6hbt26ceeaZsWTJkgxVDABVk54NAABAVZPR8Hzq1KkxYMCAePXVV+PZZ5+NDRs2xPHHHx+rV69OrTNkyJD4y1/+Eo899lhMnTo1Pv300zjjjDMyWDUAVD16NgAAAFVNjUwefPLkySUeP/DAA9G0adOYOXNmHHXUUbFixYr43//935gwYUIcc8wxERExbty42G+//eLVV1+Nb37zm5koGwCqHD0bAACAqiar7nm+YsWKiIho2LBhRETMnDkzNmzYEL169Uqt07Fjx2jTpk1MmzZtq/soKiqKwsLCEhMAULb0bAAAAL7usiY8Ly4ujiuvvDKOOOKI6Ny5c0RELF68OGrVqhX169cvsW6zZs1i8eLFW93PyJEjo6CgIDW1bt26vEsHgCpFzwYAAKAqyJrwfMCAATFr1qx49NFHd2s/w4YNixUrVqSmhQsXllGFAECEng0AAEDVkNF7nm82cODAeOqpp+LFF1+MVq1apeY3b9481q9fH8uXLy9xJduSJUuiefPmW91Xbm5u5ObmlnfJAFAl6dkAAABUFRm98jxJkhg4cGBMnDgxnn/++Wjfvn2J5V27do2aNWvGc889l5o3e/bsWLBgQXTv3r2iywWAKkvPBgAAoKrJ6JXnAwYMiAkTJsSf//znqFevXuqeqAUFBZGXlxcFBQVx0UUXxdChQ6Nhw4aRn58fgwYNiu7du8c3v/nNTJYOAFWKng0AAEBVk9Erz8eMGRMrVqyInj17RosWLVLTH/7wh9Q6d955Z5x88slx5plnxlFHHRXNmzePJ554IoNVA0DVo2cDwNfLJ598Euedd140atQo8vLy4oADDogZM2ZkuiwAyCoZvfI8SZIdrlO7du0YPXp0jB49ugIqAgC2Rs8GgK+PL7/8Mo444og4+uij4+mnn44mTZrEnDlzokGDBpkuDQCySlZ8YSgAAABQMW677bZo3bp1jBs3LjUv/ftMAIAM37YFAAAAqFiTJk2KQw89NL73ve9F06ZN4+CDD47f/va3mS4LALKOK88BAACgCvn3v/8dY8aMiaFDh8ZPf/rTeO2112Lw4MFRq1at6Nev31a3KSoqiqKiotTjwsLCMq1pwYIFsWzZsjLdZ1lq3LhxtGnTJtNlAFDBhOcAAABQhRQXF8ehhx4at9xyS0REHHzwwTFr1qwYO3bsNsPzkSNHxogRI8qlngULFkTHjvvF2rVrymX/ZSEvr0588MH7AnSAKkZ4DgAAAFVIixYtolOnTiXm7bfffvGnP/1pm9sMGzYshg4dmnpcWFgYrVu3LpN6li1bFmvXroluFw6P/BbtymSfZalw0Ucx/f4RsWzZMuE5QBUjPAcAAIAq5IgjjojZs2eXmPfhhx9G27Ztt7lNbm5u5Obmlmtd+S3aRcM2Hcr1GACwK3xhKAAAAFQhQ4YMiVdffTVuueWWmDt3bkyYMCHuu+++GDBgQKZLA4CsIjwHAACAKuSwww6LiRMnxu9///vo3Llz3HjjjTFq1Kjo27dvpksDgKziti0AAABQxZx88slx8sknZ7oMAMhqrjwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASFOq8HzPPfeMzz//fIv5y5cvjz333HO3iwIAyoaeDQCVh74NANmlVOH5Rx99FJs2bdpiflFRUXzyySe7XRQAUDb0bACoPPRtAMguNXZl5UmTJqX+/cwzz0RBQUHq8aZNm+K5556Ldu3alVlxAEDp6NkAUHno2wCQnXYpPD/ttNMiIiInJyf69etXYlnNmjWjXbt28atf/arMigMASkfPBoDKQ98GgOy0S+F5cXFxRES0b98+XnvttWjcuHG5FAUA7B49GwAqD30bALLTLoXnm82fP7+s6wAAyoGeDQCVh74NANmlVOF5RMRzzz0Xzz33XCxdujT1V/LN7r///t0uDAAoG3o2AFQe+jYAZI9ShecjRoyIG264IQ499NBo0aJF5OTklHVdAEAZ0LMBoPLQtwEgu5QqPB87dmw88MAD8f3vf7+s6wEAypCeDQCVh74NANmlWmk2Wr9+fXzrW98q61oAgDKmZwNA5aFvA0B2KVV4/oMf/CAmTJhQ1rUAAGVMzwaAykPfBoDsUqrbtqxbty7uu++++Mc//hEHHnhg1KxZs8TyO+64o0yKAwB2j54NAJWHvg0A2aVU4fnbb78dBx10UEREzJo1q8QyX2gCANlDzwaAykPfBoDsUqrw/IUXXijrOgCAcqBnA0DloW8DQHYp1T3PAQAAAADg66xUV54fffTR2/3I2PPPP1/qggCAsqNnA0DloW8DQHYpVXi++R5sm23YsCHefPPNmDVrVvTr168s6gIAyoCeDQCVh74NANmlVOH5nXfeudX5119/faxatWq3CgIAyo6eDQCVh74NANmlTO95ft5558X9999flrsEAMqBng0AlYe+DQCZUabh+bRp06J27dpluUsAoBzo2QBQeejbAJAZpbptyxlnnFHicZIksWjRopgxY0Zce+21ZVIYALD79GwAqDz0bQDILqUKzwsKCko8rlatWnTo0CFuuOGGOP7448ukMABg9+nZAFB56NsAkF1KFZ6PGzeurOsAAMqBng0AlYe+DQDZpVTh+WYzZ86M999/PyIi9t9//zj44IPLpCgAoGzp2QBQeejbAJAdShWeL126NM4555yYMmVK1K9fPyIili9fHkcffXQ8+uij0aRJk7KsEQAoJT0bACoPfRsAsku10mw0aNCgWLlyZbz77rvxxRdfxBdffBGzZs2KwsLCGDx4cFnXCACUkp4NAJWHvg0A2aVUV55Pnjw5/vGPf8R+++2XmtepU6cYPXq0LzEBgCyiZwNA5aFvA0B2KdWV58XFxVGzZs0t5tesWTOKi4t3uygAoGzo2QBQeejbAJBdShWeH3PMMXHFFVfEp59+mpr3ySefxJAhQ+LYY48ts+IAgN2jZwNA5aFvA0B2KVV4fu+990ZhYWG0a9cu9tprr9hrr72iffv2UVhYGPfcc09Z1wgAlJKeDQCVh74NANmlVPc8b926dbz++uvxj3/8Iz744IOIiNhvv/2iV69eZVocALB79GwAqDz0bQDILrt05fnzzz8fnTp1isLCwsjJyYnjjjsuBg0aFIMGDYrDDjss9t9//3jppZfKq1YAYCfp2QBQeejbAJCddik8HzVqVFx88cWRn5+/xbKCgoK49NJL44477iiz4gCA0tGzAaDy0LcBIDvtUnj+1ltvxQknnLDN5ccff3zMnDlzt4sCAHaPng0AlYe+DQDZaZfC8yVLlkTNmjW3ubxGjRrx2Wef7XZRAMDu0bMBoPLQtwEgO+1SeP6Nb3wjZs2atc3lb7/9drRo0WK3iwIAdo+eDQCVh74NANlpl8LzE088Ma699tpYt27dFsvWrl0bw4cPj5NPPnmn9/fiiy/GKaecEi1btoycnJx48sknSyzv379/5OTklJi291E2AOC/yrpnR+jbAFBeyqNvAwC7r8aurPzzn/88nnjiidh3331j4MCB0aFDh4iI+OCDD2L06NGxadOm+NnPfrbT+1u9enV06dIlLrzwwjjjjDO2us4JJ5wQ48aNSz3Ozc3dlZIBoEoq654doW8DQHkpj74NAOy+XQrPmzVrFq+88kpcdtllMWzYsEiSJCIicnJyonfv3jF69Oho1qzZTu+vT58+0adPn+2uk5ubG82bN9+VMgGgyivrnh2hbwNAeSmPvg0A7L5dCs8jItq2bRt/+9vf4ssvv4y5c+dGkiSxzz77RIMGDcqjvpgyZUo0bdo0GjRoEMccc0zcdNNN0ahRo22uX1RUFEVFRanHhYWF5VIXAGS7iu7ZEbvWt/VsAPg/mejbAMD27XJ4vlmDBg3isMMOK8tatnDCCSfEGWecEe3bt4958+bFT3/60+jTp09MmzYtqlevvtVtRo4cGSNGjCjXugCgMqmInh2x631bzwaALVVU3wYAdqzU4XlFOOecc1L/PuCAA+LAAw+MvfbaK6ZMmRLHHnvsVrcZNmxYDB06NPW4sLAwWrduXe61AkBVt6t9W88GAAAgm1XLdAG7Ys8994zGjRvH3Llzt7lObm5u5Ofnl5gAgIq3o76tZwMAAJDNKlV4/vHHH8fnn38eLVq0yHQpAMAO6NsAAABUZhm9bcuqVatKXI02f/78ePPNN6Nhw4bRsGHDGDFiRJx55pnRvHnzmDdvXvz4xz+OvffeO3r37p3BqgGgatK3AQAAqEoyGp7PmDEjjj766NTjzfc97devX4wZMybefvvtGD9+fCxfvjxatmwZxx9/fNx4442Rm5ubqZIBoMrStwEAAKhKMhqe9+zZM5Ik2ebyZ555pgKrAQC2R98GAACgKqlU9zwHAAAAAICKIDwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAIAq7NZbb42cnJy48sorM10KAGQV4TkAAABUUa+99lr85je/iQMPPDDTpQBA1hGeAwAAQBW0atWq6Nu3b/z2t7+NBg0aZLocAMg6wnMAAACoggYMGBAnnXRS9OrVK9OlAEBWqpHpAgAAAICK9eijj8brr78er7322k6tX1RUFEVFRanHhYWF5VUaVJj3338/0yVsVePGjaNNmzaZLgMI4TkAAABUKQsXLowrrrginn322ahdu/ZObTNy5MgYMWJEOVcGFWPtis8jIifOO++8TJeyVXl5deKDD94XoEMWEJ4DAABAFTJz5sxYunRpHHLIIal5mzZtihdffDHuvffeKCoqiurVq5fYZtiwYTF06NDU48LCwmjdunWF1QxlacOalRGRxEH/85No0r5jpsspoXDRRzH9/hGxbNky4TlkAeE5AAAAVCHHHntsvPPOOyXmXXDBBdGxY8f4yU9+skVwHhGRm5sbubm5FVUiVIi6TdtEwzYdMl0GkMWE5wAAAFCF1KtXLzp37lxi3h577BGNGjXaYj4AVGXVMl0AAAAAAABkG1eeAwAAQBU3ZcqUTJcAAFnHlecAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkEZ4DgAAAAAAaYTnAAAAAACQRngOAAAAAABphOcAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkEZ4DgAAAAAAaYTnAAAAAACQRngOAAAAAABphOcAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkEZ4DgAAAAAAaYTnAAAAAACQRngOAAAAAABphOcAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkEZ4DgAAAAAAaYTnAAAAAACQJqPh+YsvvhinnHJKtGzZMnJycuLJJ58ssTxJkrjuuuuiRYsWkZeXF7169Yo5c+ZkplgAqOL0bQAAAKqSjIbnq1evji5dusTo0aO3uvz222+Pu+++O8aOHRvTp0+PPfbYI3r37h3r1q2r4EoBAH0bAACAqqRGJg/ep0+f6NOnz1aXJUkSo0aNip///Odx6qmnRkTEgw8+GM2aNYsnn3wyzjnnnIosFQCqPH0bAACAqiRr73k+f/78WLx4cfTq1Ss1r6CgILp16xbTpk3b5nZFRUVRWFhYYgIAyldp+raeDQAAQDbL2vB88eLFERHRrFmzEvObNWuWWrY1I0eOjIKCgtTUunXrcq0TAChd39azAQAAyGZZG56X1rBhw2LFihWpaeHChZkuCQDYCj0bAACAbJa14Xnz5s0jImLJkiUl5i9ZsiS1bGtyc3MjPz+/xAQAlK/S9G09GwAAgGyWteF5+/bto3nz5vHcc8+l5hUWFsb06dOje/fuGawMAEinbwMAAPB1UyOTB1+1alXMnTs39Xj+/Pnx5ptvRsOGDaNNmzZx5ZVXxk033RT77LNPtG/fPq699tpo2bJlnHbaaZkrGgCqKH0bAACAqiSj4fmMGTPi6KOPTj0eOnRoRET069cvHnjggfjxj38cq1evjksuuSSWL18eRx55ZEyePDlq166dqZIBoMrStwEAAKhKMhqe9+zZM5Ik2ebynJycuOGGG+KGG26owKoAgK3RtwEAAKhKsvae5wAAAAAAkCnCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAgCpk5MiRcdhhh0W9evWiadOmcdppp8Xs2bMzXRYAZB3hOQAAAFQhU6dOjQEDBsSrr74azz77bGzYsCGOP/74WL16daZLA4CsUiPTBQAAAAAVZ/LkySUeP/DAA9G0adOYOXNmHHXUURmqCgCyjyvPAQAAoApbsWJFREQ0bNgww5UAQHZx5TkAAABUUcXFxXHllVfGEUccEZ07d97mekVFRVFUVJR6XFhYWBHlAVABFixYEMuWLct0GdvUuHHjaNOmTUaOLTwHAACAKmrAgAExa9asePnll7e73siRI2PEiBEVVBUAFWXBggXRseN+sXbtmkyXsk15eXXigw/ez0iALjwHAACAKmjgwIHx1FNPxYsvvhitWrXa7rrDhg2LoUOHph4XFhZG69aty7tEAMrZsmXLYu3aNdHtwuGR36JdpsvZQuGij2L6/SNi2bJlwnMAAACgfCVJEoMGDYqJEyfGlClTon379jvcJjc3N3JzcyugOgAyIb9Fu2jYpkOmy8g6wnMAAACoQgYMGBATJkyIP//5z1GvXr1YvHhxREQUFBREXl5ehqsDgOxRLdMFAAAAABVnzJgxsWLFiujZs2e0aNEiNf3hD3/IdGkAkFVceQ4AAABVSJIkmS4BACoFV54DAAAAAEAa4TkAAAAAAKQRngMAAAAAQBrhOQAAAAAApBGeAwAAAABAGuE5AAAAAACkyerw/Prrr4+cnJwSU8eOHTNdFgCwFfo2AAAAXyc1Ml3Ajuy///7xj3/8I/W4Ro2sLxkAqix9GwAAgK+LrD+jrVGjRjRv3jzTZQAAO0HfBgAA4Osiq2/bEhExZ86caNmyZey5557Rt2/fWLBgQaZLAgC2Qd8GAADg6yKrrzzv1q1bPPDAA9GhQ4dYtGhRjBgxIr797W/HrFmzol69elvdpqioKIqKilKPCwsLK6pcAKjSdrVv69kAAABks6wOz/v06ZP694EHHhjdunWLtm3bxh//+Me46KKLtrrNyJEjY8SIERVVIgDw/+1q39azAQAAyGZZf9uWr6pfv37su+++MXfu3G2uM2zYsFixYkVqWrhwYQVWCABstqO+rWcDAACQzSpVeL5q1aqYN29etGjRYpvr5ObmRn5+fokJAKh4O+rbejYAAADZLKvD86uuuiqmTp0aH330Ubzyyitx+umnR/Xq1ePcc8/NdGkAQBp9GwAAgK+TrL7n+ccffxznnntufP7559GkSZM48sgj49VXX40mTZpkujQAII2+DQAAwNdJVofnjz76aKZLAAB2kr4NAADA10lW37YFAAAAAAAyQXgOAAAAAABphOcAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkEZ4DgAAAAAAaYTnAAAAAACQRngOAAAAAABphOcAAAAAAJBGeA4AAAAAAGmE5wAAAAAAkEZ4DgAAAAAAaYTnAAAAAACQRngOAAAAAABpamS6AAAAAADg/7z//vuZLmGbGjduHG3atMl0GVu1YMGCWLZsWabL2KqioqLIzc3NdBlbyObftWwgPAcAAACALLB2xecRkRPnnXdepkvZpry8OvHBB+9nXYC+YMGC6Nhxv1i7dk2mS9m6nJyIJMl0Fdu0oWh9pkvISsJzAAAAAMgCG9asjIgkDvqfn0ST9h0zXc4WChd9FNPvHxHLli3LuvB82bJlsXbtmuh24fDIb9Eu0+WUsOidaTFr0n1Z+XPdXNvGjRszXUpWEp4DAAAAQBap27RNNGzTIdNlVEr5Ldpl3dgVLvooIrLz57q5NrbOF4YCAAAAAEAa4TkAAAAAAKQRngMAAAAAQBrhOQAAAAAApBGeAwAAAABAGuE5AAAAAACkEZ4DAAAAAEAa4TkAAAAAAKQRngMAAAAAQBrhOQAAAAAApBGeAwAAAABAGuE5AAAAAACkEZ4DAAAAAEAa4TkAAAAAAKQRngMAAAAAQBrhOQAAAAAApBGeAwAAAABAGuE5AAAAAACkEZ4DAAAAAEAa4TkAAAAAAKQRngMAAAAAQBrhOQAAAAAApBGeAwAAAABAGuE5AAAAAACkEZ4DAAAAAEAa4TkAAAAAAKQRngMAAAAAQBrhOQAAAAAApBGeAwAAAABAGuE5AAAAAACkEZ4DAAAAAECaShGejx49Otq1axe1a9eObt26xb/+9a9MlwQAbIO+DQCVg54NANuX9eH5H/7whxg6dGgMHz48Xn/99ejSpUv07t07li5dmunSAIA0+jYAVA56NgDsWNaH53fccUdcfPHFccEFF0SnTp1i7NixUadOnbj//vszXRoAkEbfBoDKQc8GgB3L6vB8/fr1MXPmzOjVq1dqXrVq1aJXr14xbdq0DFYGAKTTtwGgctCzAWDn1Mh0AduzbNmy2LRpUzRr1qzE/GbNmsUHH3yw1W2KioqiqKgo9XjFihUREVFYWFgmNa1atSoiIr74z+zYWLS2TPYJQOVXuHhBRPy3T+xuz9m8fZIku11XRdrVvq1nA5AJZdmzIypn3862c+1s79mbf2dmzpyZqjVbzJ49OyKyc+wKF/0nIiJWfDInatbIyXA1Jamt9Px/KJ1s/rlmc20RWXCunWSxTz75JImI5JVXXikx/+qrr04OP/zwrW4zfPjwJCJMJpPJZKr008KFCyui3ZaZXe3berbJZDKZvk5TZerbzrVNJpPJVJWnXenZWX3leePGjaN69eqxZMmSEvOXLFkSzZs33+o2w4YNi6FDh6YeFxcXxxdffBGNGjWKnJzs++tJWSosLIzWrVvHwoULIz8/P9PlVBrGrfSMXekYt9KrKmOXJEmsXLkyWrZsmelSdsmu9m09++v/u1wejF3pGLfSMW6lV5XGrjL27Yo6165KvwflzViWDeNYNoxj2TCOZWdnx7I0PTurw/NatWpF165d47nnnovTTjstIv7boJ977rkYOHDgVrfJzc2N3NzcEvPq169fzpVml/z8fP/pSsG4lZ6xKx3jVnpVYewKCgoyXcIu29W+rWdXjd/l8mLsSse4lY5xK72qMnaVrW9X9Ll2Vfk9qAjGsmwYx7JhHMuGcSw7OzOWu9qzszo8j4gYOnRo9OvXLw499NA4/PDDY9SoUbF69eq44IILMl0aAJBG3waAykHPBoAdy/rw/Oyzz47PPvssrrvuuli8eHEcdNBBMXny5C2+2AQAyDx9GwAqBz0bAHYs68PziIiBAwdu86Nj/J/c3NwYPnz4Fh+lY/uMW+kZu9IxbqVn7CoHfXvH/C6XnrErHeNWOsat9Ixd5VDePdvvQdkxlmXDOJYN41g2jGPZKc+xzEmSJCnzvQIAAAAAQCVWLdMFAAAAAABAthGeAwAAAABAGuE5AAAAAACkEZ5ngeuvvz5ycnJKTB07dkwtv++++6Jnz56Rn58fOTk5sXz58i32cfPNN8e3vvWtqFOnTtSvX3+nj/3+++/Hd77znSgoKIg99tgjDjvssFiwYEEZPKvyl6lxW7VqVQwcODBatWoVeXl50alTpxg7dmwZPauKsbtj99FHH8VFF10U7du3j7y8vNhrr71i+PDhsX79+u0ed926dTFgwIBo1KhR1K1bN84888xYsmRJeTzFcpGJcfviiy9i0KBB0aFDh8jLy4s2bdrE4MGDY8WKFeX1NMtFpn7nNkuSJPr06RM5OTnx5JNPluEzo6rRs0tP3y4dPbv09O3S0bPZGS+++GKccsop0bJly63+rJIkieuuuy5atGgReXl50atXr5gzZ06Jdb744ovo27dv5OfnR/369eOiiy6KVatWVeCzyLyRI0fGYYcdFvXq1YumTZvGaaedFrNnzy6xzs68Hi9YsCBOOumkqFOnTjRt2jSuvvrq2LhxY0U+lYwaM2ZMHHjggZGfnx/5+fnRvXv3ePrpp1PLjWHp3HrrrZGTkxNXXnllap6x3LEd9VFjuPM++eSTOO+886JRo0aRl5cXBxxwQMyYMSO1vKJ6jfA8S+y///6xaNGi1PTyyy+nlq1ZsyZOOOGE+OlPf7rN7devXx/f+9734rLLLtvpY86bNy+OPPLI6NixY0yZMiXefvvtuPbaa6N27dq79VwqUibGbejQoTF58uR4+OGH4/33348rr7wyBg4cGJMmTdqt51LRdmfsPvjggyguLo7f/OY38e6778add94ZY8eO3e5YR0QMGTIk/vKXv8Rjjz0WU6dOjU8//TTOOOOMMn1e5a2ix+3TTz+NTz/9NH75y1/GrFmz4oEHHojJkyfHRRddVObPrbxl4ndus1GjRkVOTk6ZPA/Qs0tP3y4dPbv09O3S0bPZkdWrV0eXLl1i9OjRW11+++23x9133x1jx46N6dOnxx577BG9e/eOdevWpdbp27dvvPvuu/Hss8/GU089FS+++GJccsklFfUUssLUqVNjwIAB8eqrr8azzz4bGzZsiOOPPz5Wr16dWmdHr8ebNm2Kk046KdavXx+vvPJKjB8/Ph544IG47rrrMvGUMqJVq1Zx6623xsyZM2PGjBlxzDHHxKmnnhrvvvtuRBjD0njttdfiN7/5TRx44IEl5hvLnbO9PmoMd86XX34ZRxxxRNSsWTOefvrpeO+99+JXv/pVNGjQILVOhfWahIwbPnx40qVLlx2u98ILLyQRkXz55ZfbXGfcuHFJQUHBTh337LPPTs4777ydKzILZWrc9t9//+SGG24oMe+QQw5Jfvazn+3U9tmgLMdus9tvvz1p3779NpcvX748qVmzZvLYY4+l5r3//vtJRCTTpk3bmbIzLhPjtjV//OMfk1q1aiUbNmzYpe0yKZNj98YbbyTf+MY3kkWLFiURkUycOHHHBcM26Nmlp2+Xjp5devp26ejZ7Kr0n1VxcXHSvHnz5Be/+EVq3vLly5Pc3Nzk97//fZIkSfLee+8lEZG89tprqXWefvrpJCcnJ/nkk08qrPZss3Tp0iQikqlTpyZJsnOvx3/729+SatWqJYsXL06tM2bMmCQ/Pz8pKiqq2CeQRRo0aJD87ne/M4alsHLlymSfffZJnn322aRHjx7JFVdckSSJ38edtb0+agx33k9+8pPkyCOP3Obyiuw1rjzPEnPmzImWLVvGnnvuGX379i33j2EXFxfHX//619h3332jd+/e0bRp0+jWrVul+2hkRY9bRMS3vvWtmDRpUnzyySeRJEm88MIL8eGHH8bxxx9f7scuS2U9ditWrIiGDRtuc/nMmTNjw4YN0atXr9S8jh07Rps2bWLatGm7deyKVNHjtq1t8vPzo0aNGrt17IqWibFbs2ZN/M///E+MHj06mjdvvlvHg8307NLTt0tHzy49fbt09Gx2x/z582Px4sUlXkMKCgqiW7duqdeQadOmRf369ePQQw9NrdOrV6+oVq1aTJ8+vcJrzhabb/G0+f/LzrweT5s2LQ444IBo1qxZap3evXtHYWFh6srrqmTTpk3x6KOPxurVq6N79+7GsBQGDBgQJ510Uokxi/D7uCu21UeN4c6bNGlSHHroofG9730vmjZtGgcffHD89re/TS2vyF4jPM8C3bp1S32kc8yYMTF//vz49re/HStXriy3Yy5dujRWrVoVt956a5xwwgnx97//PU4//fQ444wzYurUqeV23LKUiXGLiLjnnnuiU6dO0apVq6hVq1accMIJMXr06DjqqKPK9bhlqazHbu7cuXHPPffEpZdeus11Fi9eHLVq1driHrXNmjWLxYsXl+q4FS0T45Zu2bJlceONN1a6j7RmauyGDBkS3/rWt+LUU08t1XEgnZ5devp26ejZpadvl46eze7a/Drx1eBn8+PNyxYvXhxNmzYtsbxGjRrRsGHDSvU6U5aKi4vjyiuvjCOOOCI6d+4cETv3erx48eKtjvXmZVXFO++8E3Xr1o3c3Nz44Q9/GBMnToxOnToZw1306KOPxuuvvx4jR47cYpmx3Dnb66PGcOf9+9//jjFjxsQ+++wTzzzzTFx22WUxePDgGD9+fERUbK+pPJc/fI316dMn9e8DDzwwunXrFm3bto0//vGP5XZ/xOLi4oiIOPXUU2PIkCEREXHQQQfFK6+8EmPHjo0ePXqUy3HLUibGLeK/J+GvvvpqTJo0Kdq2bRsvvvhiDBgwIFq2bLnFX2azVVmO3SeffBInnHBCfO9734uLL764rEvNKpket8LCwjjppJOiU6dOcf311+/S8TItE2M3adKkeP755+ONN94odd2QTs8uPX27dDLdeyqzTI9dZe3bejZkxoABA2LWrFkl7o3MzuvQoUO8+eabsWLFinj88cejX79+leoig2ywcOHCuOKKK+LZZ5+tdN+rk02210fz8vIyWFnlUlxcHIceemjccsstERFx8MEHx6xZs2Ls2LHRr1+/Cq3FledZqH79+rHvvvvG3Llzy+0YjRs3jho1akSnTp1KzN9vv/0q5CPU5aEixm3t2rXx05/+NO6444445ZRT4sADD4yBAwfG2WefHb/85S/L7bjlrbRj9+mnn8bRRx8d3/rWt+K+++7b7rrNmzeP9evXx/Lly0vMX7JkSaX9aG5FjNtmK1eujBNOOCHq1asXEydOjJo1a5am5KxREWP3/PPPx7x586J+/fpRo0aN1MflzzzzzOjZs2dpS4cS9OzS07dLR88uPX27dPRsdtXm14klS5aUmP/V15DmzZvH0qVLSyzfuHFjfPHFF5X6daa0Bg4cGE899VS88MIL0apVq9T8nXk9bt68+VbHevOyqqJWrVqx9957R9euXWPkyJHRpUuXuOuuu4zhLpg5c2YsXbo0DjnkkNRr8dSpU+Puu++OGjVqRLNmzYxlKXy1j/p93HktWrTY7vlPRfYa4XkWWrVqVcybNy9atGhRbseoVatWHHbYYTF79uwS8z/88MNo27ZtuR23PFXEuG3YsCE2bNgQ1aqV/K9TvXr11JWBlVFpxu6TTz6Jnj17RteuXWPcuHFbjEm6rl27Rs2aNeO5555LzZs9e3YsWLAgunfvXuraM6kixi3iv1euHX/88VGrVq2YNGnS1+IqgIoYu2uuuSbefvvtePPNN1NTRMSdd94Z48aN253yIUXPLj19u3T07NLTt0tHz2ZXtW/fPpo3b17iNaSwsDCmT5+eeg3p3r17LF++PGbOnJla5/nnn4/i4uLo1q1bhdecKUmSxMCBA2PixInx/PPPR/v27Uss35nX4+7du8c777xTIiB69tlnIz8/f4vgqSopLi6OoqIiY7gLjj322HjnnXdKvBYfeuih0bdv39S/jeWu+2of9fu484444ojtnv9UaK/Z1W87pez96Ec/SqZMmZLMnz8/+ec//5n06tUrady4cbJ06dIkSZJk0aJFyRtvvJH89re/TSIiefHFF5M33ngj+fzzz1P7+M9//pO88cYbyYgRI5K6desmb7zxRvLGG28kK1euTK3ToUOH5Iknnkg9fuKJJ5KaNWsm9913XzJnzpzknnvuSapXr5689NJLFffkd0Omxq1Hjx7J/vvvn7zwwgvJv//972TcuHFJ7dq1k1//+tcV9+R30+6O3ccff5zsvffeybHHHpt8/PHHyaJFi1LTZh9//HHSoUOHZPr06al5P/zhD5M2bdokzz//fDJjxoyke/fuSffu3Sv2ye+GTIzbihUrkm7duiUHHHBAMnfu3BLbbNy4seIHoZQy9TuXLiKSiRMnlutz5etNzy49fbt09OzS07dLR89mZ6xcuTL1GhwRyR133JG88cYbyX/+858kSZLk1ltvTerXr5/8+c9/Tt5+++3k1FNPTdq3b5+sXbs2tY8TTjghOfjgg5Pp06cnL7/8crLPPvsk5557bqaeUkZcdtllSUFBQTJlypQS/1fWrFmTWmdHr8cbN25MOnfunBx//PHJm2++mUyePDlp0qRJMmzYsEw8pYy45pprkqlTpybz589P3n777eSaa65JcnJykr///e9JkhjD3dGjR4/kiiuuSD02lju2oz5qDHfOv/71r6RGjRrJzTffnMyZMyd55JFHkjp16iQPP/xwap2K6jXC8yxw9tlnJy1atEhq1aqVfOMb30jOPvvsZO7cuanlw4cPTyJii2ncuHGpdfr167fVdV544YXUOunbJEmS/O///m+y9957J7Vr1066dOmSPPnkk+X8bMtOpsZt0aJFSf/+/ZOWLVsmtWvXTjp06JD86le/SoqLiyvgWZeN3R27cePGbXX5V/8eN3/+/C3Gcu3atcnll1+eNGjQIKlTp05y+umnlziRynaZGLcXXnhhm9vMnz+/Ap/97snU71w6J+LsLj279PTt0tGzS0/fLh09m52xrd/1fv36JUmSJMXFxcm1116bNGvWLMnNzU2OPfbYZPbs2SX28fnnnyfnnntuUrdu3SQ/Pz+54IILSvwxtCrY1v+Vr/axnXk9/uijj5I+ffokeXl5SePGjZMf/ehHyYYNGyr42WTOhRdemLRt2zapVatW0qRJk+TYY49NBedJYgx3R3p4bix3bEd91BjuvL/85S9J586dk9zc3KRjx47JfffdV2J5RfWanCRJkgAAAAAAAFLc8xwAAAAAANIIzwEAAAAAII3wHAAAAAAA0gjPAQAAAAAgjfAcAAAAAADSCM8BAAAAACCN8BwAAAAAANIIzwEAAAAAII3wHAAAAAAA0gjPoZLKycmJJ598slyPMXv27GjevHmsXLmyXI+zK/r37x+nnXbaTq17zTXXxKBBg8q3IADYAT17x/RsALKFvr1j+jZVifAcstBnn30Wl112WbRp0yZyc3OjefPm0bt37/jnP/+ZWmfRokXRp0+fcq1j2LBhMWjQoKhXr165Hqe8XHXVVTF+/Pj497//nelSAPia0rPLhp4NQEXQt8uGvk1VIjyHLHTmmWfGG2+8EePHj48PP/wwJk2aFD179ozPP/88tU7z5s0jNze33GpYsGBBPPXUU9G/f/9yO0Z5a9y4cfTu3TvGjBmT6VIA+JrSs8uGng1ARdC3y4a+TVUiPIcss3z58njppZfitttui6OPPjratm0bhx9+eAwbNiy+853vpNb76kfJrr/++sjJydlieuCBByIiori4OEaOHBnt27ePvLy86NKlSzz++OPbreOPf/xjdOnSJb7xjW9ERERhYWHk5eXF008/XWK9iRMnRr169WLNmjUREfHOO+/EMcccE3l5edGoUaO45JJLYtWqVRERMWXKlKhVq1a89NJLqe1vv/32aNq0aSxZsiQiIhYuXBhnnXVW1K9fPxo2bBinnnpqfPTRR9us8/HHH48DDjggdbxevXrF6tWrU8tPOeWUePTRR7f7XAGgNPRsPRuAykPf1rehNITnkGXq1q0bdevWjSeffDKKiop2apurrroqFi1alJp++ctfRp06deLQQw+NiIiRI0fGgw8+GGPHjo133303hgwZEuedd15MnTp1m/t86aWXUttHROTn58fJJ58cEyZMKLHeI488EqeddlrUqVMnVq9eHb17944GDRrEa6+9Fo899lj84x//iIEDB0ZERM+ePePKK6+M73//+7FixYp444034tprr43f/e530axZs9iwYUP07t076tWrFy+99FL885//jLp168YJJ5wQ69ev36LGRYsWxbnnnhsXXnhhvP/++zFlypQ444wzIkmS1DqHH354fPzxx9t9UwAApaFn69kAVB76tr4NpZIAWefxxx9PGjRokNSuXTv51re+lQwbNix56623SqwTEcnEiRO32HbatGlJ7dq1kz/84Q9JkiTJunXrkjp16iSvvPJKifUuuuii5Nxzz91mDV26dEluuOGGEvMmTpyY1K1bN1m9enWSJEmyYsWKpHbt2snTTz+dJEmS3HfffUmDBg2SVatWpbb561//mlSrVi1ZvHhxkiRJUlRUlBx00EHJWWedlXTq1Cm5+OKLU+s+9NBDSYcOHZLi4uLUvKKioiQvLy955plnkiRJkn79+iWnnnpqkiRJMnPmzCQiko8++mibz2PFihVJRCRTpkzZ5joAUFp6tp4NQOWhb+vbsKtceQ5Z6Mwzz4xPP/00Jk2aFCeccEJMmTIlDjnkkNRHw7ZlwYIFcdppp8VVV10VZ511VkREzJ07N9asWRPHHXdc6i/tdevWjQcffDDmzZu3zX2tXbs2ateuXWLeiSeeGDVr1oxJkyZFRMSf/vSnyM/Pj169ekVExPvvvx9dunSJPfbYI7XNEUccEcXFxTF79uyIiKhVq1Y88sgj8ac//SnWrVsXd955Z2rdt956K+bOnRv16tVL1dmwYcNYt27dVmvt0qVLHHvssXHAAQfE9773vfjtb38bX375ZYl18vLyIiJSH3UDgLKkZ+vZAFQe+ra+DbuqRqYLALaudu3acdxxx8Vxxx0X1157bfzgBz+I4cOHb/NLRVavXh3f+c53onv37nHDDTek5m++B9pf//rX1D3VNtvel6A0btx4i+ZYq1at+O53vxsTJkyIc845JyZMmBBnn3121Kixay8lr7zySkREfPHFF/HFF1+k3gCsWrUqunbtGo888sgW2zRp0mSLedWrV49nn302Xnnllfj73/8e99xzT/zsZz+L6dOnR/v27VPH2Nb2AFAW9OyS9GwAspm+XZK+DdvnynOoJDp16lTiyzm+KkmSOO+886K4uDgeeuihyMnJKbFdbm5uLFiwIPbee+8SU+vWrbd5vIMPPjjee++9Leb37ds3Jk+eHO+++248//zz0bdv39Sy/fbbL956660Sdf7zn/+MatWqRYcOHSIiYt68eTFkyJD47W9/G926dYt+/fpFcXFxREQccsghMWfOnGjatOkWtRYUFGy1zpycnDjiiCNixIgR8cYbb0StWrVi4sSJqeWzZs2KmjVrxv7777/N5woAZUnP1rMBqDz0bX0btkd4Dlnm888/j2OOOSYefvjhePvtt2P+/Pnx2GOPxe233x6nnnrqVre5/vrr4x//+Ef85je/iVWrVsXixYtj8eLFsXbt2qhXr15cddVVMWTIkBg/fnzMmzcvXn/99bjnnnti/Pjx26yjd+/eMW3atNi0aVOJ+UcddVQ0b948+vbtG+3bt49u3bqllvXt2zdq164d/fr1i1mzZsULL7wQgwYNiu9///vRrFmz2LRpU5x33nnRu3fvuOCCC2LcuHHx9ttvx69+9avU9o0bN45TTz01XnrppZg/f35MmTIlBg8eHB9//PEWNU6fPj1uueWWmDFjRixYsCCeeOKJ+Oyzz2K//fZLrfPSSy/Ft7/97dRHygCgrOjZejYAlYe+rW9DqWT4nutAmnXr1iXXXHNNcsghhyQFBQVJnTp1kg4dOiQ///nPkzVr1qTWi698iUmPHj2SiNhiGjduXJIkSVJcXJyMGjUq6dChQ1KzZs2kSZMmSe/evZOpU6dus44NGzYkLVu2TCZPnrzFsh//+MdJRCTXXXfdFsvefvvt5Oijj05q166dNGzYMLn44ouTlStXJkmSJCNGjEhatGiRLFu2LLX+n/70p6RWrVrJm2++mSRJkixatCg5//zzk8aNGye5ubnJnnvumVx88cXJihUrkiQp+SUm7733XtK7d++kSZMmSW5ubrLvvvsm99xzT4l6OnTokPz+97/fwagDwK7Ts/VsACoPfVvfhtLISZIkqdC0Hqg0Ro8eHZMmTYpnnnkm06WUytNPPx0/+tGP4u23397le8UBQGWiZwNA5aFvQ+XhNxzYpksvvTSWL18eK1eujHr16mW6nF22evXqGDdunGYOwNeeng0AlYe+DZWHK88BAAAAACCNLwwFAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDTCcwAAAAAASCM8BwAAAACANMJzAAAAAABIIzwHAAAAAIA0wnMAAAAAAEgjPAcAAAAAgDT/DxlcglTaZGFgAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAAIQCAYAAABg9uTXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJKElEQVR4nO3de1RVdf7/8dfhKEdQwDHlpiheU0RREc0LKRNleCkzS79NZVZO/cJumH61Gi1/pmWlZp3Gml/lTE6NZWaNll1QB0pKxSgNUDEwJwQvKTcV8rB/f7Q8dUKNjcABzvOx1lly9v7svd+HtVqnF5+93x+LYRiGAAAAAADV4uXuAgAAAACgMSFEAQAAAIAJhCgAAAAAMIEQBQAAAAAmEKIAAAAAwARCFAAAAACYQIgCAAAAABMIUQAAAABgAiEKAAAAAEwgRAEAzik8PFy33Xabu8u4aI899pgsFku9XGvkyJEaOXKk8/2WLVtksVi0Zs2aern+bbfdpvDw8Hq5FgB4MkIUAHiY/fv366677lKXLl3UokUL+fv7a9iwYXruued06tQpd5d3QStXrpTFYnG+WrRoodDQUI0aNUrLly9XSUlJrVwnPz9fjz32mDIyMmrlfLWpIdcGAJ6imbsLAADUnw0bNuiGG26QzWbTrbfeqsjISFVUVOizzz7TzJkz9e233+rll192d5m/a/78+ercubN++uknFRQUaMuWLXrggQe0ZMkSvf/+++rbt69z7KOPPqrZs2ebOn9+fr4ef/xxhYeHq1+/ftU+7uOPPzZ1nZq4UG1/+9vfVFlZWec1AICnI0QBgIfIzc3V5MmT1alTJ23atEkhISHOfYmJicrJydGGDRvcWGH1JSQkaODAgc73c+bM0aZNmzR27Fhdc801ysrKko+PjySpWbNmatasbr/uTp48KV9fX3l7e9fpdX5P8+bN3Xp9APAU3M4HAB5i8eLFKi0t1SuvvOISoM7q1q2b7r///vMe/+OPP+qhhx5Snz591KpVK/n7+yshIUFff/11lbHPP/+8evfuLV9fX/3hD3/QwIED9cYbbzj3l5SU6IEHHlB4eLhsNpsCAwN15ZVXaufOnTX+fH/84x/1l7/8RQcOHNCqVauc28/1TNQnn3yi4cOHq3Xr1mrVqpUuvfRSPfzww5J+fo4pJiZGkjR16lTnrYMrV66U9PNzT5GRkUpPT9fll18uX19f57G/fSbqLIfDoYcffljBwcFq2bKlrrnmGh08eNBlzPmeQfv1OX+vtnM9E1VWVqYZM2YoLCxMNptNl156qZ555hkZhuEyzmKxaPr06Vq3bp0iIyNls9nUu3dvbdy48dy/cADwYMxEAYCH+Pe//60uXbpo6NChNTr+u+++07p163TDDTeoc+fOKiws1EsvvaQRI0YoMzNToaGhkn6+pey+++7TxIkTdf/99+v06dP65ptv9OWXX+qmm26SJN19991as2aNpk+froiICB07dkyfffaZsrKyNGDAgBp/xltuuUUPP/ywPv74Y02bNu2cY7799luNHTtWffv21fz582Wz2ZSTk6PPP/9cktSrVy/Nnz9fc+fO1Z///GfFxsZKksvv7dixY0pISNDkyZN18803Kygo6IJ1PfHEE7JYLPrf//1fHT58WMuWLVN8fLwyMjKcM2bVUZ3afs0wDF1zzTXavHmz7rjjDvXr108fffSRZs6cqR9++EFLly51Gf/ZZ59p7dq1uueee+Tn56fly5fr+uuv1/fff69LLrmk2nUCQJNnAACavKKiIkOSce2111b7mE6dOhlTpkxxvj99+rThcDhcxuTm5ho2m82YP3++c9u1115r9O7d+4LnDggIMBITE6tdy1mvvfaaIcnYvn37Bc/dv39/5/t58+YZv/66W7p0qSHJOHLkyHnPsX37dkOS8dprr1XZN2LECEOSsWLFinPuGzFihPP95s2bDUlG+/btjeLiYuf2t956y5BkPPfcc85tv/19n++cF6ptypQpRqdOnZzv161bZ0gyFixY4DJu4sSJhsViMXJycpzbJBne3t4u277++mtDkvH8889XuRYAeDJu5wMAD1BcXCxJ8vPzq/E5bDabvLx+/tpwOBw6duyY81a4X9+G17p1a/33v//V9u3bz3uu1q1b68svv1R+fn6N6zmfVq1aXbBLX+vWrSVJ7733Xo2bMNhsNk2dOrXa42+99VaX3/3EiRMVEhKiDz74oEbXr64PPvhAVqtV9913n8v2GTNmyDAMffjhhy7b4+Pj1bVrV+f7vn37yt/fX999912d1gkAjQ0hCgA8gL+/vyRdVAvwyspKLV26VN27d5fNZlPbtm3Vrl07ffPNNyoqKnKO+9///V+1atVKgwYNUvfu3ZWYmOi8Ve6sxYsXa/fu3QoLC9OgQYP02GOP1dr/qJeWll4wLE6aNEnDhg3TnXfeqaCgIE2ePFlvvfWWqUDVvn17U00kunfv7vLeYrGoW7duysvLq/Y5auLAgQMKDQ2t8vvo1auXc/+vdezYsco5/vCHP+j48eN1VyQANEKEKADwAP7+/goNDdXu3btrfI6FCxcqKSlJl19+uVatWqWPPvpIn3zyiXr37u0SQHr16qU9e/boX//6l4YPH6533nlHw4cP17x585xjbrzxRn333Xd6/vnnFRoaqqefflq9e/euMjNi1n//+18VFRWpW7du5x3j4+OjlJQUffrpp7rlllv0zTffaNKkSbryyivlcDiqdR0zzzFV1/kWBK5uTbXBarWec7vxmyYUAODpCFEA4CHGjh2r/fv3Ky0trUbHr1mzRnFxcXrllVc0efJkXXXVVYqPj9eJEyeqjG3ZsqUmTZqk1157Td9//73GjBmjJ554QqdPn3aOCQkJ0T333KN169YpNzdXl1xyiZ544omafjxJ0uuvvy5JGjVq1AXHeXl56YorrtCSJUuUmZmpJ554Qps2bdLmzZslnT/Q1NS+fftc3huGoZycHJdOen/4wx/O+bv87WyRmdo6deqk/Pz8KjOQ2dnZzv0AAPMIUQDgIWbNmqWWLVvqzjvvVGFhYZX9+/fv13PPPXfe461Wa5UZibfffls//PCDy7Zjx465vPf29lZERIQMw9BPP/0kh8PhcvufJAUGBio0NFTl5eVmP5bTpk2b9H//7/9V586d9ac//em843788ccq284uWnv2+i1btpSkc4aamvjHP/7hEmTWrFmjQ4cOKSEhwbmta9eu+uKLL1RRUeHctn79+iqt0M3UNnr0aDkcDr3wwgsu25cuXSqLxeJyfQBA9dHiHAA8RNeuXfXGG29o0qRJ6tWrl2699VZFRkaqoqJCW7du1dtvv33OdYrOGjt2rObPn6+pU6dq6NCh2rVrl/75z3+qS5cuLuOuuuoqBQcHa9iwYQoKClJWVpZeeOEFjRkzRn5+fjpx4oQ6dOigiRMnKioqSq1atdKnn36q7du369lnn63WZ/nwww+VnZ2tM2fOqLCwUJs2bdInn3yiTp066f3331eLFi3Oe+z8+fOVkpKiMWPGqFOnTjp8+LBefPFFdejQQcOHD3f+rlq3bq0VK1bIz89PLVu21ODBg9W5c+dq1fdbbdq00fDhwzV16lQVFhZq2bJl6tatm0sb9jvvvFNr1qzR1VdfrRtvvFH79+/XqlWrXBo9mK1t3LhxiouL0yOPPKK8vDxFRUXp448/1nvvvacHHnigyrkBANXk1t6AAIB6t3fvXmPatGlGeHi44e3tbfj5+RnDhg0znn/+eeP06dPOcedqcT5jxgwjJCTE8PHxMYYNG2akpaVVacH90ksvGZdffrlxySWXGDabzejatasxc+ZMo6ioyDAMwygvLzdmzpxpREVFGX5+fkbLli2NqKgo48UXX/zd2s+2OD/78vb2NoKDg40rr7zSeO6551zaiJ/12xbnycnJxrXXXmuEhoYa3t7eRmhoqPE///M/xt69e12Oe++994yIiAijWbNmLi3FR4wYcd4W7udrcf7mm28ac+bMMQIDAw0fHx9jzJgxxoEDB6oc/+yzzxrt27c3bDabMWzYMGPHjh1Vznmh2n7b4twwDKOkpMR48MEHjdDQUKN58+ZG9+7djaefftqorKx0GSfpnG3nz9d6HQA8mcUweFoUAAAAAKqLZ6IAAAAAwARCFAAAAACYQIgCAAAAABMIUQAAAABgAiEKAAAAAEwgRAEAAACACR6/2G5lZaXy8/Pl5+cni8Xi7nIAAAAAuIlhGCopKVFoaKi8vM4/3+TxISo/P19hYWHuLgMAAABAA3Hw4EF16NDhvPs9NkTZ7XbZ7XadOXNG0s+/KH9/fzdXBQAAAMBdiouLFRYWJj8/vwuOsxiGYdRTTQ1ScXGxAgICVFRURIgCAAAAPFh1swGNJQAAAADABEIUAAAAAJhAiAIAAAAAEwhRAAAAAGACIQoAAAAATCBEAQAAAIAJhCgAAAAAMIEQBQAAAAAmEKIAAAAAwARCFAAAAACYQIgCAAAAABMIUQAAAABggseGKLvdroiICMXExLi7FAAAAACNiMUwDMPdRbhTcXGxAgICVFRUJH9/f3eXA9Qrh8Oh1NRUHTp0SCEhIYqNjZXVanV3WQAAAG5R3WzgsTNRgKdbu3atunXrpri4ON10002Ki4tTt27dtHbtWneXBgAA0KARogAPtHbtWk2cOFF9+vRRWlqaSkpKlJaWpj59+mjixIkEKQAAgAvgdj5u54OHcTgc6tatm/r06aN33nlHn3/+ufN2vmHDhun666/X7t27tW/fPm7tAwAAHoXb+QCcU2pqqvLy8jR06FD16NHD5Xa+Hj16aMiQIcrNzVVqaqq7SwUAAGiQmrm7AAD169ChQ5KkOXPmyMfHx2VfYWGhHn74YZdxAAAAcEWIAjxMYGCg8+c//vGPGj16tHx8fHTq1Cl98MEH2rBhQ5VxAAAA+AUhCvAwDodDktSqVSvt3r3bGZokqVOnTmrVqpVKS0ud4wAAAOCKZ6IAD3P2WafS0lKdPn1aL7/8svLz8/Xyyy/r9OnTKi0tdRkHAAAAV8xEAR6msrJSktSjRw+Vl5frz3/+s3NfeHi4evToob179zrHAQAAwBUzUYCHadOmjSSpvLy8yj7DMJzbz44DAACAK2aiAA8THBwsSTpw4IC8vFz/jnLw4EHnDNTZcQAAAHDFTBTgYX4djn57y96v194mRAEAAJwbIQrwMGeDk81mk9Vqddnn5eUlm83mMg4AAACuuJ0P8DApKSmSfn4mKjAwUCNGjHC2Nf/Pf/6jw4cPO8ddeeWV7iwVAACgQSJEAR7m7AxTaGioCgsL9fbbbzv3Wa1WhYaGKj8/n5koAACA8yBEAR7mbNe9/Px8jRkzRqNHj5aPj49OnTqlDz74wLn4Lt35AAAAzq1JPBO1dOlS9e7dWxEREbrvvvtcHo4H4CowMNDlff/+/TVx4kT179//guMAAADws0Y/E3XkyBG98MIL+vbbb9W8eXNdfvnl+uKLLzRkyBB3lwY0SMeOHXP+vGnTJufMkyT5+vqecxwAAAB+0SRmos6cOaPTp0/rp59+0k8//cRf0IELaNeunaSfZ6DO/vzrfWdnpH67DwAAAD9ze4hKSUnRuHHjFBoaKovFonXr1lUZY7fbFR4erhYtWmjw4MHatm2bc1+7du300EMPqWPHjgoNDVV8fLy6du1aj58AaFzat28vScrIyFDfvn31wgsv6JVXXtELL7ygPn36KCMjw2UcAAAAXLn9dr6ysjJFRUXp9ttv14QJE6rsX716tZKSkrRixQoNHjxYy5Yt06hRo7Rnzx4FBgbq+PHjWr9+vfLy8uTj46OEhASlpKTo8ssvd8OnARq+2NhYhYeHq23bttq9e7fWr1/v3Ne5c2dFR0fr2LFjio2NdWOVAAAADZfbQ1RCQoISEhLOu3/JkiWaNm2apk6dKklasWKFNmzYoFdffVWzZ8/Wp59+qm7dujk7iY0ZM0ZffPHFeUNUeXm5ysvLne+Li4tr8dMA7nPy5EllZ2dXa2xiYqJmzZql4cOH67rrrlNJSYn8/Py0Y8cOffbZZ1q8eLG+/vrral+7Z8+eLs9TAQAANGVuD1EXUlFRofT0dM2ZM8e5zcvLS/Hx8UpLS5MkhYWFaevWrTp9+rSaN2+uLVu26M9//vN5z7lo0SI9/vjjdV47UN+ys7MVHR1t6pjU1FSlpqZW2T5z5kxT50lPT9eAAQNMHQMAANBYNegQdfToUTkcDgUFBblsDwoKcv7F/bLLLtPo0aPVv39/eXl56YorrtA111xz3nPOmTNHSUlJzvfFxcUKCwurmw8A1KOePXsqPT3d1DEOh0Pr1q3TwoUL9fDDD2v8+PGyWq01ujYAAICnaNAhqrqeeOIJPfHEE9Uaa7PZZLPZ6rgioP75+vrWaDbIarVq4cKFuv7665lNAgAAqAa3d+e7kLZt28pqtaqwsNBle2FhoYKDgy/q3Ha7XREREYqJibmo8wAAAADwLA06RHl7eys6OlrJycnObZWVlUpOTr7oxXQTExOVmZmp7du3X2yZAAAAADyI22/nKy0tVU5OjvN9bm6uMjIy1KZNG3Xs2FFJSUmaMmWKBg4cqEGDBmnZsmUqKytzdusDAAAAgPrk9hC1Y8cOxcXFOd+fbfowZcoUrVy5UpMmTdKRI0c0d+5cFRQUqF+/ftq4cWOVZhMAAAAAUB/cHqJGjhwpwzAuOGb69OmaPn16rV7XbrfLbrfL4XDU6nmBi7Vv3z6VlJTU2/WysrJc/q0vfn5+6t69e71eEwAAoDZYjN9LME1ccXGxAgICVFRUJH9/f3eXAw+3b98+9ejRw91l1Ju9e/cSpAAAQINR3Wzg9pkoAL84OwO1atUq9erVq16ueerUKeXl5Sk8PFw+Pj71cs2srCzdfPPN9TrjBgAAUFsIUUAD1KtXr3pds2nYsGH1di0AAIDGrkG3OAcAAACAhsZjQxSL7QIAAACoCY+9nS8xMVGJiYnOh8eAhiK4lUU+J/ZK+U33bxw+J/YquJXF3WUAAADUiMeGKKChuivaW71S7pJS3F1J3emlnz8nAABAY0SIAhqQkydP6qX0CkXdOFs9e/asl2uWl5crPz9foaGhstls9XLN3NxcvZT+iK6pl6sBAADULkIU0IBkZ2eroNTQhMTH3V1KvfDz83N3CQAAAKZ5bIiy2+2y2+1yOBzuLgVwGj9+vCSpZ8+e8vX1rZdrnl2zqT7XppJ+DlAstAsAABoji2EYhruLcKfqrkoMNFU7d+5UdHS00tPT63VtKgAAgIamutmg6bb/AgAAAIA6QIgCAAAAABMIUQAAAABgAiEKAAAAAEzw2BBlt9sVERGhmJgYd5cCAAAAoBHx2BCVmJiozMxMbd++3d2lAAAAAGhEPDZEAQAAAEBNEKIAAAAAwARCFAAAAACYQIgCAAAAABOaubsAALXj5MmTys7ONn1cVlaWy7810bNnT/n6+tb4eAAAgMaEEAU0EdnZ2YqOjq7x8TfffHONj01PT9eAAQNqfDwAAEBj4rEhym63y263y+FwuLsUoFb07NlT6enppo87deqU8vLyFB4eLh8fnxpfGwAAwFNYDMMw3F2EOxUXFysgIEBFRUXy9/d3dzlAvaqoqNCLL76o/fv3q2vXrrrnnnvk7e3t7rIAAADcorrZwGNnogBPN2vWLC1ZssRlNvahhx5SUlKSFi9e7MbKAAAAGjZCFOCBZs2apaeffrrKdofD4dxOkAIAADg3WpwDHqaiokLPPPOMJKlZs2aaPXu2cnJyNHv2bDVr9vPfVZ555hlVVFS4s0wAAIAGixAFeJhly5bJMAxZrVYVFxdr1KhR2rZtm0aNGqXi4mJZrVYZhqFly5a5u1QAAIAGicYSNJaAh+nTp492796thIQEZWVlKS8vz7kvPDxcl156qT766CNFRkZq165d7isUAACgntFYAsA5nb1N78MPP6zS0rywsNAZqridDwAA4Ny4nQ/wMCNGjHD+HBcXp7S0NJWUlCgtLU1xcXHnHAcAAIBfEKIAD3Pdddc5f96xY4e++eYbFRcX65tvvtGOHTvOOQ4AAAC/8Njb+ex2u+x2u8saOYAn2Lp1q/Pnw4cP66677jrvuISEhPoqCwAAoNHw2JmoxMREZWZmavv27e4uBXCLoUOHnnP7kCFD6rkSAACAxsVjQxTgqUaOHClJ8vLyUmlpqRITE3XVVVcpMTFRpaWlslqtLuMAAADgymNv5wM81ciRI9WuXTt99tlnmjx5sh5++GFFRkZq9+7dmjx5sj777DMFBgYSogAAAM6DEAV4GKvVqhUrVuj6669XcnKy1q9f79zn6+srSfrrX//qnJECAACAK27nAzzQhAkT9M477ygwMNBle2BgoN555x1NmDDBTZUBAAA0fBbDMAx3F+FO1V2VGGiKHA6HUlNTdejQIYWEhCg2NpYZKAAA4LGqmw24nQ/wYFarlWefAAAATOJ2PgAAAAAwgRAFAAAAACYQogAAAADABEIUAAAAAJhAiAIAAAAAEwhRAAAAAGCCx4You92uiIgIxcTEuLsUAAAAAI0Ii+2y2C4AAAAAVT8beOxMFAAAAADUBCEKAAAAAEwgRAEAAACACYQoAAAAADCBEAUAAAAAJhCiAAAAAMAEQhQAAAAAmECIAgAAAAATCFEAAAAAYAIhCgAAAABMIEQBAAAAgAmEKAAAAAAwgRAFAAAAACYQogAAAADABEIUAAAAAJjQ6EPUnj171K9fP+fLx8dH69atc3dZAAAAAJqoZu4u4GJdeumlysjIkCSVlpYqPDxcV155pXuLAgAAANBkNfqZqF97//33dcUVV6hly5buLgUAAABAE+X2EJWSkqJx48YpNDRUFovlnLfi2e12hYeHq0WLFho8eLC2bdt2znO99dZbmjRpUh1XDAAAAMCTuT1ElZWVKSoqSna7/Zz7V69eraSkJM2bN087d+5UVFSURo0apcOHD7uMKy4u1tatWzV69Oj6KBsAAACAh7IYhmG4u4izLBaL3n33XY0fP965bfDgwYqJidELL7wgSaqsrFRYWJjuvfdezZ492znu9ddf10cffaRVq1Zd8Brl5eUqLy93vi8uLlZYWJiKiork7+9fux8IAAAAQKNRXFysgICA380Gbp+JupCKigqlp6crPj7euc3Ly0vx8fFKS0tzGVvdW/kWLVqkgIAA5yssLKzW6wYAAADQdDXoEHX06FE5HA4FBQW5bA8KClJBQYHzfVFRkbZt26ZRo0b97jnnzJmjoqIi5+vgwYO1XjcAAACApqvRtziXpICAABUWFlZrrM1mk81mq+OKAAAAADRVDXomqm3btrJarVUCUmFhoYKDg91UFQAAAABP1qBDlLe3t6Kjo5WcnOzcVllZqeTkZA0ZMuSizm232xUREaGYmJiLLRMAAACAB3H77XylpaXKyclxvs/NzVVGRobatGmjjh07KikpSVOmTNHAgQM1aNAgLVu2TGVlZZo6depFXTcxMVGJiYnODhwAAAAAUB1uD1E7duxQXFyc831SUpIkacqUKVq5cqUmTZqkI0eOaO7cuSooKFC/fv20cePGKs0mAAAAAKA+NKh1otyhur3gAQAAADRtTWKdqLrEM1EAAAAAaoKZKGaiAAAAAIiZKAAAAACoE4QoAAAAADCBEAUAAAAAJnhsiKKxBAAAAICaoLEEjSUAAAAAiMYSAAAAAFAnCFEAAAAAYAIhCgAAAABM8NgQRWMJAAAAADVBYwkaSwAAAAAQjSUAAAAAoE4QogAAAADABEIUAAAAAJhAiAIAAAAAEzw2RNGdDwAAAEBN0J2P7nwAAAAARHc+AAAAAKgThCgAAAAAMIEQBQAAAAAmEKIAAAAAwARCFAAAAACYQIgCAAAAABM8NkSxThQAAACAmmCdKNaJAgAAACDWiQIAAACAOkGIAgAAAAATCFEAAAAAYAIhCgAAAABMIEQBAAAAgAmEKAAAAAAwgRAFAAAAACZ4bIhisV0AAAAANcFiuyy2CwAAAEAstgsAAAAAdYIQBQAAAAAmEKIAAAAAwARCFAAAAACYQIgCAAAAABMIUQAAAABgAiEKAAAAAEwgRAEAAACACYQoAAAAADCBEAUAAAAAJhCiAAAAAMCEiw5RDodDGRkZOn78eG3UAwAAAAANmukQ9cADD+iVV16R9HOAGjFihAYMGKCwsDBt2bKltuurM3a7XREREYqJiXF3KQAAAAAaEdMhas2aNYqKipIk/fvf/1Zubq6ys7P14IMP6pFHHqn1AutKYmKiMjMztX37dneXAgAAAKARMR2ijh49quDgYEnSBx98oBtuuEE9evTQ7bffrl27dtV6gQAAAADQkJgOUUFBQcrMzJTD4dDGjRt15ZVXSpJOnjwpq9Va6wUCAAAAQEPSzOwBU6dO1Y033qiQkBBZLBbFx8dLkr788kv17Nmz1gsEAAAAgIbEdIh67LHHFBkZqYMHD+qGG26QzWaTJFmtVs2ePbvWCwQAAACAhsRiGIbh7iLcqbi4WAEBASoqKpK/v7+7ywEAAADgJtXNBtWaiVq+fHm1L3zfffdVeywAAAAANDbVmonq3Llz9U5msei777676KLqEzNRAAAAAKRanonKzc2ttcIAAAAAoDEz3eL8rIqKCu3Zs0dnzpypzXoAAAAAoEEzHaJOnjypO+64Q76+vurdu7e+//57SdK9996rJ598stYLBAAAAICGxHSImjNnjr7++mtt2bJFLVq0cG6Pj4/X6tWra7U4AAAAAGhoTK8TtW7dOq1evVqXXXaZLBaLc3vv3r21f//+Wi0OAAAAABoa0zNRR44cUWBgYJXtZWVlLqEKAAAAAJoi0yFq4MCB2rBhg/P92eD0//7f/9OQIUNqrzIAAAAAaIBM3863cOFCJSQkKDMzU2fOnNFzzz2nzMxMbd26Vf/5z3/qokYAAAAAaDBMz0QNHz5cGRkZOnPmjPr06aOPP/5YgYGBSktLU3R0dF3UCAAAAAANhsUwDKO2Tnby5En5+vrW1umqLTc3V7fffrsKCwtltVr1xRdfqGXLltU6trqrEgMAAABo2qqbDUzPRF1xxRX64Ycfqmzftm2b+vXrZ/Z0teK2227T/PnzlZmZqf/85z+y2WxuqQMAAABA02c6RLVo0UJ9+/Z1rglVWVmpxx57TMOHD9fo0aNrvcDf8+2336p58+aKjY2VJLVp00bNmpl+1AsAAAAAqsV0iNqwYYPmz5+v22+/XTfddJOGDx+uv/3tb1q/fr2WLVtmuoCUlBSNGzdOoaGhslgsWrduXZUxdrtd4eHhatGihQYPHqxt27Y59+3bt0+tWrXSuHHjNGDAAC1cuNB0DQAAAABQXTWasklMTNR///tfPfXUU2rWrJm2bNmioUOH1qiAsrIyRUVF6fbbb9eECROq7F+9erWSkpK0YsUKDR48WMuWLdOoUaO0Z88eBQYG6syZM0pNTVVGRoYCAwN19dVXKyYmRldeeWWN6gEAAACACzE9E3X8+HFdf/31+utf/6qXXnpJN954o6666iq9+OKLNSogISFBCxYs0HXXXXfO/UuWLNG0adM0depURUREaMWKFfL19dWrr74qSWrfvr0GDhyosLAw2Ww2jR49WhkZGee9Xnl5uYqLi11eAAAAAFBdpkNUZGSkCgsL9dVXX2natGlatWqVXnnlFf3lL3/RmDFjarW4iooKpaenKz4+/peCvbwUHx+vtLQ0SVJMTIwOHz6s48ePq7KyUikpKerVq9d5z7lo0SIFBAQ4X2FhYbVaMwAAAICmzXSIuvvuu5WSkqLOnTs7t02aNElff/21KioqarW4o0ePyuFwKCgoyGV7UFCQCgoKJEnNmjXTwoULdfnll6tv377q3r27xo4de95zzpkzR0VFRc7XwYMHa7VmAAAAAE2b6Wei/vKXvzh/PrvElMViUYcOHfTJJ5/UXmUmJCQkKCEhoVpjbTYbLdABAAAA1JjpmShJ+sc//qE+ffrIx8dHPj4+6tu3r15//fXark1t27aV1WpVYWGhy/bCwkIFBwfX+vUAAAAA4PeYDlFLlizR//k//0ejR4/WW2+9pbfeektXX3217r77bi1durRWi/P29lZ0dLSSk5Od2yorK5WcnKwhQ4Zc1LntdrsiIiIUExNzsWUCAAAA8CAW4+w9edXUuXNnPf7447r11ltdtv/973/XY489ptzcXFMFlJaWKicnR5LUv39/LVmyRHFxcWrTpo06duyo1atXa8qUKXrppZc0aNAgLVu2TG+99Zays7OrPCtVE8XFxQoICFBRUZH8/f0v+nwAAAAAGqfqZgPTz0QdOnTonGtCDR06VIcOHTJ7Ou3YsUNxcXHO90lJSZKkKVOmaOXKlZo0aZKOHDmiuXPnqqCgQP369dPGjRtrJUABAAAAgFmmZ6IiIyN100036eGHH3bZvmDBAq1evVq7du2q1QLrGjNRAAAAAKQ6nIl6/PHHNWnSJKWkpGjYsGGSpM8//1zJycl66623al5xPbPb7bLb7XI4HO4uBQAAAEAjUu2ZqN27dysyMlKSlJ6erqVLlyorK0uS1KtXL82YMUP9+/evu0rrCDNRAAAAAKTqZ4NqhygvLy/FxMTozjvv1OTJk+Xn51drxboTIQoAAACAVP1sUO0W5//5z3/Uu3dvzZgxQyEhIbrtttuUmppaK8UCAAAAQGNR7RAVGxurV199VYcOHdLzzz+v3NxcjRgxQj169NBTTz2lgoKCuqyz1rFOFAAAAICaMN2d79dycnL02muv6fXXX1dBQYGuvvpqvf/++7VZX53jdj4AAAAAUh08E3U+ZWVl+uc//6k5c+boxIkTja7bHSEKAAAAgFSHLc7PSklJ0auvvqp33nlHXl5euvHGG3XHHXfU9HQAAAAA0CiYClH5+flauXKlVq5cqZycHA0dOlTLly/XjTfeqJYtW9ZVjQAAAADQYFQ7RCUkJOjTTz9V27Ztdeutt+r222/XpZdeWpe11SkW2wUAAABQE9V+Juqaa67RHXfcobFjx8pqtdZ1XfWGZ6IAAAAASHXwTFRj67oHAAAAAHWh2utEAQAAAAAIUQAAAABgCiEKAAAAAEyoVogaMGCAjh8/LkmaP3++Tp48WadFAQAAAEBDVa0QlZWVpbKyMknS448/rtLS0jotqj7Y7XZFREQoJibG3aUAAAAAaESq1eJ8yJAhatWqlYYPH67HH39cDz30kFq1anXOsXPnzq31IusSLc4BAAAASNXPBtUKUXv27NG8efO0f/9+7dy5UxEREWrWrGp3dIvFop07d15c5fWMEAUAAABAquUQ9WteXl4qKChQYGDgRRfZEBCiAAAAAEh1sNjuWZWVlRdVGAAAAAA0ZqZDlCTt379fy5YtU1ZWliQpIiJC999/v7p27VqrxQEAAABAQ2N6naiPPvpIERER2rZtm/r27au+ffvqyy+/VO/evfXJJ5/URY0AAAAA0GCYfiaqf//+GjVqlJ588kmX7bNnz9bHH3/caBpL2O122e12ORwO7d27l2eiAAAAAA9XZ40lWrRooV27dql79+4u2/fu3au+ffvq9OnTNavYTWgsAQAAPJ3D4VBqaqoOHTqkkJAQxcbGymq1urssoN5VNxuYvp2vXbt2ysjIqLI9IyOjyXTsAwAA8BRr165Vt27dFBcXp5tuuklxcXHq1q2b1q5d6+7SgAbLdIiaNm2a/vznP+upp55SamqqUlNT9eSTT+quu+7StGnT6qJGAAAA1IG1a9dq4sSJ6tOnj9LS0lRSUqK0tDT16dNHEydOJEgB52H6dj7DMLRs2TI9++yzys/PlySFhoZq5syZuu+++2SxWOqk0LrC7XwAAMATORwOdevWTX369NG6devk5fXL39YrKys1fvx47d69W/v27ePWPniMOnsm6tdKSkokSX5+fjU9hdsRogAAgCfasmWL4uLilJaWpssuu6zK/rS0NA0dOlSbN2/WyJEj679AwA3qbLHdX2vM4QkAAMCTHTp0SJIUGRl5zv1nt58dB+AXpp+JAgAAQOMXEhIiSdq9e/c595/dfnYcgF8QogAAADxQbGyswsPDtXDhQlVWVrrsq6ys1KJFi9S5c2fFxsa6qUKg4SJEAQAAeCCr1apnn31W69ev1/jx4126840fP17r16/XM888Q1MJ4BxMhaiffvpJV1xxhfbt21dX9dQbu92uiIgIxcTEuLsUAAAAt5gwYYLWrFmjXbt2aejQofL399fQoUO1e/durVmzRhMmTHB3iUCDZLo7X7t27bR161Z17969rmqqV3TnAwAAns7hcCg1NVWHDh1SSEiIYmNjmYGCR6qzFucPPvigbDabnnzyyYsusiEgRAEAAACQ6rDF+ZkzZ/Tqq6/q008/VXR0tFq2bOmyf8mSJearBQAAAIBGwnSI2r17twYMGCBJ2rt3r8s+i8VSO1UBAAAAQANlOkRt3ry5LuoAAAAAgEahxi3Oc3Jy9NFHH+nUqVOSJJOPVgEAAABAo2Q6RB07dkxXXHGFevToodGjR+vQoUOSpDvuuEMzZsyo9QIBAAAAoCExHaIefPBBNW/eXN9//718fX2d2ydNmqSNGzfWanEAAAAA0NCYfibq448/1kcffaQOHTq4bO/evbsOHDhQa4UBAAAAQENkeiaqrKzMZQbqrB9//FE2m61WigIAAACAhsp0iIqNjdU//vEP53uLxaLKykotXrxYcXFxtVocAAAAADQ0pm/nW7x4sa644grt2LFDFRUVmjVrlr799lv9+OOP+vzzz+uiRgAAAABoMEzPREVGRmrv3r0aPny4rr32WpWVlWnChAn66quv1LVr17qoEQAAAAAaDIvhoQs82e122e12ORwO7d27V0VFRfL393d3WQAAAADcpLi4WAEBAb+bDWoUoo4fP65XXnlFWVlZkqSIiAhNnTpVbdq0qXnFblLdXxQAAACApq262cD07XwpKSkKDw/X8uXLdfz4cR0/flzLly9X586dlZKSclFFAwAAAEBDZ3omqk+fPhoyZIj++te/ymq1SpIcDofuuecebd26Vbt27aqTQusKM1EAAAAApDqcicrJydGMGTOcAUqSrFarkpKSlJOTU7NqAQAAAKCRMN3ifMCAAcrKytKll17qsj0rK0tRUVG1VhgAAADqR0VFhV588UXt379fXbt21T333CNvb293lwU0WNUKUd98843z5/vuu0/333+/cnJydNlll0mSvvjiC9ntdj355JN1UyUAAADqxKxZs7R06VKdOXPGuW3mzJl68MEHtXjxYjdWBjRc1XomysvLSxaLRb831GKxyOFw1Fpx9YFnogAAgKeaNWuWnn76aQUFBWnBggUaO3as1q9fr0cffVSFhYWaOXMmQQoepVZbnB84cKDaF+7UqVO1xzYEhCgAAOCJKioq1LJlS11yySX673//q2bNfrlB6cyZM+rQoYOOHTumsrIybu2Dx6huNqjW7XyNLRgBAADgwl588UWdOXNGCxYskMVi0ZYtW3To0CGFhIQoNjZW8+fP11133aUXX3xRDzzwgLvLBRoU040lJCk/P1+fffaZDh8+rMrKSpd99913X60UBgAAgLqzf/9+ST8/jtGtWzfl5eU594WHh+uRRx5xGQfgF6ZD1MqVK3XXXXfJ29tbl1xyiSwWi3OfxWIhRAEAADQCXbt2lSTdeeedGjdunN58801FRkZq9+7dWrhwoaZNm+YyDsAvTC+2GxYWprvvvltz5syRl5fpZaYaHJ6JAgAAnujUqVPy9fWVt7e3SkpKXJ57qqiokJ+fnyoqKnTy5En5+Pi4sVKg/tTZYrsnT57U5MmTm0SAAgAA8FRffvmlpJ8DU8eOHfXyyy8rPz9fL7/8sjp27KiKigqXcQB+YToJ3XHHHXr77bfrohYAAADUk0OHDkmS7r//fh09elR33XWX2rdvr7vuukvHjh3T/fff7zIOwC9MPxO1aNEijR07Vhs3blSfPn3UvHlzl/1LliypteIAAABQN0JCQiRJwcHB6tChg8uSNu3bt1dQUJDLOAC/MP1M1IIFCzR37lxdeumlCgoKqtJYYtOmTbVeZF3imSgAAOCJHA6HQkJCdOTIEY0ZM0ajR4+Wj4+PTp06pQ8++EAbNmxQYGCg8vPzZbVa3V0uUC9qdZ2oX3v22Wf16quv6rbbbruY+mpVeHi4/P395eXlpT/84Q/avHmzu0sCAABo8M7+MTw5OVkbNmxwbm/RooW7SgIaBdMhymazadiwYXVRy0XZunWrWrVq5e4yAAAAGoXU1FQdPnxYklzuLJLkbCB2+PBhpaamauTIkfVdHtCgmW4scf/99+v555+vi1oAAABQT3744QdJUkJCgoqKirR582a98cYb2rx5s06cOKGEhASXcQB+YTpEbdu2TX//+9/VpUsXjRs3ThMmTHB5mZWSkqJx48YpNDRUFotF69atqzLGbrcrPDxcLVq00ODBg7Vt2zaX/RaLRSNGjFBMTIz++c9/mq4BAADA0xw5ckSSNGHCBDVv3lwjR47U//zP/2jkyJFq3ry5xo8f7zIOwC9Mh6jWrVtrwoQJGjFihNq2bauAgACXl1llZWWKioqS3W4/5/7Vq1crKSlJ8+bN086dOxUVFaVRo0Y5p58l6bPPPlN6erref/99LVy4UN98843pOgAAADxJu3btJElr165VZWWly77KykrnH7bPjgPwC9Pd+eqSxWLRu+++6/zLhyQNHjxYMTExeuGFFyT9/B91WFiY7r33Xs2ePbvKOWbOnKnevXtXu/EF3fkAAIAn2rJli+Li4mSxWDRmzBhdffXVzu58Gzdu1IYNG2QYhjZv3swzUfAYddadrz5VVFQoPT1dc+bMcW7z8vJSfHy80tLSJP08k1VZWSk/Pz+VlpZq06ZNuvHGG897zvLycpWXlzvfFxcX190HAAAAaKBiY2MVHh4uq9WqjRs3av369c59zZo1U5cuXVRZWanY2Fg3Vgk0TKZDVOfOnat0cPm177777qIK+rWjR4/K4XA4F3s7KygoSNnZ2ZKkwsJCXXfddZJ+Xu9g2rRpiomJOe85Fy1apMcff7zWagQAAGiMrFarbrjhBj399NMKDAzULbfcoi5duui7777T66+/rv3792vmzJmsEQWcg+kQ9cADD7i8/+mnn/TVV19p48aNmjlzZm3VVW1dunTR119/Xe3xc+bMUVJSkvN9cXGxwsLC6qI0AACABsvhcOjtt9/WwIEDdeTIET377LPOfeHh4Ro4cKDWrFmjRYsWEaSA3zAdou6///5zbrfb7dqxY8dFF/Rrbdu2ldVqVWFhocv2wsJCBQcH1+icNptNNputNsoDAABotFJTU5WXl6c333xTMTExSk1N1aFDhxQSEqLY2Fht27ZNQ4cOZZ0o4BxMd+c7n4SEBL3zzju1dTpJkre3t6Kjo5WcnOzcVllZqeTkZA0ZMuSizm232xUREXHBW/8AAACaqkOHDkmSIiMjZbVaXVqcW61WRUZGuowD8ItaayyxZs0atWnTxvRxpaWlysnJcb7Pzc1VRkaG2rRpo44dOyopKUlTpkzRwIEDNWjQIC1btkxlZWWaOnXqRdWbmJioxMREZwcOAAAATxISEiJJ2r179zlnonbv3u0yDsAvTIeo/v37uzSWMAxDBQUFOnLkiF588UXTBezYsUNxcXHO92efV5oyZYpWrlypSZMm6ciRI5o7d64KCgrUr18/bdy4sUqzCQAAAFTf2e589957r44ePaq8vDznvvDwcLVt21adO3emOx9wDqbXifptZzsvLy+1a9dOI0eOVM+ePWu1uPrAOlEAAMBTzZo167zd+Q4fPqyZM2dq8eLF7i4TqDfVzQYNarHd+mS322W32+VwOLR3715CFAAA8CgOh0PdunWT1WpVXl6eHA6Hc1+zZs3UqVMnVVZWat++fXTng8cgRFUTM1EAAMATbdmyRXFxcbJYLBozZowSEhLk4+OjU6dO6cMPP9SGDRtkGIY2b95Mdz54jOpmg2o/E+Xl5XXBRXYlyWKx6MyZM9WvEgAAAG7xww8/SJKuvvpqvffee/Ly+qVp8913362xY8fqww8/dI4D8Itqh6h33333vPvS0tK0fPlyVVZW1kpRAAAAqFtHjhyRJE2YMMElQEk///F8/Pjx+vDDD53jAPyi2iHq2muvrbJtz549mj17tv7973/rT3/6k+bPn1+rxQEAAKButGvXTpK0du1a3X777S5BqrKyUuvWrXMZB+AXNVpsNz8/X9OmTVOfPn105swZZWRk6O9//7s6depU2/XVGRbbBQAAnqx9+/aSpA8//FDjx49XWlqaSkpKlJaW5pyF+vU4AL8w1ViiqKhICxcu1PPPP69+/frpqaeeavRrB9BYAgAAeKKz3fnatm2rI0eO6MCBA859Z9eJOnbsGN354FFqvbHE4sWL9dRTTyk4OFhvvvnmOW/vAwAAQONgtVr17LPPauLEiRozZoxmzpzp7M63ceNGbdiwQWvWrCFAAedQ7ZkoLy8v+fj4KD4+/oL/Ma1du7bWiqsPzEQBAABPtnbtWs2YMUN5eXnObZ07d9YzzzyjCRMmuK8wwA1qfSbq1ltv/d0W5wAAAGhcJkyYoGuvvVapqak6dOiQQkJCFBsbywwUcAEeu9iu3W6X3W6Xw+HQ3r17mYkCAAAAPFx1Z6I8NkSdxe18AAAAAKTqZ4MatTgHAAAAAE9FiAIAAAAAEwhRAAAAAGACIQoAAAAATPDYEGW32xUREaGYmBh3lwIAAACgEaE7H935AAAAAIjufAAAAABQJwhRAAAAAGACIQoAAAAATCBEAQAAAIAJzdxdAAAAAGrHyZMnlZ2dXaNjT506pby8PIWHh8vHx8f08T179pSvr2+Nrg00NoQoAACAJiI7O1vR0dFuuXZ6eroGDBjglmsD9c1jQ5TdbpfdbpfD4XB3KQAAALWiZ8+eSk9Pr9GxWVlZuvnmm7Vq1Sr16tWrRtcGPAXrRLFOFAAAgHbu3Kno6GhmlODRWCcKAAAAAOoAIQoAAAAATCBEAQAAAIAJhCgAAAAAMIEQBQAAAAAmEKIAAAAAwARCFAAAAACYQIgCAAAAABM8NkTZ7XZFREQoJibG3aUAAAAAaEQ8NkQlJiYqMzNT27dvd3cpAAAAABoRjw1RAAAAAFAThCgAAAAAMKGZuwsAAABAVfv27VNJSUm9XS8rK8vl3/ri5+en7t271+s1gYtFiAIAAGhg9u3bpx49erjl2jfffHO9X3Pv3r0EKTQqhCgAAIAG5uwM1KpVq9SrV696ueapU6eUl5en8PBw+fj41Ms1s7KydPPNN9frjBtQGwhRAAAADVSvXr00YMCAervesGHD6u1aQGNGYwkAAAAAMIEQBQAAAAAmEKIAAAAAwARCFAAAAACYQIgCAAAAABM8NkTZ7XZFREQoJibG3aUAAAAAaEQ8NkQlJiYqMzNT27dvd3cpAAAAABoRjw1RAAAAAFAThCgAAAAAMIEQBQAAAAAmEKIAAAAAwARCFAAAAACYQIgCAAAAABMIUQAAAABgAiEKAAAAAEwgRAEAAACACYQoAAAAADCBEAUAAAAAJhCiAAAAAMAEQhQAAAAAmECIAgAAAAATCFEAAAAAYAIhCgAAAABMaDIh6uTJk+rUqZMeeughd5cCAAAAoAlrMiHqiSee0GWXXebuMgAAAAA0cU0iRO3bt0/Z2dlKSEhwdykAAAAAmji3h6iUlBSNGzdOoaGhslgsWrduXZUxdrtd4eHhatGihQYPHqxt27a57H/ooYe0aNGieqoYAAAAgCdze4gqKytTVFSU7Hb7OfevXr1aSUlJmjdvnnbu3KmoqCiNGjVKhw8fliS999576tGjh3r06FGfZQMAAADwUM3cXUBCQsIFb8NbsmSJpk2bpqlTp0qSVqxYoQ0bNujVV1/V7Nmz9cUXX+hf//qX3n77bZWWluqnn36Sv7+/5s6de87zlZeXq7y83Pm+uLi4dj8QAAAAgCbN7TNRF1JRUaH09HTFx8c7t3l5eSk+Pl5paWmSpEWLFungwYPKy8vTM888o2nTpp03QJ0dHxAQ4HyFhYXV+ecAAAAA0HQ06BB19OhRORwOBQUFuWwPCgpSQUFBjc45Z84cFRUVOV8HDx6sjVIBAAAAeAi3385Xm2677bbfHWOz2WSz2eq+GAAAAABNUoOeiWrbtq2sVqsKCwtdthcWFio4ONhNVQEAAADwZA06RHl7eys6OlrJycnObZWVlUpOTtaQIUMu6tx2u10RERGKiYm52DIBAAAAeBC3385XWlqqnJwc5/vc3FxlZGSoTZs26tixo5KSkjRlyhQNHDhQgwYN0rJly1RWVubs1ldTiYmJSkxMVHFxsQICAi72YwAAAADwEG4PUTt27FBcXJzzfVJSkiRpypQpWrlypSZNmqQjR45o7ty5KigoUL9+/bRx48YqzSYAAAAAoD64PUSNHDlShmFccMz06dM1ffr0eqoIAAAAAM6vQT8TVZd4JgoAAABATXhsiEpMTFRmZqa2b9/u7lIAAAAANCIeG6IAAAAAoCbc/kwUAAAAqgpuZZHPib1SftP9m7fPib0KbmVxdxmAaR4boux2u+x2uxwOh7tLAQAAqOKuaG/1SrlLSnF3JXWnl37+nEBjYzF+rzVeE3d2naiioiL5+/u7uxwAAADt3LlTY0YM1Kb33lCvnj3dXU6dycrO1h+vvUkb/rNDAwYMcHc5QLWzgcfORAEAADRkBaWGTrXuIYX2c3cpdeZUQaUKSj367/lopJruTbYAAAAAUAcIUQAAAABgAiEKAAAAAEzw2BBlt9sVERGhmJgYd5cCAAAAoBHx2BCVmJiozMxMbd++3d2lAAAAAGhEPDZEAQAAAEBNEKIAAAAAwARCFAAAAACYQIgCAAAAABM8NkTRnQ8AAABATXhsiKI7HwAAAICa8NgQBQAAAAA1QYgCAAAAABMIUQAAAABgAiEKAAAAAEwgRAEAAACACR4bomhxDgAAAKAmPDZE0eIcAAAAQE14bIgCAAAAgJogRAEAAACACYQoAAAAADCBEAUAAAAAJhCiAAAAAMAEQhQAAAAAmNDM3QUAAADA1cmTJyVJO3furLdrnjp1Snl5eQoPD5ePj0+9XDMrK6tergPUNkIUAABAA5OdnS1JmjZtmpsrqR9+fn7uLgEwxWNDlN1ul91ul8PhcHcpAAAALsaPHy9J6tmzp3x9fevlmllZWbr55pu1atUq9erVq16uKf0coLp3715v1wNqg8UwDMPdRbhTcXGxAgICVFRUJH9/f3eXAwAA4BY7d+5UdHS00tPTNWDAAHeXA7hFdbMBjSUAAAAAwARCFAAAAACYQIgCAAAAABMIUQAAAABgAiEKAAAAAEwgRAEAAACACYQoAAAAADCBEAUAAAAAJhCiAAAAAMAEQhQAAAAAmECIAgAAAAATCFEAAAAAYILHhii73a6IiAjFxMS4uxQAAAAAjYjHhqjExERlZmZq+/bt7i4FAAAAQCPisSEKAAAAAGqCEAUAAAAAJhCiAAAAAMAEQhQAAAAAmECIAgAAAAATCFEAAAAAYAIhCgAAAABMIEQBAAAAgAmEKAAAAAAwgRAFAAAAACYQogAAAADABEIUAAAAAJhAiAIAAAAAEwhRAAAAAGACIQoAAAAATCBEAQAAAIAJjT5EnThxQgMHDlS/fv0UGRmpv/3tb+4uCQAAAEAT1szdBVwsPz8/paSkyNfXV2VlZYqMjNSECRN0ySWXuLs0AAAAAE1Qo5+Jslqt8vX1lSSVl5fLMAwZhuHmqgAAAAA0VW4PUSkpKRo3bpxCQ0NlsVi0bt26KmPsdrvCw8PVokULDR48WNu2bXPZf+LECUVFRalDhw6aOXOm2rZtW0/VAwAAAPA0bg9RZWVlioqKkt1uP+f+1atXKykpSfPmzdPOnTsVFRWlUaNG6fDhw84xrVu31tdff63c3Fy98cYbKiwsrK/yAQAAAHgYt4eohIQELViwQNddd9059y9ZskTTpk3T1KlTFRERoRUrVsjX11evvvpqlbFBQUGKiopSamrqea9XXl6u4uJilxcAAAAAVJfbQ9SFVFRUKD09XfHx8c5tXl5eio+PV1pamiSpsLBQJSUlkqSioiKlpKTo0ksvPe85Fy1apICAAOcrLCysbj8EAAAAgCalQYeoo0ePyuFwKCgoyGV7UFCQCgoKJEkHDhxQbGysoqKiFBsbq3vvvVd9+vQ57znnzJmjoqIi5+vgwYN1+hkAAAAANC2NvsX5oEGDlJGRUe3xNptNNput7goCAAAA0KQ16Jmotm3bymq1VmkUUVhYqODg4Is6t91uV0REhGJiYi7qPAAAAAA8S4MOUd7e3oqOjlZycrJzW2VlpZKTkzVkyJCLOndiYqIyMzO1ffv2iy0TAAAAgAdx++18paWlysnJcb7Pzc1VRkaG2rRpo44dOyopKUlTpkzRwIEDNWjQIC1btkxlZWWaOnWqG6sGAAAA4KncHqJ27NihuLg45/ukpCRJ0pQpU7Ry5UpNmjRJR44c0dy5c1VQUKB+/fpp48aNVZpNAAAAAEB9cHuIGjlypAzDuOCY6dOna/r06bV6XbvdLrvdLofDUavnBQAAANC0uT1EuUtiYqISExNVXFysgIAAd5cDAABw0U6ePKns7OwaHZuVleXyr1k9e/aUr69vjY4FGhuPDVEAAABNTXZ2tqKjoy/qHDfffHONjktPT9eAAQMu6tpAY0GIAgAAaCJ69uyp9PT0Gh176tQp5eXlKTw8XD4+PjW6NuApLMbvPZDUxJ29na+oqEj+/v7uLgcAAACAm1Q3GzTodaLqEovtAgAAAKgJZqKYiQIAAAAgZqIAAAAAoE4QogAAAADABEIUAAAAAJjgsSGKxhIAAAAAaoLGEjSWAAAAACAaSwAAAABAnSBEAQAAAIAJhCgAAAAAMIEQBQAAAAAmeGyIojsfAAAAgJqgOx/d+QAAAACI7nwAAAAAUCcIUQAAAABgAiEKAAAAAEwgRAEAAACACc3cXYC7ne2rUVxc7OZKAAAAALjT2Uzwe733PD5ElZSUSJLCwsLcXAkAAACAhqCkpEQBAQHn3e/xLc4rKyuVn58vPz8/WSwWd5cD1Lvi4mKFhYXp4MGDtPkHAA/G9wHw8wxUSUmJQkND5eV1/iefPH4mysvLSx06dHB3GYDb+fv786UJAOD7AB7vQjNQZ9FYAgAAAABMIEQBAAAAgAmEKMDD2Ww2zZs3Tzabzd2lAADciO8DoPo8vrEEAAAAAJjBTBQAAAAAmECIAgAAAAATCFEAAAAAYAIhCmhiLBaL1q1b5+4yAABuxHcBULcIUUAjUlBQoHvvvVddunSRzWZTWFiYxo0bp+TkZHeXJunnVb7nzp2rkJAQ+fj4KD4+Xvv27XN3WQDQpDT074K1a9fqqquu0iWXXCKLxaKMjAx3lwTUOkIU0Ejk5eUpOjpamzZt0tNPP61du3Zp48aNiouLU2JiorvLkyQtXrxYy5cv14oVK/Tll1+qZcuWGjVqlE6fPu3u0gCgSWgM3wVlZWUaPny4nnrqKXeXAtQdA0CjkJCQYLRv394oLS2tsu/48ePOnyUZ7777rvP9rFmzjO7duxs+Pj5G586djUcffdSoqKhw7s/IyDBGjhxptGrVyvDz8zMGDBhgbN++3TAMw8jLyzPGjh1rtG7d2vD19TUiIiKMDRs2nLO+yspKIzg42Hj66aed206cOGHYbDbjzTffvMhPDwAwjIb/XfBrubm5hiTjq6++qvHnBRqqZm7OcACq4ccff9TGjRv1xBNPqGXLllX2t27d+rzH+vn5aeXKlQoNDdWuXbs0bdo0+fn5adasWZKkP/3pT+rfv7/++te/ymq1KiMjQ82bN5ckJSYmqqKiQikpKWrZsqUyMzPVqlWrc14nNzdXBQUFio+Pd24LCAjQ4MGDlZaWpsmTJ1/EbwAA0Bi+CwBPQYgCGoGcnBwZhqGePXuaPvbRRx91/hweHq6HHnpI//rXv5xfnN9//71mzpzpPHf37t2d47///ntdf/316tOnjySpS5cu571OQUGBJCkoKMhle1BQkHMfAKDmGsN3AeApeCYKaAQMw6jxsatXr9awYcMUHBysVq1a6dFHH9X333/v3J+UlKQ777xT8fHxevLJJ7V//37nvvvuu08LFizQsGHDNG/ePH3zzTcX9TkAADXHdwHQcBCigEage/fuslgsys7ONnVcWlqa/vSnP2n06NFav369vvrqKz3yyCOqqKhwjnnsscf07bffasyYMdq0aZMiIiL07rvvSpLuvPNOfffdd7rlllu0a9cuDRw4UM8///w5rxUcHCxJKiwsdNleWFjo3AcAqLnG8F0AeApCFNAItGnTRqNGjZLdbldZWVmV/SdOnDjncVu3blWnTp30yCOPaODAgerevbsOHDhQZVyPHj304IMP6uOPP9aECRP02muvOfeFhYXp7rvv1tq1azVjxgz97W9/O+e1OnfurODgYJcWu8XFxfryyy81ZMgQk58YAPBbjeG7APAUhCigkbDb7XI4HBo0aJDeeecd7du3T1lZWVq+fPl5Q0r37t31/fff61//+pf279+v5cuXO/+yKEmnTp3S9OnTtWXLFh04cECff/65tm/frl69ekmSHnjgAX300UfKzc3Vzp07tXnzZue+37JYLHrggQe0YMECvf/++9q1a5duvfVWhYaGavz48bX++wAAT9TQvwuknxtgZGRkKDMzU5K0Z88eZWRk8Hwsmhb3NgcEYEZ+fr6RmJhodOrUyfD29jbat29vXHPNNcbmzZudY/SbtrYzZ840LrnkEqNVq1bGpEmTjKVLlxoBAQGGYRhGeXm5MXnyZCMsLMzw9vY2QkNDjenTpxunTp0yDMMwpk+fbnTt2tWw2WxGu3btjFtuucU4evToeeurrKw0/vKXvxhBQUGGzWYzrrjiCmPPnj118asAAI/V0L8LXnvtNUNSlde8efPq4LcBuIfFMC7iKUUAAAAA8DDczgcAAAAAJhCiAAAAAMAEQhQAAAAAmECIAgAAAAATCFEAAAAAYAIhCgAAAABMIEQBAAAAgAmEKAAAAAAwgRAFAAAAACYQogAAAADABEIUAAAAAJhAiAIAAAAAE/4/cs3V8rXhPocAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#analyze Raw Dataset (Task001) in RawData, havent preprocess\n",
    "class DatasetAnalyzer:\n",
    "    def __init__(self, dataset_id: int = 1):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.logger = Logger(\n",
    "            log_dir=os.path.join(os.environ[\"nnUNet_results\"], \"logs\"),\n",
    "            name=\"dataset_analyzer\"\n",
    "        ).get_logger(__name__)        \n",
    "        self.dataset_path = os.path.join(os.environ[\"RawData\"], f\"Task001_Lung\")\n",
    "\n",
    "    def load_dataset_json(self) -> dict:\n",
    "        \"\"\"Load and return dataset.json\"\"\"\n",
    "        json_path = os.path.join(self.dataset_path, \"dataset.json\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def get_dataset_stats(self) -> Dict:\n",
    "        \"\"\"Calculate basic dataset statistics\"\"\"\n",
    "        stats = {\n",
    "            \"num_training\": 0,\n",
    "            \"image_sizes\": [],\n",
    "            \"spacings\": []\n",
    "        }\n",
    "\n",
    "        images_path = os.path.join(self.dataset_path, \"imagesTr\")\n",
    "        for file in os.listdir(images_path):\n",
    "            if file.endswith(\"_0000.nii.gz\"):  # First channel\n",
    "                img = nib.load(os.path.join(images_path, file))\n",
    "                stats[\"num_training\"] += 1\n",
    "                stats[\"image_sizes\"].append(img.shape)\n",
    "                stats[\"spacings\"].append(img.header.get_zooms())\n",
    "        print(f\"Number of training images: {stats['num_training']}\")\n",
    "        print(f\"Image sizes: {stats['image_sizes']}\")\n",
    "        print(f\"Spacings: {stats['spacings']}\")\n",
    "        return stats\n",
    "\n",
    "    def plot_size_distribution(self):\n",
    "        \"\"\"Plot distribution of image sizes\"\"\"\n",
    "        stats = self.get_dataset_stats()\n",
    "        sizes = np.array(stats[\"image_sizes\"])\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        for i, dim in enumerate(['X', 'Y', 'Z']):\n",
    "            sns.histplot(sizes[:, i], ax=axes[i])\n",
    "            axes[i].set_title(f'{dim} Dimension Distribution')\n",
    "            axes[i].set_xlabel('Size (voxels)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def analyze_class_distribution(self):\n",
    "        \"\"\"Analyze distribution of segmentation classes\"\"\"\n",
    "        labels_path = os.path.join(self.dataset_path, \"labelsTr\")\n",
    "        class_pixels = {}\n",
    "\n",
    "        for file in os.listdir(labels_path):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                img = nib.load(os.path.join(labels_path, file))\n",
    "                data = img.get_fdata()\n",
    "                unique, counts = np.unique(data, return_counts=True)\n",
    "\n",
    "                for u, c in zip(unique, counts):\n",
    "                    if u not in class_pixels:\n",
    "                        class_pixels[u] = []\n",
    "                    class_pixels[u].append(c)\n",
    "\n",
    "        # Plot class distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        boxplot_data = [np.array(pixels) for pixels in class_pixels.values()]\n",
    "        plt.boxplot(boxplot_data, labels=[f\"Class {int(k)}\" for k in class_pixels.keys()])\n",
    "        plt.title(\"Class Distribution\")\n",
    "        plt.ylabel(\"Number of Voxels\")\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "    def validate_dataset_format(self):\n",
    "        \"\"\"Validate dataset format compliance\"\"\"\n",
    "        required_folders = ['imagesTr', 'labelsTr']\n",
    "        required_files = ['dataset.json']\n",
    "\n",
    "        # Check folders\n",
    "        for folder in required_folders:\n",
    "            folder_path = os.path.join(self.dataset_path, folder)\n",
    "            if not os.path.exists(folder_path):\n",
    "                raise FileNotFoundError(f\"Missing required folder: {folder}\")\n",
    "\n",
    "        # Check files\n",
    "        for file in required_files:\n",
    "            file_path = os.path.join(self.dataset_path, file)\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"Missing required file: {file}\")\n",
    "\n",
    "        print(\"Dataset format validation completed successfully\")\n",
    "\n",
    "    def visualize_sample_cases(self, num_samples: int = 3):\n",
    "        \"\"\"Visualize sample slices from the dataset\"\"\"\n",
    "        images_path = os.path.join(self.dataset_path, \"imagesTr\")\n",
    "        labels_path = os.path.join(self.dataset_path, \"labelsTr\")\n",
    "\n",
    "        # Get list of image files\n",
    "        image_files = [f for f in os.listdir(images_path) if f.endswith('_0000.nii.gz')]\n",
    "        samples = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n",
    "\n",
    "        for image_file in samples:\n",
    "            # Load image and corresponding label\n",
    "            image = nib.load(os.path.join(images_path, image_file)).get_fdata()\n",
    "            label = nib.load(os.path.join(labels_path, image_file.replace('_0000', ''))).get_fdata()\n",
    "\n",
    "            # Get middle slice\n",
    "            slice_idx = image.shape[0] // 2\n",
    "\n",
    "            # Plot\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax1.imshow(image[slice_idx], cmap='gray')\n",
    "            ax1.set_title('Image')\n",
    "            ax2.imshow(image[slice_idx], cmap='gray')\n",
    "            ax2.imshow(label[slice_idx], alpha=0.3, cmap='red')\n",
    "            ax2.set_title('Label Overlay')\n",
    "            plt.suptitle(f'Sample Case: {image_file}')\n",
    "            plt.show()\n",
    "\n",
    "analyzer = DatasetAnalyzer()\n",
    "analyzer.plot_size_distribution()\n",
    "analyzer.analyze_class_distribution()\n",
    "\n",
    "\n",
    "#All images have the same size of 512 voxels (pixels) in both X and Y dimensions\n",
    "#Z dimension shows more variation: ranging from ~100 to 600 voxels\n",
    "\n",
    "\n",
    "#Class 0 (Background):Around 10⁸ voxels (100 million) - Very consistent across images (small box plot range)\n",
    "#Class 1 (Lung lesions/cancer): Around 10³-10⁴ voxels (1,000-10,000) - More variation between images (larger box plot range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset path: /home/doodledaron/FYP/nnUnet/FYP-file/RawData/Task001_Lung\n",
      "Analyzing Training Set Distribution...\n",
      "\n",
      "Training Set Statistics:\n",
      "Total number of cases: 63\n",
      "Cases with cancer: 63 (100.0%)\n",
      "Cases without cancer: 0 (0.0%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "import seaborn as sns\n",
    "\n",
    "class CancerDistributionAnalyzer:\n",
    "    def __init__(self):\n",
    "        if \"RAW_DATA_PATH\" not in os.environ:\n",
    "            raise EnvironmentError(\"RAW_DATA_PATH environment variable not set\")\n",
    "        self.dataset_path = os.path.join(os.environ[\"RAW_DATA_PATH\"], \"Task001_Lung\")\n",
    "        # Validate path exists\n",
    "        if not os.path.exists(self.dataset_path):\n",
    "            raise FileNotFoundError(f\"Dataset path not found: {self.dataset_path}\")\n",
    "        print(f\"Using dataset path: {self.dataset_path}\")\n",
    "        \n",
    "    def analyze_cancer_distribution(self):\n",
    "        \"\"\"Analyze distribution of cases with and without cancer\"\"\"\n",
    "        labels_path = os.path.join(self.dataset_path, \"labelsTr\")\n",
    "        has_cancer = []\n",
    "        cases = []\n",
    "        \n",
    "        print(\"Analyzing Training Set Distribution...\")\n",
    "        for file in os.listdir(labels_path):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                img = nib.load(os.path.join(labels_path, file))\n",
    "                data = img.get_fdata()\n",
    "                # Check if there are any voxels with value 1 (cancer)\n",
    "                contains_cancer = np.any(data == 1)\n",
    "                has_cancer.append(contains_cancer)\n",
    "                cases.append(file)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_cases = len(has_cancer)\n",
    "        cancer_cases = sum(has_cancer)\n",
    "        no_cancer_cases = total_cases - cancer_cases\n",
    "        \n",
    "        print(\"\\nTraining Set Statistics:\")\n",
    "        print(f\"Total number of cases: {total_cases}\")\n",
    "        print(f\"Cases with cancer: {cancer_cases} ({(cancer_cases/total_cases*100):.1f}%)\")\n",
    "        print(f\"Cases without cancer: {no_cancer_cases} ({(no_cancer_cases/total_cases*100):.1f}%)\")\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(['Cancer', 'No Cancer'], [cancer_cases, no_cancer_cases])\n",
    "        plt.title('Distribution of Cancer vs No Cancer Cases')\n",
    "        plt.ylabel('Number of Cases')\n",
    "        for i, v in enumerate([cancer_cases, no_cancer_cases]):\n",
    "            plt.text(i, v + 0.5, str(v), ha='center')\n",
    "        plt.show()\n",
    "        \n",
    "        # Return list of cases with their cancer status for potential splitting\n",
    "        return [(case, has_cancer) for case, has_cancer in zip(cases, has_cancer)]\n",
    "\n",
    "    def analyze_test_set(self):\n",
    "        \"\"\"Analyze distribution in test set if labels are available\"\"\"\n",
    "        test_labels_path = os.path.join(self.dataset_path, \"labelsTs\")\n",
    "        \n",
    "        if not os.path.exists(test_labels_path):\n",
    "            print(\"\\nNo test set labels found.\")\n",
    "            return\n",
    "        \n",
    "        has_cancer = []\n",
    "        cases = []\n",
    "        \n",
    "        print(\"\\nAnalyzing Test Set Distribution...\")\n",
    "        for file in os.listdir(test_labels_path):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                img = nib.load(os.path.join(test_labels_path, file))\n",
    "                data = img.get_fdata()\n",
    "                contains_cancer = np.any(data == 1)\n",
    "                has_cancer.append(contains_cancer)\n",
    "                cases.append(file)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_cases = len(has_cancer)\n",
    "        cancer_cases = sum(has_cancer)\n",
    "        no_cancer_cases = total_cases - cancer_cases\n",
    "        \n",
    "        print(\"\\nTest Set Statistics:\")\n",
    "        print(f\"Total number of cases: {total_cases}\")\n",
    "        print(f\"Cases with cancer: {cancer_cases} ({(cancer_cases/total_cases*100):.1f}%)\")\n",
    "        print(f\"Cases without cancer: {no_cancer_cases} ({(no_cancer_cases/total_cases*100):.1f}%)\")\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(['Cancer', 'No Cancer'], [cancer_cases, no_cancer_cases])\n",
    "        plt.title('Distribution of Cancer vs No Cancer Cases (Test Set)')\n",
    "        plt.ylabel('Number of Cases')\n",
    "        for i, v in enumerate([cancer_cases, no_cancer_cases]):\n",
    "            plt.text(i, v + 0.5, str(v), ha='center')\n",
    "        plt.show()\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = CancerDistributionAnalyzer()\n",
    "\n",
    "# Analyze training set distribution\n",
    "train_cases = analyzer.analyze_cancer_distribution()\n",
    "\n",
    "# If you have test set labels, analyze test set as well\n",
    "# analyzer.analyze_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbvzsDWQHGhQ"
   },
   "source": [
    "# Data Splitting\n",
    "- MSD dataset does not provide ground truth label for its test set so we have to split the dataset from its training set\n",
    "- Split 80% of training data and 20% of testing data (with ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lung_071', 'lung_020', 'lung_004', 'lung_066', 'lung_075', 'lung_081', 'lung_069', 'lung_059', 'lung_006', 'lung_022', 'lung_048', 'lung_037', 'lung_061', 'lung_078', 'lung_057', 'lung_073', 'lung_034', 'lung_029', 'lung_010', 'lung_064', 'lung_058', 'lung_046', 'lung_041', 'lung_025', 'lung_095', 'lung_049', 'lung_023', 'lung_092', 'lung_044', 'lung_009', 'lung_014', 'lung_093', 'lung_033', 'lung_003', 'lung_031', 'lung_054', 'lung_047', 'lung_016', 'lung_079', 'lung_028', 'lung_026', 'lung_045', 'lung_043', 'lung_074', 'lung_015', 'lung_038', 'lung_055', 'lung_086', 'lung_053', 'lung_005', 'lung_083', 'lung_018', 'lung_096', 'lung_084', 'lung_027', 'lung_065', 'lung_070', 'lung_080', 'lung_042', 'lung_001', 'lung_036', 'lung_051', 'lung_062']\n",
      "Split complete:\n",
      "Training cases: 50\n",
      "Test cases: 13\n",
      "\n",
      "Updated dataset.json with new training and test lists\n",
      "\n",
      "Test cases:\n",
      "['lung_001', 'lung_006', 'lung_020', 'lung_026', 'lung_027', 'lung_029', 'lung_031', 'lung_057', 'lung_059', 'lung_069', 'lung_073', 'lung_074', 'lung_086']\n",
      "\n",
      "Training cases:\n",
      "['lung_003', 'lung_004', 'lung_005', 'lung_009', 'lung_010', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_022', 'lung_023', 'lung_025', 'lung_028', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_058', 'lung_061', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_070', 'lung_071', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_092', 'lung_093', 'lung_095', 'lung_096']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "class DatasetSplitter:\n",
    "    def __init__(self):\n",
    "        if \"nnUNet_raw\" not in os.environ:\n",
    "            raise EnvironmentError(\"RAW_DATA_PATH environment variable not set\")\n",
    "        self.dataset_path = os.path.join(os.environ[\"nnUNet_raw\"], \"Dataset002_Lung_split\")\n",
    "        \n",
    "    def create_train_test_split(self, test_size=13, random_seed=42):\n",
    "        \"\"\"Split dataset into training and test sets while maintaining tumor distribution\"\"\"\n",
    "        # Set random seed for reproducibility\n",
    "        random.seed(random_seed)\n",
    "        \n",
    "        # Get all cases\n",
    "        images_path = os.path.join(self.dataset_path, \"imagesTr\")\n",
    "        labels_path = os.path.join(self.dataset_path, \"labelsTr\")\n",
    "        \n",
    "         # Get unique case IDs (removing _0000.nii.gz suffix)\n",
    "        cases = [f.replace('_0000.nii.gz', '') for f in os.listdir(images_path) if f.endswith('_0000.nii.gz')]\n",
    "        print(cases)\n",
    "        # Randomly select test cases\n",
    "        num_test = test_size  # test_size should be 13\n",
    "        test_cases = set(random.sample(cases, num_test))\n",
    "        train_cases = set(cases) - test_cases\n",
    "        \n",
    "        \n",
    "        # Create test directories if they don't exist\n",
    "        os.makedirs(os.path.join(self.dataset_path, \"imagesTs\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.dataset_path, \"labelsTs\"), exist_ok=True)\n",
    "            \n",
    "        # Move test cases\n",
    "        for case in test_cases:\n",
    "            # Move image (with correct _0000 suffix)\n",
    "            src_img = os.path.join(images_path, f\"{case}_0000.nii.gz\")\n",
    "            dst_img = os.path.join(self.dataset_path, \"imagesTs\", f\"{case}_0000.nii.gz\")\n",
    "            shutil.move(src_img, dst_img)\n",
    "            \n",
    "            # Move label\n",
    "            src_lbl = os.path.join(labels_path, f\"{case}.nii.gz\")\n",
    "            dst_lbl = os.path.join(self.dataset_path, \"labelsTs\", f\"{case}.nii.gz\")\n",
    "            shutil.move(src_lbl, dst_lbl)\n",
    "        \n",
    "        # Update dataset.json\n",
    "        json_path = os.path.join(self.dataset_path, \"dataset.json\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            dataset_json = json.load(f)\n",
    "        \n",
    "        # Update counts\n",
    "        dataset_json['numTraining'] = len(train_cases)\n",
    "        dataset_json['numTest'] = len(test_cases)\n",
    "        \n",
    "        # Save updated dataset.json\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(dataset_json, f, indent=4)\n",
    "            \n",
    "        print(f\"Split complete:\")\n",
    "        print(f\"Training cases: {len(train_cases)}\")\n",
    "        print(f\"Test cases: {len(test_cases)}\")\n",
    "        print(\"\\nUpdated dataset.json with new training and test lists\")\n",
    "        \n",
    "        # Print lists of cases for verification\n",
    "        print(\"\\nTest cases:\")\n",
    "        print(sorted(test_cases))\n",
    "        print(\"\\nTraining cases:\")\n",
    "        print(sorted(train_cases))\n",
    "# Create the split\n",
    "splitter = DatasetSplitter()\n",
    "splitter.create_train_test_split()  # 80% train, 20% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbvzsDWQHGhQ"
   },
   "source": [
    "# 3.Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seaWhwbFHNPB"
   },
   "source": [
    "## 3.1 Run nnUnet Preprocessing\n",
    "The Decathlon datasets are 4D nifti files, for nnU-Net they have to be **converted to 3D nifti** files.\n",
    "\n",
    "For more information about dataset conversion see: [nnU-Net Dataset Formatting Instructions](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/dataset_format.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 751644,
     "status": "error",
     "timestamp": 1734498555530,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "P81DBxesHWdR",
    "outputId": "127d9a65-3565-4750-8685-66f2cf35c75b"
   },
   "outputs": [],
   "source": [
    "# dataset id\n",
    "# 1 -> the previous dataset (non splitted)\n",
    "# 2 -> splitted dataset for tumor (MSD Dataset)\n",
    "# 3 -> TCIA lung dataset\n",
    "\n",
    "def run_preprocessing(dataset_id: int = 2):\n",
    "    \"\"\"Run nnU-Net preprocessing\"\"\"\n",
    "    # Initialize logger\n",
    "    logger = Logger(\n",
    "        log_dir=os.path.join(os.environ[\"nnUNet_results\"], \"logs\"),\n",
    "        name=\"preprocessing\"\n",
    "    ).get_logger(__name__)\n",
    "    \n",
    "    logger.info(\"Starting preprocessing...\")\n",
    "\n",
    "    # Print raw data path for verification \n",
    "    raw_path = os.path.join(os.environ[\"nnUNet_raw\"], \"Dataset002_Lung_split\")\n",
    "    logger.info(f\"Raw data path: {raw_path}\")\n",
    "\n",
    "    try:\n",
    "        # Check GPU memory before starting\n",
    "        if torch.cuda.is_available():\n",
    "            available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            free_memory = available_memory - (torch.cuda.memory_allocated(0) / 1e9)\n",
    "            logger.info(f\"Available GPU memory: {free_memory:.2f} GB\")\n",
    "            \n",
    "            if free_memory < 2:  # Less than 2GB free\n",
    "                logger.warning(\"Low GPU memory available\")\n",
    "        #add convert dataset if needed\n",
    "\n",
    "\n",
    "        # Run preprocessing\n",
    "        preprocess_command = f'nnUNetv2_plan_and_preprocess -d {dataset_id} -np 2 --verify_dataset_integrity'\n",
    "        logger.info(f\"Running preprocessing command: {preprocess_command}\")\n",
    "        result = os.system(preprocess_command)\n",
    "        if result != 0:\n",
    "            raise RuntimeError(f\"Preprocessing failed with exit code {result}\")\n",
    "        logger.info(\"Preprocessing completed successfully\")\n",
    "\n",
    "        # Verify files were created\n",
    "        preprocessed_path = os.path.join(\n",
    "            os.environ[\"nnUNet_preprocessed\"],\n",
    "            f\"Dataset{dataset_id:03d}_Lung\"\n",
    "        )\n",
    "        logger.info(f\"Checking preprocessed path: {preprocessed_path}\")\n",
    "\n",
    "        required_files = [\"dataset.json\", \"dataset_fingerprint.json\", \"nnUNetPlans.json\"]\n",
    "        missing_files = []\n",
    "        for file in required_files:\n",
    "            file_path = os.path.join(preprocessed_path, file)\n",
    "            if os.path.exists(file_path):\n",
    "                logger.info(f\"Found required file: {file}\")\n",
    "            else:\n",
    "                logger.warning(f\"Missing required file: {file}\")\n",
    "                missing_files.append(file)\n",
    "\n",
    "        if missing_files:\n",
    "            raise FileNotFoundError(f\"Missing required files: {', '.join(missing_files)}\")\n",
    "\n",
    "        logger.info(\"All required files found\")\n",
    "        logger.info(\"Preprocessing pipeline completed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during preprocessing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Call the function\n",
    "run_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sdHL0ukIZtk"
   },
   "source": [
    "## Verify Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "executionInfo": {
     "elapsed": 523,
     "status": "error",
     "timestamp": 1734446307193,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "zyRpBZ4kIZIi",
    "outputId": "5064a5cc-aaa9-4d8a-e939-d6097242b8fc"
   },
   "outputs": [],
   "source": [
    "def verify_preprocessing(dataset_id: int = 2):\n",
    "    \"\"\"Verify preprocessing results\"\"\"\n",
    "    preprocessed_path = os.path.join(\n",
    "        os.environ[\"nnUNet_preprocessed\"],\n",
    "        f\"Dataset{dataset_id:03d}_Lung_split\"  # This is correct for preprocessed folder\n",
    "    )\n",
    "\n",
    "    # Check existence of essential files\n",
    "    required_files = [\"dataset.json\", \"dataset_fingerprint.json\", \"nnUNetPlans.json\"]\n",
    "    for file in required_files:\n",
    "        path = os.path.join(preprocessed_path, file)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing required file: {path}\")\n",
    "\n",
    "\n",
    "verify_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3pmfSHhTpH9"
   },
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUawRw1mJdXS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyTClqNziw5K",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting standard U-Net training: nnUNetv2_train 2 3d_fullres 0 --npz --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:1184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename_or_checkpoint, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-01-15 10:27:29.289024: Using torch.compile...\n",
      "2025-01-15 10:27:30.288942: do_dummy_2d_data_aug: False\n",
      "2025-01-15 10:27:30.289233: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-15 10:27:30.290102: The split file contains 5 splits.\n",
      "2025-01-15 10:27:30.290138: Desired fold for training: 0\n",
      "2025-01-15 10:27:30.290154: This split has 40 training and 10 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [252.0, 512.0, 512.0], 'spacing': [1.2450199127197266, 0.7919921875, 0.7919921875], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset002_Lung_split', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.2450199127197266, 0.7919921875, 0.7919921875], 'original_median_shape_after_transp': [251, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -272.8529357910156, 'median': -155.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 303.0, 'std': 348.3833923339844}}} \n",
      "\n",
      "2025-01-15 10:27:33.529731: unpacking dataset...\n",
      "2025-01-15 10:27:38.883144: unpacking done...\n",
      "2025-01-15 10:27:38.884349: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-01-15 10:27:39.272213: Training done.\n",
      "2025-01-15 10:27:39.286953: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-15 10:27:39.287269: The split file contains 5 splits.\n",
      "2025-01-15 10:27:39.287353: Desired fold for training: 0\n",
      "2025-01-15 10:27:39.287392: This split has 40 training and 10 validation cases.\n",
      "2025-01-15 10:27:39.287596: predicting lung_018\n",
      "2025-01-15 10:27:39.288807: lung_018, shape torch.Size([1, 274, 556, 556]), rank 0\n",
      "2025-01-15 10:29:25.629131: predicting lung_028\n",
      "2025-01-15 10:29:25.635031: lung_028, shape torch.Size([1, 251, 588, 588]), rank 0\n",
      "2025-01-15 10:31:29.891273: predicting lung_044\n",
      "2025-01-15 10:31:29.896320: lung_044, shape torch.Size([1, 282, 556, 556]), rank 0\n",
      "2025-01-15 10:33:11.540321: predicting lung_045\n",
      "2025-01-15 10:33:11.545157: lung_045, shape torch.Size([1, 252, 556, 556]), rank 0\n",
      "2025-01-15 10:34:38.803338: predicting lung_055\n",
      "2025-01-15 10:34:38.809407: lung_055, shape torch.Size([1, 242, 530, 530]), rank 0\n",
      "2025-01-15 10:36:05.190921: predicting lung_061\n",
      "2025-01-15 10:36:05.193918: lung_061, shape torch.Size([1, 211, 556, 556]), rank 0\n",
      "2025-01-15 10:37:17.031726: predicting lung_062\n",
      "2025-01-15 10:37:17.036271: lung_062, shape torch.Size([1, 243, 530, 530]), rank 0\n",
      "2025-01-15 10:38:43.270751: predicting lung_071\n",
      "2025-01-15 10:38:43.275305: lung_071, shape torch.Size([1, 289, 455, 455]), rank 0\n",
      "2025-01-15 10:39:50.432997: predicting lung_080\n",
      "2025-01-15 10:39:50.435898: lung_080, shape torch.Size([1, 226, 399, 399]), rank 0\n",
      "2025-01-15 10:40:28.820575: predicting lung_084\n",
      "2025-01-15 10:40:28.822755: lung_084, shape torch.Size([1, 241, 505, 505]), rank 0\n",
      "2025-01-15 10:42:18.596923: Validation complete\n",
      "2025-01-15 10:42:18.596987: Mean Validation Dice:  0.6908527931002741\n",
      "Standard U-Net training completed for 3d_fullres fold 0 -epoch 1000\n",
      "Starting standard U-Net training: nnUNetv2_train 2 3d_fullres 1 --npz --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:1184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename_or_checkpoint, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-01-15 10:42:25.645913: Using torch.compile...\n",
      "2025-01-15 10:42:26.535222: do_dummy_2d_data_aug: False\n",
      "2025-01-15 10:42:26.535591: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-15 10:42:26.535694: The split file contains 5 splits.\n",
      "2025-01-15 10:42:26.535712: Desired fold for training: 1\n",
      "2025-01-15 10:42:26.535722: This split has 40 training and 10 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [252.0, 512.0, 512.0], 'spacing': [1.2450199127197266, 0.7919921875, 0.7919921875], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset002_Lung_split', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.2450199127197266, 0.7919921875, 0.7919921875], 'original_median_shape_after_transp': [251, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -272.8529357910156, 'median': -155.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 303.0, 'std': 348.3833923339844}}} \n",
      "\n",
      "2025-01-15 10:42:28.248893: unpacking dataset...\n",
      "2025-01-15 10:42:33.231687: unpacking done...\n",
      "2025-01-15 10:42:33.232553: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-01-15 10:42:33.597198: Training done.\n",
      "2025-01-15 10:42:33.626492: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-15 10:42:33.626820: The split file contains 5 splits.\n",
      "2025-01-15 10:42:33.626922: Desired fold for training: 1\n",
      "2025-01-15 10:42:33.626975: This split has 40 training and 10 validation cases.\n",
      "2025-01-15 10:42:33.627150: predicting lung_005\n",
      "2025-01-15 10:42:33.628806: lung_005, shape torch.Size([1, 264, 423, 423]), rank 0\n",
      "2025-01-15 10:43:42.153038: predicting lung_009\n",
      "2025-01-15 10:43:42.157009: lung_009, shape torch.Size([1, 302, 530, 530]), rank 0\n",
      "2025-01-15 10:45:26.980174: predicting lung_014\n",
      "2025-01-15 10:45:26.986307: lung_014, shape torch.Size([1, 296, 480, 480]), rank 0\n",
      "2025-01-15 10:46:36.816745: predicting lung_042\n",
      "2025-01-15 10:46:36.821794: lung_042, shape torch.Size([1, 251, 473, 473]), rank 0\n",
      "2025-01-15 10:47:36.489851: predicting lung_043\n",
      "2025-01-15 10:47:36.493583: lung_043, shape torch.Size([1, 259, 497, 497]), rank 0\n",
      "2025-01-15 10:49:08.433845: predicting lung_048\n",
      "2025-01-15 10:49:08.436919: lung_048, shape torch.Size([1, 259, 527, 527]), rank 0\n",
      "2025-01-15 10:50:35.565175: predicting lung_053\n",
      "2025-01-15 10:50:35.570012: lung_053, shape torch.Size([1, 252, 606, 606]), rank 0\n",
      "2025-01-15 10:52:36.537252: predicting lung_058\n",
      "2025-01-15 10:52:36.544353: lung_058, shape torch.Size([1, 241, 556, 556]), rank 0\n",
      "2025-01-15 10:54:03.049799: predicting lung_075\n",
      "2025-01-15 10:54:03.054831: lung_075, shape torch.Size([1, 340, 516, 516]), rank 0\n",
      "2025-01-15 10:55:58.127067: predicting lung_081\n",
      "2025-01-15 10:55:58.133584: lung_081, shape torch.Size([1, 249, 569, 569]), rank 0\n",
      "2025-01-15 10:58:03.977135: Validation complete\n",
      "2025-01-15 10:58:03.977208: Mean Validation Dice:  0.7355021548853743\n",
      "Standard U-Net training completed for 3d_fullres fold 1 -epoch 1000\n",
      "Starting standard U-Net training: nnUNetv2_train 2 3d_fullres 2 --npz --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:1184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename_or_checkpoint, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-01-15 10:58:10.378446: Using torch.compile...\n",
      "2025-01-15 10:58:11.240334: do_dummy_2d_data_aug: False\n",
      "2025-01-15 10:58:11.240649: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-15 10:58:11.240740: The split file contains 5 splits.\n",
      "2025-01-15 10:58:11.240757: Desired fold for training: 2\n",
      "2025-01-15 10:58:11.240768: This split has 40 training and 10 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [252.0, 512.0, 512.0], 'spacing': [1.2450199127197266, 0.7919921875, 0.7919921875], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset002_Lung_split', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.2450199127197266, 0.7919921875, 0.7919921875], 'original_median_shape_after_transp': [251, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -272.8529357910156, 'median': -155.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 303.0, 'std': 348.3833923339844}}} \n",
      "\n",
      "2025-01-15 10:58:12.642578: unpacking dataset...\n",
      "2025-01-15 10:58:18.039913: unpacking done...\n",
      "2025-01-15 10:58:18.040780: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-01-15 10:58:18.043912: \n",
      "2025-01-15 10:58:18.044301: Epoch 600\n",
      "2025-01-15 10:58:18.044440: Current learning rate: 0.00438\n",
      "2025-01-15 11:00:56.399807: train_loss -0.7861\n",
      "2025-01-15 11:00:56.399924: val_loss -0.6887\n",
      "2025-01-15 11:00:56.399958: Pseudo dice [np.float32(0.7771)]\n",
      "2025-01-15 11:00:56.399991: Epoch time: 158.36 s\n",
      "2025-01-15 11:00:57.004790: \n",
      "2025-01-15 11:00:57.005080: Epoch 601\n",
      "2025-01-15 11:00:57.005287: Current learning rate: 0.00437\n",
      "2025-01-15 11:02:48.545321: train_loss -0.7941\n",
      "2025-01-15 11:02:48.545467: val_loss -0.6943\n",
      "2025-01-15 11:02:48.545508: Pseudo dice [np.float32(0.7984)]\n",
      "2025-01-15 11:02:48.545543: Epoch time: 111.54 s\n",
      "2025-01-15 11:02:49.157460: \n",
      "2025-01-15 11:02:49.157552: Epoch 602\n",
      "2025-01-15 11:02:49.157616: Current learning rate: 0.00436\n",
      "2025-01-15 11:04:40.794051: train_loss -0.8214\n",
      "2025-01-15 11:04:40.794207: val_loss -0.7586\n",
      "2025-01-15 11:04:40.794246: Pseudo dice [np.float32(0.8368)]\n",
      "2025-01-15 11:04:40.794279: Epoch time: 111.64 s\n",
      "2025-01-15 11:04:41.391341: \n",
      "2025-01-15 11:04:41.391466: Epoch 603\n",
      "2025-01-15 11:04:41.391572: Current learning rate: 0.00435\n",
      "2025-01-15 11:06:33.001780: train_loss -0.8277\n",
      "2025-01-15 11:06:33.001909: val_loss -0.7494\n",
      "2025-01-15 11:06:33.001945: Pseudo dice [np.float32(0.8248)]\n",
      "2025-01-15 11:06:33.001979: Epoch time: 111.61 s\n",
      "2025-01-15 11:06:33.570108: \n",
      "2025-01-15 11:06:33.570286: Epoch 604\n",
      "2025-01-15 11:06:33.570368: Current learning rate: 0.00434\n",
      "2025-01-15 11:08:25.167587: train_loss -0.8322\n",
      "2025-01-15 11:08:25.167757: val_loss -0.7893\n",
      "2025-01-15 11:08:25.167800: Pseudo dice [np.float32(0.8316)]\n",
      "2025-01-15 11:08:25.167838: Epoch time: 111.6 s\n",
      "2025-01-15 11:08:25.738243: \n",
      "2025-01-15 11:08:25.738404: Epoch 605\n",
      "2025-01-15 11:08:25.738475: Current learning rate: 0.00433\n",
      "2025-01-15 11:10:17.222386: train_loss -0.815\n",
      "2025-01-15 11:10:17.222761: val_loss -0.7515\n",
      "2025-01-15 11:10:17.222823: Pseudo dice [np.float32(0.7998)]\n",
      "2025-01-15 11:10:17.222864: Epoch time: 111.48 s\n",
      "2025-01-15 11:10:17.794892: \n",
      "2025-01-15 11:10:17.794975: Epoch 606\n",
      "2025-01-15 11:10:17.795035: Current learning rate: 0.00432\n",
      "2025-01-15 11:12:09.403587: train_loss -0.8214\n",
      "2025-01-15 11:12:09.403723: val_loss -0.6086\n",
      "2025-01-15 11:12:09.403757: Pseudo dice [np.float32(0.6803)]\n",
      "2025-01-15 11:12:09.403790: Epoch time: 111.61 s\n",
      "2025-01-15 11:12:09.973551: \n",
      "2025-01-15 11:12:09.973793: Epoch 607\n",
      "2025-01-15 11:12:09.973867: Current learning rate: 0.00431\n",
      "2025-01-15 11:14:01.564495: train_loss -0.8038\n",
      "2025-01-15 11:14:01.564615: val_loss -0.7184\n",
      "2025-01-15 11:14:01.564648: Pseudo dice [np.float32(0.812)]\n",
      "2025-01-15 11:14:01.564681: Epoch time: 111.59 s\n",
      "2025-01-15 11:14:02.131794: \n",
      "2025-01-15 11:14:02.132099: Epoch 608\n",
      "2025-01-15 11:14:02.132252: Current learning rate: 0.0043\n",
      "2025-01-15 11:15:53.810134: train_loss -0.7893\n",
      "2025-01-15 11:15:53.810357: val_loss -0.7128\n",
      "2025-01-15 11:15:53.810403: Pseudo dice [np.float32(0.818)]\n",
      "2025-01-15 11:15:53.810438: Epoch time: 111.68 s\n",
      "2025-01-15 11:15:54.382670: \n",
      "2025-01-15 11:15:54.382963: Epoch 609\n",
      "2025-01-15 11:15:54.383200: Current learning rate: 0.00429\n",
      "2025-01-15 11:17:46.034608: train_loss -0.7905\n",
      "2025-01-15 11:17:46.034748: val_loss -0.7433\n",
      "2025-01-15 11:17:46.034780: Pseudo dice [np.float32(0.8435)]\n",
      "2025-01-15 11:17:46.034812: Epoch time: 111.65 s\n",
      "2025-01-15 11:17:46.610573: \n",
      "2025-01-15 11:17:46.610664: Epoch 610\n",
      "2025-01-15 11:17:46.610726: Current learning rate: 0.00429\n",
      "2025-01-15 11:19:38.299655: train_loss -0.8126\n",
      "2025-01-15 11:19:38.299776: val_loss -0.6796\n",
      "2025-01-15 11:19:38.299810: Pseudo dice [np.float32(0.7863)]\n",
      "2025-01-15 11:19:38.299843: Epoch time: 111.69 s\n",
      "2025-01-15 11:19:38.874906: \n",
      "2025-01-15 11:19:38.875044: Epoch 611\n",
      "2025-01-15 11:19:38.875106: Current learning rate: 0.00428\n",
      "2025-01-15 11:21:30.549404: train_loss -0.8351\n",
      "2025-01-15 11:21:30.549561: val_loss -0.7815\n",
      "2025-01-15 11:21:30.549635: Pseudo dice [np.float32(0.8527)]\n",
      "2025-01-15 11:21:30.549679: Epoch time: 111.68 s\n",
      "2025-01-15 11:21:31.127084: \n",
      "2025-01-15 11:21:31.127165: Epoch 612\n",
      "2025-01-15 11:21:31.127223: Current learning rate: 0.00427\n",
      "2025-01-15 11:23:22.783040: train_loss -0.8225\n",
      "2025-01-15 11:23:22.783164: val_loss -0.6953\n",
      "2025-01-15 11:23:22.783198: Pseudo dice [np.float32(0.802)]\n",
      "2025-01-15 11:23:22.783231: Epoch time: 111.66 s\n",
      "2025-01-15 11:23:23.367719: \n",
      "2025-01-15 11:23:23.367862: Epoch 613\n",
      "2025-01-15 11:23:23.367924: Current learning rate: 0.00426\n",
      "2025-01-15 11:25:14.998264: train_loss -0.8231\n",
      "2025-01-15 11:25:14.998384: val_loss -0.7138\n",
      "2025-01-15 11:25:14.998417: Pseudo dice [np.float32(0.8141)]\n",
      "2025-01-15 11:25:14.998449: Epoch time: 111.63 s\n",
      "2025-01-15 11:25:15.919516: \n",
      "2025-01-15 11:25:15.919723: Epoch 614\n",
      "2025-01-15 11:25:15.919841: Current learning rate: 0.00425\n",
      "2025-01-15 11:27:07.535175: train_loss -0.7925\n",
      "2025-01-15 11:27:07.535344: val_loss -0.7507\n",
      "2025-01-15 11:27:07.535417: Pseudo dice [np.float32(0.825)]\n",
      "2025-01-15 11:27:07.535459: Epoch time: 111.62 s\n",
      "2025-01-15 11:27:08.118827: \n",
      "2025-01-15 11:27:08.119009: Epoch 615\n",
      "2025-01-15 11:27:08.119160: Current learning rate: 0.00424\n",
      "2025-01-15 11:28:59.560333: train_loss -0.8174\n",
      "2025-01-15 11:28:59.560462: val_loss -0.7395\n",
      "2025-01-15 11:28:59.560495: Pseudo dice [np.float32(0.8221)]\n",
      "2025-01-15 11:28:59.560527: Epoch time: 111.44 s\n",
      "2025-01-15 11:29:00.139855: \n",
      "2025-01-15 11:29:00.140264: Epoch 616\n",
      "2025-01-15 11:29:00.140373: Current learning rate: 0.00423\n",
      "2025-01-15 11:30:51.575699: train_loss -0.7711\n",
      "2025-01-15 11:30:51.575922: val_loss -0.7249\n",
      "2025-01-15 11:30:51.575969: Pseudo dice [np.float32(0.824)]\n",
      "2025-01-15 11:30:51.576010: Epoch time: 111.44 s\n",
      "2025-01-15 11:30:51.576035: Yayy! New best EMA pseudo Dice: 0.8105000257492065\n",
      "2025-01-15 11:30:52.370033: \n",
      "2025-01-15 11:30:52.370354: Epoch 617\n",
      "2025-01-15 11:30:52.370537: Current learning rate: 0.00422\n",
      "2025-01-15 11:32:43.827091: train_loss -0.8023\n",
      "2025-01-15 11:32:43.827216: val_loss -0.756\n",
      "2025-01-15 11:32:43.827250: Pseudo dice [np.float32(0.832)]\n",
      "2025-01-15 11:32:43.827284: Epoch time: 111.46 s\n",
      "2025-01-15 11:32:43.827304: Yayy! New best EMA pseudo Dice: 0.8126000165939331\n",
      "2025-01-15 11:32:44.617554: \n",
      "2025-01-15 11:32:44.617768: Epoch 618\n",
      "2025-01-15 11:32:44.617897: Current learning rate: 0.00421\n",
      "2025-01-15 11:34:38.610703: train_loss -0.8244\n",
      "2025-01-15 11:34:38.611106: val_loss -0.7333\n",
      "2025-01-15 11:34:38.611151: Pseudo dice [np.float32(0.8096)]\n",
      "2025-01-15 11:34:38.611199: Epoch time: 113.99 s\n",
      "2025-01-15 11:34:39.202219: \n",
      "2025-01-15 11:34:39.202363: Epoch 619\n",
      "2025-01-15 11:34:39.202443: Current learning rate: 0.0042\n",
      "2025-01-15 11:36:31.663232: train_loss -0.8086\n",
      "2025-01-15 11:36:31.663360: val_loss -0.6662\n",
      "2025-01-15 11:36:31.663393: Pseudo dice [np.float32(0.6379)]\n",
      "2025-01-15 11:36:31.663426: Epoch time: 112.46 s\n",
      "2025-01-15 11:36:32.254075: \n",
      "2025-01-15 11:36:32.254168: Epoch 620\n",
      "2025-01-15 11:36:32.254248: Current learning rate: 0.00419\n",
      "2025-01-15 11:38:24.263179: train_loss -0.758\n",
      "2025-01-15 11:38:24.263587: val_loss -0.7851\n",
      "2025-01-15 11:38:24.263632: Pseudo dice [np.float32(0.8291)]\n",
      "2025-01-15 11:38:24.263668: Epoch time: 112.01 s\n",
      "2025-01-15 11:38:24.850436: \n",
      "2025-01-15 11:38:24.850654: Epoch 621\n",
      "2025-01-15 11:38:24.850733: Current learning rate: 0.00418\n",
      "2025-01-15 11:40:16.915601: train_loss -0.8255\n",
      "2025-01-15 11:40:16.915727: val_loss -0.6318\n",
      "2025-01-15 11:40:16.915761: Pseudo dice [np.float32(0.736)]\n",
      "2025-01-15 11:40:16.915793: Epoch time: 112.07 s\n",
      "2025-01-15 11:40:17.497134: \n",
      "2025-01-15 11:40:17.497219: Epoch 622\n",
      "2025-01-15 11:40:17.497281: Current learning rate: 0.00417\n",
      "2025-01-15 11:42:09.643481: train_loss -0.8176\n",
      "2025-01-15 11:42:09.643628: val_loss -0.6508\n",
      "2025-01-15 11:42:09.643671: Pseudo dice [np.float32(0.6385)]\n",
      "2025-01-15 11:42:09.643706: Epoch time: 112.15 s\n",
      "2025-01-15 11:42:10.224558: \n",
      "2025-01-15 11:42:10.224693: Epoch 623\n",
      "2025-01-15 11:42:10.224763: Current learning rate: 0.00416\n",
      "2025-01-15 11:44:02.224986: train_loss -0.8268\n",
      "2025-01-15 11:44:02.225168: val_loss -0.7711\n",
      "2025-01-15 11:44:02.225216: Pseudo dice [np.float32(0.8222)]\n",
      "2025-01-15 11:44:02.225255: Epoch time: 112.0 s\n",
      "2025-01-15 11:44:02.806219: \n",
      "2025-01-15 11:44:02.806302: Epoch 624\n",
      "2025-01-15 11:44:02.806408: Current learning rate: 0.00415\n",
      "2025-01-15 11:45:55.034574: train_loss -0.8162\n",
      "2025-01-15 11:45:55.034694: val_loss -0.7277\n",
      "2025-01-15 11:45:55.034728: Pseudo dice [np.float32(0.8338)]\n",
      "2025-01-15 11:45:55.034768: Epoch time: 112.23 s\n",
      "2025-01-15 11:45:55.622833: \n",
      "2025-01-15 11:45:55.623013: Epoch 625\n",
      "2025-01-15 11:45:55.623141: Current learning rate: 0.00414\n",
      "2025-01-15 11:47:47.879510: train_loss -0.7864\n",
      "2025-01-15 11:47:47.879635: val_loss -0.7073\n",
      "2025-01-15 11:47:47.879668: Pseudo dice [np.float32(0.7876)]\n",
      "2025-01-15 11:47:47.879700: Epoch time: 112.26 s\n",
      "2025-01-15 11:47:48.465282: \n",
      "2025-01-15 11:47:48.465689: Epoch 626\n",
      "2025-01-15 11:47:48.465810: Current learning rate: 0.00413\n",
      "2025-01-15 11:49:40.728015: train_loss -0.7838\n",
      "2025-01-15 11:49:40.728358: val_loss -0.678\n",
      "2025-01-15 11:49:40.728438: Pseudo dice [np.float32(0.778)]\n",
      "2025-01-15 11:49:40.728482: Epoch time: 112.26 s\n",
      "2025-01-15 11:49:41.316189: \n",
      "2025-01-15 11:49:41.316315: Epoch 627\n",
      "2025-01-15 11:49:41.316383: Current learning rate: 0.00412\n",
      "2025-01-15 11:51:33.528200: train_loss -0.826\n",
      "2025-01-15 11:51:33.528365: val_loss -0.6723\n",
      "2025-01-15 11:51:33.528441: Pseudo dice [np.float32(0.7341)]\n",
      "2025-01-15 11:51:33.528485: Epoch time: 112.21 s\n",
      "2025-01-15 11:51:34.115774: \n",
      "2025-01-15 11:51:34.115899: Epoch 628\n",
      "2025-01-15 11:51:34.116110: Current learning rate: 0.00411\n",
      "2025-01-15 11:53:26.378763: train_loss -0.8289\n",
      "2025-01-15 11:53:26.378952: val_loss -0.7877\n",
      "2025-01-15 11:53:26.379011: Pseudo dice [np.float32(0.842)]\n",
      "2025-01-15 11:53:26.379058: Epoch time: 112.26 s\n",
      "2025-01-15 11:53:26.962073: \n",
      "2025-01-15 11:53:26.962229: Epoch 629\n",
      "2025-01-15 11:53:26.962304: Current learning rate: 0.0041\n",
      "2025-01-15 11:55:19.180202: train_loss -0.8365\n",
      "2025-01-15 11:55:19.180329: val_loss -0.7888\n",
      "2025-01-15 11:55:19.180359: Pseudo dice [np.float32(0.8349)]\n",
      "2025-01-15 11:55:19.180389: Epoch time: 112.22 s\n",
      "2025-01-15 11:55:19.768212: \n",
      "2025-01-15 11:55:19.768286: Epoch 630\n",
      "2025-01-15 11:55:19.768343: Current learning rate: 0.00409\n",
      "2025-01-15 11:57:12.115118: train_loss -0.78\n",
      "2025-01-15 11:57:12.115292: val_loss -0.6537\n",
      "2025-01-15 11:57:12.115324: Pseudo dice [np.float32(0.738)]\n",
      "2025-01-15 11:57:12.115357: Epoch time: 112.35 s\n",
      "2025-01-15 11:57:13.078114: \n",
      "2025-01-15 11:57:13.078274: Epoch 631\n",
      "2025-01-15 11:57:13.078357: Current learning rate: 0.00408\n",
      "2025-01-15 11:59:05.305746: train_loss -0.79\n",
      "2025-01-15 11:59:05.305871: val_loss -0.6938\n",
      "2025-01-15 11:59:05.305903: Pseudo dice [np.float32(0.8181)]\n",
      "2025-01-15 11:59:05.305935: Epoch time: 112.23 s\n",
      "2025-01-15 11:59:05.893876: \n",
      "2025-01-15 11:59:05.894246: Epoch 632\n",
      "2025-01-15 11:59:05.894322: Current learning rate: 0.00407\n",
      "2025-01-15 12:00:58.041212: train_loss -0.8081\n",
      "2025-01-15 12:00:58.041351: val_loss -0.7513\n",
      "2025-01-15 12:00:58.041384: Pseudo dice [np.float32(0.8217)]\n",
      "2025-01-15 12:00:58.041543: Epoch time: 112.15 s\n",
      "2025-01-15 12:00:58.632884: \n",
      "2025-01-15 12:00:58.633244: Epoch 633\n",
      "2025-01-15 12:00:58.633340: Current learning rate: 0.00406\n",
      "2025-01-15 12:02:50.653079: train_loss -0.8138\n",
      "2025-01-15 12:02:50.653212: val_loss -0.7205\n",
      "2025-01-15 12:02:50.653271: Pseudo dice [np.float32(0.8148)]\n",
      "2025-01-15 12:02:50.653340: Epoch time: 112.02 s\n",
      "2025-01-15 12:02:51.267037: \n",
      "2025-01-15 12:02:51.267201: Epoch 634\n",
      "2025-01-15 12:02:51.267272: Current learning rate: 0.00405\n",
      "2025-01-15 12:04:43.510058: train_loss -0.8172\n",
      "2025-01-15 12:04:43.510192: val_loss -0.6785\n",
      "2025-01-15 12:04:43.510228: Pseudo dice [np.float32(0.7548)]\n",
      "2025-01-15 12:04:43.510261: Epoch time: 112.24 s\n",
      "2025-01-15 12:04:44.127388: \n",
      "2025-01-15 12:04:44.127500: Epoch 635\n",
      "2025-01-15 12:04:44.127568: Current learning rate: 0.00404\n",
      "2025-01-15 12:06:36.374671: train_loss -0.8179\n",
      "2025-01-15 12:06:36.374788: val_loss -0.753\n",
      "2025-01-15 12:06:36.374821: Pseudo dice [np.float32(0.8301)]\n",
      "2025-01-15 12:06:36.374853: Epoch time: 112.25 s\n",
      "2025-01-15 12:06:36.964968: \n",
      "2025-01-15 12:06:36.965075: Epoch 636\n",
      "2025-01-15 12:06:36.965141: Current learning rate: 0.00403\n",
      "2025-01-15 12:08:29.089737: train_loss -0.8143\n",
      "2025-01-15 12:08:29.089872: val_loss -0.7295\n",
      "2025-01-15 12:08:29.089907: Pseudo dice [np.float32(0.8112)]\n",
      "2025-01-15 12:08:29.089940: Epoch time: 112.13 s\n",
      "2025-01-15 12:08:29.679731: \n",
      "2025-01-15 12:08:29.679826: Epoch 637\n",
      "2025-01-15 12:08:29.679888: Current learning rate: 0.00402\n",
      "2025-01-15 12:10:21.877757: train_loss -0.8063\n",
      "2025-01-15 12:10:21.878162: val_loss -0.7189\n",
      "2025-01-15 12:10:21.878251: Pseudo dice [np.float32(0.8054)]\n",
      "2025-01-15 12:10:21.878320: Epoch time: 112.2 s\n",
      "2025-01-15 12:10:22.461328: \n",
      "2025-01-15 12:10:22.461420: Epoch 638\n",
      "2025-01-15 12:10:22.461488: Current learning rate: 0.00401\n",
      "2025-01-15 12:12:14.672156: train_loss -0.8093\n",
      "2025-01-15 12:12:14.672282: val_loss -0.8269\n",
      "2025-01-15 12:12:14.672316: Pseudo dice [np.float32(0.8624)]\n",
      "2025-01-15 12:12:14.672349: Epoch time: 112.21 s\n",
      "2025-01-15 12:12:15.253655: \n",
      "2025-01-15 12:12:15.253848: Epoch 639\n",
      "2025-01-15 12:12:15.253947: Current learning rate: 0.004\n",
      "2025-01-15 12:14:07.453675: train_loss -0.8127\n",
      "2025-01-15 12:14:07.454037: val_loss -0.7364\n",
      "2025-01-15 12:14:07.454082: Pseudo dice [np.float32(0.8245)]\n",
      "2025-01-15 12:14:07.454117: Epoch time: 112.2 s\n",
      "2025-01-15 12:14:08.042589: \n",
      "2025-01-15 12:14:08.042945: Epoch 640\n",
      "2025-01-15 12:14:08.043152: Current learning rate: 0.00399\n",
      "2025-01-15 12:16:00.293272: train_loss -0.8213\n",
      "2025-01-15 12:16:00.293398: val_loss -0.6496\n",
      "2025-01-15 12:16:00.293433: Pseudo dice [np.float32(0.764)]\n",
      "2025-01-15 12:16:00.293465: Epoch time: 112.25 s\n",
      "2025-01-15 12:16:00.883522: \n",
      "2025-01-15 12:16:00.883850: Epoch 641\n",
      "2025-01-15 12:16:00.884074: Current learning rate: 0.00398\n",
      "2025-01-15 12:17:53.571998: train_loss -0.8067\n",
      "2025-01-15 12:17:53.572227: val_loss -0.7391\n",
      "2025-01-15 12:17:53.572272: Pseudo dice [np.float32(0.7876)]\n",
      "2025-01-15 12:17:53.572306: Epoch time: 112.69 s\n",
      "2025-01-15 12:17:54.161091: \n",
      "2025-01-15 12:17:54.161268: Epoch 642\n",
      "2025-01-15 12:17:54.161452: Current learning rate: 0.00397\n",
      "2025-01-15 12:19:46.463488: train_loss -0.8302\n",
      "2025-01-15 12:19:46.463684: val_loss -0.7237\n",
      "2025-01-15 12:19:46.463840: Pseudo dice [np.float32(0.8309)]\n",
      "2025-01-15 12:19:46.463888: Epoch time: 112.3 s\n",
      "2025-01-15 12:19:47.055898: \n",
      "2025-01-15 12:19:47.056032: Epoch 643\n",
      "2025-01-15 12:19:47.056095: Current learning rate: 0.00396\n",
      "2025-01-15 12:21:39.315352: train_loss -0.8299\n",
      "2025-01-15 12:21:39.315817: val_loss -0.7068\n",
      "2025-01-15 12:21:39.315942: Pseudo dice [np.float32(0.7873)]\n",
      "2025-01-15 12:21:39.316008: Epoch time: 112.26 s\n",
      "2025-01-15 12:21:39.908548: \n",
      "2025-01-15 12:21:39.908874: Epoch 644\n",
      "2025-01-15 12:21:39.909009: Current learning rate: 0.00395\n",
      "2025-01-15 12:23:32.208279: train_loss -0.8365\n",
      "2025-01-15 12:23:32.208400: val_loss -0.6599\n",
      "2025-01-15 12:23:32.208433: Pseudo dice [np.float32(0.7568)]\n",
      "2025-01-15 12:23:32.208464: Epoch time: 112.3 s\n",
      "2025-01-15 12:23:32.798926: \n",
      "2025-01-15 12:23:32.799095: Epoch 645\n",
      "2025-01-15 12:23:32.799168: Current learning rate: 0.00394\n",
      "2025-01-15 12:25:25.082790: train_loss -0.8076\n",
      "2025-01-15 12:25:25.082917: val_loss -0.7451\n",
      "2025-01-15 12:25:25.082951: Pseudo dice [np.float32(0.7824)]\n",
      "2025-01-15 12:25:25.082984: Epoch time: 112.28 s\n",
      "2025-01-15 12:25:25.670000: \n",
      "2025-01-15 12:25:25.670087: Epoch 646\n",
      "2025-01-15 12:25:25.670149: Current learning rate: 0.00393\n",
      "2025-01-15 12:27:17.984014: train_loss -0.8182\n",
      "2025-01-15 12:27:17.984141: val_loss -0.7193\n",
      "2025-01-15 12:27:17.984194: Pseudo dice [np.float32(0.7988)]\n",
      "2025-01-15 12:27:17.984236: Epoch time: 112.31 s\n",
      "2025-01-15 12:27:18.561745: \n",
      "2025-01-15 12:27:18.561837: Epoch 647\n",
      "2025-01-15 12:27:18.561902: Current learning rate: 0.00392\n",
      "2025-01-15 12:29:10.876493: train_loss -0.8394\n",
      "2025-01-15 12:29:10.876621: val_loss -0.6392\n",
      "2025-01-15 12:29:10.876655: Pseudo dice [np.float32(0.7768)]\n",
      "2025-01-15 12:29:10.876687: Epoch time: 112.32 s\n",
      "2025-01-15 12:29:11.467788: \n",
      "2025-01-15 12:29:11.467872: Epoch 648\n",
      "2025-01-15 12:29:11.467933: Current learning rate: 0.00391\n",
      "2025-01-15 12:31:03.663904: train_loss -0.8047\n",
      "2025-01-15 12:31:03.664035: val_loss -0.7079\n",
      "2025-01-15 12:31:03.664067: Pseudo dice [np.float32(0.792)]\n",
      "2025-01-15 12:31:03.664099: Epoch time: 112.2 s\n",
      "2025-01-15 12:31:04.643084: \n",
      "2025-01-15 12:31:04.643477: Epoch 649\n",
      "2025-01-15 12:31:04.643730: Current learning rate: 0.0039\n",
      "2025-01-15 12:32:56.840155: train_loss -0.8269\n",
      "2025-01-15 12:32:56.840491: val_loss -0.7383\n",
      "2025-01-15 12:32:56.840555: Pseudo dice [np.float32(0.8206)]\n",
      "2025-01-15 12:32:56.840596: Epoch time: 112.2 s\n",
      "2025-01-15 12:32:57.641377: \n",
      "2025-01-15 12:32:57.641531: Epoch 650\n",
      "2025-01-15 12:32:57.641603: Current learning rate: 0.00389\n",
      "2025-01-15 12:34:49.733914: train_loss -0.812\n",
      "2025-01-15 12:34:49.734047: val_loss -0.689\n",
      "2025-01-15 12:34:49.734081: Pseudo dice [np.float32(0.7702)]\n",
      "2025-01-15 12:34:49.734113: Epoch time: 112.09 s\n",
      "2025-01-15 12:34:50.320361: \n",
      "2025-01-15 12:34:50.320693: Epoch 651\n",
      "2025-01-15 12:34:50.320842: Current learning rate: 0.00388\n",
      "2025-01-15 12:36:42.502652: train_loss -0.824\n",
      "2025-01-15 12:36:42.502784: val_loss -0.7306\n",
      "2025-01-15 12:36:42.502815: Pseudo dice [np.float32(0.7987)]\n",
      "2025-01-15 12:36:42.502849: Epoch time: 112.18 s\n",
      "2025-01-15 12:36:43.093243: \n",
      "2025-01-15 12:36:43.093345: Epoch 652\n",
      "2025-01-15 12:36:43.093408: Current learning rate: 0.00387\n",
      "2025-01-15 12:38:35.292586: train_loss -0.818\n",
      "2025-01-15 12:38:35.292713: val_loss -0.7756\n",
      "2025-01-15 12:38:35.292748: Pseudo dice [np.float32(0.8387)]\n",
      "2025-01-15 12:38:35.292782: Epoch time: 112.2 s\n",
      "2025-01-15 12:38:35.883220: \n",
      "2025-01-15 12:38:35.883393: Epoch 653\n",
      "2025-01-15 12:38:35.883465: Current learning rate: 0.00386\n",
      "2025-01-15 12:40:28.131773: train_loss -0.8193\n",
      "2025-01-15 12:40:28.132193: val_loss -0.6863\n",
      "2025-01-15 12:40:28.132236: Pseudo dice [np.float32(0.7921)]\n",
      "2025-01-15 12:40:28.132271: Epoch time: 112.25 s\n",
      "2025-01-15 12:40:28.715008: \n",
      "2025-01-15 12:40:28.715170: Epoch 654\n",
      "2025-01-15 12:40:28.715235: Current learning rate: 0.00385\n",
      "2025-01-15 12:42:20.939182: train_loss -0.8247\n",
      "2025-01-15 12:42:20.939392: val_loss -0.802\n",
      "2025-01-15 12:42:20.939430: Pseudo dice [np.float32(0.8304)]\n",
      "2025-01-15 12:42:20.939465: Epoch time: 112.22 s\n",
      "2025-01-15 12:42:21.537651: \n",
      "2025-01-15 12:42:21.538013: Epoch 655\n",
      "2025-01-15 12:42:21.538148: Current learning rate: 0.00384\n",
      "2025-01-15 12:44:13.755142: train_loss -0.8012\n",
      "2025-01-15 12:44:13.755270: val_loss -0.7019\n",
      "2025-01-15 12:44:13.755303: Pseudo dice [np.float32(0.8027)]\n",
      "2025-01-15 12:44:13.755335: Epoch time: 112.22 s\n",
      "2025-01-15 12:44:14.351736: \n",
      "2025-01-15 12:44:14.352028: Epoch 656\n",
      "2025-01-15 12:44:14.352185: Current learning rate: 0.00383\n",
      "2025-01-15 12:46:06.680456: train_loss -0.8261\n",
      "2025-01-15 12:46:06.680649: val_loss -0.7605\n",
      "2025-01-15 12:46:06.680694: Pseudo dice [np.float32(0.8399)]\n",
      "2025-01-15 12:46:06.680730: Epoch time: 112.33 s\n",
      "2025-01-15 12:46:07.269366: \n",
      "2025-01-15 12:46:07.269704: Epoch 657\n",
      "2025-01-15 12:46:07.269772: Current learning rate: 0.00382\n",
      "2025-01-15 12:47:59.528155: train_loss -0.8318\n",
      "2025-01-15 12:47:59.528287: val_loss -0.7247\n",
      "2025-01-15 12:47:59.528324: Pseudo dice [np.float32(0.7977)]\n",
      "2025-01-15 12:47:59.528357: Epoch time: 112.26 s\n",
      "2025-01-15 12:48:00.109312: \n",
      "2025-01-15 12:48:00.109397: Epoch 658\n",
      "2025-01-15 12:48:00.109457: Current learning rate: 0.00381\n",
      "2025-01-15 12:49:52.298026: train_loss -0.8453\n",
      "2025-01-15 12:49:52.298162: val_loss -0.7757\n",
      "2025-01-15 12:49:52.298194: Pseudo dice [np.float32(0.8231)]\n",
      "2025-01-15 12:49:52.298228: Epoch time: 112.19 s\n",
      "2025-01-15 12:49:52.892939: \n",
      "2025-01-15 12:49:52.893400: Epoch 659\n",
      "2025-01-15 12:49:52.893552: Current learning rate: 0.0038\n",
      "2025-01-15 12:51:45.115336: train_loss -0.8513\n",
      "2025-01-15 12:51:45.115466: val_loss -0.7337\n",
      "2025-01-15 12:51:45.115500: Pseudo dice [np.float32(0.8186)]\n",
      "2025-01-15 12:51:45.115534: Epoch time: 112.22 s\n",
      "2025-01-15 12:51:45.704452: \n",
      "2025-01-15 12:51:45.704891: Epoch 660\n",
      "2025-01-15 12:51:45.704999: Current learning rate: 0.00379\n",
      "2025-01-15 12:53:38.047112: train_loss -0.8453\n",
      "2025-01-15 12:53:38.047461: val_loss -0.6952\n",
      "2025-01-15 12:53:38.047578: Pseudo dice [np.float32(0.8029)]\n",
      "2025-01-15 12:53:38.047620: Epoch time: 112.34 s\n",
      "2025-01-15 12:53:38.641306: \n",
      "2025-01-15 12:53:38.641711: Epoch 661\n",
      "2025-01-15 12:53:38.641799: Current learning rate: 0.00378\n",
      "2025-01-15 12:55:30.867984: train_loss -0.813\n",
      "2025-01-15 12:55:30.868116: val_loss -0.6595\n",
      "2025-01-15 12:55:30.868150: Pseudo dice [np.float32(0.7702)]\n",
      "2025-01-15 12:55:30.868183: Epoch time: 112.23 s\n",
      "2025-01-15 12:55:31.452599: \n",
      "2025-01-15 12:55:31.452866: Epoch 662\n",
      "2025-01-15 12:55:31.453070: Current learning rate: 0.00377\n",
      "2025-01-15 12:57:23.679048: train_loss -0.8109\n",
      "2025-01-15 12:57:23.679246: val_loss -0.7448\n",
      "2025-01-15 12:57:23.679289: Pseudo dice [np.float32(0.8265)]\n",
      "2025-01-15 12:57:23.679331: Epoch time: 112.23 s\n",
      "2025-01-15 12:57:24.268610: \n",
      "2025-01-15 12:57:24.269015: Epoch 663\n",
      "2025-01-15 12:57:24.269112: Current learning rate: 0.00376\n",
      "2025-01-15 12:59:16.552572: train_loss -0.8138\n",
      "2025-01-15 12:59:16.552786: val_loss -0.7168\n",
      "2025-01-15 12:59:16.552820: Pseudo dice [np.float32(0.7896)]\n",
      "2025-01-15 12:59:16.552852: Epoch time: 112.28 s\n",
      "2025-01-15 12:59:17.135518: \n",
      "2025-01-15 12:59:17.135925: Epoch 664\n",
      "2025-01-15 12:59:17.136069: Current learning rate: 0.00375\n",
      "2025-01-15 13:01:09.378813: train_loss -0.8291\n",
      "2025-01-15 13:01:09.378950: val_loss -0.706\n",
      "2025-01-15 13:01:09.378984: Pseudo dice [np.float32(0.7958)]\n",
      "2025-01-15 13:01:09.379017: Epoch time: 112.24 s\n",
      "2025-01-15 13:01:09.966798: \n",
      "2025-01-15 13:01:09.967156: Epoch 665\n",
      "2025-01-15 13:01:09.967290: Current learning rate: 0.00374\n",
      "2025-01-15 13:03:02.234067: train_loss -0.8238\n",
      "2025-01-15 13:03:02.234226: val_loss -0.7413\n",
      "2025-01-15 13:03:02.234261: Pseudo dice [np.float32(0.8124)]\n",
      "2025-01-15 13:03:02.234295: Epoch time: 112.27 s\n",
      "2025-01-15 13:03:02.856651: \n",
      "2025-01-15 13:03:02.856752: Epoch 666\n",
      "2025-01-15 13:03:02.856819: Current learning rate: 0.00373\n",
      "2025-01-15 13:04:55.068208: train_loss -0.8193\n",
      "2025-01-15 13:04:55.068351: val_loss -0.6064\n",
      "2025-01-15 13:04:55.068387: Pseudo dice [np.float32(0.7144)]\n",
      "2025-01-15 13:04:55.068421: Epoch time: 112.21 s\n",
      "2025-01-15 13:04:56.119777: \n",
      "2025-01-15 13:04:56.120151: Epoch 667\n",
      "2025-01-15 13:04:56.120228: Current learning rate: 0.00372\n",
      "2025-01-15 13:06:48.271323: train_loss -0.8081\n",
      "2025-01-15 13:06:48.271786: val_loss -0.7226\n",
      "2025-01-15 13:06:48.271852: Pseudo dice [np.float32(0.7956)]\n",
      "2025-01-15 13:06:48.271891: Epoch time: 112.15 s\n",
      "2025-01-15 13:06:48.884606: \n",
      "2025-01-15 13:06:48.884714: Epoch 668\n",
      "2025-01-15 13:06:48.884778: Current learning rate: 0.00371\n",
      "2025-01-15 13:08:41.002941: train_loss -0.8143\n",
      "2025-01-15 13:08:41.003082: val_loss -0.7636\n",
      "2025-01-15 13:08:41.003115: Pseudo dice [np.float32(0.7992)]\n",
      "2025-01-15 13:08:41.003148: Epoch time: 112.12 s\n",
      "2025-01-15 13:08:41.593428: \n",
      "2025-01-15 13:08:41.593532: Epoch 669\n",
      "2025-01-15 13:08:41.593595: Current learning rate: 0.0037\n",
      "2025-01-15 13:10:33.665350: train_loss -0.8327\n",
      "2025-01-15 13:10:33.665562: val_loss -0.7673\n",
      "2025-01-15 13:10:33.665626: Pseudo dice [np.float32(0.8084)]\n",
      "2025-01-15 13:10:33.665669: Epoch time: 112.07 s\n",
      "2025-01-15 13:10:34.263062: \n",
      "2025-01-15 13:10:34.263458: Epoch 670\n",
      "2025-01-15 13:10:34.263532: Current learning rate: 0.00369\n",
      "2025-01-15 13:12:26.373651: train_loss -0.8432\n",
      "2025-01-15 13:12:26.373856: val_loss -0.7248\n",
      "2025-01-15 13:12:26.374047: Pseudo dice [np.float32(0.8248)]\n",
      "2025-01-15 13:12:26.374227: Epoch time: 112.11 s\n",
      "2025-01-15 13:12:26.966332: \n",
      "2025-01-15 13:12:26.966676: Epoch 671\n",
      "2025-01-15 13:12:26.966746: Current learning rate: 0.00368\n",
      "2025-01-15 13:14:18.875630: train_loss -0.8137\n",
      "2025-01-15 13:14:18.875773: val_loss -0.7559\n",
      "2025-01-15 13:14:18.875811: Pseudo dice [np.float32(0.8369)]\n",
      "2025-01-15 13:14:18.875845: Epoch time: 111.91 s\n",
      "2025-01-15 13:14:19.464155: \n",
      "2025-01-15 13:14:19.464561: Epoch 672\n",
      "2025-01-15 13:14:19.464778: Current learning rate: 0.00367\n",
      "2025-01-15 13:16:11.184720: train_loss -0.813\n",
      "2025-01-15 13:16:11.184841: val_loss -0.7398\n",
      "2025-01-15 13:16:11.184874: Pseudo dice [np.float32(0.8119)]\n",
      "2025-01-15 13:16:11.184909: Epoch time: 111.72 s\n",
      "2025-01-15 13:16:11.776518: \n",
      "2025-01-15 13:16:11.776711: Epoch 673\n",
      "2025-01-15 13:16:11.776785: Current learning rate: 0.00366\n",
      "2025-01-15 13:18:03.391117: train_loss -0.8192\n",
      "2025-01-15 13:18:03.391247: val_loss -0.7162\n",
      "2025-01-15 13:18:03.391280: Pseudo dice [np.float32(0.7772)]\n",
      "2025-01-15 13:18:03.391311: Epoch time: 111.62 s\n",
      "2025-01-15 13:18:03.981678: \n",
      "2025-01-15 13:18:03.981765: Epoch 674\n",
      "2025-01-15 13:18:03.981875: Current learning rate: 0.00365\n",
      "2025-01-15 13:19:55.681432: train_loss -0.8159\n",
      "2025-01-15 13:19:55.681602: val_loss -0.6705\n",
      "2025-01-15 13:19:55.681717: Pseudo dice [np.float32(0.8056)]\n",
      "2025-01-15 13:19:55.681760: Epoch time: 111.7 s\n",
      "2025-01-15 13:19:56.266082: \n",
      "2025-01-15 13:19:56.266402: Epoch 675\n",
      "2025-01-15 13:19:56.266473: Current learning rate: 0.00364\n",
      "2025-01-15 13:21:47.957252: train_loss -0.8203\n",
      "2025-01-15 13:21:47.957430: val_loss -0.7544\n",
      "2025-01-15 13:21:47.957462: Pseudo dice [np.float32(0.8166)]\n",
      "2025-01-15 13:21:47.957494: Epoch time: 111.69 s\n",
      "2025-01-15 13:21:48.547193: \n",
      "2025-01-15 13:21:48.547309: Epoch 676\n",
      "2025-01-15 13:21:48.547382: Current learning rate: 0.00363\n",
      "2025-01-15 13:23:40.202036: train_loss -0.8212\n",
      "2025-01-15 13:23:40.202178: val_loss -0.6544\n",
      "2025-01-15 13:23:40.202214: Pseudo dice [np.float32(0.785)]\n",
      "2025-01-15 13:23:40.202248: Epoch time: 111.66 s\n",
      "2025-01-15 13:23:40.789367: \n",
      "2025-01-15 13:23:40.789525: Epoch 677\n",
      "2025-01-15 13:23:40.789599: Current learning rate: 0.00362\n",
      "2025-01-15 13:25:32.465524: train_loss -0.8273\n",
      "2025-01-15 13:25:32.465652: val_loss -0.7061\n",
      "2025-01-15 13:25:32.465684: Pseudo dice [np.float32(0.7894)]\n",
      "2025-01-15 13:25:32.465718: Epoch time: 111.68 s\n",
      "2025-01-15 13:25:33.050993: \n",
      "2025-01-15 13:25:33.051141: Epoch 678\n",
      "2025-01-15 13:25:33.051208: Current learning rate: 0.00361\n",
      "2025-01-15 13:27:24.745716: train_loss -0.7944\n",
      "2025-01-15 13:27:24.745839: val_loss -0.6527\n",
      "2025-01-15 13:27:24.745871: Pseudo dice [np.float32(0.7792)]\n",
      "2025-01-15 13:27:24.745903: Epoch time: 111.7 s\n",
      "2025-01-15 13:27:25.332390: \n",
      "2025-01-15 13:27:25.332800: Epoch 679\n",
      "2025-01-15 13:27:25.332904: Current learning rate: 0.0036\n",
      "2025-01-15 13:29:17.024889: train_loss -0.8173\n",
      "2025-01-15 13:29:17.025071: val_loss -0.6601\n",
      "2025-01-15 13:29:17.025157: Pseudo dice [np.float32(0.775)]\n",
      "2025-01-15 13:29:17.025236: Epoch time: 111.69 s\n",
      "2025-01-15 13:29:17.616964: \n",
      "2025-01-15 13:29:17.617147: Epoch 680\n",
      "2025-01-15 13:29:17.617218: Current learning rate: 0.00359\n",
      "2025-01-15 13:31:09.305613: train_loss -0.82\n",
      "2025-01-15 13:31:09.305741: val_loss -0.7185\n",
      "2025-01-15 13:31:09.305775: Pseudo dice [np.float32(0.7966)]\n",
      "2025-01-15 13:31:09.305808: Epoch time: 111.69 s\n",
      "2025-01-15 13:31:09.891817: \n",
      "2025-01-15 13:31:09.891908: Epoch 681\n",
      "2025-01-15 13:31:09.891975: Current learning rate: 0.00358\n",
      "2025-01-15 13:33:01.571427: train_loss -0.8169\n",
      "2025-01-15 13:33:01.571547: val_loss -0.6265\n",
      "2025-01-15 13:33:01.571579: Pseudo dice [np.float32(0.7557)]\n",
      "2025-01-15 13:33:01.571612: Epoch time: 111.68 s\n",
      "2025-01-15 13:33:02.157649: \n",
      "2025-01-15 13:33:02.157738: Epoch 682\n",
      "2025-01-15 13:33:02.157799: Current learning rate: 0.00357\n",
      "2025-01-15 13:34:53.878815: train_loss -0.773\n",
      "2025-01-15 13:34:53.879143: val_loss -0.717\n",
      "2025-01-15 13:34:53.879217: Pseudo dice [np.float32(0.8099)]\n",
      "2025-01-15 13:34:53.879260: Epoch time: 111.72 s\n",
      "2025-01-15 13:34:54.473958: \n",
      "2025-01-15 13:34:54.474048: Epoch 683\n",
      "2025-01-15 13:34:54.474115: Current learning rate: 0.00356\n",
      "2025-01-15 13:36:46.197774: train_loss -0.818\n",
      "2025-01-15 13:36:46.197928: val_loss -0.7302\n",
      "2025-01-15 13:36:46.198000: Pseudo dice [np.float32(0.818)]\n",
      "2025-01-15 13:36:46.198040: Epoch time: 111.72 s\n",
      "2025-01-15 13:36:47.182378: \n",
      "2025-01-15 13:36:47.182473: Epoch 684\n",
      "2025-01-15 13:36:47.182544: Current learning rate: 0.00355\n",
      "2025-01-15 13:38:38.908180: train_loss -0.8273\n",
      "2025-01-15 13:38:38.908309: val_loss -0.7801\n",
      "2025-01-15 13:38:38.908344: Pseudo dice [np.float32(0.8399)]\n",
      "2025-01-15 13:38:38.908380: Epoch time: 111.73 s\n",
      "2025-01-15 13:38:39.498850: \n",
      "2025-01-15 13:38:39.499039: Epoch 685\n",
      "2025-01-15 13:38:39.499109: Current learning rate: 0.00354\n",
      "2025-01-15 13:40:31.171009: train_loss -0.8445\n",
      "2025-01-15 13:40:31.171138: val_loss -0.7297\n",
      "2025-01-15 13:40:31.171170: Pseudo dice [np.float32(0.8024)]\n",
      "2025-01-15 13:40:31.171203: Epoch time: 111.67 s\n",
      "2025-01-15 13:40:31.761511: \n",
      "2025-01-15 13:40:31.761615: Epoch 686\n",
      "2025-01-15 13:40:31.761676: Current learning rate: 0.00353\n",
      "2025-01-15 13:42:23.457038: train_loss -0.8148\n",
      "2025-01-15 13:42:23.457322: val_loss -0.7328\n",
      "2025-01-15 13:42:23.457366: Pseudo dice [np.float32(0.8)]\n",
      "2025-01-15 13:42:23.457438: Epoch time: 111.7 s\n",
      "2025-01-15 13:42:24.049536: \n",
      "2025-01-15 13:42:24.049727: Epoch 687\n",
      "2025-01-15 13:42:24.049805: Current learning rate: 0.00352\n",
      "2025-01-15 13:44:15.765970: train_loss -0.8128\n",
      "2025-01-15 13:44:15.766116: val_loss -0.6723\n",
      "2025-01-15 13:44:15.766152: Pseudo dice [np.float32(0.686)]\n",
      "2025-01-15 13:44:15.766189: Epoch time: 111.72 s\n",
      "2025-01-15 13:44:16.348594: \n",
      "2025-01-15 13:44:16.348699: Epoch 688\n",
      "2025-01-15 13:44:16.348761: Current learning rate: 0.00351\n",
      "2025-01-15 13:46:08.083316: train_loss -0.7584\n",
      "2025-01-15 13:46:08.083480: val_loss -0.6886\n",
      "2025-01-15 13:46:08.083553: Pseudo dice [np.float32(0.8155)]\n",
      "2025-01-15 13:46:08.083595: Epoch time: 111.74 s\n",
      "2025-01-15 13:46:08.682109: \n",
      "2025-01-15 13:46:08.682449: Epoch 689\n",
      "2025-01-15 13:46:08.682602: Current learning rate: 0.0035\n",
      "2025-01-15 13:48:00.372229: train_loss -0.8037\n",
      "2025-01-15 13:48:00.372407: val_loss -0.7076\n",
      "2025-01-15 13:48:00.372443: Pseudo dice [np.float32(0.7655)]\n",
      "2025-01-15 13:48:00.372478: Epoch time: 111.69 s\n",
      "2025-01-15 13:48:00.971224: \n",
      "2025-01-15 13:48:00.971321: Epoch 690\n",
      "2025-01-15 13:48:00.971383: Current learning rate: 0.00349\n",
      "2025-01-15 13:49:52.690206: train_loss -0.7881\n",
      "2025-01-15 13:49:52.690331: val_loss -0.6323\n",
      "2025-01-15 13:49:52.690363: Pseudo dice [np.float32(0.7346)]\n",
      "2025-01-15 13:49:52.690396: Epoch time: 111.72 s\n",
      "2025-01-15 13:49:53.283720: \n",
      "2025-01-15 13:49:53.284002: Epoch 691\n",
      "2025-01-15 13:49:53.284075: Current learning rate: 0.00348\n",
      "2025-01-15 13:51:45.008831: train_loss -0.8314\n",
      "2025-01-15 13:51:45.008955: val_loss -0.7455\n",
      "2025-01-15 13:51:45.008986: Pseudo dice [np.float32(0.8214)]\n",
      "2025-01-15 13:51:45.009021: Epoch time: 111.73 s\n",
      "2025-01-15 13:51:45.596876: \n",
      "2025-01-15 13:51:45.597075: Epoch 692\n",
      "2025-01-15 13:51:45.597171: Current learning rate: 0.00346\n",
      "2025-01-15 13:53:37.285451: train_loss -0.8279\n",
      "2025-01-15 13:53:37.285583: val_loss -0.7177\n",
      "2025-01-15 13:53:37.285617: Pseudo dice [np.float32(0.8262)]\n",
      "2025-01-15 13:53:37.285727: Epoch time: 111.69 s\n",
      "2025-01-15 13:53:37.880133: \n",
      "2025-01-15 13:53:37.880224: Epoch 693\n",
      "2025-01-15 13:53:37.880286: Current learning rate: 0.00345\n",
      "2025-01-15 13:55:29.557020: train_loss -0.8227\n",
      "2025-01-15 13:55:29.557145: val_loss -0.7447\n",
      "2025-01-15 13:55:29.557180: Pseudo dice [np.float32(0.8261)]\n",
      "2025-01-15 13:55:29.557213: Epoch time: 111.68 s\n",
      "2025-01-15 13:55:30.147917: \n",
      "2025-01-15 13:55:30.148115: Epoch 694\n",
      "2025-01-15 13:55:30.148187: Current learning rate: 0.00344\n",
      "2025-01-15 13:57:21.848596: train_loss -0.835\n",
      "2025-01-15 13:57:21.848720: val_loss -0.7049\n",
      "2025-01-15 13:57:21.848770: Pseudo dice [np.float32(0.8032)]\n",
      "2025-01-15 13:57:21.848900: Epoch time: 111.7 s\n",
      "2025-01-15 13:57:22.447885: \n",
      "2025-01-15 13:57:22.448337: Epoch 695\n",
      "2025-01-15 13:57:22.448464: Current learning rate: 0.00343\n",
      "2025-01-15 13:59:13.984104: train_loss -0.8417\n",
      "2025-01-15 13:59:13.984344: val_loss -0.7567\n",
      "2025-01-15 13:59:13.984388: Pseudo dice [np.float32(0.8188)]\n",
      "2025-01-15 13:59:13.984423: Epoch time: 111.54 s\n",
      "2025-01-15 13:59:14.574722: \n",
      "2025-01-15 13:59:14.574934: Epoch 696\n",
      "2025-01-15 13:59:14.575010: Current learning rate: 0.00342\n",
      "2025-01-15 14:01:06.253256: train_loss -0.8389\n",
      "2025-01-15 14:01:06.253387: val_loss -0.7243\n",
      "2025-01-15 14:01:06.253423: Pseudo dice [np.float32(0.8193)]\n",
      "2025-01-15 14:01:06.253455: Epoch time: 111.68 s\n",
      "2025-01-15 14:01:06.835261: \n",
      "2025-01-15 14:01:06.835611: Epoch 697\n",
      "2025-01-15 14:01:06.835806: Current learning rate: 0.00341\n",
      "2025-01-15 14:02:58.524433: train_loss -0.8183\n",
      "2025-01-15 14:02:58.524625: val_loss -0.6584\n",
      "2025-01-15 14:02:58.524659: Pseudo dice [np.float32(0.7495)]\n",
      "2025-01-15 14:02:58.524691: Epoch time: 111.69 s\n",
      "2025-01-15 14:02:59.111372: \n",
      "2025-01-15 14:02:59.111513: Epoch 698\n",
      "2025-01-15 14:02:59.111584: Current learning rate: 0.0034\n",
      "2025-01-15 14:04:50.724730: train_loss -0.8334\n",
      "2025-01-15 14:04:50.724867: val_loss -0.6696\n",
      "2025-01-15 14:04:50.724901: Pseudo dice [np.float32(0.7721)]\n",
      "2025-01-15 14:04:50.724936: Epoch time: 111.61 s\n",
      "2025-01-15 14:04:51.327293: \n",
      "2025-01-15 14:04:51.327647: Epoch 699\n",
      "2025-01-15 14:04:51.327834: Current learning rate: 0.00339\n",
      "2025-01-15 14:06:43.133503: train_loss -0.8137\n",
      "2025-01-15 14:06:43.133642: val_loss -0.7154\n",
      "2025-01-15 14:06:43.133679: Pseudo dice [np.float32(0.8097)]\n",
      "2025-01-15 14:06:43.133712: Epoch time: 111.81 s\n",
      "2025-01-15 14:06:43.979895: \n",
      "2025-01-15 14:06:43.979988: Epoch 700\n",
      "2025-01-15 14:06:43.980052: Current learning rate: 0.00338\n",
      "2025-01-15 14:08:35.772293: train_loss -0.8173\n",
      "2025-01-15 14:08:35.772505: val_loss -0.6096\n",
      "2025-01-15 14:08:35.772540: Pseudo dice [np.float32(0.7401)]\n",
      "2025-01-15 14:08:35.772575: Epoch time: 111.79 s\n",
      "2025-01-15 14:08:36.388214: \n",
      "2025-01-15 14:08:36.388400: Epoch 701\n",
      "2025-01-15 14:08:36.388473: Current learning rate: 0.00337\n",
      "2025-01-15 14:10:28.284420: train_loss -0.821\n",
      "2025-01-15 14:10:28.284556: val_loss -0.6892\n",
      "2025-01-15 14:10:28.284589: Pseudo dice [np.float32(0.7849)]\n",
      "2025-01-15 14:10:28.284623: Epoch time: 111.9 s\n",
      "2025-01-15 14:10:29.298348: \n",
      "2025-01-15 14:10:29.298837: Epoch 702\n",
      "2025-01-15 14:10:29.298935: Current learning rate: 0.00336\n",
      "2025-01-15 14:12:20.811955: train_loss -0.8425\n",
      "2025-01-15 14:12:20.812090: val_loss -0.7391\n",
      "2025-01-15 14:12:20.812124: Pseudo dice [np.float32(0.8184)]\n",
      "2025-01-15 14:12:20.812157: Epoch time: 111.51 s\n",
      "2025-01-15 14:12:21.404291: \n",
      "2025-01-15 14:12:21.404402: Epoch 703\n",
      "2025-01-15 14:12:21.404642: Current learning rate: 0.00335\n",
      "2025-01-15 14:14:12.900056: train_loss -0.8196\n",
      "2025-01-15 14:14:12.900191: val_loss -0.6456\n",
      "2025-01-15 14:14:12.900224: Pseudo dice [np.float32(0.7552)]\n",
      "2025-01-15 14:14:12.900259: Epoch time: 111.5 s\n",
      "2025-01-15 14:14:13.488730: \n",
      "2025-01-15 14:14:13.488832: Epoch 704\n",
      "2025-01-15 14:14:13.488895: Current learning rate: 0.00334\n",
      "2025-01-15 14:16:05.191570: train_loss -0.8339\n",
      "2025-01-15 14:16:05.191694: val_loss -0.7766\n",
      "2025-01-15 14:16:05.191726: Pseudo dice [np.float32(0.851)]\n",
      "2025-01-15 14:16:05.191762: Epoch time: 111.7 s\n",
      "2025-01-15 14:16:05.780478: \n",
      "2025-01-15 14:16:05.780581: Epoch 705\n",
      "2025-01-15 14:16:05.780644: Current learning rate: 0.00333\n",
      "2025-01-15 14:17:57.368701: train_loss -0.8275\n",
      "2025-01-15 14:17:57.368834: val_loss -0.7075\n",
      "2025-01-15 14:17:57.368868: Pseudo dice [np.float32(0.7708)]\n",
      "2025-01-15 14:17:57.368901: Epoch time: 111.59 s\n",
      "2025-01-15 14:17:57.966058: \n",
      "2025-01-15 14:17:57.966159: Epoch 706\n",
      "2025-01-15 14:17:57.966221: Current learning rate: 0.00332\n",
      "2025-01-15 14:19:49.469961: train_loss -0.81\n",
      "2025-01-15 14:19:49.470179: val_loss -0.7021\n",
      "2025-01-15 14:19:49.470220: Pseudo dice [np.float32(0.8065)]\n",
      "2025-01-15 14:19:49.470261: Epoch time: 111.5 s\n",
      "2025-01-15 14:19:50.063891: \n",
      "2025-01-15 14:19:50.064350: Epoch 707\n",
      "2025-01-15 14:19:50.064532: Current learning rate: 0.00331\n",
      "2025-01-15 14:21:41.545551: train_loss -0.8198\n",
      "2025-01-15 14:21:41.545684: val_loss -0.7756\n",
      "2025-01-15 14:21:41.545826: Pseudo dice [np.float32(0.8404)]\n",
      "2025-01-15 14:21:41.546001: Epoch time: 111.48 s\n",
      "2025-01-15 14:21:42.142673: \n",
      "2025-01-15 14:21:42.142889: Epoch 708\n",
      "2025-01-15 14:21:42.142965: Current learning rate: 0.0033\n",
      "2025-01-15 14:23:33.814726: train_loss -0.8346\n",
      "2025-01-15 14:23:33.814859: val_loss -0.7255\n",
      "2025-01-15 14:23:33.814890: Pseudo dice [np.float32(0.823)]\n",
      "2025-01-15 14:23:33.814921: Epoch time: 111.67 s\n",
      "2025-01-15 14:23:34.398407: \n",
      "2025-01-15 14:23:34.398510: Epoch 709\n",
      "2025-01-15 14:23:34.398573: Current learning rate: 0.00329\n",
      "2025-01-15 14:25:25.872155: train_loss -0.8441\n",
      "2025-01-15 14:25:25.872278: val_loss -0.6928\n",
      "2025-01-15 14:25:25.872312: Pseudo dice [np.float32(0.8169)]\n",
      "2025-01-15 14:25:25.872346: Epoch time: 111.47 s\n",
      "2025-01-15 14:25:26.461420: \n",
      "2025-01-15 14:25:26.461581: Epoch 710\n",
      "2025-01-15 14:25:26.461654: Current learning rate: 0.00328\n",
      "2025-01-15 14:27:17.980050: train_loss -0.8502\n",
      "2025-01-15 14:27:17.980189: val_loss -0.8044\n",
      "2025-01-15 14:27:17.980223: Pseudo dice [np.float32(0.8456)]\n",
      "2025-01-15 14:27:17.980255: Epoch time: 111.52 s\n",
      "2025-01-15 14:27:18.569052: \n",
      "2025-01-15 14:27:18.569351: Epoch 711\n",
      "2025-01-15 14:27:18.569497: Current learning rate: 0.00327\n",
      "2025-01-15 14:29:10.285015: train_loss -0.8323\n",
      "2025-01-15 14:29:10.285152: val_loss -0.7611\n",
      "2025-01-15 14:29:10.285187: Pseudo dice [np.float32(0.8303)]\n",
      "2025-01-15 14:29:10.285227: Epoch time: 111.72 s\n",
      "2025-01-15 14:29:10.881047: \n",
      "2025-01-15 14:29:10.881377: Epoch 712\n",
      "2025-01-15 14:29:10.881504: Current learning rate: 0.00326\n",
      "2025-01-15 14:31:02.518671: train_loss -0.8394\n",
      "2025-01-15 14:31:02.518817: val_loss -0.7605\n",
      "2025-01-15 14:31:02.518889: Pseudo dice [np.float32(0.8143)]\n",
      "2025-01-15 14:31:02.518929: Epoch time: 111.64 s\n",
      "2025-01-15 14:31:03.119613: \n",
      "2025-01-15 14:31:03.119777: Epoch 713\n",
      "2025-01-15 14:31:03.119961: Current learning rate: 0.00325\n",
      "2025-01-15 14:32:54.659261: train_loss -0.8239\n",
      "2025-01-15 14:32:54.659416: val_loss -0.7268\n",
      "2025-01-15 14:32:54.659485: Pseudo dice [np.float32(0.8138)]\n",
      "2025-01-15 14:32:54.659526: Epoch time: 111.54 s\n",
      "2025-01-15 14:32:55.250945: \n",
      "2025-01-15 14:32:55.251097: Epoch 714\n",
      "2025-01-15 14:32:55.251168: Current learning rate: 0.00324\n",
      "2025-01-15 14:34:46.762097: train_loss -0.846\n",
      "2025-01-15 14:34:46.762258: val_loss -0.7696\n",
      "2025-01-15 14:34:46.762293: Pseudo dice [np.float32(0.8321)]\n",
      "2025-01-15 14:34:46.762326: Epoch time: 111.51 s\n",
      "2025-01-15 14:34:47.351084: \n",
      "2025-01-15 14:34:47.351280: Epoch 715\n",
      "2025-01-15 14:34:47.351377: Current learning rate: 0.00323\n",
      "2025-01-15 14:36:38.832432: train_loss -0.8445\n",
      "2025-01-15 14:36:38.832616: val_loss -0.7153\n",
      "2025-01-15 14:36:38.832650: Pseudo dice [np.float32(0.8156)]\n",
      "2025-01-15 14:36:38.832684: Epoch time: 111.48 s\n",
      "2025-01-15 14:36:39.426236: \n",
      "2025-01-15 14:36:39.426560: Epoch 716\n",
      "2025-01-15 14:36:39.426689: Current learning rate: 0.00322\n",
      "2025-01-15 14:38:30.942784: train_loss -0.8418\n",
      "2025-01-15 14:38:30.942921: val_loss -0.7876\n",
      "2025-01-15 14:38:30.943161: Pseudo dice [np.float32(0.85)]\n",
      "2025-01-15 14:38:30.943214: Epoch time: 111.52 s\n",
      "2025-01-15 14:38:30.943245: Yayy! New best EMA pseudo Dice: 0.8162000179290771\n",
      "2025-01-15 14:38:31.769262: \n",
      "2025-01-15 14:38:31.769600: Epoch 717\n",
      "2025-01-15 14:38:31.769754: Current learning rate: 0.00321\n",
      "2025-01-15 14:40:24.537021: train_loss -0.8485\n",
      "2025-01-15 14:40:24.537403: val_loss -0.7051\n",
      "2025-01-15 14:40:24.537448: Pseudo dice [np.float32(0.8089)]\n",
      "2025-01-15 14:40:24.537486: Epoch time: 112.77 s\n",
      "2025-01-15 14:40:25.138517: \n",
      "2025-01-15 14:40:25.138925: Epoch 718\n",
      "2025-01-15 14:40:25.139071: Current learning rate: 0.0032\n",
      "2025-01-15 14:42:17.163917: train_loss -0.8356\n",
      "2025-01-15 14:42:17.164199: val_loss -0.7292\n",
      "2025-01-15 14:42:17.164346: Pseudo dice [np.float32(0.8313)]\n",
      "2025-01-15 14:42:17.164395: Epoch time: 112.03 s\n",
      "2025-01-15 14:42:17.164421: Yayy! New best EMA pseudo Dice: 0.8170999884605408\n",
      "2025-01-15 14:42:18.391250: \n",
      "2025-01-15 14:42:18.391603: Epoch 719\n",
      "2025-01-15 14:42:18.391700: Current learning rate: 0.00319\n",
      "2025-01-15 14:44:10.382430: train_loss -0.8468\n",
      "2025-01-15 14:44:10.382769: val_loss -0.6988\n",
      "2025-01-15 14:44:10.382918: Pseudo dice [np.float32(0.7815)]\n",
      "2025-01-15 14:44:10.382972: Epoch time: 111.99 s\n",
      "2025-01-15 14:44:10.982049: \n",
      "2025-01-15 14:44:10.982412: Epoch 720\n",
      "2025-01-15 14:44:10.982483: Current learning rate: 0.00318\n",
      "2025-01-15 14:46:02.991041: train_loss -0.8485\n",
      "2025-01-15 14:46:02.991168: val_loss -0.7992\n",
      "2025-01-15 14:46:02.991200: Pseudo dice [np.float32(0.8584)]\n",
      "2025-01-15 14:46:02.991232: Epoch time: 112.01 s\n",
      "2025-01-15 14:46:02.991251: Yayy! New best EMA pseudo Dice: 0.8180000185966492\n",
      "2025-01-15 14:46:03.806648: \n",
      "2025-01-15 14:46:03.807003: Epoch 721\n",
      "2025-01-15 14:46:03.807074: Current learning rate: 0.00317\n",
      "2025-01-15 14:47:55.815673: train_loss -0.8406\n",
      "2025-01-15 14:47:55.815856: val_loss -0.7211\n",
      "2025-01-15 14:47:55.815934: Pseudo dice [np.float32(0.8383)]\n",
      "2025-01-15 14:47:55.815975: Epoch time: 112.01 s\n",
      "2025-01-15 14:47:55.815998: Yayy! New best EMA pseudo Dice: 0.8199999928474426\n",
      "2025-01-15 14:47:56.639190: \n",
      "2025-01-15 14:47:56.639518: Epoch 722\n",
      "2025-01-15 14:47:56.639682: Current learning rate: 0.00316\n",
      "2025-01-15 14:49:48.798470: train_loss -0.8493\n",
      "2025-01-15 14:49:48.798590: val_loss -0.7583\n",
      "2025-01-15 14:49:48.798624: Pseudo dice [np.float32(0.8448)]\n",
      "2025-01-15 14:49:48.798681: Epoch time: 112.16 s\n",
      "Yayy! New best EMA pseudo Dice: 0.8224999904632568\n",
      "2025-01-15 14:49:49.619626: \n",
      "2025-01-15 14:49:49.619987: Epoch 723\n",
      "2025-01-15 14:49:49.620125: Current learning rate: 0.00315\n",
      "2025-01-15 14:51:41.604614: train_loss -0.8507\n",
      "2025-01-15 14:51:41.604994: val_loss -0.6716\n",
      "2025-01-15 14:51:41.605037: Pseudo dice [np.float32(0.765)]\n",
      "2025-01-15 14:51:41.605073: Epoch time: 111.99 s\n",
      "2025-01-15 14:51:42.206895: \n",
      "2025-01-15 14:51:42.207078: Epoch 724\n",
      "2025-01-15 14:51:42.207152: Current learning rate: 0.00314\n",
      "2025-01-15 14:53:34.353746: train_loss -0.8567\n",
      "2025-01-15 14:53:34.353965: val_loss -0.7713\n",
      "2025-01-15 14:53:34.354009: Pseudo dice [np.float32(0.8157)]\n",
      "2025-01-15 14:53:34.354051: Epoch time: 112.15 s\n",
      "2025-01-15 14:53:34.958076: \n",
      "2025-01-15 14:53:34.958235: Epoch 725\n",
      "2025-01-15 14:53:34.958311: Current learning rate: 0.00313\n",
      "2025-01-15 14:55:26.903021: train_loss -0.837\n",
      "2025-01-15 14:55:26.903152: val_loss -0.7086\n",
      "2025-01-15 14:55:26.903185: Pseudo dice [np.float32(0.7804)]\n",
      "2025-01-15 14:55:26.903217: Epoch time: 111.95 s\n",
      "2025-01-15 14:55:27.507730: \n",
      "2025-01-15 14:55:27.508073: Epoch 726\n",
      "2025-01-15 14:55:27.508214: Current learning rate: 0.00312\n",
      "2025-01-15 14:57:19.523916: train_loss -0.8526\n",
      "2025-01-15 14:57:19.524338: val_loss -0.697\n",
      "2025-01-15 14:57:19.524470: Pseudo dice [np.float32(0.8035)]\n",
      "2025-01-15 14:57:19.524612: Epoch time: 112.02 s\n",
      "2025-01-15 14:57:20.123938: \n",
      "2025-01-15 14:57:20.124375: Epoch 727\n",
      "2025-01-15 14:57:20.124473: Current learning rate: 0.00311\n",
      "2025-01-15 14:59:12.087561: train_loss -0.8377\n",
      "2025-01-15 14:59:12.087968: val_loss -0.7296\n",
      "2025-01-15 14:59:12.088013: Pseudo dice [np.float32(0.814)]\n",
      "2025-01-15 14:59:12.088051: Epoch time: 111.96 s\n",
      "2025-01-15 14:59:12.694362: \n",
      "2025-01-15 14:59:12.694555: Epoch 728\n",
      "2025-01-15 14:59:12.694628: Current learning rate: 0.0031\n",
      "2025-01-15 15:01:04.652184: train_loss -0.8388\n",
      "2025-01-15 15:01:04.652308: val_loss -0.6277\n",
      "2025-01-15 15:01:04.652342: Pseudo dice [np.float32(0.7206)]\n",
      "2025-01-15 15:01:04.652377: Epoch time: 111.96 s\n",
      "2025-01-15 15:01:05.249627: \n",
      "2025-01-15 15:01:05.250079: Epoch 729\n",
      "2025-01-15 15:01:05.250231: Current learning rate: 0.00309\n",
      "2025-01-15 15:02:57.414683: train_loss -0.8474\n",
      "2025-01-15 15:02:57.414953: val_loss -0.7555\n",
      "2025-01-15 15:02:57.415114: Pseudo dice [np.float32(0.8256)]\n",
      "2025-01-15 15:02:57.415197: Epoch time: 112.17 s\n",
      "2025-01-15 15:02:58.012277: \n",
      "2025-01-15 15:02:58.012588: Epoch 730\n",
      "2025-01-15 15:02:58.012736: Current learning rate: 0.00308\n",
      "2025-01-15 15:04:50.185668: train_loss -0.8344\n",
      "2025-01-15 15:04:50.186114: val_loss -0.6792\n",
      "2025-01-15 15:04:50.186186: Pseudo dice [np.float32(0.622)]\n",
      "2025-01-15 15:04:50.186230: Epoch time: 112.17 s\n",
      "2025-01-15 15:04:50.810735: \n",
      "2025-01-15 15:04:50.811126: Epoch 731\n",
      "2025-01-15 15:04:50.811313: Current learning rate: 0.00307\n",
      "2025-01-15 15:06:42.896399: train_loss -0.8352\n",
      "2025-01-15 15:06:42.896595: val_loss -0.7254\n",
      "2025-01-15 15:06:42.896635: Pseudo dice [np.float32(0.8269)]\n",
      "2025-01-15 15:06:42.896675: Epoch time: 112.09 s\n",
      "2025-01-15 15:06:43.521684: \n",
      "2025-01-15 15:06:43.522080: Epoch 732\n",
      "2025-01-15 15:06:43.522173: Current learning rate: 0.00306\n",
      "2025-01-15 15:08:35.640349: train_loss -0.848\n",
      "2025-01-15 15:08:35.640577: val_loss -0.7291\n",
      "2025-01-15 15:08:35.640655: Pseudo dice [np.float32(0.8205)]\n",
      "2025-01-15 15:08:35.640698: Epoch time: 112.12 s\n",
      "2025-01-15 15:08:36.256166: \n",
      "2025-01-15 15:08:36.256570: Epoch 733\n",
      "2025-01-15 15:08:36.256668: Current learning rate: 0.00305\n",
      "2025-01-15 15:10:28.515948: train_loss -0.8619\n",
      "2025-01-15 15:10:28.516105: val_loss -0.6767\n",
      "2025-01-15 15:10:28.516139: Pseudo dice [np.float32(0.6765)]\n",
      "2025-01-15 15:10:28.516172: Epoch time: 112.26 s\n",
      "2025-01-15 15:10:29.131102: \n",
      "2025-01-15 15:10:29.131196: Epoch 734\n",
      "2025-01-15 15:10:29.131259: Current learning rate: 0.00304\n",
      "2025-01-15 15:12:21.255972: train_loss -0.8527\n",
      "2025-01-15 15:12:21.256122: val_loss -0.7598\n",
      "2025-01-15 15:12:21.256168: Pseudo dice [np.float32(0.8445)]\n",
      "2025-01-15 15:12:21.256205: Epoch time: 112.13 s\n",
      "2025-01-15 15:12:21.876190: \n",
      "2025-01-15 15:12:21.876445: Epoch 735\n",
      "2025-01-15 15:12:21.876683: Current learning rate: 0.00303\n",
      "2025-01-15 15:14:14.047377: train_loss -0.8439\n",
      "2025-01-15 15:14:14.047499: val_loss -0.7605\n",
      "2025-01-15 15:14:14.047531: Pseudo dice [np.float32(0.8265)]\n",
      "2025-01-15 15:14:14.047563: Epoch time: 112.17 s\n",
      "2025-01-15 15:14:15.035216: \n",
      "2025-01-15 15:14:15.035419: Epoch 736\n",
      "2025-01-15 15:14:15.035585: Current learning rate: 0.00302\n",
      "2025-01-15 15:16:06.558169: train_loss -0.8487\n",
      "2025-01-15 15:16:06.558311: val_loss -0.7102\n",
      "2025-01-15 15:16:06.558349: Pseudo dice [np.float32(0.7583)]\n",
      "2025-01-15 15:16:06.558382: Epoch time: 111.52 s\n",
      "2025-01-15 15:16:07.153939: \n",
      "2025-01-15 15:16:07.154033: Epoch 737\n",
      "2025-01-15 15:16:07.154255: Current learning rate: 0.00301\n",
      "2025-01-15 15:17:58.823893: train_loss -0.8345\n",
      "2025-01-15 15:17:58.824023: val_loss -0.7202\n",
      "2025-01-15 15:17:58.824055: Pseudo dice [np.float32(0.8224)]\n",
      "2025-01-15 15:17:58.824088: Epoch time: 111.67 s\n",
      "2025-01-15 15:17:59.420805: \n",
      "2025-01-15 15:17:59.421169: Epoch 738\n",
      "2025-01-15 15:17:59.421268: Current learning rate: 0.003\n",
      "2025-01-15 15:19:50.923748: train_loss -0.84\n",
      "2025-01-15 15:19:50.923902: val_loss -0.7348\n",
      "2025-01-15 15:19:50.923933: Pseudo dice [np.float32(0.8178)]\n",
      "2025-01-15 15:19:50.923965: Epoch time: 111.5 s\n",
      "2025-01-15 15:19:51.515814: \n",
      "2025-01-15 15:19:51.516160: Epoch 739\n",
      "2025-01-15 15:19:51.516237: Current learning rate: 0.00299\n",
      "2025-01-15 15:21:43.200684: train_loss -0.845\n",
      "2025-01-15 15:21:43.201072: val_loss -0.677\n",
      "2025-01-15 15:21:43.201139: Pseudo dice [np.float32(0.7779)]\n",
      "2025-01-15 15:21:43.201181: Epoch time: 111.69 s\n",
      "2025-01-15 15:21:43.801773: \n",
      "2025-01-15 15:21:43.802069: Epoch 740\n",
      "2025-01-15 15:21:43.802171: Current learning rate: 0.00297\n",
      "2025-01-15 15:23:35.287601: train_loss -0.8575\n",
      "2025-01-15 15:23:35.287826: val_loss -0.7769\n",
      "2025-01-15 15:23:35.287882: Pseudo dice [np.float32(0.8378)]\n",
      "2025-01-15 15:23:35.287922: Epoch time: 111.49 s\n",
      "2025-01-15 15:23:35.891214: \n",
      "2025-01-15 15:23:35.891396: Epoch 741\n",
      "2025-01-15 15:23:35.891470: Current learning rate: 0.00296\n",
      "2025-01-15 15:25:27.404114: train_loss -0.8422\n",
      "2025-01-15 15:25:27.404270: val_loss -0.7315\n",
      "2025-01-15 15:25:27.404305: Pseudo dice [np.float32(0.8133)]\n",
      "2025-01-15 15:25:27.404337: Epoch time: 111.51 s\n",
      "2025-01-15 15:25:28.004038: \n",
      "2025-01-15 15:25:28.004557: Epoch 742\n",
      "2025-01-15 15:25:28.004693: Current learning rate: 0.00295\n",
      "2025-01-15 15:27:19.657699: train_loss -0.8288\n",
      "2025-01-15 15:27:19.657831: val_loss -0.7034\n",
      "2025-01-15 15:27:19.657865: Pseudo dice [np.float32(0.7235)]\n",
      "2025-01-15 15:27:19.657898: Epoch time: 111.65 s\n",
      "2025-01-15 15:27:20.253459: \n",
      "2025-01-15 15:27:20.253651: Epoch 743\n",
      "2025-01-15 15:27:20.253735: Current learning rate: 0.00294\n",
      "2025-01-15 15:29:11.774292: train_loss -0.8011\n",
      "2025-01-15 15:29:11.774413: val_loss -0.6648\n",
      "2025-01-15 15:29:11.774444: Pseudo dice [np.float32(0.7878)]\n",
      "2025-01-15 15:29:11.774475: Epoch time: 111.52 s\n",
      "2025-01-15 15:29:12.366962: \n",
      "2025-01-15 15:29:12.367154: Epoch 744\n",
      "2025-01-15 15:29:12.367228: Current learning rate: 0.00293\n",
      "2025-01-15 15:31:03.832810: train_loss -0.834\n",
      "2025-01-15 15:31:03.832937: val_loss -0.7185\n",
      "2025-01-15 15:31:03.832972: Pseudo dice [np.float32(0.8123)]\n",
      "2025-01-15 15:31:03.833007: Epoch time: 111.47 s\n",
      "2025-01-15 15:31:04.429681: \n",
      "2025-01-15 15:31:04.429769: Epoch 745\n",
      "2025-01-15 15:31:04.429829: Current learning rate: 0.00292\n",
      "2025-01-15 15:32:55.912710: train_loss -0.8303\n",
      "2025-01-15 15:32:55.912880: val_loss -0.7451\n",
      "2025-01-15 15:32:55.913072: Pseudo dice [np.float32(0.819)]\n",
      "2025-01-15 15:32:55.913203: Epoch time: 111.48 s\n",
      "2025-01-15 15:32:56.510473: \n",
      "2025-01-15 15:32:56.510827: Epoch 746\n",
      "2025-01-15 15:32:56.510901: Current learning rate: 0.00291\n",
      "2025-01-15 15:34:48.016311: train_loss -0.8316\n",
      "2025-01-15 15:34:48.016550: val_loss -0.6852\n",
      "2025-01-15 15:34:48.016619: Pseudo dice [np.float32(0.7594)]\n",
      "2025-01-15 15:34:48.016660: Epoch time: 111.51 s\n",
      "2025-01-15 15:34:48.606723: \n",
      "2025-01-15 15:34:48.606911: Epoch 747\n",
      "2025-01-15 15:34:48.606977: Current learning rate: 0.0029\n",
      "2025-01-15 15:36:40.103829: train_loss -0.8161\n",
      "2025-01-15 15:36:40.104013: val_loss -0.7531\n",
      "2025-01-15 15:36:40.104046: Pseudo dice [np.float32(0.8365)]\n",
      "2025-01-15 15:36:40.104078: Epoch time: 111.5 s\n",
      "2025-01-15 15:36:40.713538: \n",
      "2025-01-15 15:36:40.713780: Epoch 748\n",
      "2025-01-15 15:36:40.713939: Current learning rate: 0.00289\n",
      "2025-01-15 15:38:32.189641: train_loss -0.7901\n",
      "2025-01-15 15:38:32.189806: val_loss -0.636\n",
      "2025-01-15 15:38:32.189876: Pseudo dice [np.float32(0.6475)]\n",
      "2025-01-15 15:38:32.189917: Epoch time: 111.48 s\n",
      "2025-01-15 15:38:32.785091: \n",
      "2025-01-15 15:38:32.785182: Epoch 749\n",
      "2025-01-15 15:38:32.785242: Current learning rate: 0.00288\n",
      "2025-01-15 15:40:24.275107: train_loss -0.8158\n",
      "2025-01-15 15:40:24.275232: val_loss -0.768\n",
      "2025-01-15 15:40:24.275263: Pseudo dice [np.float32(0.8186)]\n",
      "2025-01-15 15:40:24.275295: Epoch time: 111.49 s\n",
      "2025-01-15 15:40:25.096878: \n",
      "2025-01-15 15:40:25.097110: Epoch 750\n",
      "2025-01-15 15:40:25.097176: Current learning rate: 0.00287\n",
      "2025-01-15 15:42:16.618987: train_loss -0.8197\n",
      "2025-01-15 15:42:16.619108: val_loss -0.7047\n",
      "2025-01-15 15:42:16.619140: Pseudo dice [np.float32(0.8108)]\n",
      "2025-01-15 15:42:16.619173: Epoch time: 111.52 s\n",
      "2025-01-15 15:42:17.209953: \n",
      "2025-01-15 15:42:17.210275: Epoch 751\n",
      "2025-01-15 15:42:17.210443: Current learning rate: 0.00286\n",
      "2025-01-15 15:44:08.697269: train_loss -0.8343\n",
      "2025-01-15 15:44:08.697389: val_loss -0.6957\n",
      "2025-01-15 15:44:08.697422: Pseudo dice [np.float32(0.8074)]\n",
      "2025-01-15 15:44:08.697455: Epoch time: 111.49 s\n",
      "2025-01-15 15:44:09.294630: \n",
      "2025-01-15 15:44:09.294897: Epoch 752\n",
      "2025-01-15 15:44:09.295040: Current learning rate: 0.00285\n",
      "2025-01-15 15:46:00.827278: train_loss -0.8306\n",
      "2025-01-15 15:46:00.827408: val_loss -0.7108\n",
      "2025-01-15 15:46:00.827727: Pseudo dice [np.float32(0.8002)]\n",
      "2025-01-15 15:46:00.827877: Epoch time: 111.53 s\n",
      "2025-01-15 15:46:01.820130: \n",
      "2025-01-15 15:46:01.820632: Epoch 753\n",
      "2025-01-15 15:46:01.820746: Current learning rate: 0.00284\n",
      "2025-01-15 15:47:53.300306: train_loss -0.8481\n",
      "2025-01-15 15:47:53.300499: val_loss -0.6701\n",
      "2025-01-15 15:47:53.300542: Pseudo dice [np.float32(0.7956)]\n",
      "2025-01-15 15:47:53.300586: Epoch time: 111.48 s\n",
      "2025-01-15 15:47:53.897308: \n",
      "2025-01-15 15:47:53.897469: Epoch 754\n",
      "2025-01-15 15:47:53.897542: Current learning rate: 0.00283\n",
      "2025-01-15 15:49:45.424819: train_loss -0.8338\n",
      "2025-01-15 15:49:45.424978: val_loss -0.7142\n",
      "2025-01-15 15:49:45.425009: Pseudo dice [np.float32(0.7836)]\n",
      "2025-01-15 15:49:45.425042: Epoch time: 111.53 s\n",
      "2025-01-15 15:49:46.022809: \n",
      "2025-01-15 15:49:46.023070: Epoch 755\n",
      "2025-01-15 15:49:46.023139: Current learning rate: 0.00282\n",
      "2025-01-15 15:51:37.571633: train_loss -0.837\n",
      "2025-01-15 15:51:37.571865: val_loss -0.7442\n",
      "2025-01-15 15:51:37.572026: Pseudo dice [np.float32(0.7981)]\n",
      "2025-01-15 15:51:37.572155: Epoch time: 111.55 s\n",
      "2025-01-15 15:51:38.171453: \n",
      "2025-01-15 15:51:38.171551: Epoch 756\n",
      "2025-01-15 15:51:38.171613: Current learning rate: 0.00281\n",
      "2025-01-15 15:53:29.682799: train_loss -0.8123\n",
      "2025-01-15 15:53:29.683000: val_loss -0.7369\n",
      "2025-01-15 15:53:29.683036: Pseudo dice [np.float32(0.836)]\n",
      "2025-01-15 15:53:29.683069: Epoch time: 111.51 s\n",
      "2025-01-15 15:53:30.275613: \n",
      "2025-01-15 15:53:30.275971: Epoch 757\n",
      "2025-01-15 15:53:30.276240: Current learning rate: 0.0028\n",
      "2025-01-15 15:55:21.717149: train_loss -0.8421\n",
      "2025-01-15 15:55:21.717267: val_loss -0.7258\n",
      "2025-01-15 15:55:21.717504: Pseudo dice [np.float32(0.8176)]\n",
      "2025-01-15 15:55:21.717728: Epoch time: 111.44 s\n",
      "2025-01-15 15:55:22.311427: \n",
      "2025-01-15 15:55:22.311638: Epoch 758\n",
      "2025-01-15 15:55:22.311725: Current learning rate: 0.00279\n",
      "2025-01-15 15:57:13.846898: train_loss -0.8359\n",
      "2025-01-15 15:57:13.847036: val_loss -0.7753\n",
      "2025-01-15 15:57:13.847069: Pseudo dice [np.float32(0.8311)]\n",
      "2025-01-15 15:57:13.847103: Epoch time: 111.54 s\n",
      "2025-01-15 15:57:14.447478: \n",
      "2025-01-15 15:57:14.447908: Epoch 759\n",
      "2025-01-15 15:57:14.447991: Current learning rate: 0.00278\n",
      "2025-01-15 15:59:05.963959: train_loss -0.8523\n",
      "2025-01-15 15:59:05.964380: val_loss -0.7009\n",
      "2025-01-15 15:59:05.964451: Pseudo dice [np.float32(0.785)]\n",
      "2025-01-15 15:59:05.964533: Epoch time: 111.52 s\n",
      "2025-01-15 15:59:06.561127: \n",
      "2025-01-15 15:59:06.561228: Epoch 760\n",
      "2025-01-15 15:59:06.561290: Current learning rate: 0.00277\n",
      "2025-01-15 16:00:58.035998: train_loss -0.8278\n",
      "2025-01-15 16:00:58.036198: val_loss -0.7593\n",
      "2025-01-15 16:00:58.036390: Pseudo dice [np.float32(0.8301)]\n",
      "2025-01-15 16:00:58.036479: Epoch time: 111.48 s\n",
      "2025-01-15 16:00:58.628210: \n",
      "2025-01-15 16:00:58.628301: Epoch 761\n",
      "2025-01-15 16:00:58.628362: Current learning rate: 0.00276\n",
      "2025-01-15 16:02:50.242011: train_loss -0.8422\n",
      "2025-01-15 16:02:50.242490: val_loss -0.7315\n",
      "2025-01-15 16:02:50.242542: Pseudo dice [np.float32(0.821)]\n",
      "2025-01-15 16:02:50.242580: Epoch time: 111.61 s\n",
      "2025-01-15 16:02:50.838251: \n",
      "2025-01-15 16:02:50.838515: Epoch 762\n",
      "2025-01-15 16:02:50.838651: Current learning rate: 0.00275\n",
      "2025-01-15 16:04:42.316255: train_loss -0.827\n",
      "2025-01-15 16:04:42.316382: val_loss -0.7267\n",
      "2025-01-15 16:04:42.316416: Pseudo dice [np.float32(0.7854)]\n",
      "2025-01-15 16:04:42.316450: Epoch time: 111.48 s\n",
      "2025-01-15 16:04:42.920100: \n",
      "2025-01-15 16:04:42.920187: Epoch 763\n",
      "2025-01-15 16:04:42.920428: Current learning rate: 0.00274\n",
      "2025-01-15 16:06:34.688188: train_loss -0.8263\n",
      "2025-01-15 16:06:34.688318: val_loss -0.7629\n",
      "2025-01-15 16:06:34.688350: Pseudo dice [np.float32(0.753)]\n",
      "2025-01-15 16:06:34.688382: Epoch time: 111.77 s\n",
      "2025-01-15 16:06:35.309266: \n",
      "2025-01-15 16:06:35.309629: Epoch 764\n",
      "2025-01-15 16:06:35.309716: Current learning rate: 0.00273\n",
      "2025-01-15 16:08:27.083284: train_loss -0.8113\n",
      "2025-01-15 16:08:27.083418: val_loss -0.7232\n",
      "2025-01-15 16:08:27.083783: Pseudo dice [np.float32(0.8071)]\n",
      "2025-01-15 16:08:27.083857: Epoch time: 111.77 s\n",
      "2025-01-15 16:08:27.704353: \n",
      "2025-01-15 16:08:27.704438: Epoch 765\n",
      "2025-01-15 16:08:27.704499: Current learning rate: 0.00272\n",
      "2025-01-15 16:10:19.295800: train_loss -0.8152\n",
      "2025-01-15 16:10:19.295943: val_loss -0.7038\n",
      "2025-01-15 16:10:19.295978: Pseudo dice [np.float32(0.7466)]\n",
      "2025-01-15 16:10:19.296013: Epoch time: 111.59 s\n",
      "2025-01-15 16:10:19.923598: \n",
      "2025-01-15 16:10:19.923937: Epoch 766\n",
      "2025-01-15 16:10:19.924091: Current learning rate: 0.00271\n",
      "2025-01-15 16:12:11.484459: train_loss -0.8489\n",
      "2025-01-15 16:12:11.484585: val_loss -0.7072\n",
      "2025-01-15 16:12:11.484759: Pseudo dice [np.float32(0.7719)]\n",
      "2025-01-15 16:12:11.484839: Epoch time: 111.56 s\n",
      "2025-01-15 16:12:12.106049: \n",
      "2025-01-15 16:12:12.106358: Epoch 767\n",
      "2025-01-15 16:12:12.106433: Current learning rate: 0.0027\n",
      "2025-01-15 16:14:03.685639: train_loss -0.8375\n",
      "2025-01-15 16:14:03.685963: val_loss -0.7338\n",
      "2025-01-15 16:14:03.686008: Pseudo dice [np.float32(0.8418)]\n",
      "2025-01-15 16:14:03.686044: Epoch time: 111.58 s\n",
      "2025-01-15 16:14:04.308119: \n",
      "2025-01-15 16:14:04.308215: Epoch 768\n",
      "2025-01-15 16:14:04.308282: Current learning rate: 0.00268\n",
      "2025-01-15 16:15:55.880701: train_loss -0.8368\n",
      "2025-01-15 16:15:55.880892: val_loss -0.748\n",
      "2025-01-15 16:15:55.880925: Pseudo dice [np.float32(0.8354)]\n",
      "2025-01-15 16:15:55.880960: Epoch time: 111.57 s\n",
      "2025-01-15 16:15:56.482160: \n",
      "2025-01-15 16:15:56.482243: Epoch 769\n",
      "2025-01-15 16:15:56.482315: Current learning rate: 0.00267\n",
      "2025-01-15 16:17:48.005091: train_loss -0.8334\n",
      "2025-01-15 16:17:48.005215: val_loss -0.6973\n",
      "2025-01-15 16:17:48.005329: Pseudo dice [np.float32(0.8073)]\n",
      "2025-01-15 16:17:48.005371: Epoch time: 111.52 s\n",
      "2025-01-15 16:17:48.599770: \n",
      "2025-01-15 16:17:48.599966: Epoch 770\n",
      "2025-01-15 16:17:48.600053: Current learning rate: 0.00266\n",
      "2025-01-15 16:19:40.552535: train_loss -0.8377\n",
      "2025-01-15 16:19:40.552666: val_loss -0.7057\n",
      "2025-01-15 16:19:40.552699: Pseudo dice [np.float32(0.7982)]\n",
      "2025-01-15 16:19:40.552730: Epoch time: 111.95 s\n",
      "2025-01-15 16:19:41.141261: \n",
      "2025-01-15 16:19:41.141706: Epoch 771\n",
      "2025-01-15 16:19:41.141778: Current learning rate: 0.00265\n",
      "2025-01-15 16:21:32.764611: train_loss -0.8489\n",
      "2025-01-15 16:21:32.764741: val_loss -0.7011\n",
      "2025-01-15 16:21:32.764774: Pseudo dice [np.float32(0.7967)]\n",
      "2025-01-15 16:21:32.764812: Epoch time: 111.62 s\n",
      "2025-01-15 16:21:33.363955: \n",
      "2025-01-15 16:21:33.364274: Epoch 772\n",
      "2025-01-15 16:21:33.364413: Current learning rate: 0.00264\n",
      "2025-01-15 16:23:24.947120: train_loss -0.8637\n",
      "2025-01-15 16:23:24.947510: val_loss -0.7494\n",
      "2025-01-15 16:23:24.947625: Pseudo dice [np.float32(0.8215)]\n",
      "2025-01-15 16:23:24.947675: Epoch time: 111.58 s\n",
      "2025-01-15 16:23:25.550711: \n",
      "2025-01-15 16:23:25.550864: Epoch 773\n",
      "2025-01-15 16:23:25.550928: Current learning rate: 0.00263\n",
      "2025-01-15 16:25:17.045945: train_loss -0.8602\n",
      "2025-01-15 16:25:17.046073: val_loss -0.6923\n",
      "2025-01-15 16:25:17.046107: Pseudo dice [np.float32(0.8154)]\n",
      "2025-01-15 16:25:17.046140: Epoch time: 111.5 s\n",
      "2025-01-15 16:25:17.645525: \n",
      "2025-01-15 16:25:17.645622: Epoch 774\n",
      "2025-01-15 16:25:17.645684: Current learning rate: 0.00262\n",
      "2025-01-15 16:27:09.233588: train_loss -0.8513\n",
      "2025-01-15 16:27:09.233719: val_loss -0.696\n",
      "2025-01-15 16:27:09.233752: Pseudo dice [np.float32(0.8138)]\n",
      "2025-01-15 16:27:09.233785: Epoch time: 111.59 s\n",
      "2025-01-15 16:27:09.838890: \n",
      "2025-01-15 16:27:09.839155: Epoch 775\n",
      "2025-01-15 16:27:09.839280: Current learning rate: 0.00261\n",
      "2025-01-15 16:29:01.518996: train_loss -0.849\n",
      "2025-01-15 16:29:01.519120: val_loss -0.7826\n",
      "2025-01-15 16:29:01.519211: Pseudo dice [np.float32(0.8435)]\n",
      "2025-01-15 16:29:01.519328: Epoch time: 111.68 s\n",
      "2025-01-15 16:29:02.123693: \n",
      "2025-01-15 16:29:02.123906: Epoch 776\n",
      "2025-01-15 16:29:02.123998: Current learning rate: 0.0026\n",
      "2025-01-15 16:30:53.675464: train_loss -0.8458\n",
      "2025-01-15 16:30:53.675594: val_loss -0.7254\n",
      "2025-01-15 16:30:53.675628: Pseudo dice [np.float32(0.8224)]\n",
      "2025-01-15 16:30:53.675661: Epoch time: 111.55 s\n",
      "2025-01-15 16:30:54.283037: \n",
      "2025-01-15 16:30:54.283407: Epoch 777\n",
      "2025-01-15 16:30:54.283520: Current learning rate: 0.00259\n",
      "2025-01-15 16:32:45.785945: train_loss -0.8362\n",
      "2025-01-15 16:32:45.786074: val_loss -0.7234\n",
      "2025-01-15 16:32:45.786106: Pseudo dice [np.float32(0.8122)]\n",
      "2025-01-15 16:32:45.786139: Epoch time: 111.5 s\n",
      "2025-01-15 16:32:46.387966: \n",
      "2025-01-15 16:32:46.388060: Epoch 778\n",
      "2025-01-15 16:32:46.388123: Current learning rate: 0.00258\n",
      "2025-01-15 16:34:37.888166: train_loss -0.8399\n",
      "2025-01-15 16:34:37.888616: val_loss -0.7459\n",
      "2025-01-15 16:34:37.888681: Pseudo dice [np.float32(0.8406)]\n",
      "2025-01-15 16:34:37.888721: Epoch time: 111.5 s\n",
      "2025-01-15 16:34:38.485770: \n",
      "2025-01-15 16:34:38.486112: Epoch 779\n",
      "2025-01-15 16:34:38.486238: Current learning rate: 0.00257\n",
      "2025-01-15 16:36:29.916578: train_loss -0.843\n",
      "2025-01-15 16:36:29.916915: val_loss -0.727\n",
      "2025-01-15 16:36:29.917025: Pseudo dice [np.float32(0.8048)]\n",
      "2025-01-15 16:36:29.917070: Epoch time: 111.43 s\n",
      "2025-01-15 16:36:30.551586: \n",
      "2025-01-15 16:36:30.551691: Epoch 780\n",
      "2025-01-15 16:36:30.551758: Current learning rate: 0.00256\n",
      "2025-01-15 16:38:22.022099: train_loss -0.8418\n",
      "2025-01-15 16:38:22.022223: val_loss -0.7472\n",
      "2025-01-15 16:38:22.022255: Pseudo dice [np.float32(0.8436)]\n",
      "2025-01-15 16:38:22.022288: Epoch time: 111.47 s\n",
      "2025-01-15 16:38:22.628538: \n",
      "2025-01-15 16:38:22.628891: Epoch 781\n",
      "2025-01-15 16:38:22.629028: Current learning rate: 0.00255\n",
      "2025-01-15 16:40:14.118470: train_loss -0.8511\n",
      "2025-01-15 16:40:14.118667: val_loss -0.7499\n",
      "2025-01-15 16:40:14.118727: Pseudo dice [np.float32(0.8158)]\n",
      "2025-01-15 16:40:14.118767: Epoch time: 111.49 s\n",
      "2025-01-15 16:40:14.716075: \n",
      "2025-01-15 16:40:14.716267: Epoch 782\n",
      "2025-01-15 16:40:14.716428: Current learning rate: 0.00254\n",
      "2025-01-15 16:42:06.343631: train_loss -0.8578\n",
      "2025-01-15 16:42:06.343753: val_loss -0.7024\n",
      "2025-01-15 16:42:06.343784: Pseudo dice [np.float32(0.8031)]\n",
      "2025-01-15 16:42:06.343818: Epoch time: 111.63 s\n",
      "2025-01-15 16:42:06.938648: \n",
      "2025-01-15 16:42:06.938732: Epoch 783\n",
      "2025-01-15 16:42:06.938793: Current learning rate: 0.00253\n",
      "2025-01-15 16:43:58.403722: train_loss -0.8475\n",
      "2025-01-15 16:43:58.403845: val_loss -0.7476\n",
      "2025-01-15 16:43:58.403879: Pseudo dice [np.float32(0.8153)]\n",
      "2025-01-15 16:43:58.403912: Epoch time: 111.47 s\n",
      "2025-01-15 16:43:59.006253: \n",
      "2025-01-15 16:43:59.006586: Epoch 784\n",
      "2025-01-15 16:43:59.006721: Current learning rate: 0.00252\n",
      "2025-01-15 16:45:50.645554: train_loss -0.844\n",
      "2025-01-15 16:45:50.645675: val_loss -0.6849\n",
      "2025-01-15 16:45:50.645709: Pseudo dice [np.float32(0.7768)]\n",
      "2025-01-15 16:45:50.645743: Epoch time: 111.64 s\n",
      "2025-01-15 16:45:51.241134: \n",
      "2025-01-15 16:45:51.241452: Epoch 785\n",
      "2025-01-15 16:45:51.241544: Current learning rate: 0.00251\n",
      "2025-01-15 16:47:42.927964: train_loss -0.8542\n",
      "2025-01-15 16:47:42.928412: val_loss -0.711\n",
      "2025-01-15 16:47:42.928542: Pseudo dice [np.float32(0.8173)]\n",
      "2025-01-15 16:47:42.928584: Epoch time: 111.69 s\n",
      "2025-01-15 16:47:43.537419: \n",
      "2025-01-15 16:47:43.537694: Epoch 786\n",
      "2025-01-15 16:47:43.537836: Current learning rate: 0.0025\n",
      "2025-01-15 16:49:35.087253: train_loss -0.8539\n",
      "2025-01-15 16:49:35.087595: val_loss -0.6815\n",
      "2025-01-15 16:49:35.087734: Pseudo dice [np.float32(0.7902)]\n",
      "2025-01-15 16:49:35.087783: Epoch time: 111.55 s\n",
      "2025-01-15 16:49:36.082767: \n",
      "2025-01-15 16:49:36.082871: Epoch 787\n",
      "2025-01-15 16:49:36.082943: Current learning rate: 0.00249\n",
      "2025-01-15 16:51:27.585207: train_loss -0.8459\n",
      "2025-01-15 16:51:27.585606: val_loss -0.6476\n",
      "2025-01-15 16:51:27.585730: Pseudo dice [np.float32(0.7587)]\n",
      "2025-01-15 16:51:27.585780: Epoch time: 111.5 s\n",
      "2025-01-15 16:51:28.184027: \n",
      "2025-01-15 16:51:28.184441: Epoch 788\n",
      "2025-01-15 16:51:28.184531: Current learning rate: 0.00248\n",
      "2025-01-15 16:53:19.694412: train_loss -0.8473\n",
      "2025-01-15 16:53:19.694531: val_loss -0.6575\n",
      "2025-01-15 16:53:19.694563: Pseudo dice [np.float32(0.7531)]\n",
      "2025-01-15 16:53:19.694596: Epoch time: 111.51 s\n",
      "2025-01-15 16:53:20.297065: \n",
      "2025-01-15 16:53:20.297246: Epoch 789\n",
      "2025-01-15 16:53:20.297311: Current learning rate: 0.00247\n",
      "2025-01-15 16:55:11.750092: train_loss -0.8284\n",
      "2025-01-15 16:55:11.750286: val_loss -0.7655\n",
      "2025-01-15 16:55:11.750332: Pseudo dice [np.float32(0.8477)]\n",
      "2025-01-15 16:55:11.750373: Epoch time: 111.45 s\n",
      "2025-01-15 16:55:12.345228: \n",
      "2025-01-15 16:55:12.345594: Epoch 790\n",
      "2025-01-15 16:55:12.345678: Current learning rate: 0.00245\n",
      "2025-01-15 16:57:03.836919: train_loss -0.8593\n",
      "2025-01-15 16:57:03.837046: val_loss -0.7209\n",
      "2025-01-15 16:57:03.837079: Pseudo dice [np.float32(0.8039)]\n",
      "2025-01-15 16:57:03.837112: Epoch time: 111.49 s\n",
      "2025-01-15 16:57:04.434677: \n",
      "2025-01-15 16:57:04.435030: Epoch 791\n",
      "2025-01-15 16:57:04.435115: Current learning rate: 0.00244\n",
      "2025-01-15 16:58:55.885036: train_loss -0.8598\n",
      "2025-01-15 16:58:55.885249: val_loss -0.7595\n",
      "2025-01-15 16:58:55.885358: Pseudo dice [np.float32(0.8202)]\n",
      "2025-01-15 16:58:55.885401: Epoch time: 111.45 s\n",
      "2025-01-15 16:58:56.488956: \n",
      "2025-01-15 16:58:56.489332: Epoch 792\n",
      "2025-01-15 16:58:56.489488: Current learning rate: 0.00243\n",
      "2025-01-15 17:00:47.941408: train_loss -0.8496\n",
      "2025-01-15 17:00:47.941555: val_loss -0.699\n",
      "2025-01-15 17:00:47.941596: Pseudo dice [np.float32(0.7936)]\n",
      "2025-01-15 17:00:47.941638: Epoch time: 111.45 s\n",
      "2025-01-15 17:00:48.546213: \n",
      "2025-01-15 17:00:48.546500: Epoch 793\n",
      "2025-01-15 17:00:48.546665: Current learning rate: 0.00242\n",
      "2025-01-15 17:02:40.000481: train_loss -0.8436\n",
      "2025-01-15 17:02:40.000604: val_loss -0.7312\n",
      "2025-01-15 17:02:40.000636: Pseudo dice [np.float32(0.8198)]\n",
      "2025-01-15 17:02:40.000667: Epoch time: 111.45 s\n",
      "2025-01-15 17:02:40.598672: \n",
      "2025-01-15 17:02:40.598830: Epoch 794\n",
      "2025-01-15 17:02:40.598896: Current learning rate: 0.00241\n",
      "2025-01-15 17:04:32.093132: train_loss -0.8489\n",
      "2025-01-15 17:04:32.093265: val_loss -0.7965\n",
      "2025-01-15 17:04:32.093297: Pseudo dice [np.float32(0.843)]\n",
      "2025-01-15 17:04:32.093331: Epoch time: 111.5 s\n",
      "2025-01-15 17:04:32.702695: \n",
      "2025-01-15 17:04:32.703064: Epoch 795\n",
      "2025-01-15 17:04:32.703215: Current learning rate: 0.0024\n",
      "2025-01-15 17:06:24.192985: train_loss -0.854\n",
      "2025-01-15 17:06:24.193112: val_loss -0.7336\n",
      "2025-01-15 17:06:24.193475: Pseudo dice [np.float32(0.8225)]\n",
      "2025-01-15 17:06:24.193550: Epoch time: 111.49 s\n",
      "2025-01-15 17:06:24.812492: \n",
      "2025-01-15 17:06:24.812880: Epoch 796\n",
      "2025-01-15 17:06:24.813012: Current learning rate: 0.00239\n",
      "2025-01-15 17:08:16.353614: train_loss -0.8547\n",
      "2025-01-15 17:08:16.353747: val_loss -0.7705\n",
      "2025-01-15 17:08:16.353781: Pseudo dice [np.float32(0.8305)]\n",
      "2025-01-15 17:08:16.353818: Epoch time: 111.54 s\n",
      "2025-01-15 17:08:16.975193: \n",
      "2025-01-15 17:08:16.975291: Epoch 797\n",
      "2025-01-15 17:08:16.975355: Current learning rate: 0.00238\n",
      "2025-01-15 17:10:08.539983: train_loss -0.8632\n",
      "2025-01-15 17:10:08.540108: val_loss -0.6604\n",
      "2025-01-15 17:10:08.540142: Pseudo dice [np.float32(0.736)]\n",
      "2025-01-15 17:10:08.540175: Epoch time: 111.57 s\n",
      "2025-01-15 17:10:09.140446: \n",
      "2025-01-15 17:10:09.140790: Epoch 798\n",
      "2025-01-15 17:10:09.141004: Current learning rate: 0.00237\n",
      "2025-01-15 17:12:00.583491: train_loss -0.8487\n",
      "2025-01-15 17:12:00.583652: val_loss -0.7289\n",
      "2025-01-15 17:12:00.583692: Pseudo dice [np.float32(0.8148)]\n",
      "2025-01-15 17:12:00.583750: Epoch time: 111.44 s\n",
      "2025-01-15 17:12:01.186528: \n",
      "2025-01-15 17:12:01.186615: Epoch 799\n",
      "2025-01-15 17:12:01.186681: Current learning rate: 0.00236\n",
      "2025-01-15 17:13:52.656958: train_loss -0.8531\n",
      "2025-01-15 17:13:52.657279: val_loss -0.7773\n",
      "2025-01-15 17:13:52.657399: Pseudo dice [np.float32(0.829)]\n",
      "2025-01-15 17:13:52.657454: Epoch time: 111.47 s\n",
      "2025-01-15 17:13:53.483218: \n",
      "2025-01-15 17:13:53.483413: Epoch 800\n",
      "2025-01-15 17:13:53.483489: Current learning rate: 0.00235\n",
      "2025-01-15 17:15:44.993967: train_loss -0.862\n",
      "2025-01-15 17:15:44.994089: val_loss -0.649\n",
      "2025-01-15 17:15:44.994252: Pseudo dice [np.float32(0.7373)]\n",
      "2025-01-15 17:15:44.994329: Epoch time: 111.51 s\n",
      "2025-01-15 17:15:45.602941: \n",
      "2025-01-15 17:15:45.603277: Epoch 801\n",
      "2025-01-15 17:15:45.603437: Current learning rate: 0.00234\n",
      "2025-01-15 17:17:37.079144: train_loss -0.8654\n",
      "2025-01-15 17:17:37.079482: val_loss -0.7348\n",
      "2025-01-15 17:17:37.079746: Pseudo dice [np.float32(0.8127)]\n",
      "2025-01-15 17:17:37.079866: Epoch time: 111.48 s\n",
      "2025-01-15 17:17:37.693257: \n",
      "2025-01-15 17:17:37.693337: Epoch 802\n",
      "2025-01-15 17:17:37.693398: Current learning rate: 0.00233\n",
      "2025-01-15 17:19:29.175734: train_loss -0.8596\n",
      "2025-01-15 17:19:29.175861: val_loss -0.6912\n",
      "2025-01-15 17:19:29.175893: Pseudo dice [np.float32(0.8028)]\n",
      "2025-01-15 17:19:29.175927: Epoch time: 111.48 s\n",
      "2025-01-15 17:19:29.774086: \n",
      "2025-01-15 17:19:29.774487: Epoch 803\n",
      "2025-01-15 17:19:29.774559: Current learning rate: 0.00232\n",
      "2025-01-15 17:21:21.458260: train_loss -0.8536\n",
      "2025-01-15 17:21:21.458621: val_loss -0.6491\n",
      "2025-01-15 17:21:21.458729: Pseudo dice [np.float32(0.7271)]\n",
      "2025-01-15 17:21:21.458772: Epoch time: 111.68 s\n",
      "2025-01-15 17:21:22.455832: \n",
      "2025-01-15 17:21:22.455943: Epoch 804\n",
      "2025-01-15 17:21:22.456018: Current learning rate: 0.00231\n",
      "2025-01-15 17:23:13.972927: train_loss -0.853\n",
      "2025-01-15 17:23:13.973133: val_loss -0.7845\n",
      "2025-01-15 17:23:13.973180: Pseudo dice [np.float32(0.8548)]\n",
      "2025-01-15 17:23:13.973215: Epoch time: 111.52 s\n",
      "2025-01-15 17:23:14.580558: \n",
      "2025-01-15 17:23:14.580750: Epoch 805\n",
      "2025-01-15 17:23:14.580833: Current learning rate: 0.0023\n",
      "2025-01-15 17:25:06.150668: train_loss -0.8565\n",
      "2025-01-15 17:25:06.150793: val_loss -0.7388\n",
      "2025-01-15 17:25:06.150825: Pseudo dice [np.float32(0.802)]\n",
      "2025-01-15 17:25:06.150859: Epoch time: 111.57 s\n",
      "2025-01-15 17:25:06.751446: \n",
      "2025-01-15 17:25:06.751773: Epoch 806\n",
      "2025-01-15 17:25:06.751896: Current learning rate: 0.00229\n",
      "2025-01-15 17:26:58.313452: train_loss -0.8507\n",
      "2025-01-15 17:26:58.313653: val_loss -0.6932\n",
      "2025-01-15 17:26:58.313871: Pseudo dice [np.float32(0.8163)]\n",
      "2025-01-15 17:26:58.313976: Epoch time: 111.56 s\n",
      "2025-01-15 17:26:58.911922: \n",
      "2025-01-15 17:26:58.912170: Epoch 807\n",
      "2025-01-15 17:26:58.912239: Current learning rate: 0.00228\n",
      "2025-01-15 17:28:50.360381: train_loss -0.8515\n",
      "2025-01-15 17:28:50.360527: val_loss -0.7596\n",
      "2025-01-15 17:28:50.360561: Pseudo dice [np.float32(0.8176)]\n",
      "2025-01-15 17:28:50.360746: Epoch time: 111.45 s\n",
      "2025-01-15 17:28:50.969307: \n",
      "2025-01-15 17:28:50.969413: Epoch 808\n",
      "2025-01-15 17:28:50.969475: Current learning rate: 0.00226\n",
      "2025-01-15 17:30:42.454471: train_loss -0.8457\n",
      "2025-01-15 17:30:42.454595: val_loss -0.7024\n",
      "2025-01-15 17:30:42.454627: Pseudo dice [np.float32(0.7731)]\n",
      "2025-01-15 17:30:42.454658: Epoch time: 111.49 s\n",
      "2025-01-15 17:30:43.058748: \n",
      "2025-01-15 17:30:43.059001: Epoch 809\n",
      "2025-01-15 17:30:43.059072: Current learning rate: 0.00225\n",
      "2025-01-15 17:32:34.637124: train_loss -0.8568\n",
      "2025-01-15 17:32:34.637254: val_loss -0.7012\n",
      "2025-01-15 17:32:34.637286: Pseudo dice [np.float32(0.781)]\n",
      "2025-01-15 17:32:34.637319: Epoch time: 111.58 s\n",
      "2025-01-15 17:32:35.247873: \n",
      "2025-01-15 17:32:35.248244: Epoch 810\n",
      "2025-01-15 17:32:35.248325: Current learning rate: 0.00224\n",
      "2025-01-15 17:34:26.745016: train_loss -0.8525\n",
      "2025-01-15 17:34:26.745139: val_loss -0.6844\n",
      "2025-01-15 17:34:26.745169: Pseudo dice [np.float32(0.7743)]\n",
      "2025-01-15 17:34:26.745281: Epoch time: 111.5 s\n",
      "2025-01-15 17:34:27.349434: \n",
      "2025-01-15 17:34:27.349739: Epoch 811\n",
      "2025-01-15 17:34:27.349874: Current learning rate: 0.00223\n",
      "2025-01-15 17:36:18.809832: train_loss -0.8478\n",
      "2025-01-15 17:36:18.809969: val_loss -0.67\n",
      "2025-01-15 17:36:18.810002: Pseudo dice [np.float32(0.7655)]\n",
      "2025-01-15 17:36:18.810036: Epoch time: 111.46 s\n",
      "2025-01-15 17:36:19.412108: \n",
      "2025-01-15 17:36:19.412403: Epoch 812\n",
      "2025-01-15 17:36:19.412498: Current learning rate: 0.00222\n",
      "2025-01-15 17:38:10.920395: train_loss -0.8528\n",
      "2025-01-15 17:38:10.920526: val_loss -0.6898\n",
      "2025-01-15 17:38:10.920560: Pseudo dice [np.float32(0.7329)]\n",
      "2025-01-15 17:38:10.920594: Epoch time: 111.51 s\n",
      "2025-01-15 17:38:11.522687: \n",
      "2025-01-15 17:38:11.523107: Epoch 813\n",
      "2025-01-15 17:38:11.523303: Current learning rate: 0.00221\n",
      "2025-01-15 17:40:03.037980: train_loss -0.8394\n",
      "2025-01-15 17:40:03.038210: val_loss -0.6609\n",
      "2025-01-15 17:40:03.038254: Pseudo dice [np.float32(0.5896)]\n",
      "2025-01-15 17:40:03.038291: Epoch time: 111.52 s\n",
      "2025-01-15 17:40:03.641277: \n",
      "2025-01-15 17:40:03.641364: Epoch 814\n",
      "2025-01-15 17:40:03.641427: Current learning rate: 0.0022\n",
      "2025-01-15 17:41:55.133933: train_loss -0.8344\n",
      "2025-01-15 17:41:55.134063: val_loss -0.7122\n",
      "2025-01-15 17:41:55.134097: Pseudo dice [np.float32(0.777)]\n",
      "2025-01-15 17:41:55.134141: Epoch time: 111.49 s\n",
      "2025-01-15 17:41:55.746986: \n",
      "2025-01-15 17:41:55.747311: Epoch 815\n",
      "2025-01-15 17:41:55.747519: Current learning rate: 0.00219\n",
      "2025-01-15 17:43:47.263291: train_loss -0.8375\n",
      "2025-01-15 17:43:47.263413: val_loss -0.7762\n",
      "2025-01-15 17:43:47.263445: Pseudo dice [np.float32(0.8388)]\n",
      "2025-01-15 17:43:47.263479: Epoch time: 111.52 s\n",
      "2025-01-15 17:43:47.864697: \n",
      "2025-01-15 17:43:47.865005: Epoch 816\n",
      "2025-01-15 17:43:47.865318: Current learning rate: 0.00218\n",
      "2025-01-15 17:45:39.411938: train_loss -0.8624\n",
      "2025-01-15 17:45:39.412066: val_loss -0.7026\n",
      "2025-01-15 17:45:39.412099: Pseudo dice [np.float32(0.7797)]\n",
      "2025-01-15 17:45:39.412131: Epoch time: 111.55 s\n",
      "2025-01-15 17:45:40.019567: \n",
      "2025-01-15 17:45:40.019934: Epoch 817\n",
      "2025-01-15 17:45:40.020076: Current learning rate: 0.00217\n",
      "2025-01-15 17:47:31.516117: train_loss -0.8421\n",
      "2025-01-15 17:47:31.516531: val_loss -0.6635\n",
      "2025-01-15 17:47:31.516604: Pseudo dice [np.float32(0.7556)]\n",
      "2025-01-15 17:47:31.516647: Epoch time: 111.5 s\n",
      "2025-01-15 17:47:32.123259: \n",
      "2025-01-15 17:47:32.123347: Epoch 818\n",
      "2025-01-15 17:47:32.123408: Current learning rate: 0.00216\n",
      "2025-01-15 17:49:23.570427: train_loss -0.8581\n",
      "2025-01-15 17:49:23.570569: val_loss -0.7161\n",
      "2025-01-15 17:49:23.570602: Pseudo dice [np.float32(0.8166)]\n",
      "2025-01-15 17:49:23.570636: Epoch time: 111.45 s\n",
      "2025-01-15 17:49:24.176475: \n",
      "2025-01-15 17:49:24.176692: Epoch 819\n",
      "2025-01-15 17:49:24.176828: Current learning rate: 0.00215\n",
      "2025-01-15 17:51:15.699552: train_loss -0.8557\n",
      "2025-01-15 17:51:15.699716: val_loss -0.7008\n",
      "2025-01-15 17:51:15.699750: Pseudo dice [np.float32(0.7659)]\n",
      "2025-01-15 17:51:15.699783: Epoch time: 111.52 s\n",
      "2025-01-15 17:51:16.281790: \n",
      "2025-01-15 17:51:16.281906: Epoch 820\n",
      "2025-01-15 17:51:16.281991: Current learning rate: 0.00214\n",
      "2025-01-15 17:53:07.818325: train_loss -0.86\n",
      "2025-01-15 17:53:07.818510: val_loss -0.6977\n",
      "2025-01-15 17:53:07.819059: Pseudo dice [np.float32(0.776)]\n",
      "2025-01-15 17:53:07.819112: Epoch time: 111.54 s\n",
      "2025-01-15 17:53:08.806395: \n",
      "2025-01-15 17:53:08.806608: Epoch 821\n",
      "2025-01-15 17:53:08.806776: Current learning rate: 0.00213\n",
      "2025-01-15 17:55:00.278524: train_loss -0.8434\n",
      "2025-01-15 17:55:00.278896: val_loss -0.7235\n",
      "2025-01-15 17:55:00.278935: Pseudo dice [np.float32(0.7754)]\n",
      "2025-01-15 17:55:00.278969: Epoch time: 111.47 s\n",
      "2025-01-15 17:55:00.858345: \n",
      "2025-01-15 17:55:00.858448: Epoch 822\n",
      "2025-01-15 17:55:00.858510: Current learning rate: 0.00212\n",
      "2025-01-15 17:56:52.356937: train_loss -0.8049\n",
      "2025-01-15 17:56:52.357172: val_loss -0.7134\n",
      "2025-01-15 17:56:52.357214: Pseudo dice [np.float32(0.6477)]\n",
      "2025-01-15 17:56:52.357247: Epoch time: 111.5 s\n",
      "2025-01-15 17:56:52.933784: \n",
      "2025-01-15 17:56:52.934121: Epoch 823\n",
      "2025-01-15 17:56:52.934198: Current learning rate: 0.0021\n",
      "2025-01-15 17:58:44.383636: train_loss -0.8261\n",
      "2025-01-15 17:58:44.383769: val_loss -0.7223\n",
      "2025-01-15 17:58:44.383892: Pseudo dice [np.float32(0.8281)]\n",
      "2025-01-15 17:58:44.383973: Epoch time: 111.45 s\n",
      "2025-01-15 17:58:44.969736: \n",
      "2025-01-15 17:58:44.970087: Epoch 824\n",
      "2025-01-15 17:58:44.970317: Current learning rate: 0.00209\n",
      "2025-01-15 18:00:36.541895: train_loss -0.8422\n",
      "2025-01-15 18:00:36.542147: val_loss -0.6873\n",
      "2025-01-15 18:00:36.542202: Pseudo dice [np.float32(0.7799)]\n",
      "2025-01-15 18:00:36.542239: Epoch time: 111.57 s\n",
      "2025-01-15 18:00:37.125847: \n",
      "2025-01-15 18:00:37.126210: Epoch 825\n",
      "2025-01-15 18:00:37.126305: Current learning rate: 0.00208\n",
      "2025-01-15 18:02:28.822689: train_loss -0.8551\n",
      "2025-01-15 18:02:28.822873: val_loss -0.6718\n",
      "2025-01-15 18:02:28.822908: Pseudo dice [np.float32(0.7715)]\n",
      "2025-01-15 18:02:28.822940: Epoch time: 111.7 s\n",
      "2025-01-15 18:02:29.402934: \n",
      "2025-01-15 18:02:29.403320: Epoch 826\n",
      "2025-01-15 18:02:29.403410: Current learning rate: 0.00207\n",
      "2025-01-15 18:04:20.917790: train_loss -0.8508\n",
      "2025-01-15 18:04:20.917983: val_loss -0.6696\n",
      "2025-01-15 18:04:20.918021: Pseudo dice [np.float32(0.7837)]\n",
      "2025-01-15 18:04:20.918066: Epoch time: 111.52 s\n",
      "2025-01-15 18:04:21.497242: \n",
      "2025-01-15 18:04:21.497506: Epoch 827\n",
      "2025-01-15 18:04:21.497657: Current learning rate: 0.00206\n",
      "2025-01-15 18:06:12.967862: train_loss -0.8547\n",
      "2025-01-15 18:06:12.968304: val_loss -0.7215\n",
      "2025-01-15 18:06:12.968467: Pseudo dice [np.float32(0.7874)]\n",
      "2025-01-15 18:06:12.968516: Epoch time: 111.47 s\n",
      "2025-01-15 18:06:13.552644: \n",
      "2025-01-15 18:06:13.552811: Epoch 828\n",
      "2025-01-15 18:06:13.552879: Current learning rate: 0.00205\n",
      "2025-01-15 18:08:05.297422: train_loss -0.8519\n",
      "2025-01-15 18:08:05.297788: val_loss -0.7122\n",
      "2025-01-15 18:08:05.297831: Pseudo dice [np.float32(0.788)]\n",
      "2025-01-15 18:08:05.297867: Epoch time: 111.75 s\n",
      "2025-01-15 18:08:05.906836: \n",
      "2025-01-15 18:08:05.907179: Epoch 829\n",
      "2025-01-15 18:08:05.907390: Current learning rate: 0.00204\n",
      "2025-01-15 18:09:57.654312: train_loss -0.8499\n",
      "2025-01-15 18:09:57.654592: val_loss -0.6778\n",
      "2025-01-15 18:09:57.654636: Pseudo dice [np.float32(0.7935)]\n",
      "2025-01-15 18:09:57.654673: Epoch time: 111.75 s\n",
      "2025-01-15 18:09:58.258355: \n",
      "2025-01-15 18:09:58.258512: Epoch 830\n",
      "2025-01-15 18:09:58.258629: Current learning rate: 0.00203\n",
      "2025-01-15 18:11:49.996876: train_loss -0.8557\n",
      "2025-01-15 18:11:49.996999: val_loss -0.7198\n",
      "2025-01-15 18:11:49.997031: Pseudo dice [np.float32(0.836)]\n",
      "2025-01-15 18:11:49.997066: Epoch time: 111.74 s\n",
      "2025-01-15 18:11:50.574169: \n",
      "2025-01-15 18:11:50.574552: Epoch 831\n",
      "2025-01-15 18:11:50.574740: Current learning rate: 0.00202\n",
      "2025-01-15 18:13:42.224362: train_loss -0.8665\n",
      "2025-01-15 18:13:42.224583: val_loss -0.6961\n",
      "2025-01-15 18:13:42.224627: Pseudo dice [np.float32(0.8174)]\n",
      "2025-01-15 18:13:42.224661: Epoch time: 111.65 s\n",
      "2025-01-15 18:13:42.808373: \n",
      "2025-01-15 18:13:42.808573: Epoch 832\n",
      "2025-01-15 18:13:42.808671: Current learning rate: 0.00201\n",
      "2025-01-15 18:15:34.336577: train_loss -0.8573\n",
      "2025-01-15 18:15:34.336703: val_loss -0.7603\n",
      "2025-01-15 18:15:34.336735: Pseudo dice [np.float32(0.7757)]\n",
      "2025-01-15 18:15:34.336767: Epoch time: 111.53 s\n",
      "2025-01-15 18:15:34.920025: \n",
      "2025-01-15 18:15:34.920270: Epoch 833\n",
      "2025-01-15 18:15:34.920341: Current learning rate: 0.002\n",
      "2025-01-15 18:17:26.499784: train_loss -0.855\n",
      "2025-01-15 18:17:26.500006: val_loss -0.6903\n",
      "2025-01-15 18:17:26.500049: Pseudo dice [np.float32(0.7684)]\n",
      "2025-01-15 18:17:26.500085: Epoch time: 111.58 s\n",
      "2025-01-15 18:17:27.085447: \n",
      "2025-01-15 18:17:27.085596: Epoch 834\n",
      "2025-01-15 18:17:27.085745: Current learning rate: 0.00199\n",
      "2025-01-15 18:19:18.730628: train_loss -0.8535\n",
      "2025-01-15 18:19:18.730821: val_loss -0.7626\n",
      "2025-01-15 18:19:18.730856: Pseudo dice [np.float32(0.8374)]\n",
      "2025-01-15 18:19:18.730889: Epoch time: 111.65 s\n",
      "2025-01-15 18:19:19.312563: \n",
      "2025-01-15 18:19:19.312651: Epoch 835\n",
      "2025-01-15 18:19:19.312713: Current learning rate: 0.00198\n",
      "2025-01-15 18:21:10.873884: train_loss -0.851\n",
      "2025-01-15 18:21:10.874013: val_loss -0.7199\n",
      "2025-01-15 18:21:10.874046: Pseudo dice [np.float32(0.8192)]\n",
      "2025-01-15 18:21:10.874078: Epoch time: 111.56 s\n",
      "2025-01-15 18:21:11.445827: \n",
      "2025-01-15 18:21:11.445920: Epoch 836\n",
      "2025-01-15 18:21:11.445987: Current learning rate: 0.00196\n",
      "2025-01-15 18:23:02.959964: train_loss -0.8482\n",
      "2025-01-15 18:23:02.960090: val_loss -0.726\n",
      "2025-01-15 18:23:02.960123: Pseudo dice [np.float32(0.818)]\n",
      "2025-01-15 18:23:02.960157: Epoch time: 111.51 s\n",
      "2025-01-15 18:23:03.538898: \n",
      "2025-01-15 18:23:03.539127: Epoch 837\n",
      "2025-01-15 18:23:03.539285: Current learning rate: 0.00195\n",
      "2025-01-15 18:24:55.214006: train_loss -0.8497\n",
      "2025-01-15 18:24:55.214147: val_loss -0.729\n",
      "2025-01-15 18:24:55.214232: Pseudo dice [np.float32(0.768)]\n",
      "2025-01-15 18:24:55.214337: Epoch time: 111.68 s\n",
      "2025-01-15 18:24:55.793449: \n",
      "2025-01-15 18:24:55.793636: Epoch 838\n",
      "2025-01-15 18:24:55.793710: Current learning rate: 0.00194\n",
      "2025-01-15 18:26:47.341330: train_loss -0.8589\n",
      "2025-01-15 18:26:47.341527: val_loss -0.7584\n",
      "2025-01-15 18:26:47.341560: Pseudo dice [np.float32(0.8287)]\n",
      "2025-01-15 18:26:47.341593: Epoch time: 111.55 s\n",
      "2025-01-15 18:26:47.917079: \n",
      "2025-01-15 18:26:47.917239: Epoch 839\n",
      "2025-01-15 18:26:47.917315: Current learning rate: 0.00193\n",
      "2025-01-15 18:28:39.725965: train_loss -0.8725\n",
      "2025-01-15 18:28:39.726172: val_loss -0.7486\n",
      "2025-01-15 18:28:39.726310: Pseudo dice [np.float32(0.8387)]\n",
      "2025-01-15 18:28:39.726376: Epoch time: 111.81 s\n",
      "2025-01-15 18:28:40.699581: \n",
      "2025-01-15 18:28:40.699693: Epoch 840\n",
      "2025-01-15 18:28:40.699754: Current learning rate: 0.00192\n",
      "2025-01-15 18:30:32.218692: train_loss -0.8753\n",
      "2025-01-15 18:30:32.218860: val_loss -0.7287\n",
      "2025-01-15 18:30:32.218930: Pseudo dice [np.float32(0.8313)]\n",
      "2025-01-15 18:30:32.218972: Epoch time: 111.52 s\n",
      "2025-01-15 18:30:32.800260: \n",
      "2025-01-15 18:30:32.800360: Epoch 841\n",
      "2025-01-15 18:30:32.800421: Current learning rate: 0.00191\n",
      "2025-01-15 18:32:24.468303: train_loss -0.8778\n",
      "2025-01-15 18:32:24.468735: val_loss -0.7263\n",
      "2025-01-15 18:32:24.468805: Pseudo dice [np.float32(0.7748)]\n",
      "2025-01-15 18:32:24.468847: Epoch time: 111.67 s\n",
      "2025-01-15 18:32:25.057372: \n",
      "2025-01-15 18:32:25.057610: Epoch 842\n",
      "2025-01-15 18:32:25.057753: Current learning rate: 0.0019\n",
      "2025-01-15 18:34:16.770591: train_loss -0.8654\n",
      "2025-01-15 18:34:16.770723: val_loss -0.7199\n",
      "2025-01-15 18:34:16.770756: Pseudo dice [np.float32(0.8294)]\n",
      "2025-01-15 18:34:16.770789: Epoch time: 111.71 s\n",
      "2025-01-15 18:34:17.355054: \n",
      "2025-01-15 18:34:17.355449: Epoch 843\n",
      "2025-01-15 18:34:17.355544: Current learning rate: 0.00189\n",
      "2025-01-15 18:36:09.063792: train_loss -0.8452\n",
      "2025-01-15 18:36:09.064170: val_loss -0.7856\n",
      "2025-01-15 18:36:09.064286: Pseudo dice [np.float32(0.8226)]\n",
      "2025-01-15 18:36:09.064331: Epoch time: 111.71 s\n",
      "2025-01-15 18:36:09.649730: \n",
      "2025-01-15 18:36:09.649830: Epoch 844\n",
      "2025-01-15 18:36:09.649891: Current learning rate: 0.00188\n",
      "2025-01-15 18:38:01.370986: train_loss -0.8569\n",
      "2025-01-15 18:38:01.371130: val_loss -0.714\n",
      "2025-01-15 18:38:01.371163: Pseudo dice [np.float32(0.8083)]\n",
      "2025-01-15 18:38:01.371196: Epoch time: 111.72 s\n",
      "2025-01-15 18:38:01.947941: \n",
      "2025-01-15 18:38:01.948044: Epoch 845\n",
      "2025-01-15 18:38:01.948104: Current learning rate: 0.00187\n",
      "2025-01-15 18:39:53.516669: train_loss -0.8578\n",
      "2025-01-15 18:39:53.516877: val_loss -0.8052\n",
      "2025-01-15 18:39:53.516911: Pseudo dice [np.float32(0.8585)]\n",
      "2025-01-15 18:39:53.516944: Epoch time: 111.57 s\n",
      "2025-01-15 18:39:54.094968: \n",
      "2025-01-15 18:39:54.095405: Epoch 846\n",
      "2025-01-15 18:39:54.095499: Current learning rate: 0.00186\n",
      "2025-01-15 18:41:45.661015: train_loss -0.8577\n",
      "2025-01-15 18:41:45.661380: val_loss -0.7378\n",
      "2025-01-15 18:41:45.661423: Pseudo dice [np.float32(0.8296)]\n",
      "2025-01-15 18:41:45.661496: Epoch time: 111.57 s\n",
      "2025-01-15 18:41:46.246325: \n",
      "2025-01-15 18:41:46.246415: Epoch 847\n",
      "2025-01-15 18:41:46.246477: Current learning rate: 0.00185\n",
      "2025-01-15 18:43:37.924094: train_loss -0.8662\n",
      "2025-01-15 18:43:37.924217: val_loss -0.7418\n",
      "2025-01-15 18:43:37.924249: Pseudo dice [np.float32(0.8192)]\n",
      "2025-01-15 18:43:37.924281: Epoch time: 111.68 s\n",
      "2025-01-15 18:43:38.511738: \n",
      "2025-01-15 18:43:38.512141: Epoch 848\n",
      "2025-01-15 18:43:38.512230: Current learning rate: 0.00184\n",
      "2025-01-15 18:45:30.046363: train_loss -0.8623\n",
      "2025-01-15 18:45:30.046579: val_loss -0.7278\n",
      "2025-01-15 18:45:30.046621: Pseudo dice [np.float32(0.8261)]\n",
      "2025-01-15 18:45:30.046657: Epoch time: 111.54 s\n",
      "2025-01-15 18:45:30.628559: \n",
      "2025-01-15 18:45:30.628729: Epoch 849\n",
      "2025-01-15 18:45:30.628807: Current learning rate: 0.00182\n",
      "2025-01-15 18:47:22.341076: train_loss -0.8716\n",
      "2025-01-15 18:47:22.341369: val_loss -0.6833\n",
      "2025-01-15 18:47:22.341413: Pseudo dice [np.float32(0.7941)]\n",
      "2025-01-15 18:47:22.341454: Epoch time: 111.71 s\n",
      "2025-01-15 18:47:23.147961: \n",
      "2025-01-15 18:47:23.148375: Epoch 850\n",
      "2025-01-15 18:47:23.148510: Current learning rate: 0.00181\n",
      "2025-01-15 18:49:14.690673: train_loss -0.8698\n",
      "2025-01-15 18:49:14.690790: val_loss -0.7109\n",
      "2025-01-15 18:49:14.690821: Pseudo dice [np.float32(0.809)]\n",
      "2025-01-15 18:49:14.690855: Epoch time: 111.54 s\n",
      "2025-01-15 18:49:15.260695: \n",
      "2025-01-15 18:49:15.260785: Epoch 851\n",
      "2025-01-15 18:49:15.260849: Current learning rate: 0.0018\n",
      "2025-01-15 18:51:06.966947: train_loss -0.8622\n",
      "2025-01-15 18:51:06.967075: val_loss -0.7534\n",
      "2025-01-15 18:51:06.967107: Pseudo dice [np.float32(0.8388)]\n",
      "2025-01-15 18:51:06.967139: Epoch time: 111.71 s\n",
      "2025-01-15 18:51:07.537976: \n",
      "2025-01-15 18:51:07.538063: Epoch 852\n",
      "2025-01-15 18:51:07.538124: Current learning rate: 0.00179\n",
      "2025-01-15 18:52:59.090856: train_loss -0.8675\n",
      "2025-01-15 18:52:59.091046: val_loss -0.7124\n",
      "2025-01-15 18:52:59.091152: Pseudo dice [np.float32(0.8098)]\n",
      "2025-01-15 18:52:59.091200: Epoch time: 111.55 s\n",
      "2025-01-15 18:52:59.666955: \n",
      "2025-01-15 18:52:59.667303: Epoch 853\n",
      "2025-01-15 18:52:59.667391: Current learning rate: 0.00178\n",
      "2025-01-15 18:54:51.156546: train_loss -0.8607\n",
      "2025-01-15 18:54:51.156716: val_loss -0.7375\n",
      "2025-01-15 18:54:51.156822: Pseudo dice [np.float32(0.8165)]\n",
      "2025-01-15 18:54:51.156880: Epoch time: 111.49 s\n",
      "2025-01-15 18:54:51.728799: \n",
      "2025-01-15 18:54:51.728941: Epoch 854\n",
      "2025-01-15 18:54:51.729011: Current learning rate: 0.00177\n",
      "2025-01-15 18:56:43.421455: train_loss -0.8582\n",
      "2025-01-15 18:56:43.421598: val_loss -0.7179\n",
      "2025-01-15 18:56:43.421630: Pseudo dice [np.float32(0.8367)]\n",
      "2025-01-15 18:56:43.421664: Epoch time: 111.69 s\n",
      "2025-01-15 18:56:43.995317: \n",
      "2025-01-15 18:56:43.995642: Epoch 855\n",
      "2025-01-15 18:56:43.995811: Current learning rate: 0.00176\n",
      "2025-01-15 18:58:35.737363: train_loss -0.8668\n",
      "2025-01-15 18:58:35.737483: val_loss -0.7711\n",
      "2025-01-15 18:58:35.737514: Pseudo dice [np.float32(0.8426)]\n",
      "2025-01-15 18:58:35.737549: Epoch time: 111.74 s\n",
      "2025-01-15 18:58:36.312857: \n",
      "2025-01-15 18:58:36.312946: Epoch 856\n",
      "2025-01-15 18:58:36.313008: Current learning rate: 0.00175\n",
      "2025-01-15 19:00:27.849417: train_loss -0.8637\n",
      "2025-01-15 19:00:27.849537: val_loss -0.7379\n",
      "2025-01-15 19:00:27.849569: Pseudo dice [np.float32(0.8051)]\n",
      "2025-01-15 19:00:27.849602: Epoch time: 111.54 s\n",
      "2025-01-15 19:00:28.417251: \n",
      "2025-01-15 19:00:28.417629: Epoch 857\n",
      "2025-01-15 19:00:28.417693: Current learning rate: 0.00174\n",
      "2025-01-15 19:02:19.982246: train_loss -0.8641\n",
      "2025-01-15 19:02:19.982385: val_loss -0.7651\n",
      "2025-01-15 19:02:19.982417: Pseudo dice [np.float32(0.8079)]\n",
      "2025-01-15 19:02:19.982450: Epoch time: 111.57 s\n",
      "2025-01-15 19:02:20.554398: \n",
      "2025-01-15 19:02:20.554533: Epoch 858\n",
      "2025-01-15 19:02:20.554605: Current learning rate: 0.00173\n",
      "2025-01-15 19:04:12.334197: train_loss -0.8504\n",
      "2025-01-15 19:04:12.334316: val_loss -0.7322\n",
      "2025-01-15 19:04:12.334347: Pseudo dice [np.float32(0.8099)]\n",
      "2025-01-15 19:04:12.334380: Epoch time: 111.78 s\n",
      "2025-01-15 19:04:13.335291: \n",
      "2025-01-15 19:04:13.335468: Epoch 859\n",
      "2025-01-15 19:04:13.335618: Current learning rate: 0.00172\n",
      "2025-01-15 19:06:05.019372: train_loss -0.8596\n",
      "2025-01-15 19:06:05.019500: val_loss -0.7511\n",
      "2025-01-15 19:06:05.019532: Pseudo dice [np.float32(0.825)]\n",
      "2025-01-15 19:06:05.019570: Epoch time: 111.68 s\n",
      "2025-01-15 19:06:05.586118: \n",
      "2025-01-15 19:06:05.586487: Epoch 860\n",
      "2025-01-15 19:06:05.586628: Current learning rate: 0.0017\n",
      "2025-01-15 19:07:57.314598: train_loss -0.8508\n",
      "2025-01-15 19:07:57.314723: val_loss -0.7445\n",
      "2025-01-15 19:07:57.314754: Pseudo dice [np.float32(0.8084)]\n",
      "2025-01-15 19:07:57.314787: Epoch time: 111.73 s\n",
      "2025-01-15 19:07:57.887533: \n",
      "2025-01-15 19:07:57.887632: Epoch 861\n",
      "2025-01-15 19:07:57.887693: Current learning rate: 0.00169\n",
      "2025-01-15 19:09:49.674033: train_loss -0.8569\n",
      "2025-01-15 19:09:49.674170: val_loss -0.7728\n",
      "2025-01-15 19:09:49.674235: Pseudo dice [np.float32(0.8244)]\n",
      "2025-01-15 19:09:49.674270: Epoch time: 111.79 s\n",
      "2025-01-15 19:09:50.267612: \n",
      "2025-01-15 19:09:50.267979: Epoch 862\n",
      "2025-01-15 19:09:50.268079: Current learning rate: 0.00168\n",
      "2025-01-15 19:11:41.956790: train_loss -0.8589\n",
      "2025-01-15 19:11:41.957121: val_loss -0.6932\n",
      "2025-01-15 19:11:41.957264: Pseudo dice [np.float32(0.7756)]\n",
      "2025-01-15 19:11:41.957310: Epoch time: 111.69 s\n",
      "2025-01-15 19:11:42.564973: \n",
      "2025-01-15 19:11:42.565287: Epoch 863\n",
      "2025-01-15 19:11:42.565389: Current learning rate: 0.00167\n",
      "2025-01-15 19:13:34.357116: train_loss -0.8504\n",
      "2025-01-15 19:13:34.357249: val_loss -0.6939\n",
      "2025-01-15 19:13:34.357280: Pseudo dice [np.float32(0.8004)]\n",
      "2025-01-15 19:13:34.357314: Epoch time: 111.79 s\n",
      "2025-01-15 19:13:34.931754: \n",
      "2025-01-15 19:13:34.931913: Epoch 864\n",
      "2025-01-15 19:13:34.931986: Current learning rate: 0.00166\n",
      "2025-01-15 19:15:26.640316: train_loss -0.8605\n",
      "2025-01-15 19:15:26.640445: val_loss -0.7553\n",
      "2025-01-15 19:15:26.640478: Pseudo dice [np.float32(0.8117)]\n",
      "2025-01-15 19:15:26.640511: Epoch time: 111.71 s\n",
      "2025-01-15 19:15:27.214970: \n",
      "2025-01-15 19:15:27.215065: Epoch 865\n",
      "2025-01-15 19:15:27.215127: Current learning rate: 0.00165\n",
      "2025-01-15 19:17:18.925560: train_loss -0.8698\n",
      "2025-01-15 19:17:18.925750: val_loss -0.6744\n",
      "2025-01-15 19:17:18.925782: Pseudo dice [np.float32(0.8145)]\n",
      "2025-01-15 19:17:18.925813: Epoch time: 111.71 s\n",
      "2025-01-15 19:17:19.502934: \n",
      "2025-01-15 19:17:19.503021: Epoch 866\n",
      "2025-01-15 19:17:19.503083: Current learning rate: 0.00164\n",
      "2025-01-15 19:19:11.218228: train_loss -0.8721\n",
      "2025-01-15 19:19:11.218344: val_loss -0.6959\n",
      "2025-01-15 19:19:11.218373: Pseudo dice [np.float32(0.7794)]\n",
      "2025-01-15 19:19:11.218404: Epoch time: 111.72 s\n",
      "2025-01-15 19:19:11.795135: \n",
      "2025-01-15 19:19:11.795501: Epoch 867\n",
      "2025-01-15 19:19:11.795574: Current learning rate: 0.00163\n",
      "2025-01-15 19:21:03.558548: train_loss -0.8616\n",
      "2025-01-15 19:21:03.558807: val_loss -0.7666\n",
      "2025-01-15 19:21:03.558865: Pseudo dice [np.float32(0.822)]\n",
      "2025-01-15 19:21:03.558914: Epoch time: 111.76 s\n",
      "2025-01-15 19:21:04.128878: \n",
      "2025-01-15 19:21:04.129239: Epoch 868\n",
      "2025-01-15 19:21:04.129312: Current learning rate: 0.00162\n",
      "2025-01-15 19:22:55.754281: train_loss -0.8495\n",
      "2025-01-15 19:22:55.754403: val_loss -0.7708\n",
      "2025-01-15 19:22:55.754434: Pseudo dice [np.float32(0.824)]\n",
      "2025-01-15 19:22:55.754467: Epoch time: 111.63 s\n",
      "2025-01-15 19:22:56.336261: \n",
      "2025-01-15 19:22:56.336347: Epoch 869\n",
      "2025-01-15 19:22:56.336406: Current learning rate: 0.00161\n",
      "2025-01-15 19:24:47.880144: train_loss -0.8604\n",
      "2025-01-15 19:24:47.880265: val_loss -0.76\n",
      "2025-01-15 19:24:47.880298: Pseudo dice [np.float32(0.8391)]\n",
      "2025-01-15 19:24:47.880335: Epoch time: 111.54 s\n",
      "2025-01-15 19:24:48.451969: \n",
      "2025-01-15 19:24:48.452340: Epoch 870\n",
      "2025-01-15 19:24:48.452433: Current learning rate: 0.00159\n",
      "2025-01-15 19:26:40.190662: train_loss -0.8598\n",
      "2025-01-15 19:26:40.190793: val_loss -0.7539\n",
      "2025-01-15 19:26:40.190849: Pseudo dice [np.float32(0.8225)]\n",
      "2025-01-15 19:26:40.191035: Epoch time: 111.74 s\n",
      "2025-01-15 19:26:40.763131: \n",
      "2025-01-15 19:26:40.763218: Epoch 871\n",
      "2025-01-15 19:26:40.763279: Current learning rate: 0.00158\n",
      "2025-01-15 19:28:32.486856: train_loss -0.8496\n",
      "2025-01-15 19:28:32.487273: val_loss -0.7436\n",
      "2025-01-15 19:28:32.487318: Pseudo dice [np.float32(0.8099)]\n",
      "2025-01-15 19:28:32.487394: Epoch time: 111.72 s\n",
      "2025-01-15 19:28:33.060282: \n",
      "2025-01-15 19:28:33.060364: Epoch 872\n",
      "2025-01-15 19:28:33.060425: Current learning rate: 0.00157\n",
      "2025-01-15 19:30:26.793763: train_loss -0.8575\n",
      "2025-01-15 19:30:26.793946: val_loss -0.7337\n",
      "2025-01-15 19:30:26.794115: Pseudo dice [np.float32(0.8072)]\n",
      "2025-01-15 19:30:26.794283: Epoch time: 113.73 s\n",
      "2025-01-15 19:30:27.366371: \n",
      "2025-01-15 19:30:27.366581: Epoch 873\n",
      "2025-01-15 19:30:27.366713: Current learning rate: 0.00156\n",
      "2025-01-15 19:32:19.361837: train_loss -0.8661\n",
      "2025-01-15 19:32:19.362040: val_loss -0.6008\n",
      "2025-01-15 19:32:19.362084: Pseudo dice [np.float32(0.7298)]\n",
      "2025-01-15 19:32:19.362220: Epoch time: 112.0 s\n",
      "2025-01-15 19:32:19.936162: \n",
      "2025-01-15 19:32:19.936539: Epoch 874\n",
      "2025-01-15 19:32:19.936615: Current learning rate: 0.00155\n",
      "2025-01-15 19:34:12.190599: train_loss -0.8701\n",
      "2025-01-15 19:34:12.190731: val_loss -0.7519\n",
      "2025-01-15 19:34:12.190764: Pseudo dice [np.float32(0.8198)]\n",
      "2025-01-15 19:34:12.190797: Epoch time: 112.25 s\n",
      "2025-01-15 19:34:12.791249: \n",
      "2025-01-15 19:34:12.791391: Epoch 875\n",
      "2025-01-15 19:34:12.791491: Current learning rate: 0.00154\n",
      "2025-01-15 19:36:04.845045: train_loss -0.8593\n",
      "2025-01-15 19:36:04.845179: val_loss -0.7396\n",
      "2025-01-15 19:36:04.845212: Pseudo dice [np.float32(0.8039)]\n",
      "2025-01-15 19:36:04.845245: Epoch time: 112.05 s\n",
      "2025-01-15 19:36:05.422415: \n",
      "2025-01-15 19:36:05.422607: Epoch 876\n",
      "2025-01-15 19:36:05.422704: Current learning rate: 0.00153\n",
      "2025-01-15 19:37:57.569214: train_loss -0.8687\n",
      "2025-01-15 19:37:57.569425: val_loss -0.7229\n",
      "2025-01-15 19:37:57.569562: Pseudo dice [np.float32(0.8175)]\n",
      "2025-01-15 19:37:57.569614: Epoch time: 112.15 s\n",
      "2025-01-15 19:37:58.141680: \n",
      "2025-01-15 19:37:58.141973: Epoch 877\n",
      "2025-01-15 19:37:58.142094: Current learning rate: 0.00152\n",
      "2025-01-15 19:39:50.361406: train_loss -0.8638\n",
      "2025-01-15 19:39:50.361570: val_loss -0.7074\n",
      "2025-01-15 19:39:50.361696: Pseudo dice [np.float32(0.7993)]\n",
      "2025-01-15 19:39:50.361775: Epoch time: 112.22 s\n",
      "2025-01-15 19:39:51.324629: \n",
      "2025-01-15 19:39:51.324813: Epoch 878\n",
      "2025-01-15 19:39:51.324884: Current learning rate: 0.00151\n",
      "2025-01-15 19:41:43.054340: train_loss -0.8572\n",
      "2025-01-15 19:41:43.054466: val_loss -0.7304\n",
      "2025-01-15 19:41:43.054498: Pseudo dice [np.float32(0.8267)]\n",
      "2025-01-15 19:41:43.054530: Epoch time: 111.73 s\n",
      "2025-01-15 19:41:43.630417: \n",
      "2025-01-15 19:41:43.630509: Epoch 879\n",
      "2025-01-15 19:41:43.630575: Current learning rate: 0.00149\n",
      "2025-01-15 19:43:35.334049: train_loss -0.8655\n",
      "2025-01-15 19:43:35.334195: val_loss -0.7548\n",
      "2025-01-15 19:43:35.334234: Pseudo dice [np.float32(0.8341)]\n",
      "2025-01-15 19:43:35.334268: Epoch time: 111.7 s\n",
      "2025-01-15 19:43:35.897156: \n",
      "2025-01-15 19:43:35.897263: Epoch 880\n",
      "2025-01-15 19:43:35.897324: Current learning rate: 0.00148\n",
      "2025-01-15 19:45:27.636697: train_loss -0.8699\n",
      "2025-01-15 19:45:27.637069: val_loss -0.7468\n",
      "2025-01-15 19:45:27.637131: Pseudo dice [np.float32(0.8204)]\n",
      "2025-01-15 19:45:27.637171: Epoch time: 111.74 s\n",
      "2025-01-15 19:45:28.210684: \n",
      "2025-01-15 19:45:28.211079: Epoch 881\n",
      "2025-01-15 19:45:28.211179: Current learning rate: 0.00147\n",
      "2025-01-15 19:47:19.962178: train_loss -0.8746\n",
      "2025-01-15 19:47:19.962471: val_loss -0.7192\n",
      "2025-01-15 19:47:19.962546: Pseudo dice [np.float32(0.8106)]\n",
      "2025-01-15 19:47:19.962588: Epoch time: 111.75 s\n",
      "2025-01-15 19:47:20.538972: \n",
      "2025-01-15 19:47:20.539293: Epoch 882\n",
      "2025-01-15 19:47:20.539366: Current learning rate: 0.00146\n",
      "2025-01-15 19:49:12.255229: train_loss -0.8639\n",
      "2025-01-15 19:49:12.255354: val_loss -0.7838\n",
      "2025-01-15 19:49:12.255386: Pseudo dice [np.float32(0.853)]\n",
      "2025-01-15 19:49:12.255418: Epoch time: 111.72 s\n",
      "2025-01-15 19:49:12.834614: \n",
      "2025-01-15 19:49:12.834762: Epoch 883\n",
      "2025-01-15 19:49:12.834834: Current learning rate: 0.00145\n",
      "2025-01-15 19:51:04.582132: train_loss -0.8683\n",
      "2025-01-15 19:51:04.582297: val_loss -0.6946\n",
      "2025-01-15 19:51:04.582343: Pseudo dice [np.float32(0.8064)]\n",
      "2025-01-15 19:51:04.582387: Epoch time: 111.75 s\n",
      "2025-01-15 19:51:05.169199: \n",
      "2025-01-15 19:51:05.169605: Epoch 884\n",
      "2025-01-15 19:51:05.169700: Current learning rate: 0.00144\n",
      "2025-01-15 19:52:56.915812: train_loss -0.871\n",
      "2025-01-15 19:52:56.915932: val_loss -0.679\n",
      "2025-01-15 19:52:56.915964: Pseudo dice [np.float32(0.7959)]\n",
      "2025-01-15 19:52:56.915997: Epoch time: 111.75 s\n",
      "2025-01-15 19:52:57.532239: \n",
      "2025-01-15 19:52:57.532334: Epoch 885\n",
      "2025-01-15 19:52:57.532397: Current learning rate: 0.00143\n",
      "2025-01-15 19:54:49.059096: train_loss -0.8656\n",
      "2025-01-15 19:54:49.059289: val_loss -0.6549\n",
      "2025-01-15 19:54:49.059321: Pseudo dice [np.float32(0.7458)]\n",
      "2025-01-15 19:54:49.059354: Epoch time: 111.53 s\n",
      "2025-01-15 19:54:49.636019: \n",
      "2025-01-15 19:54:49.636369: Epoch 886\n",
      "2025-01-15 19:54:49.636645: Current learning rate: 0.00142\n",
      "2025-01-15 19:56:41.204879: train_loss -0.8696\n",
      "2025-01-15 19:56:41.204998: val_loss -0.7158\n",
      "2025-01-15 19:56:41.205030: Pseudo dice [np.float32(0.7876)]\n",
      "2025-01-15 19:56:41.205063: Epoch time: 111.57 s\n",
      "2025-01-15 19:56:41.772521: \n",
      "2025-01-15 19:56:41.772695: Epoch 887\n",
      "2025-01-15 19:56:41.772775: Current learning rate: 0.00141\n",
      "2025-01-15 19:58:33.301651: train_loss -0.8616\n",
      "2025-01-15 19:58:33.301780: val_loss -0.6867\n",
      "2025-01-15 19:58:33.301812: Pseudo dice [np.float32(0.7805)]\n",
      "2025-01-15 19:58:33.301844: Epoch time: 111.53 s\n",
      "2025-01-15 19:58:33.872283: \n",
      "2025-01-15 19:58:33.872621: Epoch 888\n",
      "2025-01-15 19:58:33.872717: Current learning rate: 0.00139\n",
      "2025-01-15 20:00:25.413532: train_loss -0.8713\n",
      "2025-01-15 20:00:25.413655: val_loss -0.7277\n",
      "2025-01-15 20:00:25.413686: Pseudo dice [np.float32(0.8219)]\n",
      "2025-01-15 20:00:25.413718: Epoch time: 111.54 s\n",
      "2025-01-15 20:00:25.998942: \n",
      "2025-01-15 20:00:25.999111: Epoch 889\n",
      "2025-01-15 20:00:25.999248: Current learning rate: 0.00138\n",
      "2025-01-15 20:02:17.556035: train_loss -0.8618\n",
      "2025-01-15 20:02:17.556160: val_loss -0.7474\n",
      "2025-01-15 20:02:17.556193: Pseudo dice [np.float32(0.829)]\n",
      "2025-01-15 20:02:17.556225: Epoch time: 111.56 s\n",
      "2025-01-15 20:02:18.120978: \n",
      "2025-01-15 20:02:18.121327: Epoch 890\n",
      "2025-01-15 20:02:18.121394: Current learning rate: 0.00137\n",
      "2025-01-15 20:04:09.816616: train_loss -0.8586\n",
      "2025-01-15 20:04:09.816894: val_loss -0.7225\n",
      "2025-01-15 20:04:09.816931: Pseudo dice [np.float32(0.8108)]\n",
      "2025-01-15 20:04:09.816965: Epoch time: 111.7 s\n",
      "2025-01-15 20:04:10.385964: \n",
      "2025-01-15 20:04:10.386153: Epoch 891\n",
      "2025-01-15 20:04:10.386220: Current learning rate: 0.00136\n",
      "2025-01-15 20:06:02.061557: train_loss -0.8543\n",
      "2025-01-15 20:06:02.061681: val_loss -0.7789\n",
      "2025-01-15 20:06:02.061712: Pseudo dice [np.float32(0.8016)]\n",
      "2025-01-15 20:06:02.061745: Epoch time: 111.68 s\n",
      "2025-01-15 20:06:02.634294: \n",
      "2025-01-15 20:06:02.634375: Epoch 892\n",
      "2025-01-15 20:06:02.634437: Current learning rate: 0.00135\n",
      "2025-01-15 20:07:54.150086: train_loss -0.866\n",
      "2025-01-15 20:07:54.150216: val_loss -0.7915\n",
      "2025-01-15 20:07:54.150248: Pseudo dice [np.float32(0.833)]\n",
      "2025-01-15 20:07:54.150281: Epoch time: 111.52 s\n",
      "2025-01-15 20:07:54.718449: \n",
      "2025-01-15 20:07:54.718820: Epoch 893\n",
      "2025-01-15 20:07:54.718962: Current learning rate: 0.00134\n",
      "2025-01-15 20:09:46.477376: train_loss -0.8773\n",
      "2025-01-15 20:09:46.477503: val_loss -0.68\n",
      "2025-01-15 20:09:46.477537: Pseudo dice [np.float32(0.7877)]\n",
      "2025-01-15 20:09:46.477572: Epoch time: 111.76 s\n",
      "2025-01-15 20:09:47.071108: \n",
      "2025-01-15 20:09:47.071324: Epoch 894\n",
      "2025-01-15 20:09:47.071434: Current learning rate: 0.00133\n",
      "2025-01-15 20:11:38.654745: train_loss -0.8631\n",
      "2025-01-15 20:11:38.654968: val_loss -0.7227\n",
      "2025-01-15 20:11:38.655010: Pseudo dice [np.float32(0.7903)]\n",
      "2025-01-15 20:11:38.655046: Epoch time: 111.58 s\n",
      "2025-01-15 20:11:39.250738: \n",
      "2025-01-15 20:11:39.250835: Epoch 895\n",
      "2025-01-15 20:11:39.250901: Current learning rate: 0.00132\n",
      "2025-01-15 20:13:30.923035: train_loss -0.865\n",
      "2025-01-15 20:13:30.923257: val_loss -0.6333\n",
      "2025-01-15 20:13:30.923292: Pseudo dice [np.float32(0.7697)]\n",
      "2025-01-15 20:13:30.923325: Epoch time: 111.67 s\n",
      "2025-01-15 20:13:31.523564: \n",
      "2025-01-15 20:13:31.523656: Epoch 896\n",
      "2025-01-15 20:13:31.523717: Current learning rate: 0.0013\n",
      "2025-01-15 20:15:23.213371: train_loss -0.8628\n",
      "2025-01-15 20:15:23.213544: val_loss -0.6929\n",
      "2025-01-15 20:15:23.213600: Pseudo dice [np.float32(0.7818)]\n",
      "2025-01-15 20:15:23.213636: Epoch time: 111.69 s\n",
      "2025-01-15 20:15:24.234518: \n",
      "2025-01-15 20:15:24.234891: Epoch 897\n",
      "2025-01-15 20:15:24.234962: Current learning rate: 0.00129\n",
      "2025-01-15 20:17:15.906796: train_loss -0.858\n",
      "2025-01-15 20:17:15.906939: val_loss -0.7479\n",
      "2025-01-15 20:17:15.906972: Pseudo dice [np.float32(0.8333)]\n",
      "2025-01-15 20:17:15.907008: Epoch time: 111.67 s\n",
      "2025-01-15 20:17:16.495689: \n",
      "2025-01-15 20:17:16.496088: Epoch 898\n",
      "2025-01-15 20:17:16.496267: Current learning rate: 0.00128\n",
      "2025-01-15 20:19:08.179127: train_loss -0.8681\n",
      "2025-01-15 20:19:08.179281: val_loss -0.8072\n",
      "2025-01-15 20:19:08.179322: Pseudo dice [np.float32(0.8518)]\n",
      "2025-01-15 20:19:08.179354: Epoch time: 111.68 s\n",
      "2025-01-15 20:19:08.778408: \n",
      "2025-01-15 20:19:08.778515: Epoch 899\n",
      "2025-01-15 20:19:08.778584: Current learning rate: 0.00127\n",
      "2025-01-15 20:21:00.410060: train_loss -0.8677\n",
      "2025-01-15 20:21:00.410309: val_loss -0.6831\n",
      "2025-01-15 20:21:00.410380: Pseudo dice [np.float32(0.7688)]\n",
      "2025-01-15 20:21:00.410435: Epoch time: 111.63 s\n",
      "2025-01-15 20:21:01.241238: \n",
      "2025-01-15 20:21:01.241604: Epoch 900\n",
      "2025-01-15 20:21:01.241679: Current learning rate: 0.00126\n",
      "2025-01-15 20:22:53.025251: train_loss -0.8677\n",
      "2025-01-15 20:22:53.025644: val_loss -0.6785\n",
      "2025-01-15 20:22:53.025719: Pseudo dice [np.float32(0.8007)]\n",
      "2025-01-15 20:22:53.025761: Epoch time: 111.78 s\n",
      "2025-01-15 20:22:53.620342: \n",
      "2025-01-15 20:22:53.620837: Epoch 901\n",
      "2025-01-15 20:22:53.620919: Current learning rate: 0.00125\n",
      "2025-01-15 20:24:45.223434: train_loss -0.8678\n",
      "2025-01-15 20:24:45.223578: val_loss -0.6677\n",
      "2025-01-15 20:24:45.223632: Pseudo dice [np.float32(0.7834)]\n",
      "2025-01-15 20:24:45.223687: Epoch time: 111.6 s\n",
      "2025-01-15 20:24:45.789894: \n",
      "2025-01-15 20:24:45.790265: Epoch 902\n",
      "2025-01-15 20:24:45.790537: Current learning rate: 0.00124\n",
      "2025-01-15 20:26:37.546846: train_loss -0.8662\n",
      "2025-01-15 20:26:37.546979: val_loss -0.7563\n",
      "2025-01-15 20:26:37.547012: Pseudo dice [np.float32(0.833)]\n",
      "2025-01-15 20:26:37.547051: Epoch time: 111.76 s\n",
      "2025-01-15 20:26:38.113132: \n",
      "2025-01-15 20:26:38.113676: Epoch 903\n",
      "2025-01-15 20:26:38.113767: Current learning rate: 0.00122\n",
      "2025-01-15 20:28:29.844017: train_loss -0.8701\n",
      "2025-01-15 20:28:29.844202: val_loss -0.7513\n",
      "2025-01-15 20:28:29.844247: Pseudo dice [np.float32(0.8073)]\n",
      "2025-01-15 20:28:29.844282: Epoch time: 111.73 s\n",
      "2025-01-15 20:28:30.420358: \n",
      "2025-01-15 20:28:30.420773: Epoch 904\n",
      "2025-01-15 20:28:30.420915: Current learning rate: 0.00121\n",
      "2025-01-15 20:30:22.148930: train_loss -0.8664\n",
      "2025-01-15 20:30:22.149077: val_loss -0.7141\n",
      "2025-01-15 20:30:22.149119: Pseudo dice [np.float32(0.8073)]\n",
      "2025-01-15 20:30:22.149155: Epoch time: 111.73 s\n",
      "2025-01-15 20:30:22.720866: \n",
      "2025-01-15 20:30:22.720962: Epoch 905\n",
      "2025-01-15 20:30:22.721024: Current learning rate: 0.0012\n",
      "2025-01-15 20:32:14.277399: train_loss -0.8591\n",
      "2025-01-15 20:32:14.277735: val_loss -0.7596\n",
      "2025-01-15 20:32:14.277771: Pseudo dice [np.float32(0.8266)]\n",
      "2025-01-15 20:32:14.277804: Epoch time: 111.56 s\n",
      "2025-01-15 20:32:14.850794: \n",
      "2025-01-15 20:32:14.850986: Epoch 906\n",
      "2025-01-15 20:32:14.851062: Current learning rate: 0.00119\n",
      "2025-01-15 20:34:06.607255: train_loss -0.8515\n",
      "2025-01-15 20:34:06.607493: val_loss -0.719\n",
      "2025-01-15 20:34:06.607529: Pseudo dice [np.float32(0.8111)]\n",
      "2025-01-15 20:34:06.607563: Epoch time: 111.76 s\n",
      "2025-01-15 20:34:07.181089: \n",
      "2025-01-15 20:34:07.181305: Epoch 907\n",
      "2025-01-15 20:34:07.181372: Current learning rate: 0.00118\n",
      "2025-01-15 20:35:58.759337: train_loss -0.8699\n",
      "2025-01-15 20:35:58.759568: val_loss -0.765\n",
      "2025-01-15 20:35:58.759611: Pseudo dice [np.float32(0.832)]\n",
      "2025-01-15 20:35:58.759646: Epoch time: 111.58 s\n",
      "2025-01-15 20:35:59.336526: \n",
      "2025-01-15 20:35:59.336709: Epoch 908\n",
      "2025-01-15 20:35:59.336806: Current learning rate: 0.00117\n",
      "2025-01-15 20:37:51.067579: train_loss -0.867\n",
      "2025-01-15 20:37:51.067698: val_loss -0.7056\n",
      "2025-01-15 20:37:51.067729: Pseudo dice [np.float32(0.8138)]\n",
      "2025-01-15 20:37:51.067762: Epoch time: 111.73 s\n",
      "2025-01-15 20:37:51.638422: \n",
      "2025-01-15 20:37:51.638562: Epoch 909\n",
      "2025-01-15 20:37:51.638628: Current learning rate: 0.00116\n",
      "2025-01-15 20:39:43.379081: train_loss -0.8618\n",
      "2025-01-15 20:39:43.379323: val_loss -0.776\n",
      "2025-01-15 20:39:43.379367: Pseudo dice [np.float32(0.8287)]\n",
      "2025-01-15 20:39:43.379403: Epoch time: 111.74 s\n",
      "2025-01-15 20:39:43.947024: \n",
      "2025-01-15 20:39:43.947264: Epoch 910\n",
      "2025-01-15 20:39:43.947443: Current learning rate: 0.00115\n",
      "2025-01-15 20:41:35.721958: train_loss -0.8746\n",
      "2025-01-15 20:41:35.722083: val_loss -0.7144\n",
      "2025-01-15 20:41:35.722116: Pseudo dice [np.float32(0.7977)]\n",
      "2025-01-15 20:41:35.722148: Epoch time: 111.78 s\n",
      "2025-01-15 20:41:36.301735: \n",
      "2025-01-15 20:41:36.302007: Epoch 911\n",
      "2025-01-15 20:41:36.302124: Current learning rate: 0.00113\n",
      "2025-01-15 20:43:27.951698: train_loss -0.8741\n",
      "2025-01-15 20:43:27.951826: val_loss -0.7684\n",
      "2025-01-15 20:43:27.951858: Pseudo dice [np.float32(0.8379)]\n",
      "2025-01-15 20:43:27.951891: Epoch time: 111.65 s\n",
      "2025-01-15 20:43:28.524546: \n",
      "2025-01-15 20:43:28.524896: Epoch 912\n",
      "2025-01-15 20:43:28.525023: Current learning rate: 0.00112\n",
      "2025-01-15 20:45:20.271023: train_loss -0.8789\n",
      "2025-01-15 20:45:20.271143: val_loss -0.6706\n",
      "2025-01-15 20:45:20.271174: Pseudo dice [np.float32(0.7897)]\n",
      "2025-01-15 20:45:20.271206: Epoch time: 111.75 s\n",
      "2025-01-15 20:45:20.841454: \n",
      "2025-01-15 20:45:20.841647: Epoch 913\n",
      "2025-01-15 20:45:20.841745: Current learning rate: 0.00111\n",
      "2025-01-15 20:47:12.589529: train_loss -0.8688\n",
      "2025-01-15 20:47:12.589719: val_loss -0.7416\n",
      "2025-01-15 20:47:12.589762: Pseudo dice [np.float32(0.8127)]\n",
      "2025-01-15 20:47:12.589796: Epoch time: 111.75 s\n",
      "2025-01-15 20:47:13.171717: \n",
      "2025-01-15 20:47:13.172050: Epoch 914\n",
      "2025-01-15 20:47:13.172171: Current learning rate: 0.0011\n",
      "2025-01-15 20:49:04.890870: train_loss -0.8748\n",
      "2025-01-15 20:49:04.890998: val_loss -0.7449\n",
      "2025-01-15 20:49:04.891031: Pseudo dice [np.float32(0.8166)]\n",
      "2025-01-15 20:49:04.891065: Epoch time: 111.72 s\n",
      "2025-01-15 20:49:05.462940: \n",
      "2025-01-15 20:49:05.463231: Epoch 915\n",
      "2025-01-15 20:49:05.463419: Current learning rate: 0.00109\n",
      "2025-01-15 20:50:57.209644: train_loss -0.8748\n",
      "2025-01-15 20:50:57.209838: val_loss -0.7269\n",
      "2025-01-15 20:50:57.209883: Pseudo dice [np.float32(0.8123)]\n",
      "2025-01-15 20:50:57.209918: Epoch time: 111.75 s\n",
      "2025-01-15 20:50:58.175567: \n",
      "2025-01-15 20:50:58.175766: Epoch 916\n",
      "2025-01-15 20:50:58.175840: Current learning rate: 0.00108\n",
      "2025-01-15 20:52:49.884477: train_loss -0.8746\n",
      "2025-01-15 20:52:49.884615: val_loss -0.7292\n",
      "2025-01-15 20:52:49.884647: Pseudo dice [np.float32(0.8166)]\n",
      "2025-01-15 20:52:49.884681: Epoch time: 111.71 s\n",
      "2025-01-15 20:52:50.457336: \n",
      "2025-01-15 20:52:50.457444: Epoch 917\n",
      "2025-01-15 20:52:50.457508: Current learning rate: 0.00106\n",
      "2025-01-15 20:54:42.187021: train_loss -0.8755\n",
      "2025-01-15 20:54:42.187178: val_loss -0.7566\n",
      "2025-01-15 20:54:42.187496: Pseudo dice [np.float32(0.8335)]\n",
      "2025-01-15 20:54:42.187561: Epoch time: 111.73 s\n",
      "2025-01-15 20:54:42.760370: \n",
      "2025-01-15 20:54:42.760518: Epoch 918\n",
      "2025-01-15 20:54:42.760589: Current learning rate: 0.00105\n",
      "2025-01-15 20:56:34.460496: train_loss -0.877\n",
      "2025-01-15 20:56:34.460632: val_loss -0.7588\n",
      "2025-01-15 20:56:34.460667: Pseudo dice [np.float32(0.8425)]\n",
      "2025-01-15 20:56:34.460709: Epoch time: 111.7 s\n",
      "2025-01-15 20:56:35.040758: \n",
      "2025-01-15 20:56:35.041113: Epoch 919\n",
      "2025-01-15 20:56:35.041202: Current learning rate: 0.00104\n",
      "2025-01-15 20:58:26.671547: train_loss -0.8793\n",
      "2025-01-15 20:58:26.671927: val_loss -0.7502\n",
      "2025-01-15 20:58:26.672197: Pseudo dice [np.float32(0.8081)]\n",
      "2025-01-15 20:58:26.672324: Epoch time: 111.63 s\n",
      "2025-01-15 20:58:27.245924: \n",
      "2025-01-15 20:58:27.246104: Epoch 920\n",
      "2025-01-15 20:58:27.246176: Current learning rate: 0.00103\n",
      "2025-01-15 21:00:18.982677: train_loss -0.8711\n",
      "2025-01-15 21:00:18.982943: val_loss -0.7362\n",
      "2025-01-15 21:00:18.983012: Pseudo dice [np.float32(0.8203)]\n",
      "2025-01-15 21:00:18.983055: Epoch time: 111.74 s\n",
      "2025-01-15 21:00:19.551879: \n",
      "2025-01-15 21:00:19.552083: Epoch 921\n",
      "2025-01-15 21:00:19.552227: Current learning rate: 0.00102\n",
      "2025-01-15 21:02:11.306960: train_loss -0.8699\n",
      "2025-01-15 21:02:11.307153: val_loss -0.8028\n",
      "2025-01-15 21:02:11.307188: Pseudo dice [np.float32(0.8545)]\n",
      "2025-01-15 21:02:11.307242: Epoch time: 111.76 s\n",
      "2025-01-15 21:02:11.878541: \n",
      "2025-01-15 21:02:11.878703: Epoch 922\n",
      "2025-01-15 21:02:11.879058: Current learning rate: 0.00101\n",
      "2025-01-15 21:04:03.479006: train_loss -0.8747\n",
      "2025-01-15 21:04:03.479135: val_loss -0.6772\n",
      "2025-01-15 21:04:03.479167: Pseudo dice [np.float32(0.7779)]\n",
      "2025-01-15 21:04:03.479201: Epoch time: 111.6 s\n",
      "2025-01-15 21:04:04.052428: \n",
      "2025-01-15 21:04:04.052870: Epoch 923\n",
      "2025-01-15 21:04:04.052956: Current learning rate: 0.001\n",
      "2025-01-15 21:05:55.830782: train_loss -0.8806\n",
      "2025-01-15 21:05:55.830909: val_loss -0.7333\n",
      "2025-01-15 21:05:55.830943: Pseudo dice [np.float32(0.8417)]\n",
      "2025-01-15 21:05:55.830977: Epoch time: 111.78 s\n",
      "2025-01-15 21:05:56.408512: \n",
      "2025-01-15 21:05:56.408600: Epoch 924\n",
      "2025-01-15 21:05:56.408662: Current learning rate: 0.00098\n",
      "2025-01-15 21:07:48.164460: train_loss -0.8681\n",
      "2025-01-15 21:07:48.164597: val_loss -0.7228\n",
      "2025-01-15 21:07:48.164631: Pseudo dice [np.float32(0.8216)]\n",
      "2025-01-15 21:07:48.164663: Epoch time: 111.76 s\n",
      "2025-01-15 21:07:48.735528: \n",
      "2025-01-15 21:07:48.735706: Epoch 925\n",
      "2025-01-15 21:07:48.735770: Current learning rate: 0.00097\n",
      "2025-01-15 21:09:40.463355: train_loss -0.8812\n",
      "2025-01-15 21:09:40.463481: val_loss -0.7481\n",
      "2025-01-15 21:09:40.463515: Pseudo dice [np.float32(0.8297)]\n",
      "2025-01-15 21:09:40.463548: Epoch time: 111.73 s\n",
      "2025-01-15 21:09:41.040438: \n",
      "2025-01-15 21:09:41.040595: Epoch 926\n",
      "2025-01-15 21:09:41.040660: Current learning rate: 0.00096\n",
      "2025-01-15 21:11:32.824859: train_loss -0.8829\n",
      "2025-01-15 21:11:32.824989: val_loss -0.7366\n",
      "2025-01-15 21:11:32.825022: Pseudo dice [np.float32(0.8239)]\n",
      "2025-01-15 21:11:32.825056: Epoch time: 111.78 s\n",
      "2025-01-15 21:11:33.418324: \n",
      "2025-01-15 21:11:33.418622: Epoch 927\n",
      "2025-01-15 21:11:33.418689: Current learning rate: 0.00095\n",
      "2025-01-15 21:13:25.242383: train_loss -0.8743\n",
      "2025-01-15 21:13:25.242747: val_loss -0.7814\n",
      "2025-01-15 21:13:25.242820: Pseudo dice [np.float32(0.8479)]\n",
      "2025-01-15 21:13:25.242859: Epoch time: 111.82 s\n",
      "2025-01-15 21:13:25.242883: Yayy! New best EMA pseudo Dice: 0.8233000040054321\n",
      "2025-01-15 21:13:26.065023: \n",
      "2025-01-15 21:13:26.065377: Epoch 928\n",
      "2025-01-15 21:13:26.065449: Current learning rate: 0.00094\n",
      "2025-01-15 21:15:17.858427: train_loss -0.8755\n",
      "2025-01-15 21:15:17.858791: val_loss -0.7544\n",
      "2025-01-15 21:15:17.859018: Pseudo dice [np.float32(0.8208)]\n",
      "2025-01-15 21:15:17.859078: Epoch time: 111.79 s\n",
      "2025-01-15 21:15:18.455581: \n",
      "2025-01-15 21:15:18.455671: Epoch 929\n",
      "2025-01-15 21:15:18.455734: Current learning rate: 0.00092\n",
      "2025-01-15 21:17:10.197468: train_loss -0.8727\n",
      "2025-01-15 21:17:10.197672: val_loss -0.7636\n",
      "2025-01-15 21:17:10.197803: Pseudo dice [np.float32(0.834)]\n",
      "2025-01-15 21:17:10.197873: Epoch time: 111.74 s\n",
      "2025-01-15 21:17:10.197904: Yayy! New best EMA pseudo Dice: 0.8241000175476074\n",
      "2025-01-15 21:17:10.999600: \n",
      "2025-01-15 21:17:10.999689: Epoch 930\n",
      "2025-01-15 21:17:10.999749: Current learning rate: 0.00091\n",
      "2025-01-15 21:19:02.725746: train_loss -0.8748\n",
      "2025-01-15 21:19:02.725871: val_loss -0.7105\n",
      "2025-01-15 21:19:02.725905: Pseudo dice [np.float32(0.8098)]\n",
      "2025-01-15 21:19:02.725937: Epoch time: 111.73 s\n",
      "2025-01-15 21:19:03.293008: \n",
      "2025-01-15 21:19:03.293100: Epoch 931\n",
      "2025-01-15 21:19:03.293162: Current learning rate: 0.0009\n",
      "2025-01-15 21:20:54.998972: train_loss -0.8804\n",
      "2025-01-15 21:20:54.999322: val_loss -0.7204\n",
      "2025-01-15 21:20:54.999369: Pseudo dice [np.float32(0.8201)]\n",
      "2025-01-15 21:20:54.999407: Epoch time: 111.71 s\n",
      "2025-01-15 21:20:55.570758: \n",
      "2025-01-15 21:20:55.570906: Epoch 932\n",
      "2025-01-15 21:20:55.570976: Current learning rate: 0.00089\n",
      "2025-01-15 21:22:47.086824: train_loss -0.8706\n",
      "2025-01-15 21:22:47.086950: val_loss -0.7189\n",
      "2025-01-15 21:22:47.086983: Pseudo dice [np.float32(0.8038)]\n",
      "2025-01-15 21:22:47.087015: Epoch time: 111.52 s\n",
      "2025-01-15 21:22:47.660506: \n",
      "2025-01-15 21:22:47.660590: Epoch 933\n",
      "2025-01-15 21:22:47.660650: Current learning rate: 0.00088\n",
      "2025-01-15 21:24:39.340647: train_loss -0.8791\n",
      "2025-01-15 21:24:39.340846: val_loss -0.7268\n",
      "2025-01-15 21:24:39.340878: Pseudo dice [np.float32(0.8039)]\n",
      "2025-01-15 21:24:39.340911: Epoch time: 111.68 s\n",
      "2025-01-15 21:24:39.916210: \n",
      "2025-01-15 21:24:39.916373: Epoch 934\n",
      "2025-01-15 21:24:39.916444: Current learning rate: 0.00087\n",
      "2025-01-15 21:26:31.640466: train_loss -0.8733\n",
      "2025-01-15 21:26:31.640692: val_loss -0.6669\n",
      "2025-01-15 21:26:31.640735: Pseudo dice [np.float32(0.7227)]\n",
      "2025-01-15 21:26:31.640771: Epoch time: 111.72 s\n",
      "2025-01-15 21:26:32.596071: \n",
      "2025-01-15 21:26:32.596196: Epoch 935\n",
      "2025-01-15 21:26:32.596320: Current learning rate: 0.00085\n",
      "2025-01-15 21:28:24.200644: train_loss -0.868\n",
      "2025-01-15 21:28:24.200894: val_loss -0.7681\n",
      "2025-01-15 21:28:24.201172: Pseudo dice [np.float32(0.8265)]\n",
      "2025-01-15 21:28:24.201239: Epoch time: 111.61 s\n",
      "2025-01-15 21:28:24.766048: \n",
      "2025-01-15 21:28:24.766369: Epoch 936\n",
      "2025-01-15 21:28:24.766461: Current learning rate: 0.00084\n",
      "2025-01-15 21:30:16.418497: train_loss -0.8797\n",
      "2025-01-15 21:30:16.418631: val_loss -0.6878\n",
      "2025-01-15 21:30:16.418664: Pseudo dice [np.float32(0.7998)]\n",
      "2025-01-15 21:30:16.418697: Epoch time: 111.65 s\n",
      "2025-01-15 21:30:17.010353: \n",
      "2025-01-15 21:30:17.010460: Epoch 937\n",
      "2025-01-15 21:30:17.010528: Current learning rate: 0.00083\n",
      "2025-01-15 21:32:08.530604: train_loss -0.8758\n",
      "2025-01-15 21:32:08.530723: val_loss -0.7391\n",
      "2025-01-15 21:32:08.530755: Pseudo dice [np.float32(0.8137)]\n",
      "2025-01-15 21:32:08.530787: Epoch time: 111.52 s\n",
      "2025-01-15 21:32:09.100686: \n",
      "2025-01-15 21:32:09.100868: Epoch 938\n",
      "2025-01-15 21:32:09.100941: Current learning rate: 0.00082\n",
      "2025-01-15 21:34:00.845527: train_loss -0.8803\n",
      "2025-01-15 21:34:00.845987: val_loss -0.7586\n",
      "2025-01-15 21:34:00.846058: Pseudo dice [np.float32(0.8311)]\n",
      "2025-01-15 21:34:00.846101: Epoch time: 111.75 s\n",
      "2025-01-15 21:34:01.420859: \n",
      "2025-01-15 21:34:01.420974: Epoch 939\n",
      "2025-01-15 21:34:01.421045: Current learning rate: 0.00081\n",
      "2025-01-15 21:35:52.994916: train_loss -0.8739\n",
      "2025-01-15 21:35:52.995037: val_loss -0.6899\n",
      "2025-01-15 21:35:52.995070: Pseudo dice [np.float32(0.7844)]\n",
      "2025-01-15 21:35:52.995103: Epoch time: 111.57 s\n",
      "2025-01-15 21:35:53.567787: \n",
      "2025-01-15 21:35:53.568167: Epoch 940\n",
      "2025-01-15 21:35:53.568310: Current learning rate: 0.00079\n",
      "2025-01-15 21:37:45.196749: train_loss -0.8803\n",
      "2025-01-15 21:37:45.197088: val_loss -0.7547\n",
      "2025-01-15 21:37:45.197132: Pseudo dice [np.float32(0.83)]\n",
      "2025-01-15 21:37:45.197172: Epoch time: 111.63 s\n",
      "2025-01-15 21:37:45.765857: \n",
      "2025-01-15 21:37:45.766234: Epoch 941\n",
      "2025-01-15 21:37:45.766328: Current learning rate: 0.00078\n",
      "2025-01-15 21:39:37.383005: train_loss -0.8764\n",
      "2025-01-15 21:39:37.383148: val_loss -0.7161\n",
      "2025-01-15 21:39:37.383182: Pseudo dice [np.float32(0.8023)]\n",
      "2025-01-15 21:39:37.383216: Epoch time: 111.62 s\n",
      "2025-01-15 21:39:37.955740: \n",
      "2025-01-15 21:39:37.956084: Epoch 942\n",
      "2025-01-15 21:39:37.956154: Current learning rate: 0.00077\n",
      "2025-01-15 21:41:29.509484: train_loss -0.8775\n",
      "2025-01-15 21:41:29.509614: val_loss -0.7728\n",
      "2025-01-15 21:41:29.509647: Pseudo dice [np.float32(0.8327)]\n",
      "2025-01-15 21:41:29.509685: Epoch time: 111.55 s\n",
      "2025-01-15 21:41:30.076707: \n",
      "2025-01-15 21:41:30.076862: Epoch 943\n",
      "2025-01-15 21:41:30.076935: Current learning rate: 0.00076\n",
      "2025-01-15 21:43:21.755473: train_loss -0.8787\n",
      "2025-01-15 21:43:21.755958: val_loss -0.7415\n",
      "2025-01-15 21:43:21.756092: Pseudo dice [np.float32(0.8021)]\n",
      "2025-01-15 21:43:21.756160: Epoch time: 111.68 s\n",
      "2025-01-15 21:43:22.359091: \n",
      "2025-01-15 21:43:22.359255: Epoch 944\n",
      "2025-01-15 21:43:22.359324: Current learning rate: 0.00075\n",
      "2025-01-15 21:45:13.921681: train_loss -0.871\n",
      "2025-01-15 21:45:13.921812: val_loss -0.6848\n",
      "2025-01-15 21:45:13.921843: Pseudo dice [np.float32(0.7734)]\n",
      "2025-01-15 21:45:13.921875: Epoch time: 111.56 s\n",
      "2025-01-15 21:45:14.492517: \n",
      "2025-01-15 21:45:14.492603: Epoch 945\n",
      "2025-01-15 21:45:14.492664: Current learning rate: 0.00074\n",
      "2025-01-15 21:47:06.225104: train_loss -0.8815\n",
      "2025-01-15 21:47:06.225312: val_loss -0.7469\n",
      "2025-01-15 21:47:06.225351: Pseudo dice [np.float32(0.8238)]\n",
      "2025-01-15 21:47:06.225383: Epoch time: 111.73 s\n",
      "2025-01-15 21:47:06.795973: \n",
      "2025-01-15 21:47:06.796180: Epoch 946\n",
      "2025-01-15 21:47:06.796258: Current learning rate: 0.00072\n",
      "2025-01-15 21:48:58.499957: train_loss -0.8809\n",
      "2025-01-15 21:48:58.500086: val_loss -0.6977\n",
      "2025-01-15 21:48:58.500118: Pseudo dice [np.float32(0.7948)]\n",
      "2025-01-15 21:48:58.500151: Epoch time: 111.7 s\n",
      "2025-01-15 21:48:59.073337: \n",
      "2025-01-15 21:48:59.073487: Epoch 947\n",
      "Current learning rate: 0.00071\n",
      "2025-01-15 21:50:50.812793: train_loss -0.8754\n",
      "2025-01-15 21:50:50.812918: val_loss -0.7177\n",
      "2025-01-15 21:50:50.812950: Pseudo dice [np.float32(0.8008)]\n",
      "2025-01-15 21:50:50.812984: Epoch time: 111.74 s\n",
      "2025-01-15 21:50:51.382567: \n",
      "2025-01-15 21:50:51.382666: Epoch 948\n",
      "2025-01-15 21:50:51.382726: Current learning rate: 0.0007\n",
      "2025-01-15 21:52:42.983054: train_loss -0.8578\n",
      "2025-01-15 21:52:42.983542: val_loss -0.7211\n",
      "2025-01-15 21:52:42.983612: Pseudo dice [np.float32(0.8074)]\n",
      "2025-01-15 21:52:42.983655: Epoch time: 111.6 s\n",
      "2025-01-15 21:52:43.566040: \n",
      "2025-01-15 21:52:43.566155: Epoch 949\n",
      "2025-01-15 21:52:43.566275: Current learning rate: 0.00069\n",
      "2025-01-15 21:54:35.281359: train_loss -0.8613\n",
      "2025-01-15 21:54:35.281543: val_loss -0.7556\n",
      "2025-01-15 21:54:35.281574: Pseudo dice [np.float32(0.8266)]\n",
      "2025-01-15 21:54:35.281606: Epoch time: 111.72 s\n",
      "2025-01-15 21:54:36.077447: \n",
      "2025-01-15 21:54:36.077768: Epoch 950\n",
      "2025-01-15 21:54:36.078094: Current learning rate: 0.00067\n",
      "2025-01-15 21:56:27.610999: train_loss -0.8729\n",
      "2025-01-15 21:56:27.611238: val_loss -0.8032\n",
      "2025-01-15 21:56:27.611419: Pseudo dice [np.float32(0.8578)]\n",
      "2025-01-15 21:56:27.611515: Epoch time: 111.53 s\n",
      "2025-01-15 21:56:28.175244: \n",
      "2025-01-15 21:56:28.175431: Epoch 951\n",
      "2025-01-15 21:56:28.175517: Current learning rate: 0.00066\n",
      "2025-01-15 21:58:19.903901: train_loss -0.868\n",
      "2025-01-15 21:58:19.904027: val_loss -0.6756\n",
      "2025-01-15 21:58:19.904059: Pseudo dice [np.float32(0.7404)]\n",
      "2025-01-15 21:58:19.904091: Epoch time: 111.73 s\n",
      "2025-01-15 21:58:20.480239: \n",
      "2025-01-15 21:58:20.480395: Epoch 952\n",
      "2025-01-15 21:58:20.480463: Current learning rate: 0.00065\n",
      "2025-01-15 22:00:12.059263: train_loss -0.8769\n",
      "2025-01-15 22:00:12.059388: val_loss -0.7196\n",
      "2025-01-15 22:00:12.059420: Pseudo dice [np.float32(0.7535)]\n",
      "2025-01-15 22:00:12.059454: Epoch time: 111.58 s\n",
      "2025-01-15 22:00:12.631271: \n",
      "2025-01-15 22:00:12.631420: Epoch 953\n",
      "2025-01-15 22:00:12.631500: Current learning rate: 0.00064\n",
      "2025-01-15 22:02:04.398970: train_loss -0.8637\n",
      "2025-01-15 22:02:04.399081: val_loss -0.6788\n",
      "2025-01-15 22:02:04.399113: Pseudo dice [np.float32(0.7633)]\n",
      "2025-01-15 22:02:04.399180: Epoch time: 111.77 s\n",
      "2025-01-15 22:02:05.369310: \n",
      "2025-01-15 22:02:05.369428: Epoch 954\n",
      "2025-01-15 22:02:05.369579: Current learning rate: 0.00063\n",
      "2025-01-15 22:03:57.134827: train_loss -0.8862\n",
      "2025-01-15 22:03:57.134951: val_loss -0.6613\n",
      "2025-01-15 22:03:57.134983: Pseudo dice [np.float32(0.7842)]\n",
      "2025-01-15 22:03:57.135015: Epoch time: 111.77 s\n",
      "2025-01-15 22:03:57.708318: \n",
      "2025-01-15 22:03:57.708436: Epoch 955\n",
      "2025-01-15 22:03:57.708501: Current learning rate: 0.00061\n",
      "2025-01-15 22:05:49.298373: train_loss -0.8749\n",
      "2025-01-15 22:05:49.298498: val_loss -0.771\n",
      "2025-01-15 22:05:49.298532: Pseudo dice [np.float32(0.843)]\n",
      "2025-01-15 22:05:49.298567: Epoch time: 111.59 s\n",
      "2025-01-15 22:05:49.878276: \n",
      "2025-01-15 22:05:49.878616: Epoch 956\n",
      "2025-01-15 22:05:49.878701: Current learning rate: 0.0006\n",
      "2025-01-15 22:07:41.502969: train_loss -0.8734\n",
      "2025-01-15 22:07:41.503142: val_loss -0.7092\n",
      "2025-01-15 22:07:41.503269: Pseudo dice [np.float32(0.82)]\n",
      "2025-01-15 22:07:41.503313: Epoch time: 111.63 s\n",
      "2025-01-15 22:07:42.090698: \n",
      "2025-01-15 22:07:42.091026: Epoch 957\n",
      "2025-01-15 22:07:42.091217: Current learning rate: 0.00059\n",
      "2025-01-15 22:09:33.803864: train_loss -0.8821\n",
      "2025-01-15 22:09:33.803984: val_loss -0.7616\n",
      "2025-01-15 22:09:33.804016: Pseudo dice [np.float32(0.8387)]\n",
      "2025-01-15 22:09:33.804048: Epoch time: 111.71 s\n",
      "2025-01-15 22:09:34.384372: \n",
      "2025-01-15 22:09:34.384839: Epoch 958\n",
      "2025-01-15 22:09:34.384953: Current learning rate: 0.00058\n",
      "2025-01-15 22:11:26.026730: train_loss -0.8855\n",
      "2025-01-15 22:11:26.026860: val_loss -0.7416\n",
      "2025-01-15 22:11:26.026963: Pseudo dice [np.float32(0.8291)]\n",
      "2025-01-15 22:11:26.027133: Epoch time: 111.64 s\n",
      "2025-01-15 22:11:26.620725: \n",
      "2025-01-15 22:11:26.621068: Epoch 959\n",
      "2025-01-15 22:11:26.621144: Current learning rate: 0.00056\n",
      "2025-01-15 22:13:18.243505: train_loss -0.8803\n",
      "2025-01-15 22:13:18.243636: val_loss -0.7351\n",
      "2025-01-15 22:13:18.243670: Pseudo dice [np.float32(0.8161)]\n",
      "2025-01-15 22:13:18.243704: Epoch time: 111.62 s\n",
      "2025-01-15 22:13:18.846889: \n",
      "2025-01-15 22:13:18.847214: Epoch 960\n",
      "2025-01-15 22:13:18.847281: Current learning rate: 0.00055\n",
      "2025-01-15 22:15:10.655179: train_loss -0.8729\n",
      "2025-01-15 22:15:10.655381: val_loss -0.6874\n",
      "2025-01-15 22:15:10.655506: Pseudo dice [np.float32(0.8021)]\n",
      "2025-01-15 22:15:10.655583: Epoch time: 111.81 s\n",
      "2025-01-15 22:15:11.277016: \n",
      "2025-01-15 22:15:11.277122: Epoch 961\n",
      "2025-01-15 22:15:11.277199: Current learning rate: 0.00054\n",
      "2025-01-15 22:17:03.054306: train_loss -0.8767\n",
      "2025-01-15 22:17:03.054427: val_loss -0.7753\n",
      "2025-01-15 22:17:03.054458: Pseudo dice [np.float32(0.8348)]\n",
      "2025-01-15 22:17:03.054492: Epoch time: 111.78 s\n",
      "2025-01-15 22:17:03.641604: \n",
      "2025-01-15 22:17:03.642040: Epoch 962\n",
      "2025-01-15 22:17:03.642134: Current learning rate: 0.00053\n",
      "2025-01-15 22:18:55.185600: train_loss -0.885\n",
      "2025-01-15 22:18:55.185741: val_loss -0.7355\n",
      "2025-01-15 22:18:55.185812: Pseudo dice [np.float32(0.8293)]\n",
      "2025-01-15 22:18:55.185855: Epoch time: 111.54 s\n",
      "2025-01-15 22:18:55.763412: \n",
      "2025-01-15 22:18:55.763777: Epoch 963\n",
      "2025-01-15 22:18:55.763865: Current learning rate: 0.00051\n",
      "2025-01-15 22:20:47.488022: train_loss -0.8714\n",
      "2025-01-15 22:20:47.488137: val_loss -0.7294\n",
      "2025-01-15 22:20:47.488170: Pseudo dice [np.float32(0.8153)]\n",
      "2025-01-15 22:20:47.488221: Epoch time: 111.73 s\n",
      "2025-01-15 22:20:48.075222: \n",
      "2025-01-15 22:20:48.075310: Epoch 964\n",
      "2025-01-15 22:20:48.075372: Current learning rate: 0.0005\n",
      "2025-01-15 22:22:39.810892: train_loss -0.8801\n",
      "2025-01-15 22:22:39.811115: val_loss -0.7838\n",
      "2025-01-15 22:22:39.811161: Pseudo dice [np.float32(0.844)]\n",
      "2025-01-15 22:22:39.811232: Epoch time: 111.74 s\n",
      "2025-01-15 22:22:40.387920: \n",
      "2025-01-15 22:22:40.388008: Epoch 965\n",
      "2025-01-15 22:22:40.388070: Current learning rate: 0.00049\n",
      "2025-01-15 22:24:32.109775: train_loss -0.8761\n",
      "2025-01-15 22:24:32.109927: val_loss -0.7479\n",
      "2025-01-15 22:24:32.109961: Pseudo dice [np.float32(0.8252)]\n",
      "2025-01-15 22:24:32.109995: Epoch time: 111.72 s\n",
      "2025-01-15 22:24:32.694114: \n",
      "2025-01-15 22:24:32.694230: Epoch 966\n",
      "2025-01-15 22:24:32.694292: Current learning rate: 0.00048\n",
      "2025-01-15 22:26:24.412035: train_loss -0.8778\n",
      "2025-01-15 22:26:24.412276: val_loss -0.7133\n",
      "2025-01-15 22:26:24.412316: Pseudo dice [np.float32(0.8064)]\n",
      "2025-01-15 22:26:24.412351: Epoch time: 111.72 s\n",
      "2025-01-15 22:26:24.992030: \n",
      "2025-01-15 22:26:24.992210: Epoch 967\n",
      "2025-01-15 22:26:24.992288: Current learning rate: 0.00046\n",
      "2025-01-15 22:28:16.567390: train_loss -0.8837\n",
      "2025-01-15 22:28:16.567510: val_loss -0.6628\n",
      "2025-01-15 22:28:16.567544: Pseudo dice [np.float32(0.7937)]\n",
      "2025-01-15 22:28:16.567578: Epoch time: 111.58 s\n",
      "2025-01-15 22:28:17.157611: \n",
      "2025-01-15 22:28:17.157696: Epoch 968\n",
      "2025-01-15 22:28:17.157757: Current learning rate: 0.00045\n",
      "2025-01-15 22:30:08.973516: train_loss -0.886\n",
      "2025-01-15 22:30:08.973650: val_loss -0.7494\n",
      "2025-01-15 22:30:08.973683: Pseudo dice [np.float32(0.7856)]\n",
      "2025-01-15 22:30:08.973717: Epoch time: 111.82 s\n",
      "2025-01-15 22:30:09.555616: \n",
      "2025-01-15 22:30:09.555700: Epoch 969\n",
      "2025-01-15 22:30:09.555763: Current learning rate: 0.00044\n",
      "2025-01-15 22:32:01.146624: train_loss -0.8752\n",
      "2025-01-15 22:32:01.146759: val_loss -0.7421\n",
      "2025-01-15 22:32:01.146795: Pseudo dice [np.float32(0.8342)]\n",
      "2025-01-15 22:32:01.146830: Epoch time: 111.59 s\n",
      "2025-01-15 22:32:01.722843: \n",
      "2025-01-15 22:32:01.722934: Epoch 970\n",
      "2025-01-15 22:32:01.722994: Current learning rate: 0.00043\n",
      "2025-01-15 22:33:53.502827: train_loss -0.8786\n",
      "2025-01-15 22:33:53.502962: val_loss -0.7671\n",
      "2025-01-15 22:33:53.503009: Pseudo dice [np.float32(0.8057)]\n",
      "2025-01-15 22:33:53.503062: Epoch time: 111.78 s\n",
      "2025-01-15 22:33:54.085199: \n",
      "2025-01-15 22:33:54.085388: Epoch 971\n",
      "2025-01-15 22:33:54.085484: Current learning rate: 0.00041\n",
      "2025-01-15 22:35:46.010720: train_loss -0.8855\n",
      "2025-01-15 22:35:46.010919: val_loss -0.7211\n",
      "2025-01-15 22:35:46.010963: Pseudo dice [np.float32(0.7761)]\n",
      "2025-01-15 22:35:46.011000: Epoch time: 111.93 s\n",
      "2025-01-15 22:35:46.588307: \n",
      "2025-01-15 22:35:46.588636: Epoch 972\n",
      "Current learning rate: 0.0004\n",
      "2025-01-15 22:37:38.327154: train_loss -0.8771\n",
      "2025-01-15 22:37:38.327282: val_loss -0.7675\n",
      "2025-01-15 22:37:38.327313: Pseudo dice [np.float32(0.827)]\n",
      "2025-01-15 22:37:38.327345: Epoch time: 111.74 s\n",
      "2025-01-15 22:37:39.305682: \n",
      "2025-01-15 22:37:39.305856: Epoch 973\n",
      "2025-01-15 22:37:39.305926: Current learning rate: 0.00039\n",
      "2025-01-15 22:39:30.996394: train_loss -0.8822\n",
      "2025-01-15 22:39:30.996554: val_loss -0.7363\n",
      "2025-01-15 22:39:30.996623: Pseudo dice [np.float32(0.8283)]\n",
      "2025-01-15 22:39:30.996664: Epoch time: 111.69 s\n",
      "2025-01-15 22:39:31.580663: \n",
      "2025-01-15 22:39:31.580762: Epoch 974\n",
      "2025-01-15 22:39:31.580823: Current learning rate: 0.00037\n",
      "2025-01-15 22:41:23.183170: train_loss -0.8725\n",
      "2025-01-15 22:41:23.183306: val_loss -0.693\n",
      "2025-01-15 22:41:23.183340: Pseudo dice [np.float32(0.787)]\n",
      "2025-01-15 22:41:23.183372: Epoch time: 111.6 s\n",
      "2025-01-15 22:41:23.770195: \n",
      "2025-01-15 22:41:23.770351: Epoch 975\n",
      "2025-01-15 22:41:23.770479: Current learning rate: 0.00036\n",
      "2025-01-15 22:43:15.499677: train_loss -0.8747\n",
      "2025-01-15 22:43:15.499796: val_loss -0.7448\n",
      "2025-01-15 22:43:15.499831: Pseudo dice [np.float32(0.8033)]\n",
      "2025-01-15 22:43:15.499864: Epoch time: 111.73 s\n",
      "2025-01-15 22:43:16.088425: \n",
      "2025-01-15 22:43:16.088581: Epoch 976\n",
      "2025-01-15 22:43:16.088730: Current learning rate: 0.00035\n",
      "2025-01-15 22:45:07.830257: train_loss -0.8748\n",
      "2025-01-15 22:45:07.830378: val_loss -0.7335\n",
      "2025-01-15 22:45:07.830411: Pseudo dice [np.float32(0.8247)]\n",
      "2025-01-15 22:45:07.830443: Epoch time: 111.74 s\n",
      "2025-01-15 22:45:08.408539: \n",
      "2025-01-15 22:45:08.408735: Epoch 977\n",
      "2025-01-15 22:45:08.408819: Current learning rate: 0.00034\n",
      "2025-01-15 22:46:59.951114: train_loss -0.8888\n",
      "2025-01-15 22:46:59.951519: val_loss -0.7406\n",
      "2025-01-15 22:46:59.951628: Pseudo dice [np.float32(0.8431)]\n",
      "2025-01-15 22:46:59.951673: Epoch time: 111.54 s\n",
      "2025-01-15 22:47:00.530976: \n",
      "2025-01-15 22:47:00.531153: Epoch 978\n",
      "2025-01-15 22:47:00.531366: Current learning rate: 0.00032\n",
      "2025-01-15 22:48:52.179826: train_loss -0.8796\n",
      "2025-01-15 22:48:52.179965: val_loss -0.7803\n",
      "2025-01-15 22:48:52.179996: Pseudo dice [np.float32(0.8275)]\n",
      "2025-01-15 22:48:52.180030: Epoch time: 111.65 s\n",
      "2025-01-15 22:48:52.766392: \n",
      "2025-01-15 22:48:52.766711: Epoch 979\n",
      "2025-01-15 22:48:52.766787: Current learning rate: 0.00031\n",
      "2025-01-15 22:50:44.477488: train_loss -0.8874\n",
      "2025-01-15 22:50:44.477694: val_loss -0.658\n",
      "2025-01-15 22:50:44.477727: Pseudo dice [np.float32(0.7771)]\n",
      "2025-01-15 22:50:44.477761: Epoch time: 111.71 s\n",
      "2025-01-15 22:50:45.060143: \n",
      "2025-01-15 22:50:45.060312: Epoch 980\n",
      "2025-01-15 22:50:45.060385: Current learning rate: 0.0003\n",
      "2025-01-15 22:52:36.634781: train_loss -0.8796\n",
      "2025-01-15 22:52:36.634905: val_loss -0.675\n",
      "2025-01-15 22:52:36.634940: Pseudo dice [np.float32(0.7947)]\n",
      "2025-01-15 22:52:36.634975: Epoch time: 111.58 s\n",
      "2025-01-15 22:52:37.209431: \n",
      "2025-01-15 22:52:37.209792: Epoch 981\n",
      "2025-01-15 22:52:37.209881: Current learning rate: 0.00028\n",
      "2025-01-15 22:54:28.950922: train_loss -0.879\n",
      "2025-01-15 22:54:28.951044: val_loss -0.7328\n",
      "2025-01-15 22:54:28.951079: Pseudo dice [np.float32(0.7848)]\n",
      "2025-01-15 22:54:28.951112: Epoch time: 111.74 s\n",
      "2025-01-15 22:54:29.547641: \n",
      "2025-01-15 22:54:29.547837: Epoch 982\n",
      "2025-01-15 22:54:29.547981: Current learning rate: 0.00027\n",
      "2025-01-15 22:56:21.090821: train_loss -0.8789\n",
      "2025-01-15 22:56:21.090948: val_loss -0.7434\n",
      "2025-01-15 22:56:21.090982: Pseudo dice [np.float32(0.8282)]\n",
      "2025-01-15 22:56:21.091016: Epoch time: 111.54 s\n",
      "2025-01-15 22:56:21.669719: \n",
      "2025-01-15 22:56:21.670074: Epoch 983\n",
      "2025-01-15 22:56:21.670176: Current learning rate: 0.00026\n",
      "2025-01-15 22:58:13.412840: train_loss -0.8843\n",
      "2025-01-15 22:58:13.412967: val_loss -0.7612\n",
      "2025-01-15 22:58:13.412999: Pseudo dice [np.float32(0.8166)]\n",
      "2025-01-15 22:58:13.413031: Epoch time: 111.74 s\n",
      "2025-01-15 22:58:13.996709: \n",
      "2025-01-15 22:58:13.996967: Epoch 984\n",
      "2025-01-15 22:58:13.997085: Current learning rate: 0.00024\n",
      "2025-01-15 23:00:05.700167: train_loss -0.8824\n",
      "2025-01-15 23:00:05.700323: val_loss -0.6988\n",
      "2025-01-15 23:00:05.700394: Pseudo dice [np.float32(0.7996)]\n",
      "2025-01-15 23:00:05.700436: Epoch time: 111.7 s\n",
      "2025-01-15 23:00:06.276754: \n",
      "2025-01-15 23:00:06.276916: Epoch 985\n",
      "2025-01-15 23:00:06.277007: Current learning rate: 0.00023\n",
      "2025-01-15 23:01:57.965699: train_loss -0.8817\n",
      "2025-01-15 23:01:57.965833: val_loss -0.7187\n",
      "2025-01-15 23:01:57.965870: Pseudo dice [np.float32(0.7953)]\n",
      "2025-01-15 23:01:57.965911: Epoch time: 111.69 s\n",
      "2025-01-15 23:01:58.543829: \n",
      "2025-01-15 23:01:58.544173: Epoch 986\n",
      "2025-01-15 23:01:58.544359: Current learning rate: 0.00021\n",
      "2025-01-15 23:03:50.243710: train_loss -0.8818\n",
      "2025-01-15 23:03:50.243846: val_loss -0.7093\n",
      "2025-01-15 23:03:50.243882: Pseudo dice [np.float32(0.789)]\n",
      "2025-01-15 23:03:50.243914: Epoch time: 111.7 s\n",
      "2025-01-15 23:03:50.840253: \n",
      "2025-01-15 23:03:50.840584: Epoch 987\n",
      "2025-01-15 23:03:50.840657: Current learning rate: 0.0002\n",
      "2025-01-15 23:05:42.582947: train_loss -0.8743\n",
      "2025-01-15 23:05:42.583064: val_loss -0.7016\n",
      "2025-01-15 23:05:42.583097: Pseudo dice [np.float32(0.8139)]\n",
      "2025-01-15 23:05:42.583129: Epoch time: 111.74 s\n",
      "2025-01-15 23:05:43.167366: \n",
      "2025-01-15 23:05:43.167478: Epoch 988\n",
      "2025-01-15 23:05:43.167542: Current learning rate: 0.00019\n",
      "2025-01-15 23:07:34.972600: train_loss -0.8839\n",
      "2025-01-15 23:07:34.972709: val_loss -0.7261\n",
      "2025-01-15 23:07:34.972739: Pseudo dice [np.float32(0.7844)]\n",
      "2025-01-15 23:07:34.972771: Epoch time: 111.81 s\n",
      "2025-01-15 23:07:35.545882: \n",
      "2025-01-15 23:07:35.546169: Epoch 989\n",
      "2025-01-15 23:07:35.546485: Current learning rate: 0.00017\n",
      "2025-01-15 23:09:27.371187: train_loss -0.8825\n",
      "2025-01-15 23:09:27.371312: val_loss -0.7177\n",
      "2025-01-15 23:09:27.371344: Pseudo dice [np.float32(0.784)]\n",
      "2025-01-15 23:09:27.371376: Epoch time: 111.83 s\n",
      "2025-01-15 23:09:27.951851: \n",
      "2025-01-15 23:09:27.951939: Epoch 990\n",
      "2025-01-15 23:09:27.952002: Current learning rate: 0.00016\n",
      "2025-01-15 23:11:19.688204: train_loss -0.8826\n",
      "2025-01-15 23:11:19.688391: val_loss -0.6533\n",
      "2025-01-15 23:11:19.688425: Pseudo dice [np.float32(0.7854)]\n",
      "2025-01-15 23:11:19.688456: Epoch time: 111.74 s\n",
      "2025-01-15 23:11:20.272801: \n",
      "2025-01-15 23:11:20.272984: Epoch 991\n",
      "2025-01-15 23:11:20.273085: Current learning rate: 0.00014\n",
      "2025-01-15 23:13:12.317424: train_loss -0.8835\n",
      "2025-01-15 23:13:12.317560: val_loss -0.7266\n",
      "2025-01-15 23:13:12.317591: Pseudo dice [np.float32(0.8088)]\n",
      "2025-01-15 23:13:12.317629: Epoch time: 112.05 s\n",
      "2025-01-15 23:13:12.909539: \n",
      "2025-01-15 23:13:12.909644: Epoch 992\n",
      "2025-01-15 23:13:12.909714: Current learning rate: 0.00013\n",
      "2025-01-15 23:15:04.723207: train_loss -0.881\n",
      "2025-01-15 23:15:04.723429: val_loss -0.7584\n",
      "2025-01-15 23:15:04.723470: Pseudo dice [np.float32(0.8303)]\n",
      "2025-01-15 23:15:04.723506: Epoch time: 111.81 s\n",
      "2025-01-15 23:15:05.327352: \n",
      "2025-01-15 23:15:05.327788: Epoch 993\n",
      "2025-01-15 23:15:05.327888: Current learning rate: 0.00011\n",
      "2025-01-15 23:16:57.019374: train_loss -0.8857\n",
      "2025-01-15 23:16:57.019517: val_loss -0.7209\n",
      "2025-01-15 23:16:57.019551: Pseudo dice [np.float32(0.8146)]\n",
      "2025-01-15 23:16:57.019586: Epoch time: 111.69 s\n",
      "2025-01-15 23:16:57.629106: \n",
      "2025-01-15 23:16:57.629204: Epoch 994\n",
      "2025-01-15 23:16:57.629266: Current learning rate: 0.0001\n",
      "2025-01-15 23:18:49.454972: train_loss -0.8834\n",
      "2025-01-15 23:18:49.455100: val_loss -0.7569\n",
      "2025-01-15 23:18:49.455134: Pseudo dice [np.float32(0.7648)]\n",
      "2025-01-15 23:18:49.455169: Epoch time: 111.83 s\n",
      "2025-01-15 23:18:50.032786: \n",
      "2025-01-15 23:18:50.032962: Epoch 995\n",
      "2025-01-15 23:18:50.033036: Current learning rate: 8e-05\n",
      "2025-01-15 23:20:41.790952: train_loss -0.8848\n",
      "2025-01-15 23:20:41.791167: val_loss -0.627\n",
      "2025-01-15 23:20:41.791213: Pseudo dice [np.float32(0.7782)]\n",
      "2025-01-15 23:20:41.791249: Epoch time: 111.76 s\n",
      "2025-01-15 23:20:42.374485: \n",
      "2025-01-15 23:20:42.374662: Epoch 996\n",
      "2025-01-15 23:20:42.374735: Current learning rate: 7e-05\n",
      "2025-01-15 23:22:34.071888: train_loss -0.8854\n",
      "2025-01-15 23:22:34.072019: val_loss -0.652\n",
      "2025-01-15 23:22:34.072054: Pseudo dice [np.float32(0.7649)]\n",
      "2025-01-15 23:22:34.072088: Epoch time: 111.7 s\n",
      "2025-01-15 23:22:34.655677: \n",
      "2025-01-15 23:22:34.655995: Epoch 997\n",
      "Current learning rate: 5e-05\n",
      "2025-01-15 23:24:26.228022: train_loss -0.8848\n",
      "2025-01-15 23:24:26.228224: val_loss -0.7473\n",
      "2025-01-15 23:24:26.228258: Pseudo dice [np.float32(0.8271)]\n",
      "2025-01-15 23:24:26.228291: Epoch time: 111.57 s\n",
      "2025-01-15 23:24:26.805808: \n",
      "2025-01-15 23:24:26.805901: Epoch 998\n",
      "2025-01-15 23:24:26.805966: Current learning rate: 4e-05\n",
      "2025-01-15 23:26:18.561283: train_loss -0.8777\n",
      "2025-01-15 23:26:18.561482: val_loss -0.7599\n",
      "2025-01-15 23:26:18.561527: Pseudo dice [np.float32(0.8207)]\n",
      "2025-01-15 23:26:18.561563: Epoch time: 111.76 s\n",
      "2025-01-15 23:26:19.139836: \n",
      "2025-01-15 23:26:19.139974: Epoch 999\n",
      "2025-01-15 23:26:19.140037: Current learning rate: 2e-05\n",
      "2025-01-15 23:28:10.849800: train_loss -0.8837\n",
      "2025-01-15 23:28:10.849929: val_loss -0.7682\n",
      "2025-01-15 23:28:10.849963: Pseudo dice [np.float32(0.8402)]\n",
      "2025-01-15 23:28:10.849996: Epoch time: 111.71 s\n",
      "2025-01-15 23:28:11.687603: Training done.\n",
      "2025-01-15 23:28:11.692142: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-15 23:28:11.692265: The split file contains 5 splits.\n",
      "2025-01-15 23:28:11.692284: Desired fold for training: 2\n",
      "2025-01-15 23:28:11.692297: This split has 40 training and 10 validation cases.\n",
      "2025-01-15 23:28:11.692361: predicting lung_003\n",
      "2025-01-15 23:28:11.692754: lung_003, shape torch.Size([1, 288, 606, 606]), rank 0\n",
      "2025-01-15 23:30:43.293748: predicting lung_010\n",
      "2025-01-15 23:30:43.298204: lung_010, shape torch.Size([1, 242, 386, 386]), rank 0\n",
      "2025-01-15 23:31:29.471082: predicting lung_016\n",
      "2025-01-15 23:31:29.472967: lung_016, shape torch.Size([1, 228, 503, 503]), rank 0\n",
      "2025-01-15 23:32:41.580050: predicting lung_036\n",
      "2025-01-15 23:32:41.582588: lung_036, shape torch.Size([1, 271, 505, 505]), rank 0\n",
      "2025-01-15 23:34:08.120450: predicting lung_046\n",
      "2025-01-15 23:34:08.123515: lung_046, shape torch.Size([1, 226, 505, 505]), rank 0\n",
      "2025-01-15 23:35:20.223075: predicting lung_047\n",
      "2025-01-15 23:35:20.225786: lung_047, shape torch.Size([1, 511, 508, 508]), rank 0\n",
      "2025-01-15 23:38:13.437311: predicting lung_049\n",
      "2025-01-15 23:38:13.442378: lung_049, shape torch.Size([1, 292, 455, 455]), rank 0\n",
      "2025-01-15 23:39:20.776929: predicting lung_065\n",
      "2025-01-15 23:39:20.782222: lung_065, shape torch.Size([1, 257, 470, 470]), rank 0\n",
      "2025-01-15 23:40:18.535161: predicting lung_078\n",
      "2025-01-15 23:40:18.538793: lung_078, shape torch.Size([1, 229, 390, 390]), rank 0\n",
      "2025-01-15 23:40:57.022448: predicting lung_083\n",
      "2025-01-15 23:40:57.025197: lung_083, shape torch.Size([1, 256, 505, 505]), rank 0\n",
      "2025-01-15 23:42:47.945598: Validation complete\n",
      "2025-01-15 23:42:47.946047: Mean Validation Dice:  0.6538259901093416\n",
      "Standard U-Net training completed for 3d_fullres fold 2 -epoch 1000\n",
      "Starting standard U-Net training: nnUNetv2_train 2 3d_fullres 3 --npz --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "WARNING: Cannot continue training because there seems to be no checkpoint available to continue from. Starting a new training...\n",
      "2025-01-15 23:42:53.652214: do_dummy_2d_data_aug: False\n",
      "2025-01-15 23:42:53.652479: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-15 23:42:53.652566: The split file contains 5 splits.\n",
      "2025-01-15 23:42:53.652584: Desired fold for training: 3\n",
      "2025-01-15 23:42:53.652596: This split has 40 training and 10 validation cases.\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pin_memory on device 0\n",
      "2025-01-15 23:43:03.118758: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [252.0, 512.0, 512.0], 'spacing': [1.2450199127197266, 0.7919921875, 0.7919921875], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset002_Lung_split', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.2450199127197266, 0.7919921875, 0.7919921875], 'original_median_shape_after_transp': [251, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -272.8529357910156, 'median': -155.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 303.0, 'std': 348.3833923339844}}} \n",
      "\n",
      "2025-01-15 23:43:04.396471: unpacking dataset...\n",
      "2025-01-15 23:43:08.309912: unpacking done...\n",
      "2025-01-15 23:43:08.310907: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-01-15 23:43:08.315049: \n",
      "2025-01-15 23:43:08.315107: Epoch 0\n",
      "2025-01-15 23:43:08.315172: Current learning rate: 0.01\n",
      "2025-01-15 23:45:18.146881: train_loss 0.0893\n",
      "2025-01-15 23:45:18.147022: val_loss 0.0109\n",
      "2025-01-15 23:45:18.147058: Pseudo dice [np.float32(0.0)]\n",
      "2025-01-15 23:45:18.147091: Epoch time: 129.83 s\n",
      "2025-01-15 23:45:18.147241: Yayy! New best EMA pseudo Dice: 0.0\n",
      "2025-01-15 23:45:18.817202: \n",
      "2025-01-15 23:45:18.817384: Epoch 1\n",
      "2025-01-15 23:45:18.817453: Current learning rate: 0.00999\n",
      "2025-01-15 23:47:09.545371: train_loss -0.0594\n",
      "2025-01-15 23:47:09.545748: val_loss -0.1233\n",
      "2025-01-15 23:47:09.545871: Pseudo dice [np.float32(0.0)]\n",
      "2025-01-15 23:47:09.545928: Epoch time: 110.73 s\n",
      "2025-01-15 23:47:09.545954: Yayy! New best EMA pseudo Dice: 0.0\n",
      "2025-01-15 23:47:10.422272: \n",
      "2025-01-15 23:47:10.422370: Epoch 2\n",
      "2025-01-15 23:47:10.422433: Current learning rate: 0.00998\n",
      "2025-01-15 23:49:01.133504: train_loss -0.2461\n",
      "2025-01-15 23:49:01.133631: val_loss -0.4227\n",
      "2025-01-15 23:49:01.133665: Pseudo dice [np.float32(0.4273)]\n",
      "2025-01-15 23:49:01.133697: Epoch time: 110.71 s\n",
      "2025-01-15 23:49:01.133716: Yayy! New best EMA pseudo Dice: 0.04270000010728836\n",
      "2025-01-15 23:49:01.873746: \n",
      "2025-01-15 23:49:01.873837: Epoch 3\n",
      "2025-01-15 23:49:01.873898: Current learning rate: 0.00997\n",
      "2025-01-15 23:50:52.544641: train_loss -0.3853\n",
      "2025-01-15 23:50:52.544763: val_loss -0.3513\n",
      "2025-01-15 23:50:52.544796: Pseudo dice [np.float32(0.3323)]\n",
      "2025-01-15 23:50:52.544829: Epoch time: 110.67 s\n",
      "2025-01-15 23:50:52.544850: Yayy! New best EMA pseudo Dice: 0.07169999927282333\n",
      "2025-01-15 23:50:53.262054: \n",
      "2025-01-15 23:50:53.262144: Epoch 4\n",
      "2025-01-15 23:50:53.262207: Current learning rate: 0.00996\n",
      "2025-01-15 23:52:43.936283: train_loss -0.4185\n",
      "2025-01-15 23:52:43.936421: val_loss -0.4366\n",
      "2025-01-15 23:52:43.936453: Pseudo dice [np.float32(0.4757)]\n",
      "2025-01-15 23:52:43.936485: Epoch time: 110.67 s\n",
      "2025-01-15 23:52:43.936504: Yayy! New best EMA pseudo Dice: 0.11209999769926071\n",
      "2025-01-15 23:52:44.681373: \n",
      "2025-01-15 23:52:44.681516: Epoch 5\n",
      "2025-01-15 23:52:44.681590: Current learning rate: 0.00995\n",
      "2025-01-15 23:54:35.391932: train_loss -0.4225\n",
      "2025-01-15 23:54:35.392130: val_loss -0.3927\n",
      "2025-01-15 23:54:35.392176: Pseudo dice [np.float32(0.4011)]\n",
      "2025-01-15 23:54:35.392210: Epoch time: 110.71 s\n",
      "2025-01-15 23:54:35.392232: Yayy! New best EMA pseudo Dice: 0.14100000262260437\n",
      "2025-01-15 23:54:36.120182: \n",
      "2025-01-15 23:54:36.120380: Epoch 6\n",
      "2025-01-15 23:54:36.120447: Current learning rate: 0.00995\n",
      "2025-01-15 23:56:29.823399: train_loss -0.4705\n",
      "2025-01-15 23:56:29.823601: val_loss -0.5326\n",
      "2025-01-15 23:56:29.823637: Pseudo dice [np.float32(0.488)]\n",
      "2025-01-15 23:56:29.823672: Epoch time: 113.7 s\n",
      "2025-01-15 23:56:29.823699: Yayy! New best EMA pseudo Dice: 0.17569999396800995\n",
      "2025-01-15 23:56:30.582023: \n",
      "2025-01-15 23:56:30.582121: Epoch 7\n",
      "2025-01-15 23:56:30.582192: Current learning rate: 0.00994\n",
      "2025-01-15 23:58:24.920082: train_loss -0.4686\n",
      "2025-01-15 23:58:24.920211: val_loss -0.513\n",
      "2025-01-15 23:58:24.920246: Pseudo dice [np.float32(0.5973)]\n",
      "2025-01-15 23:58:24.920278: Epoch time: 114.34 s\n",
      "2025-01-15 23:58:24.920298: Yayy! New best EMA pseudo Dice: 0.21789999306201935\n",
      "2025-01-15 23:58:25.680103: \n",
      "2025-01-15 23:58:25.680188: Epoch 8\n",
      "2025-01-15 23:58:25.680252: Current learning rate: 0.00993\n",
      "2025-01-16 00:00:17.427389: train_loss -0.5322\n",
      "2025-01-16 00:00:17.427528: val_loss -0.4667\n",
      "2025-01-16 00:00:17.427563: Pseudo dice [np.float32(0.5107)]\n",
      "2025-01-16 00:00:17.427598: Epoch time: 111.75 s\n",
      "2025-01-16 00:00:17.427617: Yayy! New best EMA pseudo Dice: 0.24709999561309814\n",
      "2025-01-16 00:00:18.185494: \n",
      "2025-01-16 00:00:18.185578: Epoch 9\n",
      "2025-01-16 00:00:18.185646: Current learning rate: 0.00992\n",
      "2025-01-16 00:02:10.675189: train_loss -0.5322\n",
      "2025-01-16 00:02:10.675315: val_loss -0.4986\n",
      "2025-01-16 00:02:10.675349: Pseudo dice [np.float32(0.5079)]\n",
      "2025-01-16 00:02:10.675383: Epoch time: 112.49 s\n",
      "2025-01-16 00:02:10.675404: Yayy! New best EMA pseudo Dice: 0.27320000529289246\n",
      "2025-01-16 00:02:11.410258: \n",
      "2025-01-16 00:02:11.410344: Epoch 10\n",
      "2025-01-16 00:02:11.410406: Current learning rate: 0.00991\n",
      "2025-01-16 00:04:02.860626: train_loss -0.4972\n",
      "2025-01-16 00:04:02.860755: val_loss -0.4631\n",
      "2025-01-16 00:04:02.860788: Pseudo dice [np.float32(0.4571)]\n",
      "2025-01-16 00:04:02.860820: Epoch time: 111.45 s\n",
      "2025-01-16 00:04:02.860838: Yayy! New best EMA pseudo Dice: 0.29159998893737793\n",
      "2025-01-16 00:04:03.592242: \n",
      "2025-01-16 00:04:03.592329: Epoch 11\n",
      "2025-01-16 00:04:03.592391: Current learning rate: 0.0099\n",
      "2025-01-16 00:05:54.692960: train_loss -0.5149\n",
      "2025-01-16 00:05:54.693351: val_loss -0.5176\n",
      "2025-01-16 00:05:54.693491: Pseudo dice [np.float32(0.6061)]\n",
      "2025-01-16 00:05:54.693542: Epoch time: 111.1 s\n",
      "2025-01-16 00:05:54.693567: Yayy! New best EMA pseudo Dice: 0.3230000138282776\n",
      "2025-01-16 00:05:55.431278: \n",
      "2025-01-16 00:05:55.431498: Epoch 12\n",
      "2025-01-16 00:05:55.431576: Current learning rate: 0.00989\n",
      "2025-01-16 00:07:46.470605: train_loss -0.5686\n",
      "2025-01-16 00:07:46.470732: val_loss -0.5259\n",
      "2025-01-16 00:07:46.470880: Pseudo dice [np.float32(0.5803)]\n",
      "2025-01-16 00:07:46.470988: Epoch time: 111.04 s\n",
      "2025-01-16 00:07:46.471021: Yayy! New best EMA pseudo Dice: 0.34880000352859497\n",
      "2025-01-16 00:07:47.214724: \n",
      "2025-01-16 00:07:47.215059: Epoch 13\n",
      "2025-01-16 00:07:47.215172: Current learning rate: 0.00988\n",
      "2025-01-16 00:09:38.338357: train_loss -0.5503\n",
      "2025-01-16 00:09:38.338490: val_loss -0.466\n",
      "2025-01-16 00:09:38.338527: Pseudo dice [np.float32(0.5691)]\n",
      "2025-01-16 00:09:38.338561: Epoch time: 111.12 s\n",
      "2025-01-16 00:09:38.338581: Yayy! New best EMA pseudo Dice: 0.3707999885082245\n",
      "2025-01-16 00:09:39.322357: \n",
      "2025-01-16 00:09:39.322454: Epoch 14\n",
      "2025-01-16 00:09:39.322517: Current learning rate: 0.00987\n",
      "2025-01-16 00:11:30.406892: train_loss -0.5362\n",
      "2025-01-16 00:11:30.407017: val_loss -0.5034\n",
      "2025-01-16 00:11:30.407055: Pseudo dice [np.float32(0.5239)]\n",
      "2025-01-16 00:11:30.407089: Epoch time: 111.09 s\n",
      "2025-01-16 00:11:30.407109: Yayy! New best EMA pseudo Dice: 0.38609999418258667\n",
      "2025-01-16 00:11:31.168713: \n",
      "2025-01-16 00:11:31.169128: Epoch 15\n",
      "2025-01-16 00:11:31.169221: Current learning rate: 0.00986\n",
      "2025-01-16 00:13:22.324432: train_loss -0.5455\n",
      "2025-01-16 00:13:22.324582: val_loss -0.5652\n",
      "2025-01-16 00:13:22.324616: Pseudo dice [np.float32(0.6137)]\n",
      "2025-01-16 00:13:22.324652: Epoch time: 111.16 s\n",
      "2025-01-16 00:13:22.324673: Yayy! New best EMA pseudo Dice: 0.4088999927043915\n",
      "2025-01-16 00:13:23.115120: \n",
      "2025-01-16 00:13:23.115494: Epoch 16\n",
      "2025-01-16 00:13:23.115632: Current learning rate: 0.00986\n",
      "2025-01-16 00:15:14.282239: train_loss -0.5733\n",
      "2025-01-16 00:15:14.282660: val_loss -0.5168\n",
      "2025-01-16 00:15:14.282707: Pseudo dice [np.float32(0.6488)]\n",
      "2025-01-16 00:15:14.282748: Epoch time: 111.17 s\n",
      "2025-01-16 00:15:14.282774: Yayy! New best EMA pseudo Dice: 0.43290001153945923\n",
      "2025-01-16 00:15:15.075894: \n",
      "2025-01-16 00:15:15.076100: Epoch 17\n",
      "2025-01-16 00:15:15.076273: Current learning rate: 0.00985\n",
      "2025-01-16 00:17:06.237173: train_loss -0.5759\n",
      "2025-01-16 00:17:06.237431: val_loss -0.4744\n",
      "2025-01-16 00:17:06.240128: Pseudo dice [np.float32(0.6255)]\n",
      "2025-01-16 00:17:06.240241: Epoch time: 111.16 s\n",
      "2025-01-16 00:17:06.240361: Yayy! New best EMA pseudo Dice: 0.45210000872612\n",
      "2025-01-16 00:17:07.050344: \n",
      "2025-01-16 00:17:07.050450: Epoch 18\n",
      "2025-01-16 00:17:07.050516: Current learning rate: 0.00984\n",
      "2025-01-16 00:18:57.812237: train_loss -0.5228\n",
      "2025-01-16 00:18:57.812385: val_loss -0.4784\n",
      "2025-01-16 00:18:57.812421: Pseudo dice [np.float32(0.4897)]\n",
      "2025-01-16 00:18:57.812454: Epoch time: 110.76 s\n",
      "2025-01-16 00:18:57.812473: Yayy! New best EMA pseudo Dice: 0.45590001344680786\n",
      "2025-01-16 00:18:58.608793: \n",
      "2025-01-16 00:18:58.608887: Epoch 19\n",
      "2025-01-16 00:18:58.608952: Current learning rate: 0.00983\n",
      "2025-01-16 00:20:49.383598: train_loss -0.5623\n",
      "2025-01-16 00:20:49.383736: val_loss -0.5279\n",
      "2025-01-16 00:20:49.383768: Pseudo dice [np.float32(0.5874)]\n",
      "2025-01-16 00:20:49.383802: Epoch time: 110.78 s\n",
      "Yayy! New best EMA pseudo Dice: 0.4690999984741211\n",
      "2025-01-16 00:20:50.183155: \n",
      "2025-01-16 00:20:50.183256: Epoch 20\n",
      "2025-01-16 00:20:50.183324: Current learning rate: 0.00982\n",
      "2025-01-16 00:22:40.972905: train_loss -0.5514\n",
      "2025-01-16 00:22:40.973049: val_loss -0.5698\n",
      "2025-01-16 00:22:40.973083: Pseudo dice [np.float32(0.7184)]\n",
      "2025-01-16 00:22:40.973116: Epoch time: 110.79 s\n",
      "2025-01-16 00:22:40.973137: Yayy! New best EMA pseudo Dice: 0.49399998784065247\n",
      "2025-01-16 00:22:41.770768: \n",
      "2025-01-16 00:22:41.771261: Epoch 21\n",
      "2025-01-16 00:22:41.771375: Current learning rate: 0.00981\n",
      "2025-01-16 00:24:32.521353: train_loss -0.5643\n",
      "2025-01-16 00:24:32.521485: val_loss -0.4374\n",
      "2025-01-16 00:24:32.521524: Pseudo dice [np.float32(0.5506)]\n",
      "2025-01-16 00:24:32.521558: Epoch time: 110.75 s\n",
      "2025-01-16 00:24:32.521577: Yayy! New best EMA pseudo Dice: 0.49959999322891235\n",
      "2025-01-16 00:24:33.302394: \n",
      "2025-01-16 00:24:33.302483: Epoch 22\n",
      "2025-01-16 00:24:33.302549: Current learning rate: 0.0098\n",
      "2025-01-16 00:26:24.027204: train_loss -0.5123\n",
      "2025-01-16 00:26:24.027329: val_loss -0.564\n",
      "2025-01-16 00:26:24.027364: Pseudo dice [np.float32(0.667)]\n",
      "2025-01-16 00:26:24.027398: Epoch time: 110.73 s\n",
      "2025-01-16 00:26:24.027418: Yayy! New best EMA pseudo Dice: 0.5163999795913696\n",
      "2025-01-16 00:26:24.779871: \n",
      "2025-01-16 00:26:24.780056: Epoch 23\n",
      "2025-01-16 00:26:24.780207: Current learning rate: 0.00979\n",
      "2025-01-16 00:28:15.477658: train_loss -0.6528\n",
      "2025-01-16 00:28:15.477795: val_loss -0.4187\n",
      "2025-01-16 00:28:15.477831: Pseudo dice [np.float32(0.421)]\n",
      "2025-01-16 00:28:15.477864: Epoch time: 110.7 s\n",
      "2025-01-16 00:28:16.003563: \n",
      "2025-01-16 00:28:16.003652: Epoch 24\n",
      "2025-01-16 00:28:16.003714: Current learning rate: 0.00978\n",
      "2025-01-16 00:30:06.716803: train_loss -0.538\n",
      "2025-01-16 00:30:06.716959: val_loss -0.5739\n",
      "2025-01-16 00:30:06.717015: Pseudo dice [np.float32(0.5948)]\n",
      "2025-01-16 00:30:06.717053: Epoch time: 110.71 s\n",
      "2025-01-16 00:30:07.242283: \n",
      "2025-01-16 00:30:07.242525: Epoch 25\n",
      "2025-01-16 00:30:07.242669: Current learning rate: 0.00977\n",
      "2025-01-16 00:31:57.921184: train_loss -0.58\n",
      "2025-01-16 00:31:57.921349: val_loss -0.5098\n",
      "2025-01-16 00:31:57.921426: Pseudo dice [np.float32(0.602)]\n",
      "2025-01-16 00:31:57.921469: Epoch time: 110.68 s\n",
      "2025-01-16 00:31:57.921499: Yayy! New best EMA pseudo Dice: 0.5242999792098999\n",
      "2025-01-16 00:31:58.899331: \n",
      "2025-01-16 00:31:58.899745: Epoch 26\n",
      "2025-01-16 00:31:58.899885: Current learning rate: 0.00977\n",
      "2025-01-16 00:33:49.601374: train_loss -0.6359\n",
      "2025-01-16 00:33:49.601571: val_loss -0.4267\n",
      "2025-01-16 00:33:49.601709: Pseudo dice [np.float32(0.5222)]\n",
      "2025-01-16 00:33:49.601772: Epoch time: 110.7 s\n",
      "2025-01-16 00:33:50.130008: \n",
      "2025-01-16 00:33:50.130116: Epoch 27\n",
      "2025-01-16 00:33:50.130216: Current learning rate: 0.00976\n",
      "2025-01-16 00:35:40.872694: train_loss -0.6004\n",
      "2025-01-16 00:35:40.872855: val_loss -0.48\n",
      "2025-01-16 00:35:40.873007: Pseudo dice [np.float32(0.54)]\n",
      "2025-01-16 00:35:40.873050: Epoch time: 110.74 s\n",
      "2025-01-16 00:35:40.873072: Yayy! New best EMA pseudo Dice: 0.5256999731063843\n",
      "2025-01-16 00:35:41.626967: \n",
      "2025-01-16 00:35:41.627291: Epoch 28\n",
      "2025-01-16 00:35:41.627366: Current learning rate: 0.00975\n",
      "2025-01-16 00:37:32.338031: train_loss -0.5727\n",
      "2025-01-16 00:37:32.338165: val_loss -0.5104\n",
      "2025-01-16 00:37:32.338199: Pseudo dice [np.float32(0.5791)]\n",
      "2025-01-16 00:37:32.338231: Epoch time: 110.71 s\n",
      "2025-01-16 00:37:32.338252: Yayy! New best EMA pseudo Dice: 0.531000018119812\n",
      "2025-01-16 00:37:33.084111: \n",
      "2025-01-16 00:37:33.084437: Epoch 29\n",
      "2025-01-16 00:37:33.084568: Current learning rate: 0.00974\n",
      "2025-01-16 00:39:23.787546: train_loss -0.5553\n",
      "2025-01-16 00:39:23.787740: val_loss -0.4928\n",
      "2025-01-16 00:39:23.787827: Pseudo dice [np.float32(0.6093)]\n",
      "2025-01-16 00:39:23.787868: Epoch time: 110.7 s\n",
      "2025-01-16 00:39:23.787891: Yayy! New best EMA pseudo Dice: 0.5388000011444092\n",
      "2025-01-16 00:39:24.539961: \n",
      "2025-01-16 00:39:24.540056: Epoch 30\n",
      "2025-01-16 00:39:24.540122: Current learning rate: 0.00973\n",
      "2025-01-16 00:41:15.249878: train_loss -0.5978\n",
      "2025-01-16 00:41:15.250087: val_loss -0.4599\n",
      "2025-01-16 00:41:15.250124: Pseudo dice [np.float32(0.5689)]\n",
      "2025-01-16 00:41:15.250158: Epoch time: 110.71 s\n",
      "2025-01-16 00:41:15.250179: Yayy! New best EMA pseudo Dice: 0.5418000221252441\n",
      "2025-01-16 00:41:16.010584: \n",
      "2025-01-16 00:41:16.010679: Epoch 31\n",
      "2025-01-16 00:41:16.010744: Current learning rate: 0.00972\n",
      "2025-01-16 00:43:06.667819: train_loss -0.6015\n",
      "2025-01-16 00:43:06.667946: val_loss -0.3979\n",
      "2025-01-16 00:43:06.667979: Pseudo dice [np.float32(0.4598)]\n",
      "2025-01-16 00:43:06.668012: Epoch time: 110.66 s\n",
      "2025-01-16 00:43:07.198883: \n",
      "2025-01-16 00:43:07.199248: Epoch 32\n",
      "2025-01-16 00:43:07.199348: Current learning rate: 0.00971\n",
      "2025-01-16 00:44:57.884516: train_loss -0.5466\n",
      "2025-01-16 00:44:57.884653: val_loss -0.5475\n",
      "2025-01-16 00:44:57.884694: Pseudo dice [np.float32(0.6418)]\n",
      "2025-01-16 00:44:57.884732: Epoch time: 110.69 s\n",
      "2025-01-16 00:44:57.884753: Yayy! New best EMA pseudo Dice: 0.5444999933242798\n",
      "2025-01-16 00:44:58.641501: \n",
      "2025-01-16 00:44:58.641876: Epoch 33\n",
      "2025-01-16 00:44:58.641962: Current learning rate: 0.0097\n",
      "2025-01-16 00:46:49.351817: train_loss -0.6105\n",
      "2025-01-16 00:46:49.351944: val_loss -0.4856\n",
      "2025-01-16 00:46:49.351977: Pseudo dice [np.float32(0.487)]\n",
      "2025-01-16 00:46:49.352009: Epoch time: 110.71 s\n",
      "2025-01-16 00:46:49.906401: \n",
      "2025-01-16 00:46:49.906697: Epoch 34\n",
      "2025-01-16 00:46:49.906823: Current learning rate: 0.00969\n",
      "2025-01-16 00:48:40.622054: train_loss -0.5955\n",
      "2025-01-16 00:48:40.622185: val_loss -0.5866\n",
      "2025-01-16 00:48:40.622217: Pseudo dice [np.float32(0.6236)]\n",
      "2025-01-16 00:48:40.622249: Epoch time: 110.72 s\n",
      "2025-01-16 00:48:40.622269: Yayy! New best EMA pseudo Dice: 0.5472000241279602\n",
      "2025-01-16 00:48:41.386691: \n",
      "2025-01-16 00:48:41.387012: Epoch 35\n",
      "2025-01-16 00:48:41.387161: Current learning rate: 0.00968\n",
      "2025-01-16 00:50:32.125230: train_loss -0.6202\n",
      "2025-01-16 00:50:32.125359: val_loss -0.573\n",
      "2025-01-16 00:50:32.125392: Pseudo dice [np.float32(0.7136)]\n",
      "2025-01-16 00:50:32.125424: Epoch time: 110.74 s\n",
      "2025-01-16 00:50:32.125444: Yayy! New best EMA pseudo Dice: 0.5637999773025513\n",
      "2025-01-16 00:50:32.892082: \n",
      "2025-01-16 00:50:32.892171: Epoch 36\n",
      "2025-01-16 00:50:32.892232: Current learning rate: 0.00968\n",
      "2025-01-16 00:52:23.635376: train_loss -0.5965\n",
      "2025-01-16 00:52:23.635576: val_loss -0.4634\n",
      "2025-01-16 00:52:23.635614: Pseudo dice [np.float32(0.5473)]\n",
      "2025-01-16 00:52:23.635647: Epoch time: 110.74 s\n",
      "2025-01-16 00:52:24.401401: \n",
      "2025-01-16 00:52:24.401494: Epoch 37\n",
      "2025-01-16 00:52:24.401693: Current learning rate: 0.00967\n",
      "2025-01-16 00:54:15.079861: train_loss -0.5873\n",
      "2025-01-16 00:54:15.080034: val_loss -0.5986\n",
      "2025-01-16 00:54:15.080107: Pseudo dice [np.float32(0.6753)]\n",
      "2025-01-16 00:54:15.080151: Epoch time: 110.68 s\n",
      "2025-01-16 00:54:15.080178: Yayy! New best EMA pseudo Dice: 0.5734999775886536\n",
      "2025-01-16 00:54:15.848769: \n",
      "2025-01-16 00:54:15.849102: Epoch 38\n",
      "2025-01-16 00:54:15.849172: Current learning rate: 0.00966\n",
      "2025-01-16 00:56:06.506340: train_loss -0.6124\n",
      "2025-01-16 00:56:06.506542: val_loss -0.552\n",
      "2025-01-16 00:56:06.506577: Pseudo dice [np.float32(0.581)]\n",
      "2025-01-16 00:56:06.506610: Epoch time: 110.66 s\n",
      "2025-01-16 00:56:06.506630: Yayy! New best EMA pseudo Dice: 0.5741999745368958\n",
      "2025-01-16 00:56:07.290314: \n",
      "2025-01-16 00:56:07.290419: Epoch 39\n",
      "2025-01-16 00:56:07.290482: Current learning rate: 0.00965\n",
      "2025-01-16 00:57:57.989501: train_loss -0.6239\n",
      "2025-01-16 00:57:57.989736: val_loss -0.5086\n",
      "2025-01-16 00:57:57.989782: Pseudo dice [np.float32(0.5368)]\n",
      "2025-01-16 00:57:57.989833: Epoch time: 110.7 s\n",
      "2025-01-16 00:57:58.547417: \n",
      "2025-01-16 00:57:58.547518: Epoch 40\n",
      "2025-01-16 00:57:58.547582: Current learning rate: 0.00964\n",
      "2025-01-16 00:59:49.320976: train_loss -0.6484\n",
      "2025-01-16 00:59:49.321167: val_loss -0.6024\n",
      "2025-01-16 00:59:49.321205: Pseudo dice [np.float32(0.6586)]\n",
      "2025-01-16 00:59:49.321239: Epoch time: 110.77 s\n",
      "2025-01-16 00:59:49.321259: Yayy! New best EMA pseudo Dice: 0.5792999863624573\n",
      "2025-01-16 00:59:50.093774: \n",
      "2025-01-16 00:59:50.093928: Epoch 41\n",
      "2025-01-16 00:59:50.094002: Current learning rate: 0.00963\n",
      "2025-01-16 01:01:40.785140: train_loss -0.6315\n",
      "2025-01-16 01:01:40.785322: val_loss -0.5106\n",
      "2025-01-16 01:01:40.785412: Pseudo dice [np.float32(0.6019)]\n",
      "2025-01-16 01:01:40.785456: Epoch time: 110.69 s\n",
      "2025-01-16 01:01:40.785478: Yayy! New best EMA pseudo Dice: 0.58160001039505\n",
      "2025-01-16 01:01:41.533702: \n",
      "2025-01-16 01:01:41.534037: Epoch 42\n",
      "2025-01-16 01:01:41.534122: Current learning rate: 0.00962\n",
      "2025-01-16 01:03:32.248402: train_loss -0.6461\n",
      "2025-01-16 01:03:32.248536: val_loss -0.5291\n",
      "2025-01-16 01:03:32.248571: Pseudo dice [np.float32(0.6408)]\n",
      "2025-01-16 01:03:32.248609: Epoch time: 110.72 s\n",
      "2025-01-16 01:03:32.248629: Yayy! New best EMA pseudo Dice: 0.5874999761581421\n",
      "2025-01-16 01:03:32.996124: \n",
      "2025-01-16 01:03:32.996276: Epoch 43\n",
      "2025-01-16 01:03:32.996343: Current learning rate: 0.00961\n",
      "2025-01-16 01:05:23.675269: train_loss -0.6001\n",
      "2025-01-16 01:05:23.675391: val_loss -0.5202\n",
      "2025-01-16 01:05:23.675424: Pseudo dice [np.float32(0.5961)]\n",
      "2025-01-16 01:05:23.675456: Epoch time: 110.68 s\n",
      "2025-01-16 01:05:23.675474: Yayy! New best EMA pseudo Dice: 0.5884000062942505\n",
      "2025-01-16 01:05:24.432277: \n",
      "2025-01-16 01:05:24.432724: Epoch 44\n",
      "2025-01-16 01:05:24.432833: Current learning rate: 0.0096\n",
      "2025-01-16 01:07:15.076459: train_loss -0.6114\n",
      "2025-01-16 01:07:15.076672: val_loss -0.5745\n",
      "2025-01-16 01:07:15.076792: Pseudo dice [np.float32(0.5487)]\n",
      "2025-01-16 01:07:15.076892: Epoch time: 110.64 s\n",
      "2025-01-16 01:07:15.609570: \n",
      "2025-01-16 01:07:15.609667: Epoch 45\n",
      "2025-01-16 01:07:15.609737: Current learning rate: 0.00959\n",
      "2025-01-16 01:09:06.293352: train_loss -0.6393\n",
      "2025-01-16 01:09:06.293748: val_loss -0.6321\n",
      "2025-01-16 01:09:06.293871: Pseudo dice [np.float32(0.6824)]\n",
      "2025-01-16 01:09:06.293921: Epoch time: 110.68 s\n",
      "2025-01-16 01:09:06.293944: Yayy! New best EMA pseudo Dice: 0.5942000150680542\n",
      "2025-01-16 01:09:07.056031: \n",
      "2025-01-16 01:09:07.056162: Epoch 46\n",
      "2025-01-16 01:09:07.056349: Current learning rate: 0.00959\n",
      "2025-01-16 01:10:57.753404: train_loss -0.6104\n",
      "2025-01-16 01:10:57.753604: val_loss -0.5337\n",
      "2025-01-16 01:10:57.753761: Pseudo dice [np.float32(0.6017)]\n",
      "2025-01-16 01:10:57.753868: Epoch time: 110.7 s\n",
      "2025-01-16 01:10:57.753961: Yayy! New best EMA pseudo Dice: 0.5949000120162964\n",
      "2025-01-16 01:10:58.507899: \n",
      "2025-01-16 01:10:58.507978: Epoch 47\n",
      "2025-01-16 01:10:58.508037: Current learning rate: 0.00958\n",
      "2025-01-16 01:12:49.046420: train_loss -0.6477\n",
      "2025-01-16 01:12:49.046545: val_loss -0.6648\n",
      "2025-01-16 01:12:49.046577: Pseudo dice [np.float32(0.7007)]\n",
      "2025-01-16 01:12:49.046609: Epoch time: 110.54 s\n",
      "2025-01-16 01:12:49.046629: Yayy! New best EMA pseudo Dice: 0.6054999828338623\n",
      "2025-01-16 01:12:49.793703: \n",
      "2025-01-16 01:12:49.793800: Epoch 48\n",
      "2025-01-16 01:12:49.793859: Current learning rate: 0.00957\n",
      "2025-01-16 01:14:40.491511: train_loss -0.6632\n",
      "2025-01-16 01:14:40.491644: val_loss -0.4511\n",
      "2025-01-16 01:14:40.491678: Pseudo dice [np.float32(0.5548)]\n",
      "2025-01-16 01:14:40.491713: Epoch time: 110.7 s\n",
      "2025-01-16 01:14:41.299184: \n",
      "2025-01-16 01:14:41.299289: Epoch 49\n",
      "2025-01-16 01:14:41.299353: Current learning rate: 0.00956\n",
      "2025-01-16 01:16:31.901270: train_loss -0.6208\n",
      "2025-01-16 01:16:31.901420: val_loss -0.5464\n",
      "2025-01-16 01:16:31.901455: Pseudo dice [np.float32(0.6122)]\n",
      "2025-01-16 01:16:31.901501: Epoch time: 110.6 s\n",
      "2025-01-16 01:16:32.656046: \n",
      "2025-01-16 01:16:32.656277: Epoch 50\n",
      "2025-01-16 01:16:32.656385: Current learning rate: 0.00955\n",
      "2025-01-16 01:18:23.423120: train_loss -0.6261\n",
      "2025-01-16 01:18:23.423261: val_loss -0.5938\n",
      "2025-01-16 01:18:23.423296: Pseudo dice [np.float32(0.6988)]\n",
      "2025-01-16 01:18:23.423331: Epoch time: 110.77 s\n",
      "2025-01-16 01:18:23.423352: Yayy! New best EMA pseudo Dice: 0.611299991607666\n",
      "2025-01-16 01:18:24.209088: \n",
      "2025-01-16 01:18:24.209198: Epoch 51\n",
      "2025-01-16 01:18:24.209263: Current learning rate: 0.00954\n",
      "2025-01-16 01:20:14.989470: train_loss -0.622\n",
      "2025-01-16 01:20:14.989671: val_loss -0.6061\n",
      "2025-01-16 01:20:14.989707: Pseudo dice [np.float32(0.6294)]\n",
      "2025-01-16 01:20:14.989741: Epoch time: 110.78 s\n",
      "2025-01-16 01:20:14.989760: Yayy! New best EMA pseudo Dice: 0.613099992275238\n",
      "2025-01-16 01:20:15.786350: \n",
      "2025-01-16 01:20:15.786712: Epoch 52\n",
      "2025-01-16 01:20:15.786785: Current learning rate: 0.00953\n",
      "2025-01-16 01:22:06.441099: train_loss -0.6227\n",
      "2025-01-16 01:22:06.441245: val_loss -0.6211\n",
      "2025-01-16 01:22:06.441381: Pseudo dice [np.float32(0.6219)]\n",
      "2025-01-16 01:22:06.441467: Epoch time: 110.66 s\n",
      "2025-01-16 01:22:06.441497: Yayy! New best EMA pseudo Dice: 0.6140000224113464\n",
      "2025-01-16 01:22:07.219306: \n",
      "2025-01-16 01:22:07.219417: Epoch 53\n",
      "2025-01-16 01:22:07.219490: Current learning rate: 0.00952\n",
      "2025-01-16 01:23:58.002681: train_loss -0.6585\n",
      "2025-01-16 01:23:58.002829: val_loss -0.5351\n",
      "2025-01-16 01:23:58.002864: Pseudo dice [np.float32(0.5683)]\n",
      "2025-01-16 01:23:58.002898: Epoch time: 110.78 s\n",
      "2025-01-16 01:23:58.565594: \n",
      "2025-01-16 01:23:58.565694: Epoch 54\n",
      "2025-01-16 01:23:58.565764: Current learning rate: 0.00951\n",
      "2025-01-16 01:25:49.321620: train_loss -0.6387\n",
      "2025-01-16 01:25:49.322117: val_loss -0.602\n",
      "2025-01-16 01:25:49.322265: Pseudo dice [np.float32(0.6284)]\n",
      "2025-01-16 01:25:49.322319: Epoch time: 110.76 s\n",
      "2025-01-16 01:25:49.882565: \n",
      "2025-01-16 01:25:49.882663: Epoch 55\n",
      "2025-01-16 01:25:49.882727: Current learning rate: 0.0095\n",
      "2025-01-16 01:27:40.680531: train_loss -0.5871\n",
      "2025-01-16 01:27:40.680663: val_loss -0.445\n",
      "2025-01-16 01:27:40.680699: Pseudo dice [np.float32(0.3334)]\n",
      "2025-01-16 01:27:40.680731: Epoch time: 110.8 s\n",
      "2025-01-16 01:27:41.219285: \n",
      "2025-01-16 01:27:41.219494: Epoch 56\n",
      "2025-01-16 01:27:41.219590: Current learning rate: 0.00949\n",
      "2025-01-16 01:29:31.956828: train_loss -0.5253\n",
      "2025-01-16 01:29:31.956996: val_loss -0.6595\n",
      "2025-01-16 01:29:31.957071: Pseudo dice [np.float32(0.699)]\n",
      "2025-01-16 01:29:31.957113: Epoch time: 110.74 s\n",
      "2025-01-16 01:29:32.499473: \n",
      "2025-01-16 01:29:32.499565: Epoch 57\n",
      "2025-01-16 01:29:32.499626: Current learning rate: 0.00949\n",
      "2025-01-16 01:31:23.027401: train_loss -0.632\n",
      "2025-01-16 01:31:23.027531: val_loss -0.603\n",
      "2025-01-16 01:31:23.027562: Pseudo dice [np.float32(0.6457)]\n",
      "2025-01-16 01:31:23.027595: Epoch time: 110.53 s\n",
      "2025-01-16 01:31:23.572525: \n",
      "2025-01-16 01:31:23.572617: Epoch 58\n",
      "2025-01-16 01:31:23.572680: Current learning rate: 0.00948\n",
      "2025-01-16 01:33:14.274844: train_loss -0.6717\n",
      "2025-01-16 01:33:14.274984: val_loss -0.486\n",
      "2025-01-16 01:33:14.275019: Pseudo dice [np.float32(0.5645)]\n",
      "2025-01-16 01:33:14.275053: Epoch time: 110.7 s\n",
      "2025-01-16 01:33:14.818468: \n",
      "2025-01-16 01:33:14.818554: Epoch 59\n",
      "2025-01-16 01:33:14.818615: Current learning rate: 0.00947\n",
      "2025-01-16 01:35:05.526270: train_loss -0.6476\n",
      "2025-01-16 01:35:05.526397: val_loss -0.5663\n",
      "2025-01-16 01:35:05.526429: Pseudo dice [np.float32(0.6379)]\n",
      "2025-01-16 01:35:05.526462: Epoch time: 110.71 s\n",
      "2025-01-16 01:35:06.074648: \n",
      "2025-01-16 01:35:06.074792: Epoch 60\n",
      "2025-01-16 01:35:06.074866: Current learning rate: 0.00946\n",
      "2025-01-16 01:36:56.767672: train_loss -0.6555\n",
      "2025-01-16 01:36:56.767800: val_loss -0.6362\n",
      "2025-01-16 01:36:56.767833: Pseudo dice [np.float32(0.6767)]\n",
      "2025-01-16 01:36:56.767865: Epoch time: 110.69 s\n",
      "2025-01-16 01:36:57.537879: \n",
      "2025-01-16 01:36:57.538245: Epoch 61\n",
      "2025-01-16 01:36:57.538327: Current learning rate: 0.00945\n",
      "2025-01-16 01:38:48.209636: train_loss -0.64\n",
      "2025-01-16 01:38:48.209807: val_loss -0.4754\n",
      "2025-01-16 01:38:48.209879: Pseudo dice [np.float32(0.5681)]\n",
      "2025-01-16 01:38:48.209920: Epoch time: 110.67 s\n",
      "2025-01-16 01:38:48.758285: \n",
      "2025-01-16 01:38:48.758560: Epoch 62\n",
      "2025-01-16 01:38:48.758713: Current learning rate: 0.00944\n",
      "2025-01-16 01:40:39.479205: train_loss -0.6381\n",
      "2025-01-16 01:40:39.479350: val_loss -0.5621\n",
      "2025-01-16 01:40:39.479387: Pseudo dice [np.float32(0.5781)]\n",
      "2025-01-16 01:40:39.479420: Epoch time: 110.72 s\n",
      "2025-01-16 01:40:40.019913: \n",
      "2025-01-16 01:40:40.020301: Epoch 63\n",
      "2025-01-16 01:40:40.020449: Current learning rate: 0.00943\n",
      "2025-01-16 01:42:30.689388: train_loss -0.6799\n",
      "2025-01-16 01:42:30.689523: val_loss -0.5785\n",
      "2025-01-16 01:42:30.689557: Pseudo dice [np.float32(0.6091)]\n",
      "2025-01-16 01:42:30.689603: Epoch time: 110.67 s\n",
      "2025-01-16 01:42:31.234155: \n",
      "2025-01-16 01:42:31.234276: Epoch 64\n",
      "2025-01-16 01:42:31.234337: Current learning rate: 0.00942\n",
      " train_loss -0.7063.940201:\n",
      "2025-01-16 01:44:21.940541: val_loss -0.632\n",
      "2025-01-16 01:44:21.940616: Pseudo dice [np.float32(0.656)]\n",
      "2025-01-16 01:44:21.940724: Epoch time: 110.71 s\n",
      "2025-01-16 01:44:22.492185: \n",
      "2025-01-16 01:44:22.492286: Epoch 65\n",
      "2025-01-16 01:44:22.492349: Current learning rate: 0.00941\n",
      "2025-01-16 01:46:13.223114: train_loss -0.6626\n",
      "2025-01-16 01:46:13.223564: val_loss -0.65\n",
      "2025-01-16 01:46:13.223679: Pseudo dice [np.float32(0.6645)]\n",
      "2025-01-16 01:46:13.223732: Epoch time: 110.73 s\n",
      "2025-01-16 01:46:13.784127: \n",
      "2025-01-16 01:46:13.784232: Epoch 66\n",
      "2025-01-16 01:46:13.784295: Current learning rate: 0.0094\n",
      "2025-01-16 01:48:04.493276: train_loss -0.5969\n",
      "2025-01-16 01:48:04.493454: val_loss -0.5499\n",
      "2025-01-16 01:48:04.493487: Pseudo dice [np.float32(0.6078)]\n",
      "2025-01-16 01:48:04.493520: Epoch time: 110.71 s\n",
      "2025-01-16 01:48:05.042684: \n",
      "2025-01-16 01:48:05.042773: Epoch 67\n",
      "2025-01-16 01:48:05.042842: Current learning rate: 0.00939\n",
      "2025-01-16 01:49:55.704318: train_loss -0.5912\n",
      "2025-01-16 01:49:55.704455: val_loss -0.6287\n",
      "2025-01-16 01:49:55.704492: Pseudo dice [np.float32(0.6981)]\n",
      "2025-01-16 01:49:55.704523: Epoch time: 110.66 s\n",
      "2025-01-16 01:49:55.704543: Yayy! New best EMA pseudo Dice: 0.621399998664856\n",
      "2025-01-16 01:49:56.478802: \n",
      "2025-01-16 01:49:56.478891: Epoch 68\n",
      "2025-01-16 01:49:56.478962: Current learning rate: 0.00939\n",
      "2025-01-16 01:51:47.218409: train_loss -0.6428\n",
      "2025-01-16 01:51:47.218538: val_loss -0.5174\n",
      "2025-01-16 01:51:47.218571: Pseudo dice [np.float32(0.622)]\n",
      "2025-01-16 01:51:47.218604: Epoch time: 110.74 s\n",
      "2025-01-16 01:51:47.218626: Yayy! New best EMA pseudo Dice: 0.6215000152587891\n",
      "2025-01-16 01:51:47.997957: \n",
      "2025-01-16 01:51:47.998051: Epoch 69\n",
      "2025-01-16 01:51:47.998121: Current learning rate: 0.00938\n",
      "2025-01-16 01:53:38.718158: train_loss -0.6553\n",
      "2025-01-16 01:53:38.718341: val_loss -0.6127\n",
      "2025-01-16 01:53:38.718379: Pseudo dice [np.float32(0.6648)]\n",
      "2025-01-16 01:53:38.718411: Epoch time: 110.72 s\n",
      "2025-01-16 01:53:38.718432: Yayy! New best EMA pseudo Dice: 0.6258000135421753\n",
      "2025-01-16 01:53:39.488799: \n",
      "2025-01-16 01:53:39.488985: Epoch 70\n",
      "2025-01-16 01:53:39.489056: Current learning rate: 0.00937\n",
      "2025-01-16 01:55:30.027581: train_loss -0.6644\n",
      "2025-01-16 01:55:30.027714: val_loss -0.3674\n",
      "2025-01-16 01:55:30.027750: Pseudo dice [np.float32(0.3885)]\n",
      "2025-01-16 01:55:30.027785: Epoch time: 110.54 s\n",
      "2025-01-16 01:55:30.579266: \n",
      "2025-01-16 01:55:30.579350: Epoch 71\n",
      "2025-01-16 01:55:30.579408: Current learning rate: 0.00936\n",
      "2025-01-16 01:57:21.306489: train_loss -0.5908\n",
      "2025-01-16 01:57:21.306625: val_loss -0.5873\n",
      "2025-01-16 01:57:21.306659: Pseudo dice [np.float32(0.5454)]\n",
      "2025-01-16 01:57:21.306692: Epoch time: 110.73 s\n",
      "2025-01-16 01:57:21.866385: \n",
      "2025-01-16 01:57:21.866472: Epoch 72\n",
      "2025-01-16 01:57:21.866537: Current learning rate: 0.00935\n",
      "2025-01-16 01:59:12.613091: train_loss -0.6493\n",
      "2025-01-16 01:59:12.613243: val_loss -0.5743\n",
      "2025-01-16 01:59:12.613313: Pseudo dice [np.float32(0.6084)]\n",
      "2025-01-16 01:59:12.613355: Epoch time: 110.75 s\n",
      "2025-01-16 01:59:13.391847: \n",
      "2025-01-16 01:59:13.391964: Epoch 73\n",
      "2025-01-16 01:59:13.392031: Current learning rate: 0.00934\n",
      "2025-01-16 02:01:03.944633: train_loss -0.6658\n",
      "2025-01-16 02:01:03.944772: val_loss -0.633\n",
      "2025-01-16 02:01:03.944832: Pseudo dice [np.float32(0.6403)]\n",
      "2025-01-16 02:01:03.944870: Epoch time: 110.55 s\n",
      "2025-01-16 02:01:04.501643: \n",
      "2025-01-16 02:01:04.501818: Epoch 74\n",
      "2025-01-16 02:01:04.502077: Current learning rate: 0.00933\n",
      "2025-01-16 02:02:55.210279: train_loss -0.6372\n",
      "2025-01-16 02:02:55.210407: val_loss -0.5573\n",
      "2025-01-16 02:02:55.210443: Pseudo dice [np.float32(0.6208)]\n",
      "2025-01-16 02:02:55.210477: Epoch time: 110.71 s\n",
      "2025-01-16 02:02:55.778722: \n",
      "2025-01-16 02:02:55.779056: Epoch 75\n",
      "2025-01-16 02:02:55.779131: Current learning rate: 0.00932\n",
      "2025-01-16 02:04:46.493260: train_loss -0.6644\n",
      "2025-01-16 02:04:46.493534: val_loss -0.5237\n",
      "2025-01-16 02:04:46.493570: Pseudo dice [np.float32(0.6228)]\n",
      "2025-01-16 02:04:46.493603: Epoch time: 110.72 s\n",
      "2025-01-16 02:04:47.046921: \n",
      "2025-01-16 02:04:47.047068: Epoch 76\n",
      "2025-01-16 02:04:47.047141: Current learning rate: 0.00931\n",
      "2025-01-16 02:06:37.759465: train_loss -0.6548\n",
      "2025-01-16 02:06:37.759594: val_loss -0.4614\n",
      "2025-01-16 02:06:37.759628: Pseudo dice [np.float32(0.6273)]\n",
      "2025-01-16 02:06:37.759661: Epoch time: 110.71 s\n",
      "2025-01-16 02:06:38.319465: \n",
      "2025-01-16 02:06:38.319660: Epoch 77\n",
      "2025-01-16 02:06:38.319735: Current learning rate: 0.0093\n",
      "2025-01-16 02:08:28.916006: train_loss -0.5848\n",
      "2025-01-16 02:08:28.916157: val_loss -0.4367\n",
      "2025-01-16 02:08:28.916196: Pseudo dice [np.float32(0.5298)]\n",
      "2025-01-16 02:08:28.916234: Epoch time: 110.6 s\n",
      "2025-01-16 02:08:29.483248: \n",
      "2025-01-16 02:08:29.483350: Epoch 78\n",
      "2025-01-16 02:08:29.483415: Current learning rate: 0.0093\n",
      "2025-01-16 02:10:20.257920: train_loss -0.6902\n",
      "2025-01-16 02:10:20.258075: val_loss -0.5608\n",
      "2025-01-16 02:10:20.258145: Pseudo dice [np.float32(0.6018)]\n",
      "2025-01-16 02:10:20.258188: Epoch time: 110.78 s\n",
      "2025-01-16 02:10:20.831432: \n",
      "2025-01-16 02:10:20.831738: Epoch 79\n",
      "2025-01-16 02:10:20.831814: Current learning rate: 0.00929\n",
      "2025-01-16 02:12:11.557476: train_loss -0.6244\n",
      "2025-01-16 02:12:11.557606: val_loss -0.4462\n",
      "2025-01-16 02:12:11.557639: Pseudo dice [np.float32(0.5575)]\n",
      "2025-01-16 02:12:11.557673: Epoch time: 110.73 s\n",
      "2025-01-16 02:12:12.120499: \n",
      "2025-01-16 02:12:12.120600: Epoch 80\n",
      "2025-01-16 02:12:12.120664: Current learning rate: 0.00928\n",
      "2025-01-16 02:14:02.759849: train_loss -0.5907\n",
      "2025-01-16 02:14:02.759970: val_loss -0.5304\n",
      "2025-01-16 02:14:02.760004: Pseudo dice [np.float32(0.6081)]\n",
      "2025-01-16 02:14:02.760038: Epoch time: 110.64 s\n",
      "2025-01-16 02:14:03.327311: \n",
      "2025-01-16 02:14:03.327593: Epoch 81\n",
      "2025-01-16 02:14:03.327683: Current learning rate: 0.00927\n",
      "2025-01-16 02:15:54.024645: train_loss -0.6276\n",
      "2025-01-16 02:15:54.024819: val_loss -0.5661\n",
      "2025-01-16 02:15:54.024868: Pseudo dice [np.float32(0.667)]\n",
      "2025-01-16 02:15:54.024907: Epoch time: 110.7 s\n",
      "2025-01-16 02:15:54.609885: \n",
      "2025-01-16 02:15:54.609973: Epoch 82\n",
      "2025-01-16 02:15:54.610038: Current learning rate: 0.00926\n",
      "2025-01-16 02:17:45.418819: train_loss -0.6445\n",
      "2025-01-16 02:17:45.418968: val_loss -0.5984\n",
      "2025-01-16 02:17:45.419001: Pseudo dice [np.float32(0.6577)]\n",
      "2025-01-16 02:17:45.419035: Epoch time: 110.81 s\n",
      "2025-01-16 02:17:45.983532: \n",
      "2025-01-16 02:17:45.983939: Epoch 83\n",
      "2025-01-16 02:17:45.984101: Current learning rate: 0.00925\n",
      "2025-01-16 02:19:36.809681: train_loss -0.6859\n",
      "2025-01-16 02:19:36.809823: val_loss -0.5872\n",
      "2025-01-16 02:19:36.809859: Pseudo dice [np.float32(0.5551)]\n",
      "2025-01-16 02:19:36.809893: Epoch time: 110.83 s\n",
      "2025-01-16 02:19:37.645257: \n",
      "2025-01-16 02:19:37.645361: Epoch 84\n",
      "2025-01-16 02:19:37.645434: Current learning rate: 0.00924\n",
      "2025-01-16 02:21:28.435041: train_loss -0.6968\n",
      "2025-01-16 02:21:28.435192: val_loss -0.6227\n",
      "2025-01-16 02:21:28.435225: Pseudo dice [np.float32(0.6845)]\n",
      "2025-01-16 02:21:28.435272: Epoch time: 110.79 s\n",
      "2025-01-16 02:21:29.016258: \n",
      "2025-01-16 02:21:29.016361: Epoch 85\n",
      "2025-01-16 02:21:29.016425: Current learning rate: 0.00923\n",
      "2025-01-16 02:23:19.770256: train_loss -0.6585\n",
      "2025-01-16 02:23:19.770376: val_loss -0.6339\n",
      "2025-01-16 02:23:19.770406: Pseudo dice [np.float32(0.6854)]\n",
      "2025-01-16 02:23:19.770438: Epoch time: 110.75 s\n",
      "2025-01-16 02:23:20.313743: \n",
      "2025-01-16 02:23:20.314043: Epoch 86\n",
      "2025-01-16 02:23:20.314213: Current learning rate: 0.00922\n",
      "2025-01-16 02:25:10.995945: train_loss -0.6696\n",
      "2025-01-16 02:25:10.996145: val_loss -0.5615\n",
      "2025-01-16 02:25:10.996243: Pseudo dice [np.float32(0.5852)]\n",
      "2025-01-16 02:25:10.996326: Epoch time: 110.68 s\n",
      "2025-01-16 02:25:11.532506: \n",
      "2025-01-16 02:25:11.532804: Epoch 87\n",
      "2025-01-16 02:25:11.532945: Current learning rate: 0.00921\n",
      "2025-01-16 02:27:02.287640: train_loss -0.6605\n",
      "2025-01-16 02:27:02.288035: val_loss -0.5483\n",
      "2025-01-16 02:27:02.288075: Pseudo dice [np.float32(0.5975)]\n",
      "2025-01-16 02:27:02.288107: Epoch time: 110.76 s\n",
      "2025-01-16 02:27:02.825723: \n",
      "2025-01-16 02:27:02.825844: Epoch 88\n",
      "2025-01-16 02:27:02.826036: Current learning rate: 0.0092\n",
      "2025-01-16 02:28:53.590229: train_loss -0.686\n",
      "2025-01-16 02:28:53.590380: val_loss -0.6283\n",
      "[np.float32(0.6312)]590430: Pseudo dice \n",
      "2025-01-16 02:28:53.590528: Epoch time: 110.77 s\n",
      "2025-01-16 02:28:54.145789: \n",
      "2025-01-16 02:28:54.146083: Epoch 89\n",
      "2025-01-16 02:28:54.146226: Current learning rate: 0.0092\n",
      "2025-01-16 02:30:44.859287: train_loss -0.6913\n",
      "2025-01-16 02:30:44.859416: val_loss -0.5201\n",
      "2025-01-16 02:30:44.859452: Pseudo dice [np.float32(0.4654)]\n",
      "2025-01-16 02:30:44.859483: Epoch time: 110.71 s\n",
      "2025-01-16 02:30:45.403722: \n",
      "2025-01-16 02:30:45.403917: Epoch 90\n",
      "2025-01-16 02:30:45.404017: Current learning rate: 0.00919\n",
      "2025-01-16 02:32:36.123265: train_loss -0.6714\n",
      "2025-01-16 02:32:36.123391: val_loss -0.5709\n",
      "2025-01-16 02:32:36.123425: Pseudo dice [np.float32(0.5865)]\n",
      "2025-01-16 02:32:36.123458: Epoch time: 110.72 s\n",
      "2025-01-16 02:32:36.664348: \n",
      "2025-01-16 02:32:36.664467: Epoch 91\n",
      "2025-01-16 02:32:36.664656: Current learning rate: 0.00918\n",
      "2025-01-16 02:34:27.417556: train_loss -0.6941\n",
      "2025-01-16 02:34:27.417714: val_loss -0.555\n",
      "2025-01-16 02:34:27.417785: Pseudo dice [np.float32(0.5303)]\n",
      "2025-01-16 02:34:27.417827: Epoch time: 110.75 s\n",
      "2025-01-16 02:34:27.958708: \n",
      "2025-01-16 02:34:27.958788: Epoch 92\n",
      "2025-01-16 02:34:27.958848: Current learning rate: 0.00917\n",
      "2025-01-16 02:36:18.685573: train_loss -0.705\n",
      "2025-01-16 02:36:18.685699: val_loss -0.6286\n",
      "2025-01-16 02:36:18.685735: Pseudo dice [np.float32(0.5958)]\n",
      "2025-01-16 02:36:18.685828: Epoch time: 110.73 s\n",
      "2025-01-16 02:36:19.217047: \n",
      "2025-01-16 02:36:19.217134: Epoch 93\n",
      "2025-01-16 02:36:19.217195: Current learning rate: 0.00916\n",
      "2025-01-16 02:38:09.945791: train_loss -0.6803\n",
      "2025-01-16 02:38:09.945915: val_loss -0.5917\n",
      "2025-01-16 02:38:09.945949: Pseudo dice [np.float32(0.6363)]\n",
      "2025-01-16 02:38:09.945983: Epoch time: 110.73 s\n",
      "2025-01-16 02:38:10.476496: \n",
      "2025-01-16 02:38:10.476896: Epoch 94\n",
      "2025-01-16 02:38:10.477026: Current learning rate: 0.00915\n",
      "2025-01-16 02:40:01.080179: train_loss -0.6573\n",
      "2025-01-16 02:40:01.080303: val_loss -0.4452\n",
      "2025-01-16 02:40:01.080349: Pseudo dice [np.float32(0.3015)]\n",
      "2025-01-16 02:40:01.080388: Epoch time: 110.6 s\n",
      "2025-01-16 02:40:01.625618: \n",
      "2025-01-16 02:40:01.625841: Epoch 95\n",
      "2025-01-16 02:40:01.625939: Current learning rate: 0.00914\n",
      "2025-01-16 02:41:52.165577: train_loss -0.6352\n",
      "2025-01-16 02:41:52.165719: val_loss -0.5781\n",
      "2025-01-16 02:41:52.165755: Pseudo dice [np.float32(0.6136)]\n",
      "2025-01-16 02:41:52.165788: Epoch time: 110.54 s\n",
      "2025-01-16 02:41:52.702536: \n",
      "2025-01-16 02:41:52.702627: Epoch 96\n",
      "2025-01-16 02:41:52.702689: Current learning rate: 0.00913\n",
      "2025-01-16 02:43:43.366006: train_loss -0.6925\n",
      "2025-01-16 02:43:43.366154: val_loss -0.5997\n",
      "2025-01-16 02:43:43.366187: Pseudo dice [np.float32(0.634)]\n",
      "2025-01-16 02:43:43.366220: Epoch time: 110.66 s\n",
      "2025-01-16 02:43:44.136321: \n",
      "2025-01-16 02:43:44.136522: Epoch 97\n",
      "2025-01-16 02:43:44.136597: Current learning rate: 0.00912\n",
      "2025-01-16 02:45:34.794732: train_loss -0.6932\n",
      "2025-01-16 02:45:34.795200: val_loss -0.6308\n",
      "2025-01-16 02:45:34.795273: Pseudo dice [np.float32(0.6915)]\n",
      "2025-01-16 02:45:34.795314: Epoch time: 110.66 s\n",
      "2025-01-16 02:45:35.336722: \n",
      "2025-01-16 02:45:35.337105: Epoch 98\n",
      "2025-01-16 02:45:35.337205: Current learning rate: 0.00911\n",
      "2025-01-16 02:47:26.027643: train_loss -0.6653\n",
      "2025-01-16 02:47:26.027769: val_loss -0.5106\n",
      "2025-01-16 02:47:26.027801: Pseudo dice [np.float32(0.6149)]\n",
      "2025-01-16 02:47:26.027833: Epoch time: 110.69 s\n",
      "2025-01-16 02:47:26.568964: \n",
      "2025-01-16 02:47:26.569189: Epoch 99\n",
      "2025-01-16 02:47:26.569382: Current learning rate: 0.0091\n",
      "2025-01-16 02:49:17.093240: train_loss -0.6682\n",
      "2025-01-16 02:49:17.093466: val_loss -0.5502\n",
      "2025-01-16 02:49:17.093512: Pseudo dice [np.float32(0.6002)]\n",
      "2025-01-16 02:49:17.093547: Epoch time: 110.52 s\n",
      "2025-01-16 02:49:17.865355: \n",
      "2025-01-16 02:49:17.865457: Epoch 100\n",
      "2025-01-16 02:49:17.865521: Current learning rate: 0.0091\n",
      "2025-01-16 02:51:08.551140: train_loss -0.6388\n",
      "2025-01-16 02:51:08.551271: val_loss -0.5555\n",
      "2025-01-16 02:51:08.551307: Pseudo dice [np.float32(0.638)]\n",
      "2025-01-16 02:51:08.551411: Epoch time: 110.69 s\n",
      "2025-01-16 02:51:09.099164: \n",
      "2025-01-16 02:51:09.099263: Epoch 101\n",
      "2025-01-16 02:51:09.099324: Current learning rate: 0.00909\n",
      "2025-01-16 02:52:59.807317: train_loss -0.6593\n",
      "2025-01-16 02:52:59.807453: val_loss -0.5684\n",
      "2025-01-16 02:52:59.807509: Pseudo dice [np.float32(0.5277)]\n",
      "2025-01-16 02:52:59.807549: Epoch time: 110.71 s\n",
      "2025-01-16 02:53:00.348162: \n",
      "2025-01-16 02:53:00.348253: Epoch 102\n",
      "2025-01-16 02:53:00.348314: Current learning rate: 0.00908\n",
      "2025-01-16 02:54:51.027385: train_loss -0.6566\n",
      "2025-01-16 02:54:51.027519: val_loss -0.5376\n",
      "2025-01-16 02:54:51.027554: Pseudo dice [np.float32(0.5883)]\n",
      "2025-01-16 02:54:51.027589: Epoch time: 110.68 s\n",
      "2025-01-16 02:54:51.575339: \n",
      "2025-01-16 02:54:51.575547: Epoch 103\n",
      "2025-01-16 02:54:51.575676: Current learning rate: 0.00907\n",
      "2025-01-16 02:56:42.133726: train_loss -0.6774\n",
      "2025-01-16 02:56:42.133853: val_loss -0.6104\n",
      "2025-01-16 02:56:42.133886: Pseudo dice [np.float32(0.6866)]\n",
      "2025-01-16 02:56:42.133920: Epoch time: 110.56 s\n",
      "2025-01-16 02:56:42.680374: \n",
      "2025-01-16 02:56:42.680696: Epoch 104\n",
      "2025-01-16 02:56:42.680787: Current learning rate: 0.00906\n",
      "2025-01-16 02:58:33.418512: train_loss -0.6943\n",
      "2025-01-16 02:58:33.418724: val_loss -0.5502\n",
      "2025-01-16 02:58:33.418760: Pseudo dice [np.float32(0.6069)]\n",
      "2025-01-16 02:58:33.418792: Epoch time: 110.74 s\n",
      "2025-01-16 02:58:33.959585: \n",
      "2025-01-16 02:58:33.959872: Epoch 105\n",
      "2025-01-16 02:58:33.959984: Current learning rate: 0.00905\n",
      "2025-01-16 03:00:24.674326: train_loss -0.655\n",
      "2025-01-16 03:00:24.674453: val_loss -0.533\n",
      "2025-01-16 03:00:24.674486: Pseudo dice [np.float32(0.5666)]\n",
      "2025-01-16 03:00:24.674518: Epoch time: 110.72 s\n",
      "2025-01-16 03:00:25.219196: \n",
      "2025-01-16 03:00:25.219399: Epoch 106\n",
      "2025-01-16 03:00:25.219535: Current learning rate: 0.00904\n",
      "2025-01-16 03:02:15.916177: train_loss -0.6751\n",
      "2025-01-16 03:02:15.916302: val_loss -0.6459\n",
      "2025-01-16 03:02:15.916337: Pseudo dice [np.float32(0.7416)]\n",
      "2025-01-16 03:02:15.916370: Epoch time: 110.7 s\n",
      "2025-01-16 03:02:16.458463: \n",
      "2025-01-16 03:02:16.458553: Epoch 107\n",
      "2025-01-16 03:02:16.458616: Current learning rate: 0.00903\n",
      "2025-01-16 03:04:07.177934: train_loss -0.6906\n",
      "2025-01-16 03:04:07.178291: val_loss -0.6444\n",
      "2025-01-16 03:04:07.178330: Pseudo dice [np.float32(0.7173)]\n",
      "2025-01-16 03:04:07.178362: Epoch time: 110.72 s\n",
      "2025-01-16 03:04:07.721505: \n",
      "2025-01-16 03:04:07.721851: Epoch 108\n",
      "2025-01-16 03:04:07.721919: Current learning rate: 0.00902\n",
      "2025-01-16 03:05:58.442806: train_loss -0.706\n",
      "2025-01-16 03:05:58.442944: val_loss -0.5905\n",
      "2025-01-16 03:05:58.442979: Pseudo dice [np.float32(0.6153)]\n",
      "2025-01-16 03:05:58.443012: Epoch time: 110.72 s\n",
      "2025-01-16 03:05:59.256084: \n",
      "2025-01-16 03:05:59.256478: Epoch 109\n",
      "2025-01-16 03:05:59.256567: Current learning rate: 0.00901\n",
      "2025-01-16 03:07:49.942205: train_loss -0.7283\n",
      "2025-01-16 03:07:49.942420: val_loss -0.5748\n",
      "2025-01-16 03:07:49.942465: Pseudo dice [np.float32(0.6329)]\n",
      "2025-01-16 03:07:49.942499: Epoch time: 110.69 s\n",
      "2025-01-16 03:07:50.482583: \n",
      "2025-01-16 03:07:50.482769: Epoch 110\n",
      "2025-01-16 03:07:50.482843: Current learning rate: 0.009\n",
      "2025-01-16 03:09:41.186301: train_loss -0.7103\n",
      "2025-01-16 03:09:41.186424: val_loss -0.5851\n",
      "2025-01-16 03:09:41.186459: Pseudo dice [np.float32(0.6253)]\n",
      "2025-01-16 03:09:41.186494: Epoch time: 110.7 s\n",
      "2025-01-16 03:09:41.760795: \n",
      "2025-01-16 03:09:41.760890: Epoch 111\n",
      "2025-01-16 03:09:41.760951: Current learning rate: 0.009\n",
      "2025-01-16 03:11:32.341848: train_loss -0.6279\n",
      "2025-01-16 03:11:32.341977: val_loss -0.5237\n",
      "2025-01-16 03:11:32.342010: Pseudo dice [np.float32(0.5873)]\n",
      "2025-01-16 03:11:32.342042: Epoch time: 110.58 s\n",
      "2025-01-16 03:11:32.899122: \n",
      "2025-01-16 03:11:32.899258: Epoch 112\n",
      "2025-01-16 03:11:32.899380: Current learning rate: 0.00899\n",
      "2025-01-16 03:13:23.657362: train_loss -0.6318\n",
      "2025-01-16 03:13:23.657771: val_loss -0.5844\n",
      "2025-01-16 03:13:23.657897: Pseudo dice [np.float32(0.6372)]\n",
      "2025-01-16 03:13:23.657962: Epoch time: 110.76 s\n",
      "2025-01-16 03:13:24.209504: \n",
      "2025-01-16 03:13:24.209948: Epoch 113\n",
      "2025-01-16 03:13:24.210043: Current learning rate: 0.00898\n",
      "2025-01-16 03:15:15.001012: train_loss -0.6326\n",
      "2025-01-16 03:15:15.001156: val_loss -0.467\n",
      "2025-01-16 03:15:15.001193: Pseudo dice [np.float32(0.556)]\n",
      "2025-01-16 03:15:15.001236: Epoch time: 110.79 s\n",
      "2025-01-16 03:15:15.567653: \n",
      "2025-01-16 03:15:15.567755: Epoch 114\n",
      "2025-01-16 03:15:15.567821: Current learning rate: 0.00897\n",
      "2025-01-16 03:17:06.401057: train_loss -0.6418\n",
      "2025-01-16 03:17:06.401188: val_loss -0.61\n",
      "2025-01-16 03:17:06.401222: Pseudo dice [np.float32(0.6532)]\n",
      "2025-01-16 03:17:06.401257: Epoch time: 110.83 s\n",
      "2025-01-16 03:17:06.970479: \n",
      "2025-01-16 03:17:06.970582: Epoch 115\n",
      "2025-01-16 03:17:06.970654: Current learning rate: 0.00896\n",
      "2025-01-16 03:18:57.788214: train_loss -0.6535\n",
      "2025-01-16 03:18:57.788346: val_loss -0.6486\n",
      "2025-01-16 03:18:57.788379: Pseudo dice [np.float32(0.6938)]\n",
      "2025-01-16 03:18:57.788412: Epoch time: 110.82 s\n",
      "2025-01-16 03:18:57.788436: Yayy! New best EMA pseudo Dice: 0.6259999871253967\n",
      "2025-01-16 03:18:58.590159: \n",
      "2025-01-16 03:18:58.590546: Epoch 116\n",
      "2025-01-16 03:18:58.590742: Current learning rate: 0.00895\n",
      "2025-01-16 03:20:49.349375: train_loss -0.6856\n",
      "2025-01-16 03:20:49.349527: val_loss -0.5659\n",
      "2025-01-16 03:20:49.349567: Pseudo dice [np.float32(0.5994)]\n",
      "2025-01-16 03:20:49.349602: Epoch time: 110.76 s\n",
      "2025-01-16 03:20:49.967601: \n",
      "2025-01-16 03:20:49.967968: Epoch 117\n",
      "2025-01-16 03:20:49.968115: Current learning rate: 0.00894\n",
      "2025-01-16 03:22:40.775405: train_loss -0.6493\n",
      "2025-01-16 03:22:40.775532: val_loss -0.5918\n",
      "2025-01-16 03:22:40.775566: Pseudo dice [np.float32(0.6347)]\n",
      "2025-01-16 03:22:40.775599: Epoch time: 110.81 s\n",
      "2025-01-16 03:22:41.323573: \n",
      "2025-01-16 03:22:41.323685: Epoch 118\n",
      "2025-01-16 03:22:41.323845: Current learning rate: 0.00893\n",
      "2025-01-16 03:24:31.922607: train_loss -0.7054\n",
      "2025-01-16 03:24:31.922741: val_loss -0.6203\n",
      "2025-01-16 03:24:31.922779: Pseudo dice [np.float32(0.6512)]\n",
      "2025-01-16 03:24:31.922813: Epoch time: 110.6 s\n",
      "2025-01-16 03:24:31.922833: Yayy! New best EMA pseudo Dice: 0.6272000074386597\n",
      "2025-01-16 03:24:32.687510: \n",
      "2025-01-16 03:24:32.687593: Epoch 119\n",
      "2025-01-16 03:24:32.687657: Current learning rate: 0.00892\n",
      "2025-01-16 03:26:23.436298: train_loss -0.6809\n",
      "2025-01-16 03:26:23.436418: val_loss -0.5315\n",
      "2025-01-16 03:26:23.436453: Pseudo dice [np.float32(0.5485)]\n",
      "2025-01-16 03:26:23.436489: Epoch time: 110.75 s\n",
      "2025-01-16 03:26:23.987480: \n",
      "2025-01-16 03:26:23.987786: Epoch 120\n",
      "2025-01-16 03:26:23.987941: Current learning rate: 0.00891\n",
      "2025-01-16 03:28:14.792298: train_loss -0.6927\n",
      "2025-01-16 03:28:14.792490: val_loss -0.5277\n",
      "2025-01-16 03:28:14.792522: Pseudo dice [np.float32(0.629)]\n",
      "2025-01-16 03:28:14.792554: Epoch time: 110.81 s\n",
      "2025-01-16 03:28:15.581944: \n",
      "2025-01-16 03:28:15.582267: Epoch 121\n",
      "2025-01-16 03:28:15.582401: Current learning rate: 0.0089\n",
      "2025-01-16 03:30:06.341954: train_loss -0.7131\n",
      "2025-01-16 03:30:06.342130: val_loss -0.4935\n",
      "2025-01-16 03:30:06.342169: Pseudo dice [np.float32(0.62)]\n",
      "2025-01-16 03:30:06.342202: Epoch time: 110.76 s\n",
      "2025-01-16 03:30:06.899166: \n",
      "2025-01-16 03:30:06.899271: Epoch 122\n",
      "2025-01-16 03:30:06.899334: Current learning rate: 0.00889\n",
      "2025-01-16 03:31:57.620278: train_loss -0.6998\n",
      "2025-01-16 03:31:57.620412: val_loss -0.6193\n",
      "2025-01-16 03:31:57.620445: Pseudo dice [np.float32(0.6516)]\n",
      "2025-01-16 03:31:57.620479: Epoch time: 110.72 s\n",
      "2025-01-16 03:31:58.165317: \n",
      "2025-01-16 03:31:58.165508: Epoch 123\n",
      "2025-01-16 03:31:58.165582: Current learning rate: 0.00889\n",
      "2025-01-16 03:33:48.816262: train_loss -0.7076\n",
      "2025-01-16 03:33:48.816386: val_loss -0.597\n",
      "2025-01-16 03:33:48.816418: Pseudo dice [np.float32(0.6584)]\n",
      "2025-01-16 03:33:48.816450: Epoch time: 110.65 s\n",
      "2025-01-16 03:33:49.372198: \n",
      "2025-01-16 03:33:49.372541: Epoch 124\n",
      "2025-01-16 03:33:49.372616: Current learning rate: 0.00888\n",
      "2025-01-16 03:35:40.148557: train_loss -0.7002\n",
      "2025-01-16 03:35:40.148689: val_loss -0.4911\n",
      "2025-01-16 03:35:40.148860: Pseudo dice [np.float32(0.5711)]\n",
      "2025-01-16 03:35:40.148932: Epoch time: 110.78 s\n",
      "2025-01-16 03:35:40.701716: \n",
      "2025-01-16 03:35:40.702086: Epoch 125\n",
      "2025-01-16 03:35:40.702162: Current learning rate: 0.00887\n",
      "2025-01-16 03:37:31.430953: train_loss -0.7216\n",
      "2025-01-16 03:37:31.431278: val_loss -0.5216\n",
      "2025-01-16 03:37:31.431348: Pseudo dice [np.float32(0.4586)]\n",
      "2025-01-16 03:37:31.431391: Epoch time: 110.73 s\n",
      "2025-01-16 03:37:31.992225: \n",
      "2025-01-16 03:37:31.992528: Epoch 126\n",
      "2025-01-16 03:37:31.992716: Current learning rate: 0.00886\n",
      "2025-01-16 03:39:22.754761: train_loss -0.7265\n",
      "2025-01-16 03:39:22.754885: val_loss -0.6458\n",
      "2025-01-16 03:39:22.754983: Pseudo dice [np.float32(0.6265)]\n",
      "2025-01-16 03:39:22.755095: Epoch time: 110.76 s\n",
      "2025-01-16 03:39:23.309611: \n",
      "2025-01-16 03:39:23.309921: Epoch 127\n",
      "2025-01-16 03:39:23.309991: Current learning rate: 0.00885\n",
      "2025-01-16 03:41:14.080404: train_loss -0.6772\n",
      "2025-01-16 03:41:14.080652: val_loss -0.5162\n",
      "2025-01-16 03:41:14.080693: Pseudo dice [np.float32(0.6159)]\n",
      "2025-01-16 03:41:14.080726: Epoch time: 110.77 s\n",
      "2025-01-16 03:41:14.633183: \n",
      "2025-01-16 03:41:14.633335: Epoch 128\n",
      "2025-01-16 03:41:14.633539: Current learning rate: 0.00884\n",
      "2025-01-16 03:43:05.369737: train_loss -0.7056\n",
      "2025-01-16 03:43:05.369897: val_loss -0.4393\n",
      "2025-01-16 03:43:05.369941: Pseudo dice [np.float32(0.508)]\n",
      "2025-01-16 03:43:05.370006: Epoch time: 110.74 s\n",
      "2025-01-16 03:43:05.923300: \n",
      "2025-01-16 03:43:05.923704: Epoch 129\n",
      "2025-01-16 03:43:05.923790: Current learning rate: 0.00883\n",
      "2025-01-16 03:44:56.695841: train_loss -0.6979\n",
      "2025-01-16 03:44:56.695973: val_loss -0.4637\n",
      "2025-01-16 03:44:56.696008: Pseudo dice [np.float32(0.5649)]\n",
      "2025-01-16 03:44:56.696041: Epoch time: 110.77 s\n",
      "2025-01-16 03:44:57.253356: \n",
      "2025-01-16 03:44:57.253674: Epoch 130\n",
      "2025-01-16 03:44:57.253801: Current learning rate: 0.00882\n",
      "2025-01-16 03:46:48.036774: train_loss -0.6967\n",
      "2025-01-16 03:46:48.036938: val_loss -0.5784\n",
      "2025-01-16 03:46:48.036976: Pseudo dice [np.float32(0.6227)]\n",
      "2025-01-16 03:46:48.037007: Epoch time: 110.78 s\n",
      "2025-01-16 03:46:48.592665: \n",
      "2025-01-16 03:46:48.592757: Epoch 131\n",
      "2025-01-16 03:46:48.592819: Current learning rate: 0.00881\n",
      "2025-01-16 03:48:39.312855: train_loss -0.7187\n",
      "2025-01-16 03:48:39.313186: val_loss -0.6928\n",
      "2025-01-16 03:48:39.313227: Pseudo dice [np.float32(0.6915)]\n",
      "2025-01-16 03:48:39.313260: Epoch time: 110.72 s\n",
      "2025-01-16 03:48:39.864393: \n",
      "2025-01-16 03:48:39.864474: Epoch 132\n",
      "2025-01-16 03:48:39.864535: Current learning rate: 0.0088\n",
      "2025-01-16 03:50:30.644717: train_loss -0.6935\n",
      "2025-01-16 03:50:30.644842: val_loss -0.5735\n",
      "2025-01-16 03:50:30.644876: Pseudo dice [np.float32(0.5577)]\n",
      "2025-01-16 03:50:30.644911: Epoch time: 110.78 s\n",
      "2025-01-16 03:50:31.435290: \n",
      "2025-01-16 03:50:31.435588: Epoch 133\n",
      "2025-01-16 03:50:31.435686: Current learning rate: 0.00879\n",
      "2025-01-16 03:52:22.211065: train_loss -0.6778\n",
      "2025-01-16 03:52:22.211193: val_loss -0.6157\n",
      "2025-01-16 03:52:22.211227: Pseudo dice [np.float32(0.6749)]\n",
      "2025-01-16 03:52:22.211259: Epoch time: 110.78 s\n",
      "2025-01-16 03:52:22.770467: \n",
      "2025-01-16 03:52:22.770616: Epoch 134\n",
      "2025-01-16 03:52:22.770689: Current learning rate: 0.00879\n",
      "2025-01-16 03:54:13.426898: train_loss -0.6867\n",
      "2025-01-16 03:54:13.427080: val_loss -0.6475\n",
      "2025-01-16 03:54:13.427115: Pseudo dice [np.float32(0.6569)]\n",
      "2025-01-16 03:54:13.427147: Epoch time: 110.66 s\n",
      "2025-01-16 03:54:13.993975: \n",
      "2025-01-16 03:54:13.994308: Epoch 135\n",
      "2025-01-16 03:54:13.994474: Current learning rate: 0.00878\n",
      "2025-01-16 03:56:04.716899: train_loss -0.7271\n",
      "2025-01-16 03:56:04.717022: val_loss -0.6748\n",
      "2025-01-16 03:56:04.717058: Pseudo dice [np.float32(0.658)]\n",
      "2025-01-16 03:56:04.717092: Epoch time: 110.72 s\n",
      "2025-01-16 03:56:05.279549: \n",
      "2025-01-16 03:56:05.279644: Epoch 136\n",
      "2025-01-16 03:56:05.279705: Current learning rate: 0.00877\n",
      "2025-01-16 03:57:56.007614: train_loss -0.7071\n",
      "2025-01-16 03:57:56.007876: val_loss -0.5932\n",
      "2025-01-16 03:57:56.007913: Pseudo dice [np.float32(0.61)]\n",
      "2025-01-16 03:57:56.007946: Epoch time: 110.73 s\n",
      "2025-01-16 03:57:56.566586: \n",
      "2025-01-16 03:57:56.566765: Epoch 137\n",
      "2025-01-16 03:57:56.566838: Current learning rate: 0.00876\n",
      "2025-01-16 03:59:47.325495: train_loss -0.7301\n",
      "2025-01-16 03:59:47.325662: val_loss -0.616\n",
      "2025-01-16 03:59:47.325737: Pseudo dice [np.float32(0.6646)]\n",
      "2025-01-16 03:59:47.325788: Epoch time: 110.76 s\n",
      "2025-01-16 03:59:47.883000: \n",
      "2025-01-16 03:59:47.883103: Epoch 138\n",
      "2025-01-16 03:59:47.883169: Current learning rate: 0.00875\n",
      "2025-01-16 04:01:38.491993: train_loss -0.7414\n",
      "2025-01-16 04:01:38.492178: val_loss -0.6417\n",
      "2025-01-16 04:01:38.492229: Pseudo dice [np.float32(0.5601)]\n",
      "2025-01-16 04:01:38.492391: Epoch time: 110.61 s\n",
      "2025-01-16 04:01:39.041391: \n",
      "2025-01-16 04:01:39.041566: Epoch 139\n",
      "2025-01-16 04:01:39.041649: Current learning rate: 0.00874\n",
      "2025-01-16 04:03:29.803827: train_loss -0.7141\n",
      "2025-01-16 04:03:29.804013: val_loss -0.5861\n",
      "2025-01-16 04:03:29.804047: Pseudo dice [np.float32(0.6575)]\n",
      "2025-01-16 04:03:29.804080: Epoch time: 110.76 s\n",
      "2025-01-16 04:03:30.355448: \n",
      "2025-01-16 04:03:30.355548: Epoch 140\n",
      "2025-01-16 04:03:30.355633: Current learning rate: 0.00873\n",
      "2025-01-16 04:05:21.115188: train_loss -0.707\n",
      "2025-01-16 04:05:21.115313: val_loss -0.6039\n",
      "2025-01-16 04:05:21.115348: Pseudo dice [np.float32(0.6168)]\n",
      "2025-01-16 04:05:21.115382: Epoch time: 110.76 s\n",
      "2025-01-16 04:05:21.674395: \n",
      "2025-01-16 04:05:21.674983: Epoch 141\n",
      "2025-01-16 04:05:21.675072: Current learning rate: 0.00872\n",
      "2025-01-16 04:07:12.232490: train_loss -0.6888\n",
      "2025-01-16 04:07:12.232633: val_loss -0.5967\n",
      "2025-01-16 04:07:12.232671: Pseudo dice [np.float32(0.5792)]\n",
      "2025-01-16 04:07:12.232716: Epoch time: 110.56 s\n",
      "2025-01-16 04:07:12.786458: \n",
      "2025-01-16 04:07:12.786548: Epoch 142\n",
      "2025-01-16 04:07:12.786610: Current learning rate: 0.00871\n",
      "2025-01-16 04:09:03.499054: train_loss -0.6912\n",
      "2025-01-16 04:09:03.499191: val_loss -0.6588\n",
      "2025-01-16 04:09:03.499314: Pseudo dice [np.float32(0.6725)]\n",
      "2025-01-16 04:09:03.499409: Epoch time: 110.71 s\n",
      "2025-01-16 04:09:04.047821: \n",
      "2025-01-16 04:09:04.047914: Epoch 143\n",
      "2025-01-16 04:09:04.047980: Current learning rate: 0.0087\n",
      "2025-01-16 04:10:54.709746: train_loss -0.7071\n",
      "2025-01-16 04:10:54.709949: val_loss -0.5561\n",
      "2025-01-16 04:10:54.710063: Pseudo dice [np.float32(0.5171)]\n",
      "2025-01-16 04:10:54.710105: Epoch time: 110.66 s\n",
      "2025-01-16 04:10:55.266101: \n",
      "2025-01-16 04:10:55.266496: Epoch 144\n",
      "2025-01-16 04:10:55.266591: Current learning rate: 0.00869\n",
      "2025-01-16 04:12:45.828691: train_loss -0.7152\n",
      "2025-01-16 04:12:45.828816: val_loss -0.6811\n",
      "2025-01-16 04:12:45.828848: Pseudo dice [np.float32(0.6951)]\n",
      "2025-01-16 04:12:45.828881: Epoch time: 110.56 s\n",
      "2025-01-16 04:12:46.376648: \n",
      "2025-01-16 04:12:46.376813: Epoch 145\n",
      "2025-01-16 04:12:46.376896: Current learning rate: 0.00868\n",
      "2025-01-16 04:14:36.990936: train_loss -0.6989\n",
      "2025-01-16 04:14:36.991098: val_loss -0.5468\n",
      "2025-01-16 04:14:36.991178: Pseudo dice [np.float32(0.6116)]\n",
      "2025-01-16 04:14:36.991218: Epoch time: 110.61 s\n",
      "2025-01-16 04:14:37.781524: \n",
      "2025-01-16 04:14:37.781746: Epoch 146\n",
      "2025-01-16 04:14:37.781843: Current learning rate: 0.00868\n",
      "2025-01-16 04:16:28.479929: train_loss -0.6496\n",
      "2025-01-16 04:16:28.480113: val_loss -0.5575\n",
      "2025-01-16 04:16:28.480149: Pseudo dice [np.float32(0.6042)]\n",
      "2025-01-16 04:16:28.480183: Epoch time: 110.7 s\n",
      "2025-01-16 04:16:29.053709: \n",
      "2025-01-16 04:16:29.054084: Epoch 147\n",
      "2025-01-16 04:16:29.054230: Current learning rate: 0.00867\n",
      "2025-01-16 04:18:19.683787: train_loss -0.6938\n",
      "2025-01-16 04:18:19.683923: val_loss -0.4529\n",
      "2025-01-16 04:18:19.683973: Pseudo dice [np.float32(0.4583)]\n",
      "2025-01-16 04:18:19.684008: Epoch time: 110.63 s\n",
      "2025-01-16 04:18:20.259013: \n",
      "2025-01-16 04:18:20.259102: Epoch 148\n",
      "2025-01-16 04:18:20.259169: Current learning rate: 0.00866\n",
      "2025-01-16 04:20:10.936590: train_loss -0.6935\n",
      "2025-01-16 04:20:10.936762: val_loss -0.6057\n",
      "2025-01-16 04:20:10.936808: Pseudo dice [np.float32(0.7235)]\n",
      "2025-01-16 04:20:10.936849: Epoch time: 110.68 s\n",
      "2025-01-16 04:20:11.510583: \n",
      "2025-01-16 04:20:11.510942: Epoch 149\n",
      "2025-01-16 04:20:11.511029: Current learning rate: 0.00865\n",
      "2025-01-16 04:22:02.335894: train_loss -0.7078\n",
      "2025-01-16 04:22:02.336253: val_loss -0.4914\n",
      "2025-01-16 04:22:02.336302: Pseudo dice [np.float32(0.5337)]\n",
      "2025-01-16 04:22:02.336338: Epoch time: 110.83 s\n",
      "2025-01-16 04:22:03.146713: \n",
      "2025-01-16 04:22:03.147139: Epoch 150\n",
      "2025-01-16 04:22:03.147294: Current learning rate: 0.00864\n",
      "2025-01-16 04:23:53.796559: train_loss -0.7095\n",
      "2025-01-16 04:23:53.796696: val_loss -0.5889\n",
      "2025-01-16 04:23:53.796730: Pseudo dice [np.float32(0.6706)]\n",
      "2025-01-16 04:23:53.796764: Epoch time: 110.65 s\n",
      "2025-01-16 04:23:54.356530: \n",
      "2025-01-16 04:23:54.356726: Epoch 151\n",
      "2025-01-16 04:23:54.356805: Current learning rate: 0.00863\n",
      "2025-01-16 04:25:45.081950: train_loss -0.6944\n",
      "2025-01-16 04:25:45.082113: val_loss -0.5499\n",
      "2025-01-16 04:25:45.082152: Pseudo dice [np.float32(0.6134)]\n",
      "2025-01-16 04:25:45.082205: Epoch time: 110.73 s\n",
      "2025-01-16 04:25:45.641026: \n",
      "2025-01-16 04:25:45.641127: Epoch 152\n",
      "2025-01-16 04:25:45.641190: Current learning rate: 0.00862\n",
      "2025-01-16 04:27:36.243134: train_loss -0.7264\n",
      "2025-01-16 04:27:36.243283: val_loss -0.6519\n",
      "2025-01-16 04:27:36.243323: Pseudo dice [np.float32(0.639)]\n",
      "2025-01-16 04:27:36.243360: Epoch time: 110.6 s\n",
      "2025-01-16 04:27:36.805690: \n",
      "2025-01-16 04:27:36.805898: Epoch 153\n",
      "2025-01-16 04:27:36.805997: Current learning rate: 0.00861\n",
      "2025-01-16 04:29:27.575361: train_loss -0.7337\n",
      "2025-01-16 04:29:27.575482: val_loss -0.536\n",
      "2025-01-16 04:29:27.575588: Pseudo dice [np.float32(0.5997)]\n",
      "2025-01-16 04:29:27.575638: Epoch time: 110.77 s\n",
      "2025-01-16 04:29:28.133848: \n",
      "2025-01-16 04:29:28.134127: Epoch 154\n",
      "2025-01-16 04:29:28.134220: Current learning rate: 0.0086\n",
      "2025-01-16 04:31:18.889282: train_loss -0.7018\n",
      "2025-01-16 04:31:18.889668: val_loss -0.5211\n",
      "2025-01-16 04:31:18.889864: Pseudo dice [np.float32(0.5558)]\n",
      "2025-01-16 04:31:18.889931: Epoch time: 110.76 s\n",
      "2025-01-16 04:31:19.449753: \n",
      "2025-01-16 04:31:19.449851: Epoch 155\n",
      "2025-01-16 04:31:19.449915: Current learning rate: 0.00859\n",
      "2025-01-16 04:33:10.236352: train_loss -0.7276\n",
      "2025-01-16 04:33:10.236487: val_loss -0.5774\n",
      "2025-01-16 04:33:10.236524: Pseudo dice [np.float32(0.6082)]\n",
      "2025-01-16 04:33:10.236575: Epoch time: 110.79 s\n",
      "2025-01-16 04:33:10.798454: \n",
      "2025-01-16 04:33:10.798714: Epoch 156\n",
      "2025-01-16 04:33:10.798785: Current learning rate: 0.00858\n",
      "2025-01-16 04:35:01.560687: train_loss -0.7284\n",
      "2025-01-16 04:35:01.560851: val_loss -0.69\n",
      "2025-01-16 04:35:01.560892: Pseudo dice [np.float32(0.7112)]\n",
      "2025-01-16 04:35:01.560926: Epoch time: 110.76 s\n",
      "2025-01-16 04:35:02.338962: \n",
      "2025-01-16 04:35:02.339104: Epoch 157\n",
      "2025-01-16 04:35:02.339191: Current learning rate: 0.00858\n",
      "2025-01-16 04:36:53.065956: train_loss -0.7191\n",
      "2025-01-16 04:36:53.066091: val_loss -0.5545\n",
      "2025-01-16 04:36:53.066122: Pseudo dice [np.float32(0.5625)]\n",
      "2025-01-16 04:36:53.066154: Epoch time: 110.73 s\n",
      "2025-01-16 04:36:53.628583: \n",
      "2025-01-16 04:36:53.628836: Epoch 158\n",
      "2025-01-16 04:36:53.628926: Current learning rate: 0.00857\n",
      "2025-01-16 04:38:44.291816: train_loss -0.6982\n",
      "2025-01-16 04:38:44.291997: val_loss -0.6587\n",
      "2025-01-16 04:38:44.292032: Pseudo dice [np.float32(0.6512)]\n",
      "2025-01-16 04:38:44.292065: Epoch time: 110.66 s\n",
      "2025-01-16 04:38:44.856128: \n",
      "2025-01-16 04:38:44.856495: Epoch 159\n",
      "2025-01-16 04:38:44.856590: Current learning rate: 0.00856\n",
      "2025-01-16 04:40:35.650357: train_loss -0.7267\n",
      "2025-01-16 04:40:35.650487: val_loss -0.5792\n",
      "2025-01-16 04:40:35.650522: Pseudo dice [np.float32(0.5055)]\n",
      "2025-01-16 04:40:35.650555: Epoch time: 110.79 s\n",
      "2025-01-16 04:40:36.211664: \n",
      "2025-01-16 04:40:36.211768: Epoch 160\n",
      "2025-01-16 04:40:36.211835: Current learning rate: 0.00855\n",
      "2025-01-16 04:42:26.874217: train_loss -0.718\n",
      "2025-01-16 04:42:26.874546: val_loss -0.5507\n",
      "2025-01-16 04:42:26.874705: Pseudo dice [np.float32(0.5658)]\n",
      "2025-01-16 04:42:26.874784: Epoch time: 110.66 s\n",
      "2025-01-16 04:42:27.439080: \n",
      "2025-01-16 04:42:27.439283: Epoch 161\n",
      "2025-01-16 04:42:27.439352: Current learning rate: 0.00854\n",
      "2025-01-16 04:44:18.144067: train_loss -0.6922\n",
      "2025-01-16 04:44:18.144248: val_loss -0.6123\n",
      "2025-01-16 04:44:18.144318: Pseudo dice [np.float32(0.6095)]\n",
      "2025-01-16 04:44:18.144360: Epoch time: 110.71 s\n",
      "2025-01-16 04:44:18.708164: \n",
      "2025-01-16 04:44:18.708328: Epoch 162\n",
      "2025-01-16 04:44:18.708400: Current learning rate: 0.00853\n",
      "2025-01-16 04:46:09.468641: train_loss -0.7244\n",
      "2025-01-16 04:46:09.468767: val_loss -0.6396\n",
      "2025-01-16 04:46:09.468805: Pseudo dice [np.float32(0.6512)]\n",
      "2025-01-16 04:46:09.468854: Epoch time: 110.76 s\n",
      "2025-01-16 04:46:10.029934: \n",
      "2025-01-16 04:46:10.030267: Epoch 163\n",
      "2025-01-16 04:46:10.030436: Current learning rate: 0.00852\n",
      "2025-01-16 04:48:00.781184: train_loss -0.7155\n",
      "2025-01-16 04:48:00.781473: val_loss -0.6645\n",
      "2025-01-16 04:48:00.781516: Pseudo dice [np.float32(0.704)]\n",
      "2025-01-16 04:48:00.781551: Epoch time: 110.75 s\n",
      "2025-01-16 04:48:01.348434: \n",
      "2025-01-16 04:48:01.348840: Epoch 164\n",
      "2025-01-16 04:48:01.348908: Current learning rate: 0.00851\n",
      "2025-01-16 04:49:52.020864: train_loss -0.6646\n",
      "2025-01-16 04:49:52.021011: val_loss -0.5093\n",
      "2025-01-16 04:49:52.021044: Pseudo dice [np.float32(0.5378)]\n",
      "2025-01-16 04:49:52.021079: Epoch time: 110.67 s\n",
      "2025-01-16 04:49:52.568265: \n",
      "2025-01-16 04:49:52.568413: Epoch 165\n",
      "2025-01-16 04:49:52.568486: Current learning rate: 0.0085\n",
      "2025-01-16 04:51:43.344614: train_loss -0.6404\n",
      "2025-01-16 04:51:43.344808: val_loss -0.6001\n",
      "2025-01-16 04:51:43.344899: Pseudo dice [np.float32(0.6458)]\n",
      "2025-01-16 04:51:43.344943: Epoch time: 110.78 s\n",
      "2025-01-16 04:51:43.892599: \n",
      "2025-01-16 04:51:43.892688: Epoch 166\n",
      "2025-01-16 04:51:43.892753: Current learning rate: 0.00849\n",
      "2025-01-16 04:53:34.674805: train_loss -0.6846\n",
      "2025-01-16 04:53:34.674979: val_loss -0.6683\n",
      "2025-01-16 04:53:34.675024: Pseudo dice [np.float32(0.6662)]\n",
      "2025-01-16 04:53:34.675060: Epoch time: 110.78 s\n",
      "2025-01-16 04:53:35.224767: \n",
      "2025-01-16 04:53:35.224890: Epoch 167\n",
      "2025-01-16 04:53:35.225094: Current learning rate: 0.00848\n",
      "2025-01-16 04:55:25.963014: train_loss -0.6908\n",
      "2025-01-16 04:55:25.963138: val_loss -0.5955\n",
      "2025-01-16 04:55:25.963171: Pseudo dice [np.float32(0.6092)]\n",
      "2025-01-16 04:55:25.963204: Epoch time: 110.74 s\n",
      "2025-01-16 04:55:26.516232: \n",
      "2025-01-16 04:55:26.516315: Epoch 168\n",
      "2025-01-16 04:55:26.516374: Current learning rate: 0.00847\n",
      "2025-01-16 04:57:17.275705: train_loss -0.7003\n",
      "2025-01-16 04:57:17.275831: val_loss -0.6622\n",
      "2025-01-16 04:57:17.276167: Pseudo dice [np.float32(0.6124)]\n",
      "2025-01-16 04:57:17.276236: Epoch time: 110.76 s\n",
      "2025-01-16 04:57:17.832556: \n",
      "2025-01-16 04:57:17.832643: Epoch 169\n",
      "2025-01-16 04:57:17.832704: Current learning rate: 0.00847\n",
      "2025-01-16 04:59:08.587665: train_loss -0.6888\n",
      "2025-01-16 04:59:08.587845: val_loss -0.5411\n",
      "2025-01-16 04:59:08.587889: Pseudo dice [np.float32(0.6145)]\n",
      "2025-01-16 04:59:08.587926: Epoch time: 110.76 s\n",
      "2025-01-16 04:59:09.393600: \n",
      "2025-01-16 04:59:09.393980: Epoch 170\n",
      "2025-01-16 04:59:09.394122: Current learning rate: 0.00846\n",
      "2025-01-16 05:01:00.181518: train_loss -0.6906\n",
      "2025-01-16 05:01:00.181646: val_loss -0.607\n",
      "2025-01-16 05:01:00.181679: Pseudo dice [np.float32(0.6811)]\n",
      "2025-01-16 05:01:00.181714: Epoch time: 110.79 s\n",
      "2025-01-16 05:01:00.744903: \n",
      "2025-01-16 05:01:00.745105: Epoch 171\n",
      "2025-01-16 05:01:00.745243: Current learning rate: 0.00845\n",
      "2025-01-16 05:02:51.496644: train_loss -0.6924\n",
      "2025-01-16 05:02:51.496774: val_loss -0.5408\n",
      "2025-01-16 05:02:51.496809: Pseudo dice [np.float32(0.6454)]\n",
      "2025-01-16 05:02:51.496850: Epoch time: 110.75 s\n",
      "2025-01-16 05:02:52.053984: \n",
      "2025-01-16 05:02:52.054077: Epoch 172\n",
      "2025-01-16 05:02:52.054142: Current learning rate: 0.00844\n",
      "2025-01-16 05:04:42.830408: train_loss -0.7129\n",
      "2025-01-16 05:04:42.830560: val_loss -0.6924\n",
      "2025-01-16 05:04:42.830601: Pseudo dice [np.float32(0.693)]\n",
      "2025-01-16 05:04:42.830633: Epoch time: 110.78 s\n",
      "2025-01-16 05:04:42.830654: Yayy! New best EMA pseudo Dice: 0.6319000124931335\n",
      "2025-01-16 05:04:43.612486: \n",
      "2025-01-16 05:04:43.612824: Epoch 173\n",
      "2025-01-16 05:04:43.612911: Current learning rate: 0.00843\n",
      "2025-01-16 05:06:34.352571: train_loss -0.7312\n",
      "2025-01-16 05:06:34.352782: val_loss -0.6203\n",
      "2025-01-16 05:06:34.352829: Pseudo dice [np.float32(0.6418)]\n",
      "2025-01-16 05:06:34.352865: Epoch time: 110.74 s\n",
      "2025-01-16 05:06:34.352892: Yayy! New best EMA pseudo Dice: 0.6328999996185303\n",
      "2025-01-16 05:06:35.128486: \n",
      "2025-01-16 05:06:35.128684: Epoch 174\n",
      "2025-01-16 05:06:35.128768: Current learning rate: 0.00842\n",
      "2025-01-16 05:08:25.815730: train_loss -0.7487\n",
      "2025-01-16 05:08:25.816200: val_loss -0.6571\n",
      "2025-01-16 05:08:25.816260: Pseudo dice [np.float32(0.5804)]\n",
      "2025-01-16 05:08:25.816301: Epoch time: 110.69 s\n",
      "2025-01-16 05:08:26.368873: \n",
      "2025-01-16 05:08:26.369011: Epoch 175\n",
      "2025-01-16 05:08:26.369082: Current learning rate: 0.00841\n",
      "2025-01-16 05:10:17.158422: train_loss -0.742\n",
      "2025-01-16 05:10:17.158556: val_loss -0.625\n",
      "2025-01-16 05:10:17.158590: Pseudo dice [np.float32(0.6998)]\n",
      "2025-01-16 05:10:17.158622: Epoch time: 110.79 s\n",
      "2025-01-16 05:10:17.158643: Yayy! New best EMA pseudo Dice: 0.6348000168800354\n",
      "2025-01-16 05:10:17.932637: \n",
      "2025-01-16 05:10:17.933023: Epoch 176\n",
      "2025-01-16 05:10:17.933459: Current learning rate: 0.0084\n",
      "2025-01-16 05:12:08.515887: train_loss -0.7263\n",
      "2025-01-16 05:12:08.516062: val_loss -0.6534\n",
      "2025-01-16 05:12:08.516144: Pseudo dice [np.float32(0.6275)]\n",
      "2025-01-16 05:12:08.516186: Epoch time: 110.58 s\n",
      "2025-01-16 05:12:09.079748: \n",
      "2025-01-16 05:12:09.079975: Epoch 177\n",
      "2025-01-16 05:12:09.080044: Current learning rate: 0.00839\n",
      "2025-01-16 05:13:59.838408: train_loss -0.7292\n",
      "2025-01-16 05:13:59.838576: val_loss -0.6629\n",
      "2025-01-16 05:13:59.838615: Pseudo dice [np.float32(0.6308)]\n",
      "2025-01-16 05:13:59.838648: Epoch time: 110.76 s\n",
      "2025-01-16 05:14:00.394356: \n",
      "2025-01-16 05:14:00.394480: Epoch 178\n",
      "2025-01-16 05:14:00.394560: Current learning rate: 0.00838\n",
      "2025-01-16 05:15:51.121949: train_loss -0.7562\n",
      "2025-01-16 05:15:51.122082: val_loss -0.6324\n",
      "2025-01-16 05:15:51.122161: Pseudo dice [np.float32(0.6138)]\n",
      "2025-01-16 05:15:51.122303: Epoch time: 110.73 s\n",
      "2025-01-16 05:15:51.681595: \n",
      "2025-01-16 05:15:51.681687: Epoch 179\n",
      "2025-01-16 05:15:51.681751: Current learning rate: 0.00837\n",
      "2025-01-16 05:17:42.432670: train_loss -0.7499\n",
      "2025-01-16 05:17:42.433004: val_loss -0.5882\n",
      "2025-01-16 05:17:42.433059: Pseudo dice [np.float32(0.5543)]\n",
      "2025-01-16 05:17:42.433095: Epoch time: 110.75 s\n",
      "2025-01-16 05:17:43.015112: \n",
      "2025-01-16 05:17:43.015467: Epoch 180\n",
      "2025-01-16 05:17:43.015543: Current learning rate: 0.00836\n",
      "2025-01-16 05:19:33.645524: train_loss -0.7724\n",
      "2025-01-16 05:19:33.645878: val_loss -0.5782\n",
      "2025-01-16 05:19:33.645922: Pseudo dice [np.float32(0.5612)]\n",
      "2025-01-16 05:19:33.645958: Epoch time: 110.63 s\n",
      "2025-01-16 05:19:34.496619: \n",
      "2025-01-16 05:19:34.496970: Epoch 181\n",
      "2025-01-16 05:19:34.497123: Current learning rate: 0.00836\n",
      "2025-01-16 05:21:25.282489: train_loss -0.7489\n",
      "2025-01-16 05:21:25.282678: val_loss -0.6484\n",
      "2025-01-16 05:21:25.282758: Pseudo dice [np.float32(0.639)]\n",
      "2025-01-16 05:21:25.282802: Epoch time: 110.79 s\n",
      "2025-01-16 05:21:25.865516: \n",
      "2025-01-16 05:21:25.865623: Epoch 182\n",
      "2025-01-16 05:21:25.865691: Current learning rate: 0.00835\n",
      "2025-01-16 05:23:16.651434: train_loss -0.7654\n",
      "2025-01-16 05:23:16.651673: val_loss -0.6364\n",
      "2025-01-16 05:23:16.651716: Pseudo dice [np.float32(0.654)]\n",
      "2025-01-16 05:23:16.651753: Epoch time: 110.79 s\n",
      "2025-01-16 05:23:17.237003: \n",
      "2025-01-16 05:23:17.237351: Epoch 183\n",
      "2025-01-16 05:23:17.237455: Current learning rate: 0.00834\n",
      "2025-01-16 05:25:08.035845: train_loss -0.7642\n",
      "2025-01-16 05:25:08.035986: val_loss -0.5584\n",
      "2025-01-16 05:25:08.036022: Pseudo dice [np.float32(0.5931)]\n",
      "2025-01-16 05:25:08.036055: Epoch time: 110.8 s\n",
      "2025-01-16 05:25:08.591398: \n",
      "2025-01-16 05:25:08.591560: Epoch 184\n",
      "2025-01-16 05:25:08.591625: Current learning rate: 0.00833\n",
      "2025-01-16 05:26:59.187469: train_loss -0.7688\n",
      "2025-01-16 05:26:59.187640: val_loss -0.6148\n",
      "2025-01-16 05:26:59.187672: Pseudo dice [np.float32(0.6868)]\n",
      "2025-01-16 05:26:59.187704: Epoch time: 110.6 s\n",
      "2025-01-16 05:26:59.751070: \n",
      "2025-01-16 05:26:59.751185: Epoch 185\n",
      "2025-01-16 05:26:59.751251: Current learning rate: 0.00832\n",
      "2025-01-16 05:28:50.326038: train_loss -0.7303\n",
      "2025-01-16 05:28:50.326474: val_loss -0.5551\n",
      "2025-01-16 05:28:50.326544: Pseudo dice [np.float32(0.5689)]\n",
      "2025-01-16 05:28:50.326586: Epoch time: 110.58 s\n",
      "2025-01-16 05:28:50.885998: \n",
      "2025-01-16 05:28:50.886168: Epoch 186\n",
      "2025-01-16 05:28:50.886248: Current learning rate: 0.00831\n",
      "2025-01-16 05:30:41.509820: train_loss -0.6916\n",
      "2025-01-16 05:30:41.509937: val_loss -0.4466\n",
      "2025-01-16 05:30:41.509969: Pseudo dice [np.float32(0.5498)]\n",
      "2025-01-16 05:30:41.510002: Epoch time: 110.62 s\n",
      "2025-01-16 05:30:42.076021: \n",
      "2025-01-16 05:30:42.076164: Epoch 187\n",
      "2025-01-16 05:30:42.076230: Current learning rate: 0.0083\n",
      "2025-01-16 05:32:32.836818: train_loss -0.7141\n",
      "2025-01-16 05:32:32.836973: val_loss -0.5764\n",
      "2025-01-16 05:32:32.837013: Pseudo dice [np.float32(0.5931)]\n",
      "2025-01-16 05:32:32.837111: Epoch time: 110.76 s\n",
      "2025-01-16 05:32:33.406075: \n",
      "2025-01-16 05:32:33.406157: Epoch 188\n",
      "2025-01-16 05:32:33.406219: Current learning rate: 0.00829\n",
      "2025-01-16 05:34:24.163623: train_loss -0.7317\n",
      "2025-01-16 05:34:24.163875: val_loss -0.5845\n",
      "2025-01-16 05:34:24.163922: Pseudo dice [np.float32(0.569)]\n",
      "2025-01-16 05:34:24.163974: Epoch time: 110.76 s\n",
      "2025-01-16 05:34:24.724605: \n",
      "2025-01-16 05:34:24.724688: Epoch 189\n",
      "2025-01-16 05:34:24.724749: Current learning rate: 0.00828\n",
      "2025-01-16 05:36:15.450344: train_loss -0.7224\n",
      "2025-01-16 05:36:15.450471: val_loss -0.619\n",
      "2025-01-16 05:36:15.450505: Pseudo dice [np.float32(0.6559)]\n",
      "2025-01-16 05:36:15.450537: Epoch time: 110.73 s\n",
      "2025-01-16 05:36:16.009833: \n",
      "2025-01-16 05:36:16.009921: Epoch 190\n",
      "2025-01-16 05:36:16.009984: Current learning rate: 0.00827\n",
      "2025-01-16 05:38:06.559250: train_loss -0.7328\n",
      "2025-01-16 05:38:06.559452: val_loss -0.5691\n",
      "2025-01-16 05:38:06.559486: Pseudo dice [np.float32(0.6523)]\n",
      "2025-01-16 05:38:06.559519: Epoch time: 110.55 s\n",
      "2025-01-16 05:38:07.121016: \n",
      "2025-01-16 05:38:07.121304: Epoch 191\n",
      "2025-01-16 05:38:07.121534: Current learning rate: 0.00826\n",
      "2025-01-16 05:39:57.779399: train_loss -0.7589\n",
      "2025-01-16 05:39:57.779534: val_loss -0.6284\n",
      "2025-01-16 05:39:57.779569: Pseudo dice [np.float32(0.6717)]\n",
      "2025-01-16 05:39:57.779608: Epoch time: 110.66 s\n",
      "2025-01-16 05:39:58.347303: \n",
      "2025-01-16 05:39:58.347387: Epoch 192\n",
      "2025-01-16 05:39:58.347447: Current learning rate: 0.00825\n",
      "2025-01-16 05:41:48.944872: train_loss -0.756\n",
      "2025-01-16 05:41:48.945007: val_loss -0.6492\n",
      "2025-01-16 05:41:48.945039: Pseudo dice [np.float32(0.6335)]\n",
      "2025-01-16 05:41:48.945072: Epoch time: 110.6 s\n",
      "2025-01-16 05:41:49.763170: \n",
      "2025-01-16 05:41:49.763324: Epoch 193\n",
      "2025-01-16 05:41:49.763393: Current learning rate: 0.00824\n",
      "2025-01-16 05:43:40.342071: train_loss -0.7521\n",
      "2025-01-16 05:43:40.342195: val_loss -0.6222\n",
      "2025-01-16 05:43:40.342227: Pseudo dice [np.float32(0.5793)]\n",
      "2025-01-16 05:43:40.342258: Epoch time: 110.58 s\n",
      "2025-01-16 05:43:40.927140: \n",
      "2025-01-16 05:43:40.927251: Epoch 194\n",
      "2025-01-16 05:43:40.927317: Current learning rate: 0.00824\n",
      "2025-01-16 05:45:31.705491: train_loss -0.7192\n",
      "2025-01-16 05:45:31.705629: val_loss -0.6253\n",
      "2025-01-16 05:45:31.705669: Pseudo dice [np.float32(0.6223)]\n",
      "2025-01-16 05:45:31.705701: Epoch time: 110.78 s\n",
      "2025-01-16 05:45:32.268561: \n",
      "2025-01-16 05:45:32.268663: Epoch 195\n",
      "2025-01-16 05:45:32.268725: Current learning rate: 0.00823\n",
      "2025-01-16 05:47:23.060562: train_loss -0.7207\n",
      "2025-01-16 05:47:23.060699: val_loss -0.6233\n",
      "2025-01-16 05:47:23.060735: Pseudo dice [np.float32(0.6241)]\n",
      "2025-01-16 05:47:23.060768: Epoch time: 110.79 s\n",
      "2025-01-16 05:47:23.631461: \n",
      "2025-01-16 05:47:23.631560: Epoch 196\n",
      "2025-01-16 05:47:23.631624: Current learning rate: 0.00822\n",
      "2025-01-16 05:49:14.349637: train_loss -0.6904\n",
      "2025-01-16 05:49:14.349773: val_loss -0.6032\n",
      "2025-01-16 05:49:14.349813: Pseudo dice [np.float32(0.5917)]\n",
      "2025-01-16 05:49:14.349854: Epoch time: 110.72 s\n",
      "2025-01-16 05:49:14.919149: \n",
      "2025-01-16 05:49:14.919261: Epoch 197\n",
      "2025-01-16 05:49:14.919321: Current learning rate: 0.00821\n",
      "2025-01-16 05:51:05.687093: train_loss -0.7125\n",
      "2025-01-16 05:51:05.687232: val_loss -0.6817\n",
      "2025-01-16 05:51:05.687264: Pseudo dice [np.float32(0.7026)]\n",
      "2025-01-16 05:51:05.687296: Epoch time: 110.77 s\n",
      "2025-01-16 05:51:06.253068: \n",
      "2025-01-16 05:51:06.253176: Epoch 198\n",
      "2025-01-16 05:51:06.253241: Current learning rate: 0.0082\n",
      "2025-01-16 05:52:57.018633: train_loss -0.7532\n",
      "2025-01-16 05:52:57.018756: val_loss -0.66\n",
      "2025-01-16 05:52:57.018785: Pseudo dice [np.float32(0.7283)]\n",
      "2025-01-16 05:52:57.018817: Epoch time: 110.77 s\n",
      "2025-01-16 05:52:57.018839: Yayy! New best EMA pseudo Dice: 0.635699987411499\n",
      "2025-01-16 05:52:57.802545: \n",
      "2025-01-16 05:52:57.802811: Epoch 199\n",
      "2025-01-16 05:52:57.803038: Current learning rate: 0.00819\n",
      "2025-01-16 05:54:48.469936: train_loss -0.7624\n",
      "2025-01-16 05:54:48.470062: val_loss -0.6389\n",
      "2025-01-16 05:54:48.470094: Pseudo dice [np.float32(0.6579)]\n",
      "2025-01-16 05:54:48.470127: Epoch time: 110.67 s\n",
      "2025-01-16 05:54:48.675134: Yayy! New best EMA pseudo Dice: 0.6378999948501587\n",
      "2025-01-16 05:54:49.458703: \n",
      "2025-01-16 05:54:49.458977: Epoch 200\n",
      "2025-01-16 05:54:49.459062: Current learning rate: 0.00818\n",
      "2025-01-16 05:56:40.178507: train_loss -0.7037\n",
      "2025-01-16 05:56:40.178664: val_loss -0.5735\n",
      "2025-01-16 05:56:40.178695: Pseudo dice [np.float32(0.4713)]\n",
      "2025-01-16 05:56:40.178729: Epoch time: 110.72 s\n",
      "2025-01-16 05:56:40.748369: \n",
      "2025-01-16 05:56:40.748457: Epoch 201\n",
      "2025-01-16 05:56:40.748520: Current learning rate: 0.00817\n",
      "2025-01-16 05:58:31.361539: train_loss -0.7205\n",
      "2025-01-16 05:58:31.361660: val_loss -0.5561\n",
      "2025-01-16 05:58:31.361694: Pseudo dice [np.float32(0.5198)]\n",
      "2025-01-16 05:58:31.361726: Epoch time: 110.61 s\n",
      "2025-01-16 05:58:31.930229: \n",
      "2025-01-16 05:58:31.930637: Epoch 202\n",
      "2025-01-16 05:58:31.930734: Current learning rate: 0.00816\n",
      "2025-01-16 06:00:22.495399: train_loss -0.7323\n",
      "2025-01-16 06:00:22.495519: val_loss -0.609\n",
      "2025-01-16 06:00:22.495551: Pseudo dice [np.float32(0.5769)]\n",
      "2025-01-16 06:00:22.495587: Epoch time: 110.57 s\n",
      "2025-01-16 06:00:23.061817: \n",
      "2025-01-16 06:00:23.062211: Epoch 203\n",
      "2025-01-16 06:00:23.062303: Current learning rate: 0.00815\n",
      "2025-01-16 06:02:13.667956: train_loss -0.7299\n",
      "2025-01-16 06:02:13.668079: val_loss -0.6163\n",
      "2025-01-16 06:02:13.668112: Pseudo dice [np.float32(0.6646)]\n",
      "2025-01-16 06:02:13.668146: Epoch time: 110.61 s\n",
      "2025-01-16 06:02:14.240722: \n",
      "2025-01-16 06:02:14.240807: Epoch 204\n",
      "2025-01-16 06:02:14.240871: Current learning rate: 0.00814\n",
      "2025-01-16 06:04:05.014124: train_loss -0.7059\n",
      "2025-01-16 06:04:05.014271: val_loss -0.6756\n",
      "2025-01-16 06:04:05.014305: Pseudo dice [np.float32(0.6519)]\n",
      "2025-01-16 06:04:05.014340: Epoch time: 110.77 s\n",
      "2025-01-16 06:04:05.824460: \n",
      "2025-01-16 06:04:05.824560: Epoch 205\n",
      "2025-01-16 06:04:05.824622: Current learning rate: 0.00813\n",
      "2025-01-16 06:05:56.545743: train_loss -0.7125\n",
      "2025-01-16 06:05:56.545934: val_loss -0.6268\n",
      "2025-01-16 06:05:56.545970: Pseudo dice [np.float32(0.6175)]\n",
      "2025-01-16 06:05:56.546004: Epoch time: 110.72 s\n",
      "2025-01-16 06:05:57.086568: \n",
      "2025-01-16 06:05:57.086668: Epoch 206\n",
      "2025-01-16 06:05:57.086733: Current learning rate: 0.00813\n",
      "2025-01-16 06:07:47.842289: train_loss -0.69\n",
      "2025-01-16 06:07:47.842421: val_loss -0.5571\n",
      "2025-01-16 06:07:47.842453: Pseudo dice [np.float32(0.5168)]\n",
      "2025-01-16 06:07:47.842488: Epoch time: 110.76 s\n",
      "2025-01-16 06:07:48.387583: \n",
      "2025-01-16 06:07:48.387964: Epoch 207\n",
      "2025-01-16 06:07:48.388039: Current learning rate: 0.00812\n",
      "2025-01-16 06:09:39.131285: train_loss -0.7007\n",
      "2025-01-16 06:09:39.131414: val_loss -0.5625\n",
      "2025-01-16 06:09:39.131447: Pseudo dice [np.float32(0.644)]\n",
      "2025-01-16 06:09:39.131479: Epoch time: 110.74 s\n",
      "2025-01-16 06:09:39.674914: \n",
      "2025-01-16 06:09:39.675091: Epoch 208\n",
      "2025-01-16 06:09:39.675166: Current learning rate: 0.00811\n",
      "2025-01-16 06:11:30.409068: train_loss -0.7451\n",
      "2025-01-16 06:11:30.409216: val_loss -0.6255\n",
      "2025-01-16 06:11:30.409249: Pseudo dice [np.float32(0.5936)]\n",
      "2025-01-16 06:11:30.409282: Epoch time: 110.73 s\n",
      "2025-01-16 06:11:30.961864: \n",
      "2025-01-16 06:11:30.962225: Epoch 209\n",
      "2025-01-16 06:11:30.962294: Current learning rate: 0.0081\n",
      "2025-01-16 06:13:21.681178: train_loss -0.7246\n",
      "2025-01-16 06:13:21.681318: val_loss -0.6535\n",
      "2025-01-16 06:13:21.681356: Pseudo dice [np.float32(0.6711)]\n",
      "2025-01-16 06:13:21.681396: Epoch time: 110.72 s\n",
      "2025-01-16 06:13:22.229232: \n",
      "2025-01-16 06:13:22.229328: Epoch 210\n",
      "2025-01-16 06:13:22.229471: Current learning rate: 0.00809\n",
      "2025-01-16 06:15:12.995046: train_loss -0.7551\n",
      "2025-01-16 06:15:12.995277: val_loss -0.6856\n",
      "2025-01-16 06:15:12.995346: Pseudo dice [np.float32(0.7059)]\n",
      "2025-01-16 06:15:12.995416: Epoch time: 110.77 s\n",
      "2025-01-16 06:15:13.537663: \n",
      "2025-01-16 06:15:13.537760: Epoch 211\n",
      "2025-01-16 06:15:13.537870: Current learning rate: 0.00808\n",
      "2025-01-16 06:17:04.303866: train_loss -0.7559\n",
      "2025-01-16 06:17:04.303991: val_loss -0.6728\n",
      "2025-01-16 06:17:04.304024: Pseudo dice [np.float32(0.6625)]\n",
      "2025-01-16 06:17:04.304055: Epoch time: 110.77 s\n",
      "2025-01-16 06:17:04.851501: \n",
      "Epoch 2126 06:17:04.851585: \n",
      "2025-01-16 06:17:04.851711: Current learning rate: 0.00807\n",
      "2025-01-16 06:18:55.471006: train_loss -0.7886\n",
      "2025-01-16 06:18:55.471143: val_loss -0.5939\n",
      "2025-01-16 06:18:55.471176: Pseudo dice [np.float32(0.6422)]\n",
      "2025-01-16 06:18:55.471209: Epoch time: 110.62 s\n",
      "2025-01-16 06:18:56.038244: \n",
      "2025-01-16 06:18:56.038622: Epoch 213\n",
      "2025-01-16 06:18:56.038699: Current learning rate: 0.00806\n",
      "2025-01-16 06:20:46.858691: train_loss -0.7707\n",
      "2025-01-16 06:20:46.859044: val_loss -0.6435\n",
      "2025-01-16 06:20:46.859107: Pseudo dice [np.float32(0.6321)]\n",
      "2025-01-16 06:20:46.859149: Epoch time: 110.82 s\n",
      "2025-01-16 06:20:47.427585: \n",
      "2025-01-16 06:20:47.427682: Epoch 214\n",
      "2025-01-16 06:20:47.427748: Current learning rate: 0.00805\n",
      "2025-01-16 06:22:38.242952: train_loss -0.7505\n",
      "2025-01-16 06:22:38.243096: val_loss -0.6046\n",
      "2025-01-16 06:22:38.243129: Pseudo dice [np.float32(0.6314)]\n",
      "2025-01-16 06:22:38.243163: Epoch time: 110.82 s\n",
      "2025-01-16 06:22:38.812562: \n",
      "2025-01-16 06:22:38.812712: Epoch 215\n",
      "2025-01-16 06:22:38.812788: Current learning rate: 0.00804\n",
      "2025-01-16 06:24:29.591418: train_loss -0.7541\n",
      "2025-01-16 06:24:29.591537: val_loss -0.6408\n",
      "2025-01-16 06:24:29.591569: Pseudo dice [np.float32(0.6409)]\n",
      "2025-01-16 06:24:29.591604: Epoch time: 110.78 s\n",
      "2025-01-16 06:24:30.160349: \n",
      "2025-01-16 06:24:30.160439: Epoch 216\n",
      "2025-01-16 06:24:30.160503: Current learning rate: 0.00803\n",
      "2025-01-16 06:26:20.915725: train_loss -0.7331\n",
      "2025-01-16 06:26:20.915873: val_loss -0.5897\n",
      "2025-01-16 06:26:20.916052: Pseudo dice [np.float32(0.6304)]\n",
      "2025-01-16 06:26:20.916124: Epoch time: 110.76 s\n",
      "2025-01-16 06:26:21.463340: \n",
      "2025-01-16 06:26:21.463813: Epoch 217\n",
      "2025-01-16 06:26:21.463884: Current learning rate: 0.00802\n",
      "2025-01-16 06:28:12.257303: train_loss -0.7214\n",
      "2025-01-16 06:28:12.257528: val_loss -0.6837\n",
      "2025-01-16 06:28:12.257563: Pseudo dice [np.float32(0.6898)]\n",
      "2025-01-16 06:28:12.257598: Epoch time: 110.79 s\n",
      "2025-01-16 06:28:13.044873: \n",
      "2025-01-16 06:28:13.045247: Epoch 218\n",
      "2025-01-16 06:28:13.045374: Current learning rate: 0.00801\n",
      "2025-01-16 06:30:03.807384: train_loss -0.6772\n",
      "2025-01-16 06:30:03.807515: val_loss -0.628\n",
      "2025-01-16 06:30:03.807550: Pseudo dice [np.float32(0.633)]\n",
      "2025-01-16 06:30:03.807581: Epoch time: 110.76 s\n",
      "2025-01-16 06:30:04.348360: \n",
      "2025-01-16 06:30:04.348770: Epoch 219\n",
      "2025-01-16 06:30:04.348865: Current learning rate: 0.00801\n",
      "2025-01-16 06:31:55.069945: train_loss -0.7178\n",
      "2025-01-16 06:31:55.070091: val_loss -0.7265\n",
      "2025-01-16 06:31:55.070125: Pseudo dice [np.float32(0.7233)]\n",
      "2025-01-16 06:31:55.070159: Epoch time: 110.72 s\n",
      "2025-01-16 06:31:55.070180: Yayy! New best EMA pseudo Dice: 0.6452000141143799\n",
      "2025-01-16 06:31:55.840613: \n",
      "2025-01-16 06:31:55.840715: Epoch 220\n",
      "2025-01-16 06:31:55.840778: Current learning rate: 0.008\n",
      "2025-01-16 06:33:46.610666: train_loss -0.7409\n",
      "2025-01-16 06:33:46.610866: val_loss -0.6917\n",
      "2025-01-16 06:33:46.610901: Pseudo dice [np.float32(0.6626)]\n",
      "2025-01-16 06:33:46.610935: Epoch time: 110.77 s\n",
      "2025-01-16 06:33:46.610959: Yayy! New best EMA pseudo Dice: 0.6468999981880188\n",
      "2025-01-16 06:33:47.373686: \n",
      "2025-01-16 06:33:47.373852: Epoch 221\n",
      "2025-01-16 06:33:47.373919: Current learning rate: 0.00799\n",
      "2025-01-16 06:35:38.149323: train_loss -0.7418\n",
      "2025-01-16 06:35:38.149712: val_loss -0.6394\n",
      "2025-01-16 06:35:38.149757: Pseudo dice [np.float32(0.6386)]\n",
      "2025-01-16 06:35:38.149805: Epoch time: 110.78 s\n",
      "2025-01-16 06:35:38.692455: \n",
      "2025-01-16 06:35:38.692638: Epoch 222\n",
      "2025-01-16 06:35:38.692708: Current learning rate: 0.00798\n",
      "2025-01-16 06:37:29.424744: train_loss -0.7101\n",
      "2025-01-16 06:37:29.424869: val_loss -0.6312\n",
      "2025-01-16 06:37:29.424902: Pseudo dice [np.float32(0.6414)]\n",
      "2025-01-16 06:37:29.424934: Epoch time: 110.73 s\n",
      "2025-01-16 06:37:29.982900: \n",
      "2025-01-16 06:37:29.983273: Epoch 223\n",
      "2025-01-16 06:37:29.983455: Current learning rate: 0.00797\n",
      "2025-01-16 06:39:20.676399: train_loss -0.7478\n",
      "2025-01-16 06:39:20.676567: val_loss -0.6569\n",
      "2025-01-16 06:39:20.676599: Pseudo dice [np.float32(0.6941)]\n",
      "2025-01-16 06:39:20.676633: Epoch time: 110.69 s\n",
      "2025-01-16 06:39:20.676654: Yayy! New best EMA pseudo Dice: 0.6504999995231628\n",
      "2025-01-16 06:39:21.413974: \n",
      "2025-01-16 06:39:21.414151: Epoch 224\n",
      "2025-01-16 06:39:21.414227: Current learning rate: 0.00796\n",
      "2025-01-16 06:41:12.017343: train_loss -0.7376\n",
      "2025-01-16 06:41:12.017500: val_loss -0.6253\n",
      "2025-01-16 06:41:12.017602: Pseudo dice [np.float32(0.6354)]\n",
      "2025-01-16 06:41:12.017661: Epoch time: 110.6 s\n",
      "2025-01-16 06:41:12.548307: \n",
      "2025-01-16 06:41:12.548452: Epoch 225\n",
      "2025-01-16 06:41:12.548522: Current learning rate: 0.00795\n",
      "2025-01-16 06:43:03.038468: train_loss -0.7388\n",
      "2025-01-16 06:43:03.038590: val_loss -0.6068\n",
      "2025-01-16 06:43:03.038622: Pseudo dice [np.float32(0.5877)]\n",
      "2025-01-16 06:43:03.038654: Epoch time: 110.49 s\n",
      "2025-01-16 06:43:03.561629: \n",
      "2025-01-16 06:43:03.561715: Epoch 226\n",
      "2025-01-16 06:43:03.561776: Current learning rate: 0.00794\n",
      "2025-01-16 06:44:54.205342: train_loss -0.7595\n",
      "2025-01-16 06:44:54.205488: val_loss -0.6486\n",
      "2025-01-16 06:44:54.205524: Pseudo dice [np.float32(0.6265)]\n",
      "2025-01-16 06:44:54.205555: Epoch time: 110.64 s\n",
      "2025-01-16 06:44:54.726686: \n",
      "2025-01-16 06:44:54.726769: Epoch 227\n",
      "2025-01-16 06:44:54.726830: Current learning rate: 0.00793\n",
      "2025-01-16 06:46:45.220051: train_loss -0.7314\n",
      "2025-01-16 06:46:45.220244: val_loss -0.6016\n",
      "2025-01-16 06:46:45.220292: Pseudo dice [np.float32(0.6255)]\n",
      "2025-01-16 06:46:45.220328: Epoch time: 110.49 s\n",
      "2025-01-16 06:46:45.743340: \n",
      "2025-01-16 06:46:45.743499: Epoch 228\n",
      "2025-01-16 06:46:45.743572: Current learning rate: 0.00792\n",
      "2025-01-16 06:48:36.363646: train_loss -0.7542\n",
      "2025-01-16 06:48:36.363834: val_loss -0.5879\n",
      "2025-01-16 06:48:36.363977: Pseudo dice [np.float32(0.6114)]\n",
      "2025-01-16 06:48:36.364021: Epoch time: 110.62 s\n",
      "2025-01-16 06:48:36.880105: \n",
      "2025-01-16 06:48:36.880248: Epoch 229\n",
      "2025-01-16 06:48:36.880317: Current learning rate: 0.00791\n",
      "2025-01-16 06:50:27.505894: train_loss -0.7415\n",
      "2025-01-16 06:50:27.506076: val_loss -0.5713\n",
      "2025-01-16 06:50:27.506109: Pseudo dice [np.float32(0.7021)]\n",
      "2025-01-16 06:50:27.506142: Epoch time: 110.63 s\n",
      "2025-01-16 06:50:28.027993: \n",
      "2025-01-16 06:50:28.028206: Epoch 230\n",
      "2025-01-16 06:50:28.028333: Current learning rate: 0.0079\n",
      "2025-01-16 06:52:18.648042: train_loss -0.7445\n",
      "2025-01-16 06:52:18.648218: val_loss -0.5986\n",
      "2025-01-16 06:52:18.648252: Pseudo dice [np.float32(0.6448)]\n",
      "2025-01-16 06:52:18.648284: Epoch time: 110.62 s\n",
      "2025-01-16 06:52:19.387149: \n",
      "2025-01-16 06:52:19.387317: Epoch 231\n",
      "2025-01-16 06:52:19.387393: Current learning rate: 0.00789\n",
      "2025-01-16 06:54:09.835280: train_loss -0.7084\n",
      "2025-01-16 06:54:09.835404: val_loss -0.6035\n",
      "2025-01-16 06:54:09.835443: Pseudo dice [np.float32(0.5137)]\n",
      "2025-01-16 06:54:09.835476: Epoch time: 110.45 s\n",
      "2025-01-16 06:54:10.358637: \n",
      "2025-01-16 06:54:10.358794: Epoch 232\n",
      "2025-01-16 06:54:10.358862: Current learning rate: 0.00789\n",
      "2025-01-16 06:56:00.950954: train_loss -0.733\n",
      "2025-01-16 06:56:00.951070: val_loss -0.6398\n",
      "2025-01-16 06:56:00.951102: Pseudo dice [np.float32(0.5824)]\n",
      "2025-01-16 06:56:00.951135: Epoch time: 110.59 s\n",
      "2025-01-16 06:56:01.476832: \n",
      "2025-01-16 06:56:01.476925: Epoch 233\n",
      "2025-01-16 06:56:01.476984: Current learning rate: 0.00788\n",
      "2025-01-16 06:57:52.116221: train_loss -0.74\n",
      "2025-01-16 06:57:52.116414: val_loss -0.6349\n",
      "2025-01-16 06:57:52.116446: Pseudo dice [np.float32(0.6819)]\n",
      "2025-01-16 06:57:52.116480: Epoch time: 110.64 s\n",
      "2025-01-16 06:57:52.637538: \n",
      "2025-01-16 06:57:52.637632: Epoch 234\n",
      "2025-01-16 06:57:52.637694: Current learning rate: 0.00787\n",
      "2025-01-16 06:59:43.237658: train_loss -0.7431\n",
      "2025-01-16 06:59:43.237852: val_loss -0.5942\n",
      "2025-01-16 06:59:43.237885: Pseudo dice [np.float32(0.6531)]\n",
      "2025-01-16 06:59:43.237918: Epoch time: 110.6 s\n",
      "2025-01-16 06:59:43.764317: \n",
      "2025-01-16 06:59:43.764411: Epoch 235\n",
      "2025-01-16 06:59:43.764471: Current learning rate: 0.00786\n",
      "2025-01-16 07:01:34.408099: train_loss -0.6833\n",
      "2025-01-16 07:01:34.408259: val_loss -0.6219\n",
      "2025-01-16 07:01:34.408399: Pseudo dice [np.float32(0.6496)]\n",
      "2025-01-16 07:01:34.408502: Epoch time: 110.64 s\n",
      "2025-01-16 07:01:34.934463: \n",
      "2025-01-16 07:01:34.934550: Epoch 236\n",
      "2025-01-16 07:01:34.934613: Current learning rate: 0.00785\n",
      "2025-01-16 07:03:25.401880: train_loss -0.6881\n",
      "2025-01-16 07:03:25.402124: val_loss -0.619\n",
      "2025-01-16 07:03:25.402223: Pseudo dice [np.float32(0.6305)]\n",
      "2025-01-16 07:03:25.402318: Epoch time: 110.47 s\n",
      "2025-01-16 07:03:25.938159: \n",
      "2025-01-16 07:03:25.938245: Epoch 237\n",
      "2025-01-16 07:03:25.938304: Current learning rate: 0.00784\n",
      "2025-01-16 07:05:16.573680: train_loss -0.7471\n",
      "2025-01-16 07:05:16.573870: val_loss -0.6587\n",
      "2025-01-16 07:05:16.573915: Pseudo dice [np.float32(0.5939)]\n",
      "2025-01-16 07:05:16.573949: Epoch time: 110.64 s\n",
      "2025-01-16 07:05:17.103592: \n",
      "2025-01-16 07:05:17.103736: Epoch 238\n",
      "2025-01-16 07:05:17.103807: Current learning rate: 0.00783\n",
      "2025-01-16 07:07:07.756770: train_loss -0.7522\n",
      "2025-01-16 07:07:07.756963: val_loss -0.6373\n",
      "2025-01-16 07:07:07.756997: Pseudo dice [np.float32(0.5836)]\n",
      "2025-01-16 07:07:07.757031: Epoch time: 110.65 s\n",
      "2025-01-16 07:07:08.279617: \n",
      "2025-01-16 07:07:08.279787: Epoch 239\n",
      "2025-01-16 07:07:08.279851: Current learning rate: 0.00782\n",
      "2025-01-16 07:08:58.781151: train_loss -0.7611\n",
      "2025-01-16 07:08:58.781273: val_loss -0.6259\n",
      "2025-01-16 07:08:58.781306: Pseudo dice [np.float32(0.5954)]\n",
      "2025-01-16 07:08:58.781339: Epoch time: 110.5 s\n",
      "2025-01-16 07:08:59.310366: \n",
      "2025-01-16 07:08:59.310549: Epoch 240\n",
      "2025-01-16 07:08:59.310629: Current learning rate: 0.00781\n",
      "2025-01-16 07:10:49.950687: train_loss -0.7593\n",
      "2025-01-16 07:10:49.950806: val_loss -0.6124\n",
      "2025-01-16 07:10:49.950839: Pseudo dice [np.float32(0.6276)]\n",
      "2025-01-16 07:10:49.950872: Epoch time: 110.64 s\n",
      "2025-01-16 07:10:50.476758: \n",
      "2025-01-16 07:10:50.476966: Epoch 241\n",
      "2025-01-16 07:10:50.477048: Current learning rate: 0.0078\n",
      "2025-01-16 07:12:41.142569: train_loss -0.7289\n",
      "2025-01-16 07:12:41.142720: val_loss -0.5894\n",
      "2025-01-16 07:12:41.142791: Pseudo dice [np.float32(0.4197)]\n",
      "2025-01-16 07:12:41.142832: Epoch time: 110.67 s\n",
      "2025-01-16 07:12:41.673845: \n",
      "2025-01-16 07:12:41.673934: Epoch 242\n",
      "2025-01-16 07:12:41.673995: Current learning rate: 0.00779\n",
      "2025-01-16 07:14:32.312466: train_loss -0.6972\n",
      "2025-01-16 07:14:32.312591: val_loss -0.6757\n",
      "2025-01-16 07:14:32.312623: Pseudo dice [np.float32(0.7027)]\n",
      "2025-01-16 07:14:32.312655: Epoch time: 110.64 s\n",
      "2025-01-16 07:14:32.847313: \n",
      "2025-01-16 07:14:32.847469: Epoch 243\n",
      "2025-01-16 07:14:32.847539: Current learning rate: 0.00778\n",
      "2025-01-16 07:16:23.559726: train_loss -0.7732\n",
      "2025-01-16 07:16:23.559841: val_loss -0.6552\n",
      "2025-01-16 07:16:23.559872: Pseudo dice [np.float32(0.6741)]\n",
      "2025-01-16 07:16:23.559904: Epoch time: 110.71 s\n",
      "2025-01-16 07:16:24.096178: \n",
      "2025-01-16 07:16:24.096358: Epoch 244\n",
      "2025-01-16 07:16:24.096431: Current learning rate: 0.00777\n",
      "2025-01-16 07:18:14.750260: train_loss -0.7371\n",
      "2025-01-16 07:18:14.750692: val_loss -0.5793\n",
      "2025-01-16 07:18:14.750753: Pseudo dice [np.float32(0.5942)]\n",
      "2025-01-16 07:18:14.750792: Epoch time: 110.65 s\n",
      "2025-01-16 07:18:15.286916: \n",
      "2025-01-16 07:18:15.287077: Epoch 245\n",
      "2025-01-16 07:18:15.287150: Current learning rate: 0.00777\n",
      "2025-01-16 07:20:05.957029: train_loss -0.7467\n",
      "2025-01-16 07:20:05.957160: val_loss -0.6162\n",
      "2025-01-16 07:20:05.957195: Pseudo dice [np.float32(0.6286)]\n",
      "2025-01-16 07:20:05.957227: Epoch time: 110.67 s\n",
      "2025-01-16 07:20:06.513152: \n",
      "2025-01-16 07:20:06.513512: Epoch 246\n",
      "2025-01-16 07:20:06.513611: Current learning rate: 0.00776\n",
      "2025-01-16 07:21:57.195014: train_loss -0.7517\n",
      "2025-01-16 07:21:57.195135: val_loss -0.6045\n",
      "2025-01-16 07:21:57.195167: Pseudo dice [np.float32(0.6806)]\n",
      "2025-01-16 07:21:57.195200: Epoch time: 110.68 s\n",
      "2025-01-16 07:21:57.736006: \n",
      "2025-01-16 07:21:57.736153: Epoch 247\n",
      "2025-01-16 07:21:57.736228: Current learning rate: 0.00775\n",
      "2025-01-16 07:23:48.381112: train_loss -0.7557\n",
      "2025-01-16 07:23:48.381555: val_loss -0.6214\n",
      "2025-01-16 07:23:48.381602: Pseudo dice [np.float32(0.6534)]\n",
      "2025-01-16 07:23:48.381639: Epoch time: 110.65 s\n",
      "2025-01-16 07:23:48.913707: \n",
      "2025-01-16 07:23:48.913845: Epoch 248\n",
      "2025-01-16 07:23:48.913909: Current learning rate: 0.00774\n",
      "2025-01-16 07:25:39.549535: train_loss -0.7549\n",
      "2025-01-16 07:25:39.549692: val_loss -0.6508\n",
      "2025-01-16 07:25:39.549769: Pseudo dice [np.float32(0.6511)]\n",
      "2025-01-16 07:25:39.549811: Epoch time: 110.64 s\n",
      "2025-01-16 07:25:40.087873: \n",
      "2025-01-16 07:25:40.088009: Epoch 249\n",
      "2025-01-16 07:25:40.088073: Current learning rate: 0.00773\n",
      "2025-01-16 07:27:30.742109: train_loss -0.7644\n",
      "2025-01-16 07:27:30.742335: val_loss -0.5892\n",
      "2025-01-16 07:27:30.742393: Pseudo dice [np.float32(0.6155)]\n",
      "2025-01-16 07:27:30.742434: Epoch time: 110.65 s\n",
      "2025-01-16 07:27:31.480899: \n",
      "2025-01-16 07:27:31.481077: Epoch 250\n",
      "2025-01-16 07:27:31.481151: Current learning rate: 0.00772\n",
      "2025-01-16 07:29:22.007879: train_loss -0.7128\n",
      "2025-01-16 07:29:22.008058: val_loss -0.5287\n",
      "2025-01-16 07:29:22.008089: Pseudo dice [np.float32(0.6004)]\n",
      "2025-01-16 07:29:22.008121: Epoch time: 110.53 s\n",
      "2025-01-16 07:29:22.540737: \n",
      "2025-01-16 07:29:22.540826: Epoch 251\n",
      "2025-01-16 07:29:22.540888: Current learning rate: 0.00771\n",
      "2025-01-16 07:31:13.205712: train_loss -0.7066\n",
      "2025-01-16 07:31:13.205828: val_loss -0.6622\n",
      "2025-01-16 07:31:13.205860: Pseudo dice [np.float32(0.6468)]\n",
      "2025-01-16 07:31:13.205892: Epoch time: 110.67 s\n",
      "2025-01-16 07:31:13.741212: \n",
      "2025-01-16 07:31:13.741358: Epoch 252\n",
      "2025-01-16 07:31:13.741421: Current learning rate: 0.0077\n",
      "2025-01-16 07:33:04.420114: train_loss -0.6732\n",
      "2025-01-16 07:33:04.420240: val_loss -0.5581\n",
      "2025-01-16 07:33:04.420272: Pseudo dice [np.float32(0.6472)]\n",
      "2025-01-16 07:33:04.420304: Epoch time: 110.68 s\n",
      "2025-01-16 07:33:04.952088: \n",
      "2025-01-16 07:33:04.952266: Epoch 253\n",
      "2025-01-16 07:33:04.952412: Current learning rate: 0.00769\n",
      "2025-01-16 07:34:55.613526: train_loss -0.7139\n",
      "2025-01-16 07:34:55.613655: val_loss -0.559\n",
      "2025-01-16 07:34:55.613687: Pseudo dice [np.float32(0.5737)]\n",
      "2025-01-16 07:34:55.613721: Epoch time: 110.66 s\n",
      "2025-01-16 07:34:56.153770: \n",
      "2025-01-16 07:34:56.153892: Epoch 254\n",
      "2025-01-16 07:34:56.153961: Current learning rate: 0.00768\n",
      "2025-01-16 07:36:46.801502: train_loss -0.7573\n",
      "2025-01-16 07:36:46.801635: val_loss -0.6621\n",
      "2025-01-16 07:36:46.801669: Pseudo dice [np.float32(0.6792)]\n",
      "2025-01-16 07:36:46.801702: Epoch time: 110.65 s\n",
      "2025-01-16 07:36:47.331822: \n",
      "2025-01-16 07:36:47.332017: Epoch 255\n",
      "2025-01-16 07:36:47.332116: Current learning rate: 0.00767\n",
      "2025-01-16 07:38:37.980620: train_loss -0.7439\n",
      "2025-01-16 07:38:37.980802: val_loss -0.6671\n",
      "2025-01-16 07:38:37.980834: Pseudo dice [np.float32(0.7105)]\n",
      "2025-01-16 07:38:37.980867: Epoch time: 110.65 s\n",
      "2025-01-16 07:38:38.737170: \n",
      "2025-01-16 07:38:38.737265: Epoch 256\n",
      "2025-01-16 07:38:38.737337: Current learning rate: 0.00766\n",
      "2025-01-16 07:40:29.413116: train_loss -0.7565\n",
      "2025-01-16 07:40:29.413242: val_loss -0.6599\n",
      "2025-01-16 07:40:29.413274: Pseudo dice [np.float32(0.6238)]\n",
      "2025-01-16 07:40:29.413304: Epoch time: 110.68 s\n",
      "2025-01-16 07:40:29.972129: \n",
      "2025-01-16 07:40:29.972325: Epoch 257\n",
      "2025-01-16 07:40:29.972398: Current learning rate: 0.00765\n",
      "2025-01-16 07:42:20.611599: train_loss -0.7615\n",
      "2025-01-16 07:42:20.611720: val_loss -0.651\n",
      "2025-01-16 07:42:20.611752: Pseudo dice [np.float32(0.6498)]\n",
      "2025-01-16 07:42:20.611784: Epoch time: 110.64 s\n",
      "2025-01-16 07:42:21.143023: \n",
      "2025-01-16 07:42:21.143215: Epoch 258\n",
      "2025-01-16 07:42:21.143288: Current learning rate: 0.00764\n",
      "2025-01-16 07:44:11.853910: train_loss -0.7702\n",
      "2025-01-16 07:44:11.854034: val_loss -0.6226\n",
      "2025-01-16 07:44:11.854067: Pseudo dice [np.float32(0.6436)]\n",
      "2025-01-16 07:44:11.854101: Epoch time: 110.71 s\n",
      "2025-01-16 07:44:12.389460: \n",
      "2025-01-16 07:44:12.389559: Epoch 259\n",
      "2025-01-16 07:44:12.389621: Current learning rate: 0.00764\n",
      "2025-01-16 07:46:03.147538: train_loss -0.7674\n",
      "2025-01-16 07:46:03.147757: val_loss -0.6372\n",
      "2025-01-16 07:46:03.147791: Pseudo dice [np.float32(0.6626)]\n",
      "2025-01-16 07:46:03.147822: Epoch time: 110.76 s\n",
      "2025-01-16 07:46:03.686676: \n",
      "2025-01-16 07:46:03.686871: Epoch 260\n",
      "2025-01-16 07:46:03.686945: Current learning rate: 0.00763\n",
      "2025-01-16 07:47:54.280987: train_loss -0.7493\n",
      "2025-01-16 07:47:54.281110: val_loss -0.7581\n",
      "2025-01-16 07:47:54.281144: Pseudo dice [np.float32(0.7835)]\n",
      "2025-01-16 07:47:54.281177: Epoch time: 110.59 s\n",
      "2025-01-16 07:47:54.281197: Yayy! New best EMA pseudo Dice: 0.6547999978065491\n",
      "2025-01-16 07:47:55.044514: \n",
      "2025-01-16 07:47:55.044695: Epoch 261\n",
      "2025-01-16 07:47:55.044820: Current learning rate: 0.00762\n",
      "2025-01-16 07:49:45.709783: train_loss -0.7688\n",
      "2025-01-16 07:49:45.709901: val_loss -0.6062\n",
      "2025-01-16 07:49:45.709933: Pseudo dice [np.float32(0.6111)]\n",
      "2025-01-16 07:49:45.709965: Epoch time: 110.67 s\n",
      "2025-01-16 07:49:46.257376: \n",
      "2025-01-16 07:49:46.257465: Epoch 262\n",
      "2025-01-16 07:49:46.257526: Current learning rate: 0.00761\n",
      "2025-01-16 07:51:36.998577: train_loss -0.69\n",
      "2025-01-16 07:51:36.998777: val_loss -0.6386\n",
      "2025-01-16 07:51:36.998823: Pseudo dice [np.float32(0.6459)]\n",
      "2025-01-16 07:51:36.998858: Epoch time: 110.74 s\n",
      "2025-01-16 07:51:37.550474: \n",
      "2025-01-16 07:51:37.550637: Epoch 263\n",
      "2025-01-16 07:51:37.550703: Current learning rate: 0.0076\n",
      "2025-01-16 07:53:28.294248: train_loss -0.6811\n",
      "2025-01-16 07:53:28.294369: val_loss -0.5089\n",
      "2025-01-16 07:53:28.294400: Pseudo dice [np.float32(0.5976)]\n",
      "2025-01-16 07:53:28.294432: Epoch time: 110.74 s\n",
      "2025-01-16 07:53:28.846863: \n",
      "2025-01-16 07:53:28.846954: Epoch 264\n",
      "2025-01-16 07:53:28.847015: Current learning rate: 0.00759\n",
      "2025-01-16 07:55:19.402612: train_loss -0.7017\n",
      "2025-01-16 07:55:19.402750: val_loss -0.5788\n",
      "2025-01-16 07:55:19.402786: Pseudo dice [np.float32(0.6172)]\n",
      "2025-01-16 07:55:19.402821: Epoch time: 110.56 s\n",
      "2025-01-16 07:55:19.949944: \n",
      "2025-01-16 07:55:19.950104: Epoch 265\n",
      "2025-01-16 07:55:19.950241: Current learning rate: 0.00758\n",
      "2025-01-16 07:57:10.691010: train_loss -0.7072\n",
      "2025-01-16 07:57:10.691145: val_loss -0.6378\n",
      "2025-01-16 07:57:10.691179: Pseudo dice [np.float32(0.6831)]\n",
      "2025-01-16 07:57:10.691212: Epoch time: 110.74 s\n",
      "2025-01-16 07:57:11.245491: \n",
      "2025-01-16 07:57:11.245808: Epoch 266\n",
      "2025-01-16 07:57:11.245999: Current learning rate: 0.00757\n",
      "2025-01-16 07:59:01.829985: train_loss -0.757\n",
      "2025-01-16 07:59:01.830177: val_loss -0.6673\n",
      "2025-01-16 07:59:01.830213: Pseudo dice [np.float32(0.6544)]\n",
      "2025-01-16 07:59:01.830247: Epoch time: 110.59 s\n",
      "2025-01-16 07:59:02.376681: \n",
      "2025-01-16 07:59:02.376817: Epoch 267\n",
      "2025-01-16 07:59:02.376878: Current learning rate: 0.00756\n",
      "2025-01-16 08:00:52.917944: train_loss -0.7093\n",
      "2025-01-16 08:00:52.918101: val_loss -0.6278\n",
      "2025-01-16 08:00:52.918275: Pseudo dice [np.float32(0.6597)]\n",
      "2025-01-16 08:00:52.918393: Epoch time: 110.54 s\n",
      "2025-01-16 08:00:53.466378: \n",
      "2025-01-16 08:00:53.466742: Epoch 268\n",
      "2025-01-16 08:00:53.466834: Current learning rate: 0.00755\n",
      "2025-01-16 08:02:44.173712: train_loss -0.7094\n",
      "2025-01-16 08:02:44.173836: val_loss -0.586\n",
      "2025-01-16 08:02:44.173869: Pseudo dice [np.float32(0.6168)]\n",
      "2025-01-16 08:02:44.173901: Epoch time: 110.71 s\n",
      "2025-01-16 08:02:44.959086: \n",
      "2025-01-16 08:02:44.959393: Epoch 269\n",
      "2025-01-16 08:02:44.959532: Current learning rate: 0.00754\n",
      "2025-01-16 08:04:35.513300: train_loss -0.6989\n",
      "2025-01-16 08:04:35.513424: val_loss -0.527\n",
      "2025-01-16 08:04:35.513455: Pseudo dice [np.float32(0.5903)]\n",
      "2025-01-16 08:04:35.513488: Epoch time: 110.55 s\n",
      "2025-01-16 08:04:36.067258: \n",
      "2025-01-16 08:04:36.067362: Epoch 270\n",
      "2025-01-16 08:04:36.067425: Current learning rate: 0.00753\n",
      "2025-01-16 08:06:26.730016: train_loss -0.7352\n",
      "2025-01-16 08:06:26.730185: val_loss -0.7032\n",
      "2025-01-16 08:06:26.730220: Pseudo dice [np.float32(0.7031)]\n",
      "2025-01-16 08:06:26.730252: Epoch time: 110.66 s\n",
      "2025-01-16 08:06:27.274133: \n",
      "2025-01-16 08:06:27.274363: Epoch 271\n",
      "2025-01-16 08:06:27.274451: Current learning rate: 0.00752\n",
      "2025-01-16 08:08:17.856590: train_loss -0.7411\n",
      "2025-01-16 08:08:17.856793: val_loss -0.7185\n",
      "2025-01-16 08:08:17.856837: Pseudo dice [np.float32(0.7399)]\n",
      "2025-01-16 08:08:17.856870: Epoch time: 110.58 s\n",
      "2025-01-16 08:08:17.856889: Yayy! New best EMA pseudo Dice: 0.6553000211715698\n",
      "2025-01-16 08:08:18.641729: \n",
      "2025-01-16 08:08:18.642050: Epoch 272\n",
      "2025-01-16 08:08:18.642199: Current learning rate: 0.00751\n",
      "2025-01-16 08:10:09.407748: train_loss -0.6954\n",
      "2025-01-16 08:10:09.407886: val_loss -0.6508\n",
      "2025-01-16 08:10:09.407925: Pseudo dice [np.float32(0.7103)]\n",
      "2025-01-16 08:10:09.407960: Epoch time: 110.77 s\n",
      "2025-01-16 08:10:09.407980: Yayy! New best EMA pseudo Dice: 0.6607999801635742\n",
      "2025-01-16 08:10:10.188787: \n",
      "2025-01-16 08:10:10.188902: Epoch 273\n",
      "2025-01-16 08:10:10.188962: Current learning rate: 0.00751\n",
      "2025-01-16 08:12:00.919155: train_loss -0.75\n",
      "2025-01-16 08:12:00.919280: val_loss -0.5918\n",
      "2025-01-16 08:12:00.919315: Pseudo dice [np.float32(0.6337)]\n",
      "2025-01-16 08:12:00.919348: Epoch time: 110.73 s\n",
      "2025-01-16 08:12:01.474616: \n",
      "2025-01-16 08:12:01.474720: Epoch 274\n",
      "2025-01-16 08:12:01.474784: Current learning rate: 0.0075\n",
      "2025-01-16 08:13:52.038485: train_loss -0.7789\n",
      "2025-01-16 08:13:52.038618: val_loss -0.6224\n",
      "2025-01-16 08:13:52.038651: Pseudo dice [np.float32(0.7144)]\n",
      "2025-01-16 08:13:52.038683: Epoch time: 110.56 s\n",
      "2025-01-16 08:13:52.038703: Yayy! New best EMA pseudo Dice: 0.6638000011444092\n",
      "2025-01-16 08:13:52.803774: \n",
      "2025-01-16 08:13:52.804158: Epoch 275\n",
      "2025-01-16 08:13:52.804267: Current learning rate: 0.00749\n",
      "2025-01-16 08:15:43.590455: train_loss -0.781\n",
      "2025-01-16 08:15:43.590587: val_loss -0.6397\n",
      "2025-01-16 08:15:43.590621: Pseudo dice [np.float32(0.6199)]\n",
      "2025-01-16 08:15:43.590668: Epoch time: 110.79 s\n",
      "2025-01-16 08:15:44.140124: \n",
      "2025-01-16 08:15:44.140221: Epoch 276\n",
      "2025-01-16 08:15:44.140295: Current learning rate: 0.00748\n",
      "2025-01-16 08:17:34.704507: train_loss -0.7449\n",
      "2025-01-16 08:17:34.704689: val_loss -0.6257\n",
      "2025-01-16 08:17:34.704722: Pseudo dice [np.float32(0.6508)]\n",
      "2025-01-16 08:17:34.704756: Epoch time: 110.57 s\n",
      "2025-01-16 08:17:35.259910: \n",
      "2025-01-16 08:17:35.260003: Epoch 277\n",
      "2025-01-16 08:17:35.260066: Current learning rate: 0.00747\n",
      "2025-01-16 08:19:25.835290: train_loss -0.7145\n",
      "2025-01-16 08:19:25.835485: val_loss -0.5841\n",
      "2025-01-16 08:19:25.835520: Pseudo dice [np.float32(0.6009)]\n",
      "2025-01-16 08:19:25.835553: Epoch time: 110.58 s\n",
      "2025-01-16 08:19:26.385122: \n",
      "2025-01-16 08:19:26.385212: Epoch 278\n",
      "2025-01-16 08:19:26.385274: Current learning rate: 0.00746\n",
      "2025-01-16 08:21:17.110961: train_loss -0.7246\n",
      "2025-01-16 08:21:17.111089: val_loss -0.6633\n",
      "2025-01-16 08:21:17.111122: Pseudo dice [np.float32(0.6763)]\n",
      "2025-01-16 08:21:17.111154: Epoch time: 110.73 s\n",
      "2025-01-16 08:21:17.659779: \n",
      "2025-01-16 08:21:17.659868: Epoch 279\n",
      "2025-01-16 08:21:17.659932: Current learning rate: 0.00745\n",
      "2025-01-16 08:23:08.261667: train_loss -0.7444\n",
      "2025-01-16 08:23:08.261907: val_loss -0.6767\n",
      "2025-01-16 08:23:08.261951: Pseudo dice [np.float32(0.6559)]\n",
      "2025-01-16 08:23:08.262029: Epoch time: 110.6 s\n",
      "2025-01-16 08:23:08.812480: \n",
      "2025-01-16 08:23:08.812567: Epoch 280\n",
      "2025-01-16 08:23:08.812631: Current learning rate: 0.00744\n",
      "2025-01-16 08:24:59.542972: train_loss -0.7725\n",
      "2025-01-16 08:24:59.543194: val_loss -0.6302\n",
      "2025-01-16 08:24:59.543258: Pseudo dice [np.float32(0.6796)]\n",
      "2025-01-16 08:24:59.543300: Epoch time: 110.73 s\n",
      "2025-01-16 08:25:00.326814: \n",
      "2025-01-16 08:25:00.326928: Epoch 281\n",
      "2025-01-16 08:25:00.326993: Current learning rate: 0.00743\n",
      "2025-01-16 08:26:51.012552: train_loss -0.7545\n",
      "2025-01-16 08:26:51.012682: val_loss -0.6364\n",
      "2025-01-16 08:26:51.012716: Pseudo dice [np.float32(0.6693)]\n",
      "2025-01-16 08:26:51.012748: Epoch time: 110.69 s\n",
      "2025-01-16 08:26:51.567700: \n",
      "2025-01-16 08:26:51.568067: Epoch 282\n",
      "2025-01-16 08:26:51.568195: Current learning rate: 0.00742\n",
      "2025-01-16 08:28:42.345414: train_loss -0.7758\n",
      "2025-01-16 08:28:42.345670: val_loss -0.6905\n",
      "2025-01-16 08:28:42.345716: Pseudo dice [np.float32(0.7363)]\n",
      "2025-01-16 08:28:42.345756: Epoch time: 110.78 s\n",
      "2025-01-16 08:28:42.345782: Yayy! New best EMA pseudo Dice: 0.6665999889373779\n",
      "2025-01-16 08:28:43.115072: \n",
      "2025-01-16 08:28:43.115436: Epoch 283\n",
      "2025-01-16 08:28:43.115512: Current learning rate: 0.00741\n",
      "2025-01-16 08:30:33.834207: train_loss -0.7954\n",
      "2025-01-16 08:30:33.834390: val_loss -0.7075\n",
      "2025-01-16 08:30:33.834421: Pseudo dice [np.float32(0.7434)]\n",
      "2025-01-16 08:30:33.834454: Epoch time: 110.72 s\n",
      "2025-01-16 08:30:33.834474: Yayy! New best EMA pseudo Dice: 0.6741999983787537\n",
      "2025-01-16 08:30:34.612823: \n",
      "2025-01-16 08:30:34.613142: Epoch 284\n",
      "2025-01-16 08:30:34.613214: Current learning rate: 0.0074\n",
      "2025-01-16 08:32:25.342355: train_loss -0.7634\n",
      "2025-01-16 08:32:25.342513: val_loss -0.6957\n",
      "2025-01-16 08:32:25.342548: Pseudo dice [np.float32(0.6725)]\n",
      "2025-01-16 08:32:25.342582: Epoch time: 110.73 s\n",
      "2025-01-16 08:32:25.905629: \n",
      "2025-01-16 08:32:25.905725: Epoch 285\n",
      "2025-01-16 08:32:25.905786: Current learning rate: 0.00739\n",
      "2025-01-16 08:34:16.705022: train_loss -0.772\n",
      "2025-01-16 08:34:16.705158: val_loss -0.6421\n",
      "2025-01-16 08:34:16.705191: Pseudo dice [np.float32(0.7)]\n",
      "2025-01-16 08:34:16.705225: Epoch time: 110.8 s\n",
      "2025-01-16 08:34:16.705246: Yayy! New best EMA pseudo Dice: 0.6766999959945679\n",
      "2025-01-16 08:34:17.485910: \n",
      "2025-01-16 08:34:17.486090: Epoch 286\n",
      "2025-01-16 08:34:17.486181: Current learning rate: 0.00738\n",
      "2025-01-16 08:36:08.239208: train_loss -0.7491\n",
      "2025-01-16 08:36:08.239329: val_loss -0.6966\n",
      "2025-01-16 08:36:08.239360: Pseudo dice [np.float32(0.731)]\n",
      "2025-01-16 08:36:08.239398: Epoch time: 110.75 s\n",
      "2025-01-16 08:36:08.239423: Yayy! New best EMA pseudo Dice: 0.6820999979972839\n",
      "2025-01-16 08:36:09.024288: \n",
      "2025-01-16 08:36:09.024440: Epoch 287\n",
      "2025-01-16 08:36:09.024512: Current learning rate: 0.00738\n",
      "2025-01-16 08:37:59.765003: train_loss -0.7605\n",
      "2025-01-16 08:37:59.765127: val_loss -0.647\n",
      "2025-01-16 08:37:59.765217: Pseudo dice [np.float32(0.6402)]\n",
      "2025-01-16 08:37:59.765322: Epoch time: 110.74 s\n",
      "2025-01-16 08:38:00.326048: \n",
      "2025-01-16 08:38:00.326259: Epoch 288\n",
      "2025-01-16 08:38:00.326334: Current learning rate: 0.00737\n",
      "2025-01-16 08:39:51.060435: train_loss -0.7755\n",
      "2025-01-16 08:39:51.060642: val_loss -0.5366\n",
      "2025-01-16 08:39:51.060679: Pseudo dice [np.float32(0.5953)]\n",
      "2025-01-16 08:39:51.060712: Epoch time: 110.73 s\n",
      "2025-01-16 08:39:51.617084: \n",
      "2025-01-16 08:39:51.617173: Epoch 289\n",
      "2025-01-16 08:39:51.617235: Current learning rate: 0.00736\n",
      "2025-01-16 08:41:42.368280: train_loss -0.7554\n",
      "2025-01-16 08:41:42.368416: val_loss -0.6494\n",
      "2025-01-16 08:41:42.368451: Pseudo dice [np.float32(0.6465)]\n",
      "2025-01-16 08:41:42.368484: Epoch time: 110.75 s\n",
      "2025-01-16 08:41:42.927979: \n",
      "2025-01-16 08:41:42.928064: Epoch 290\n",
      "2025-01-16 08:41:42.928127: Current learning rate: 0.00735\n",
      "2025-01-16 08:43:33.728596: train_loss -0.7468\n",
      "2025-01-16 08:43:33.728783: val_loss -0.6568\n",
      "2025-01-16 08:43:33.728817: Pseudo dice [np.float32(0.692)]\n",
      "2025-01-16 08:43:33.728850: Epoch time: 110.8 s\n",
      "2025-01-16 08:43:34.293532: \n",
      "2025-01-16 08:43:34.293884: Epoch 291\n",
      "2025-01-16 08:43:34.293952: Current learning rate: 0.00734\n",
      "2025-01-16 08:45:25.095352: train_loss -0.7508\n",
      "2025-01-16 08:45:25.095491: val_loss -0.6842\n",
      "2025-01-16 08:45:25.095614: Pseudo dice [np.float32(0.6877)]\n",
      "2025-01-16 08:45:25.095777: Epoch time: 110.8 s\n",
      "2025-01-16 08:45:25.658657: \n",
      "2025-01-16 08:45:25.659040: Epoch 292\n",
      "2025-01-16 08:45:25.659133: Current learning rate: 0.00733\n",
      "2025-01-16 08:47:16.466716: train_loss -0.759\n",
      "2025-01-16 08:47:16.466973: val_loss -0.6742\n",
      "2025-01-16 08:47:16.467021: Pseudo dice [np.float32(0.6613)]\n",
      "2025-01-16 08:47:16.467055: Epoch time: 110.81 s\n",
      "2025-01-16 08:47:17.022583: \n",
      "2025-01-16 08:47:17.022668: Epoch 293\n",
      "2025-01-16 08:47:17.022736: Current learning rate: 0.00732\n",
      "2025-01-16 08:49:07.749007: train_loss -0.773\n",
      "2025-01-16 08:49:07.749141: val_loss -0.6663\n",
      "2025-01-16 08:49:07.749176: Pseudo dice [np.float32(0.6681)]\n",
      "2025-01-16 08:49:07.749210: Epoch time: 110.73 s\n",
      "2025-01-16 08:49:08.561495: \n",
      "2025-01-16 08:49:08.561598: Epoch 294\n",
      "2025-01-16 08:49:08.561661: Current learning rate: 0.00731\n",
      "2025-01-16 08:50:59.294734: train_loss -0.7649\n",
      "2025-01-16 08:50:59.294863: val_loss -0.6269\n",
      "2025-01-16 08:50:59.294895: Pseudo dice [np.float32(0.6473)]\n",
      "2025-01-16 08:50:59.294927: Epoch time: 110.73 s\n",
      "2025-01-16 08:50:59.851371: \n",
      "2025-01-16 08:50:59.851539: Epoch 295\n",
      "2025-01-16 08:50:59.851602: Current learning rate: 0.0073\n",
      "2025-01-16 08:52:50.405313: train_loss -0.7848\n",
      "2025-01-16 08:52:50.405434: val_loss -0.6832\n",
      "2025-01-16 08:52:50.405468: Pseudo dice [np.float32(0.6929)]\n",
      "2025-01-16 08:52:50.405536: Epoch time: 110.55 s\n",
      "2025-01-16 08:52:50.962081: \n",
      "2025-01-16 08:52:50.962178: Epoch 296\n",
      "2025-01-16 08:52:50.962237: Current learning rate: 0.00729\n",
      "2025-01-16 08:54:41.703686: train_loss -0.7627\n",
      "2025-01-16 08:54:41.703793: val_loss -0.656\n",
      "2025-01-16 08:54:41.703820: Pseudo dice [np.float32(0.7197)]\n",
      "2025-01-16 08:54:41.703853: Epoch time: 110.74 s\n",
      "2025-01-16 08:54:42.263875: \n",
      "2025-01-16 08:54:42.263999: Epoch 297\n",
      "2025-01-16 08:54:42.264093: Current learning rate: 0.00728\n",
      "2025-01-16 08:56:32.996623: train_loss -0.779\n",
      "2025-01-16 08:56:32.997113: val_loss -0.6919\n",
      "2025-01-16 08:56:32.997241: Pseudo dice [np.float32(0.7017)]\n",
      "2025-01-16 08:56:32.997293: Epoch time: 110.73 s\n",
      "2025-01-16 08:56:33.582499: \n",
      "2025-01-16 08:56:33.582609: Epoch 298\n",
      "2025-01-16 08:56:33.582795: Current learning rate: 0.00727\n",
      "2025-01-16 08:58:24.354119: train_loss -0.7623\n",
      "2025-01-16 08:58:24.354302: val_loss -0.6883\n",
      "2025-01-16 08:58:24.354426: Pseudo dice [np.float32(0.709)]\n",
      "2025-01-16 08:58:24.354479: Epoch time: 110.77 s\n",
      "2025-01-16 08:58:24.915602: \n",
      "2025-01-16 08:58:24.915709: Epoch 299\n",
      "2025-01-16 08:58:24.915770: Current learning rate: 0.00726\n",
      "2025-01-16 09:00:15.670613: train_loss -0.7444\n",
      "2025-01-16 09:00:15.670809: val_loss -0.5189\n",
      "2025-01-16 09:00:15.670844: Pseudo dice [np.float32(0.4007)]\n",
      "2025-01-16 09:00:15.670878: Epoch time: 110.76 s\n",
      "2025-01-16 09:00:16.459033: \n",
      "2025-01-16 09:00:16.459184: Epoch 300\n",
      "2025-01-16 09:00:16.459247: Current learning rate: 0.00725\n",
      "2025-01-16 09:02:07.253897: train_loss -0.6746\n",
      "2025-01-16 09:02:07.254040: val_loss -0.6009\n",
      "2025-01-16 09:02:07.254081: Pseudo dice [np.float32(0.5526)]\n",
      "2025-01-16 09:02:07.254112: Epoch time: 110.8 s\n",
      "2025-01-16 09:02:07.805984: \n",
      "2025-01-16 09:02:07.806066: Epoch 301\n",
      "2025-01-16 09:02:07.806126: Current learning rate: 0.00724\n",
      "2025-01-16 09:03:58.571677: train_loss -0.7252\n",
      "2025-01-16 09:03:58.571893: val_loss -0.6081\n",
      "2025-01-16 09:03:58.571937: Pseudo dice [np.float32(0.6736)]\n",
      "2025-01-16 09:03:58.571972: Epoch time: 110.77 s\n",
      "2025-01-16 09:03:59.129324: \n",
      "2025-01-16 09:03:59.129735: Epoch 302\n",
      "2025-01-16 09:03:59.129876: Current learning rate: 0.00724\n",
      "2025-01-16 09:05:49.890190: train_loss -0.7373\n",
      "2025-01-16 09:05:49.890309: val_loss -0.555\n",
      "2025-01-16 09:05:49.890343: Pseudo dice [np.float32(0.5899)]\n",
      "2025-01-16 09:05:49.890375: Epoch time: 110.76 s\n",
      "2025-01-16 09:05:50.452675: \n",
      "2025-01-16 09:05:50.452877: Epoch 303\n",
      "2025-01-16 09:05:50.453047: Current learning rate: 0.00723\n",
      "2025-01-16 09:07:41.229769: train_loss -0.7451\n",
      "2025-01-16 09:07:41.229895: val_loss -0.7535\n",
      "2025-01-16 09:07:41.229928: Pseudo dice [np.float32(0.7537)]\n",
      "2025-01-16 09:07:41.229959: Epoch time: 110.78 s\n",
      "2025-01-16 09:07:41.797947: \n",
      "2025-01-16 09:07:41.798033: Epoch 304\n",
      "2025-01-16 09:07:41.798097: Current learning rate: 0.00722\n",
      "2025-01-16 09:09:32.576325: train_loss -0.7538\n",
      "2025-01-16 09:09:32.576455: val_loss -0.658\n",
      "2025-01-16 09:09:32.576489: Pseudo dice [np.float32(0.6596)]\n",
      "2025-01-16 09:09:32.576522: Epoch time: 110.78 s\n",
      "2025-01-16 09:09:33.138268: \n",
      "2025-01-16 09:09:33.138431: Epoch 305\n",
      "2025-01-16 09:09:33.138604: Current learning rate: 0.00721\n",
      "2025-01-16 09:11:23.899547: train_loss -0.7473\n",
      "2025-01-16 09:11:23.899821: val_loss -0.6528\n",
      "2025-01-16 09:11:23.899880: Pseudo dice [np.float32(0.693)]\n",
      "2025-01-16 09:11:23.899930: Epoch time: 110.76 s\n",
      "2025-01-16 09:11:24.691412: \n",
      "2025-01-16 09:11:24.691641: Epoch 306\n",
      "2025-01-16 09:11:24.691710: Current learning rate: 0.0072\n",
      "2025-01-16 09:13:15.375263: train_loss -0.7964\n",
      "2025-01-16 09:13:15.375397: val_loss -0.6743\n",
      "2025-01-16 09:13:15.375430: Pseudo dice [np.float32(0.6689)]\n",
      "2025-01-16 09:13:15.375528: Epoch time: 110.68 s\n",
      "2025-01-16 09:13:15.936109: \n",
      "2025-01-16 09:13:15.936460: Epoch 307\n",
      "2025-01-16 09:13:15.936535: Current learning rate: 0.00719\n",
      "2025-01-16 09:15:06.678337: train_loss -0.7464\n",
      "2025-01-16 09:15:06.678465: val_loss -0.59\n",
      "2025-01-16 09:15:06.678497: Pseudo dice [np.float32(0.5527)]\n",
      "2025-01-16 09:15:06.678530: Epoch time: 110.74 s\n",
      "2025-01-16 09:15:07.243970: \n",
      "2025-01-16 09:15:07.244049: Epoch 308\n",
      "2025-01-16 09:15:07.244113: Current learning rate: 0.00718\n",
      "2025-01-16 09:16:57.962439: train_loss -0.7497\n",
      "2025-01-16 09:16:57.962575: val_loss -0.6633\n",
      "2025-01-16 09:16:57.962606: Pseudo dice [np.float32(0.6489)]\n",
      "2025-01-16 09:16:57.962642: Epoch time: 110.72 s\n",
      "2025-01-16 09:16:58.532576: \n",
      "2025-01-16 09:16:58.532760: Epoch 309\n",
      "2025-01-16 09:16:58.532952: Current learning rate: 0.00717\n",
      "2025-01-16 09:18:49.235453: train_loss -0.7628\n",
      "2025-01-16 09:18:49.235646: val_loss -0.6896\n",
      "2025-01-16 09:18:49.235698: Pseudo dice [np.float32(0.6909)]\n",
      "2025-01-16 09:18:49.235739: Epoch time: 110.7 s\n",
      "2025-01-16 09:18:49.801636: \n",
      "2025-01-16 09:18:49.801883: Epoch 310\n",
      "2025-01-16 09:18:49.801977: Current learning rate: 0.00716\n",
      "2025-01-16 09:20:40.558067: train_loss -0.702\n",
      "2025-01-16 09:20:40.558196: val_loss -0.5955\n",
      "2025-01-16 09:20:40.558231: Pseudo dice [np.float32(0.5953)]\n",
      "2025-01-16 09:20:40.558266: Epoch time: 110.76 s\n",
      "2025-01-16 09:20:41.121852: \n",
      "2025-01-16 09:20:41.121951: Epoch 311\n",
      "2025-01-16 09:20:41.122013: Current learning rate: 0.00715\n",
      "2025-01-16 09:22:31.890200: train_loss -0.706\n",
      "2025-01-16 09:22:31.890393: val_loss -0.6712\n",
      "2025-01-16 09:22:31.890428: Pseudo dice [np.float32(0.6737)]\n",
      "2025-01-16 09:22:31.890463: Epoch time: 110.77 s\n",
      "2025-01-16 09:22:32.456561: \n",
      "2025-01-16 09:22:32.456654: Epoch 312\n",
      "2025-01-16 09:22:32.456717: Current learning rate: 0.00714\n",
      "2025-01-16 09:24:23.201504: train_loss -0.7347\n",
      "2025-01-16 09:24:23.201908: val_loss -0.6199\n",
      "2025-01-16 09:24:23.201950: Pseudo dice [np.float32(0.6809)]\n",
      "2025-01-16 09:24:23.201984: Epoch time: 110.75 s\n",
      "2025-01-16 09:24:23.766986: \n",
      "2025-01-16 09:24:23.767148: Epoch 313\n",
      "2025-01-16 09:24:23.767223: Current learning rate: 0.00713\n",
      "2025-01-16 09:26:14.482954: train_loss -0.759\n",
      "2025-01-16 09:26:14.483136: val_loss -0.5893\n",
      "2025-01-16 09:26:14.483184: Pseudo dice [np.float32(0.5659)]\n",
      "2025-01-16 09:26:14.483221: Epoch time: 110.72 s\n",
      "2025-01-16 09:26:15.046287: \n",
      "2025-01-16 09:26:15.046421: Epoch 314\n",
      "2025-01-16 09:26:15.046494: Current learning rate: 0.00712\n",
      "2025-01-16 09:28:05.807051: train_loss -0.7394\n",
      "2025-01-16 09:28:05.807176: val_loss -0.5757\n",
      "2025-01-16 09:28:05.807222: Pseudo dice [np.float32(0.637)]\n",
      "2025-01-16 09:28:05.807273: Epoch time: 110.76 s\n",
      "2025-01-16 09:28:06.376036: \n",
      "2025-01-16 09:28:06.376354: Epoch 315\n",
      "2025-01-16 09:28:06.376521: Current learning rate: 0.00711\n",
      "2025-01-16 09:29:56.936800: train_loss -0.7287\n",
      "2025-01-16 09:29:56.936985: val_loss -0.5437\n",
      "2025-01-16 09:29:56.937028: Pseudo dice [np.float32(0.6639)]\n",
      "2025-01-16 09:29:56.937062: Epoch time: 110.56 s\n",
      "2025-01-16 09:29:57.496711: \n",
      "2025-01-16 09:29:57.496799: Epoch 316\n",
      "2025-01-16 09:29:57.496862: Current learning rate: 0.0071\n",
      "2025-01-16 09:31:48.234727: train_loss -0.7225\n",
      "2025-01-16 09:31:48.234848: val_loss -0.6326\n",
      "2025-01-16 09:31:48.234879: Pseudo dice [np.float32(0.6844)]\n",
      "2025-01-16 09:31:48.234912: Epoch time: 110.74 s\n",
      "2025-01-16 09:31:48.803029: \n",
      "2025-01-16 09:31:48.803118: Epoch 317\n",
      "2025-01-16 09:31:48.803181: Current learning rate: 0.0071\n",
      "2025-01-16 09:33:39.603509: train_loss -0.7354\n",
      "2025-01-16 09:33:39.603636: val_loss -0.6295\n",
      "2025-01-16 09:33:39.603667: Pseudo dice [np.float32(0.6384)]\n",
      "2025-01-16 09:33:39.603705: Epoch time: 110.8 s\n",
      "2025-01-16 09:33:40.404218: \n",
      "2025-01-16 09:33:40.404368: Epoch 318\n",
      "2025-01-16 09:33:40.404442: Current learning rate: 0.00709\n",
      "2025-01-16 09:35:31.156369: train_loss -0.7589\n",
      "2025-01-16 09:35:31.156672: val_loss -0.6033\n",
      "2025-01-16 09:35:31.156866: Pseudo dice [np.float32(0.6154)]\n",
      "2025-01-16 09:35:31.156942: Epoch time: 110.75 s\n",
      "2025-01-16 09:35:31.722577: \n",
      "2025-01-16 09:35:31.722685: Epoch 319\n",
      "2025-01-16 09:35:31.722749: Current learning rate: 0.00708\n",
      "2025-01-16 09:37:22.420181: train_loss -0.7503\n",
      "2025-01-16 09:37:22.420308: val_loss -0.6183\n",
      "2025-01-16 09:37:22.420342: Pseudo dice [np.float32(0.6751)]\n",
      "2025-01-16 09:37:22.420377: Epoch time: 110.7 s\n",
      "2025-01-16 09:37:22.994097: \n",
      "2025-01-16 09:37:22.994224: Epoch 320\n",
      "2025-01-16 09:37:22.994288: Current learning rate: 0.00707\n",
      "2025-01-16 09:39:13.720015: train_loss -0.7435\n",
      "2025-01-16 09:39:13.720139: val_loss -0.4712\n",
      "2025-01-16 09:39:13.720175: Pseudo dice [np.float32(0.5405)]\n",
      "2025-01-16 09:39:13.720208: Epoch time: 110.73 s\n",
      "2025-01-16 09:39:14.282514: \n",
      "2025-01-16 09:39:14.282689: Epoch 321\n",
      "2025-01-16 09:39:14.282769: Current learning rate: 0.00706\n",
      "2025-01-16 09:41:04.848742: train_loss -0.7501\n",
      "2025-01-16 09:41:04.848876: val_loss -0.6761\n",
      "2025-01-16 09:41:04.848911: Pseudo dice [np.float32(0.7261)]\n",
      "2025-01-16 09:41:04.848946: Epoch time: 110.57 s\n",
      "2025-01-16 09:41:05.421774: \n",
      "2025-01-16 09:41:05.422107: Epoch 322\n",
      "2025-01-16 09:41:05.422192: Current learning rate: 0.00705\n",
      "2025-01-16 09:42:56.132166: train_loss -0.7531\n",
      "2025-01-16 09:42:56.132403: val_loss -0.5902\n",
      "2025-01-16 09:42:56.132479: Pseudo dice [np.float32(0.6261)]\n",
      "2025-01-16 09:42:56.132536: Epoch time: 110.71 s\n",
      "2025-01-16 09:42:56.701323: \n",
      "2025-01-16 09:42:56.701458: Epoch 323\n",
      "2025-01-16 09:42:56.701546: Current learning rate: 0.00704\n",
      "2025-01-16 09:44:47.437487: train_loss -0.7786\n",
      "2025-01-16 09:44:47.437794: val_loss -0.6349\n",
      "2025-01-16 09:44:47.437830: Pseudo dice [np.float32(0.6081)]\n",
      "2025-01-16 09:44:47.437863: Epoch time: 110.74 s\n",
      "2025-01-16 09:44:47.996236: \n",
      "2025-01-16 09:44:47.996549: Epoch 324\n",
      "2025-01-16 09:44:47.996643: Current learning rate: 0.00703\n",
      "2025-01-16 09:46:38.771783: train_loss -0.7726\n",
      "2025-01-16 09:46:38.771920: val_loss -0.589\n",
      "2025-01-16 09:46:38.771958: Pseudo dice [np.float32(0.5932)]\n",
      "2025-01-16 09:46:38.771990: Epoch time: 110.78 s\n",
      "2025-01-16 09:46:39.338267: \n",
      "2025-01-16 09:46:39.338459: Epoch 325\n",
      "2025-01-16 09:46:39.338533: Current learning rate: 0.00702\n",
      "2025-01-16 09:48:29.886305: train_loss -0.8001\n",
      "2025-01-16 09:48:29.886433: val_loss -0.6651\n",
      "2025-01-16 09:48:29.886468: Pseudo dice [np.float32(0.6668)]\n",
      "2025-01-16 09:48:29.886568: Epoch time: 110.55 s\n",
      "2025-01-16 09:48:30.453474: \n",
      "2025-01-16 09:48:30.453565: Epoch 326\n",
      "2025-01-16 09:48:30.453626: Current learning rate: 0.00701\n",
      "2025-01-16 09:50:21.169412: train_loss -0.7657\n",
      "2025-01-16 09:50:21.169543: val_loss -0.7108\n",
      "2025-01-16 09:50:21.169579: Pseudo dice [np.float32(0.727)]\n",
      "2025-01-16 09:50:21.169613: Epoch time: 110.72 s\n",
      "2025-01-16 09:50:21.743342: \n",
      "2025-01-16 09:50:21.743516: Epoch 327\n",
      "2025-01-16 09:50:21.743612: Current learning rate: 0.007\n",
      "2025-01-16 09:52:12.535742: train_loss -0.7269\n",
      "2025-01-16 09:52:12.535916: val_loss -0.5869\n",
      "2025-01-16 09:52:12.535955: Pseudo dice [np.float32(0.6027)]\n",
      "2025-01-16 09:52:12.536001: Epoch time: 110.79 s\n",
      "2025-01-16 09:52:13.100204: \n",
      "2025-01-16 09:52:13.100291: Epoch 328\n",
      "2025-01-16 09:52:13.100352: Current learning rate: 0.00699\n",
      "2025-01-16 09:54:03.869100: train_loss -0.7062\n",
      "2025-01-16 09:54:03.869231: val_loss -0.6148\n",
      "2025-01-16 09:54:03.869264: Pseudo dice [np.float32(0.6227)]\n",
      "2025-01-16 09:54:03.869297: Epoch time: 110.77 s\n",
      "2025-01-16 09:54:04.434737: \n",
      "2025-01-16 09:54:04.434922: Epoch 329\n",
      "2025-01-16 09:54:04.434999: Current learning rate: 0.00698\n",
      "2025-01-16 09:55:55.098404: train_loss -0.7443\n",
      "2025-01-16 09:55:55.098534: val_loss -0.6524\n",
      "2025-01-16 09:55:55.098566: Pseudo dice [np.float32(0.6578)]\n",
      "2025-01-16 09:55:55.098599: Epoch time: 110.66 s\n",
      "2025-01-16 09:55:55.894433: \n",
      "2025-01-16 09:55:55.894532: Epoch 330\n",
      "2025-01-16 09:55:55.894596: Current learning rate: 0.00697\n",
      "2025-01-16 09:57:46.641689: train_loss -0.736\n",
      "2025-01-16 09:57:46.641820: val_loss -0.6808\n",
      "2025-01-16 09:57:46.641853: Pseudo dice [np.float32(0.6742)]\n",
      "2025-01-16 09:57:46.641885: Epoch time: 110.75 s\n",
      "2025-01-16 09:57:47.213084: \n",
      "2025-01-16 09:57:47.213283: Epoch 331\n",
      "2025-01-16 09:57:47.213379: Current learning rate: 0.00696\n",
      "2025-01-16 09:59:37.975803: train_loss -0.7258\n",
      "2025-01-16 09:59:37.975932: val_loss -0.6293\n",
      "2025-01-16 09:59:37.975965: Pseudo dice [np.float32(0.6761)]\n",
      "2025-01-16 09:59:37.976003: Epoch time: 110.76 s\n",
      "2025-01-16 09:59:38.540240: \n",
      "2025-01-16 09:59:38.540344: Epoch 332\n",
      "2025-01-16 09:59:38.540405: Current learning rate: 0.00696\n",
      "2025-01-16 10:01:29.266937: train_loss -0.7594\n",
      "2025-01-16 10:01:29.267076: val_loss -0.7392\n",
      "2025-01-16 10:01:29.267110: Pseudo dice [np.float32(0.7658)]\n",
      "2025-01-16 10:01:29.267145: Epoch time: 110.73 s\n",
      "2025-01-16 10:01:29.835209: \n",
      "2025-01-16 10:01:29.835373: Epoch 333\n",
      "2025-01-16 10:01:29.835458: Current learning rate: 0.00695\n",
      "2025-01-16 10:03:20.416261: train_loss -0.7497\n",
      "2025-01-16 10:03:20.416464: val_loss -0.6378\n",
      "2025-01-16 10:03:20.416499: Pseudo dice [np.float32(0.6483)]\n",
      "2025-01-16 10:03:20.416531: Epoch time: 110.58 s\n",
      "2025-01-16 10:03:20.986664: \n",
      "2025-01-16 10:03:20.987040: Epoch 334\n",
      "2025-01-16 10:03:20.987128: Current learning rate: 0.00694\n",
      "2025-01-16 10:05:11.785571: train_loss -0.7408\n",
      "2025-01-16 10:05:11.785761: val_loss -0.6671\n",
      "2025-01-16 10:05:11.785815: Pseudo dice [np.float32(0.6206)]\n",
      "2025-01-16 10:05:11.785854: Epoch time: 110.8 s\n",
      "2025-01-16 10:05:12.371536: \n",
      "2025-01-16 10:05:12.371902: Epoch 335\n",
      "2025-01-16 10:05:12.372041: Current learning rate: 0.00693\n",
      "2025-01-16 10:07:03.118169: train_loss -0.7604\n",
      "2025-01-16 10:07:03.118334: val_loss -0.6362\n",
      "2025-01-16 10:07:03.118438: Pseudo dice [np.float32(0.6604)]\n",
      "2025-01-16 10:07:03.118483: Epoch time: 110.75 s\n",
      "2025-01-16 10:07:03.690396: \n",
      "2025-01-16 10:07:03.690737: Epoch 336\n",
      "2025-01-16 10:07:03.690972: Current learning rate: 0.00692\n",
      "2025-01-16 10:08:54.453067: train_loss -0.7546\n",
      "2025-01-16 10:08:54.453185: val_loss -0.6968\n",
      "2025-01-16 10:08:54.453216: Pseudo dice [np.float32(0.7197)]\n",
      "2025-01-16 10:08:54.453248: Epoch time: 110.76 s\n",
      "2025-01-16 10:08:55.026121: \n",
      "2025-01-16 10:08:55.026273: Epoch 337\n",
      "2025-01-16 10:08:55.026336: Current learning rate: 0.00691\n",
      "2025-01-16 10:10:45.758970: train_loss -0.7662\n",
      "2025-01-16 10:10:45.759235: val_loss -0.6896\n",
      "2025-01-16 10:10:45.759284: Pseudo dice [np.float32(0.7172)]\n",
      "2025-01-16 10:10:45.759323: Epoch time: 110.73 s\n",
      "2025-01-16 10:10:46.358567: \n",
      "2025-01-16 10:10:46.358927: Epoch 338\n",
      "2025-01-16 10:10:46.359002: Current learning rate: 0.0069\n",
      "2025-01-16 10:12:36.908531: train_loss -0.7591\n",
      "2025-01-16 10:12:36.908667: val_loss -0.6844\n",
      "2025-01-16 10:12:36.908699: Pseudo dice [np.float32(0.7368)]\n",
      "2025-01-16 10:12:36.908731: Epoch time: 110.55 s\n",
      "2025-01-16 10:12:37.482458: \n",
      "2025-01-16 10:12:37.482548: Epoch 339\n",
      "2025-01-16 10:12:37.482609: Current learning rate: 0.00689\n",
      "2025-01-16 10:14:28.198841: train_loss -0.7661\n",
      "2025-01-16 10:14:28.198966: val_loss -0.5959\n",
      "2025-01-16 10:14:28.198998: Pseudo dice [np.float32(0.6012)]\n",
      "2025-01-16 10:14:28.199030: Epoch time: 110.72 s\n",
      "2025-01-16 10:14:28.774341: \n",
      "2025-01-16 10:14:28.774514: Epoch 340\n",
      "2025-01-16 10:14:28.774692: Current learning rate: 0.00688\n",
      "2025-01-16 10:16:19.376193: train_loss -0.7815\n",
      "2025-01-16 10:16:19.376676: val_loss -0.5729\n",
      "2025-01-16 10:16:19.376736: Pseudo dice [np.float32(0.5816)]\n",
      "2025-01-16 10:16:19.376777: Epoch time: 110.6 s\n",
      "2025-01-16 10:16:19.939111: \n",
      "2025-01-16 10:16:19.939277: Epoch 341\n",
      "2025-01-16 10:16:19.939341: Current learning rate: 0.00687\n",
      "2025-01-16 10:18:10.633211: train_loss -0.7537\n",
      "2025-01-16 10:18:10.633335: val_loss -0.5792\n",
      "2025-01-16 10:18:10.633367: Pseudo dice [np.float32(0.544)]\n",
      "2025-01-16 10:18:10.633400: Epoch time: 110.69 s\n",
      "2025-01-16 10:18:11.448282: \n",
      "2025-01-16 10:18:11.448635: Epoch 342\n",
      "2025-01-16 10:18:11.448814: Current learning rate: 0.00686\n",
      "2025-01-16 10:20:02.203471: train_loss -0.7385\n",
      "2025-01-16 10:20:02.203601: val_loss -0.574\n",
      "2025-01-16 10:20:02.203635: Pseudo dice [np.float32(0.57)]\n",
      "2025-01-16 10:20:02.203670: Epoch time: 110.76 s\n",
      "2025-01-16 10:20:02.788032: \n",
      "2025-01-16 10:20:02.788151: Epoch 343\n",
      "2025-01-16 10:20:02.788218: Current learning rate: 0.00685\n",
      "2025-01-16 10:21:53.413186: train_loss -0.7308\n",
      "2025-01-16 10:21:53.413311: val_loss -0.6197\n",
      "2025-01-16 10:21:53.413342: Pseudo dice [np.float32(0.6394)]\n",
      "2025-01-16 10:21:53.413375: Epoch time: 110.63 s\n",
      "2025-01-16 10:21:53.987020: \n",
      "2025-01-16 10:21:53.987125: Epoch 344\n",
      "2025-01-16 10:21:53.987189: Current learning rate: 0.00684\n",
      "2025-01-16 10:23:44.593885: train_loss -0.7176\n",
      "2025-01-16 10:23:44.594021: val_loss -0.5566\n",
      "2025-01-16 10:23:44.594054: Pseudo dice [np.float32(0.5825)]\n",
      "2025-01-16 10:23:44.594089: Epoch time: 110.61 s\n",
      "2025-01-16 10:23:45.169049: \n",
      "2025-01-16 10:23:45.169146: Epoch 345\n",
      "2025-01-16 10:23:45.169208: Current learning rate: 0.00683\n",
      "2025-01-16 10:25:35.759525: train_loss -0.7763\n",
      "2025-01-16 10:25:35.759654: val_loss -0.6071\n",
      "2025-01-16 10:25:35.759687: Pseudo dice [np.float32(0.6474)]\n",
      "2025-01-16 10:25:35.759804: Epoch time: 110.59 s\n",
      "2025-01-16 10:25:36.339913: \n",
      "2025-01-16 10:25:36.340310: Epoch 346\n",
      "2025-01-16 10:25:36.340401: Current learning rate: 0.00682\n",
      "2025-01-16 10:27:26.898460: train_loss -0.7514\n",
      "2025-01-16 10:27:26.898806: val_loss -0.5401\n",
      "2025-01-16 10:27:26.898842: Pseudo dice [np.float32(0.6773)]\n",
      "2025-01-16 10:27:26.898876: Epoch time: 110.56 s\n",
      "2025-01-16 10:27:27.477085: \n",
      "2025-01-16 10:27:27.477243: Epoch 347\n",
      "2025-01-16 10:27:27.477341: Current learning rate: 0.00681\n",
      "2025-01-16 10:29:18.039291: train_loss -0.7673\n",
      "2025-01-16 10:29:18.039410: val_loss -0.74\n",
      "2025-01-16 10:29:18.039442: Pseudo dice [np.float32(0.7551)]\n",
      "2025-01-16 10:29:18.039477: Epoch time: 110.56 s\n",
      "2025-01-16 10:29:18.619444: \n",
      "2025-01-16 10:29:18.619822: Epoch 348\n",
      "2025-01-16 10:29:18.619963: Current learning rate: 0.0068\n",
      "2025-01-16 10:31:09.359347: train_loss -0.777\n",
      "2025-01-16 10:31:09.359490: val_loss -0.5735\n",
      "2025-01-16 10:31:09.359659: Pseudo dice [np.float32(0.5683)]\n",
      "2025-01-16 10:31:09.359742: Epoch time: 110.74 s\n",
      "2025-01-16 10:31:09.932716: \n",
      "2025-01-16 10:31:09.932806: Epoch 349\n",
      "2025-01-16 10:31:09.932869: Current learning rate: 0.0068\n",
      "2025-01-16 10:33:00.455438: train_loss -0.7402\n",
      "2025-01-16 10:33:00.455569: val_loss -0.6181\n",
      "2025-01-16 10:33:00.455603: Pseudo dice [np.float32(0.6448)]\n",
      "2025-01-16 10:33:00.455638: Epoch time: 110.52 s\n",
      "2025-01-16 10:33:01.247365: \n",
      "2025-01-16 10:33:01.247555: Epoch 350\n",
      "2025-01-16 10:33:01.247677: Current learning rate: 0.00679\n",
      "2025-01-16 10:34:51.893344: train_loss -0.7721\n",
      "2025-01-16 10:34:51.893565: val_loss -0.599\n",
      "2025-01-16 10:34:51.893605: Pseudo dice [np.float32(0.564)]\n",
      "2025-01-16 10:34:51.893639: Epoch time: 110.65 s\n",
      "2025-01-16 10:34:52.469261: \n",
      "2025-01-16 10:34:52.469357: Epoch 351\n",
      "2025-01-16 10:34:52.469421: Current learning rate: 0.00678\n",
      "2025-01-16 10:36:42.984862: train_loss -0.7574\n",
      "2025-01-16 10:36:42.984987: val_loss -0.6091\n",
      "2025-01-16 10:36:42.985020: Pseudo dice [np.float32(0.6286)]\n",
      "2025-01-16 10:36:42.985055: Epoch time: 110.52 s\n",
      "2025-01-16 10:36:43.559486: \n",
      "2025-01-16 10:36:43.559796: Epoch 352\n",
      "2025-01-16 10:36:43.560052: Current learning rate: 0.00677\n",
      "2025-01-16 10:38:34.231970: train_loss -0.7758\n",
      "2025-01-16 10:38:34.232095: val_loss -0.6756\n",
      "2025-01-16 10:38:34.232128: Pseudo dice [np.float32(0.6656)]\n",
      "2025-01-16 10:38:34.232162: Epoch time: 110.67 s\n",
      "2025-01-16 10:38:34.807840: \n",
      "2025-01-16 10:38:34.807925: Epoch 353\n",
      "2025-01-16 10:38:34.807987: Current learning rate: 0.00676\n",
      "2025-01-16 10:40:25.484772: train_loss -0.7778\n",
      "2025-01-16 10:40:25.484964: val_loss -0.5957\n",
      "2025-01-16 10:40:25.485002: Pseudo dice [np.float32(0.629)]\n",
      "2025-01-16 10:40:25.485041: Epoch time: 110.68 s\n",
      "2025-01-16 10:40:26.309673: \n",
      "2025-01-16 10:40:26.309789: Epoch 354\n",
      "2025-01-16 10:40:26.309853: Current learning rate: 0.00675\n",
      "2025-01-16 10:42:16.862123: train_loss -0.7213\n",
      "2025-01-16 10:42:16.862478: val_loss -0.6037\n",
      "2025-01-16 10:42:16.862521: Pseudo dice [np.float32(0.6489)]\n",
      "2025-01-16 10:42:16.862554: Epoch time: 110.55 s\n",
      "2025-01-16 10:42:17.439837: \n",
      "2025-01-16 10:42:17.439941: Epoch 355\n",
      "2025-01-16 10:42:17.440004: Current learning rate: 0.00674\n",
      "2025-01-16 10:44:07.936655: train_loss -0.7372\n",
      "2025-01-16 10:44:07.936790: val_loss -0.669\n",
      "2025-01-16 10:44:07.936980: Pseudo dice [np.float32(0.6437)]\n",
      "2025-01-16 10:44:07.937054: Epoch time: 110.5 s\n",
      "2025-01-16 10:44:08.507727: \n",
      "2025-01-16 10:44:08.508075: Epoch 356\n",
      "2025-01-16 10:44:08.508262: Current learning rate: 0.00673\n",
      "2025-01-16 10:45:59.048347: train_loss -0.7432\n",
      "2025-01-16 10:45:59.048585: val_loss -0.6207\n",
      "2025-01-16 10:45:59.048629: Pseudo dice [np.float32(0.6355)]\n",
      "2025-01-16 10:45:59.048669: Epoch time: 110.54 s\n",
      "2025-01-16 10:45:59.625494: \n",
      "2025-01-16 10:45:59.625590: Epoch 357\n",
      "2025-01-16 10:45:59.625653: Current learning rate: 0.00672\n",
      "2025-01-16 10:47:50.187801: train_loss -0.7268\n",
      "2025-01-16 10:47:50.187933: val_loss -0.5743\n",
      "2025-01-16 10:47:50.187973: Pseudo dice [np.float32(0.5677)]\n",
      "2025-01-16 10:47:50.188011: Epoch time: 110.56 s\n",
      "2025-01-16 10:47:50.761580: \n",
      "2025-01-16 10:47:50.761685: Epoch 358\n",
      "2025-01-16 10:47:50.761750: Current learning rate: 0.00671\n",
      "2025-01-16 10:49:41.318605: train_loss -0.7598\n",
      "2025-01-16 10:49:41.318763: val_loss -0.5926\n",
      "2025-01-16 10:49:41.318852: Pseudo dice [np.float32(0.6285)]\n",
      "2025-01-16 10:49:41.318895: Epoch time: 110.56 s\n",
      "2025-01-16 10:49:41.897753: \n",
      "2025-01-16 10:49:41.897892: Epoch 359\n",
      "2025-01-16 10:49:41.897990: Current learning rate: 0.0067\n",
      "2025-01-16 10:51:32.443581: train_loss -0.7561\n",
      "2025-01-16 10:51:32.443754: val_loss -0.6525\n",
      "2025-01-16 10:51:32.443806: Pseudo dice [np.float32(0.6138)]\n",
      "2025-01-16 10:51:32.443912: Epoch time: 110.55 s\n",
      "2025-01-16 10:51:33.022325: \n",
      "2025-01-16 10:51:33.022477: Epoch 360\n",
      "2025-01-16 10:51:33.022549: Current learning rate: 0.00669\n",
      "2025-01-16 10:53:23.680902: train_loss -0.763\n",
      "2025-01-16 10:53:23.681030: val_loss -0.7506\n",
      "2025-01-16 10:53:23.681061: Pseudo dice [np.float32(0.7754)]\n",
      "2025-01-16 10:53:23.681094: Epoch time: 110.66 s\n",
      "2025-01-16 10:53:24.254317: \n",
      "2025-01-16 10:53:24.254402: Epoch 361\n",
      "2025-01-16 10:53:24.254464: Current learning rate: 0.00668\n",
      "2025-01-16 10:55:14.785934: train_loss -0.7766\n",
      "2025-01-16 10:55:14.786082: val_loss -0.5946\n",
      "2025-01-16 10:55:14.786116: Pseudo dice [np.float32(0.6118)]\n",
      "2025-01-16 10:55:14.786183: Epoch time: 110.53 s\n",
      "2025-01-16 10:55:15.364275: \n",
      "2025-01-16 10:55:15.364598: Epoch 362\n",
      "2025-01-16 10:55:15.364672: Current learning rate: 0.00667\n",
      "2025-01-16 10:57:05.898217: train_loss -0.7906\n",
      "2025-01-16 10:57:05.898588: val_loss -0.674\n",
      "2025-01-16 10:57:05.898728: Pseudo dice [np.float32(0.7017)]\n",
      "2025-01-16 10:57:05.898782: Epoch time: 110.53 s\n",
      "2025-01-16 10:57:06.466844: \n",
      "2025-01-16 10:57:06.466935: Epoch 363\n",
      "2025-01-16 10:57:06.466998: Current learning rate: 0.00666\n",
      "2025-01-16 10:58:57.043841: train_loss -0.7659\n",
      "2025-01-16 10:58:57.044038: val_loss -0.4545\n",
      "2025-01-16 10:58:57.044070: Pseudo dice [np.float32(0.5628)]\n",
      "2025-01-16 10:58:57.044104: Epoch time: 110.58 s\n",
      "2025-01-16 10:58:57.619495: \n",
      "2025-01-16 10:58:57.619584: Epoch 364\n",
      "2025-01-16 10:58:57.619645: Current learning rate: 0.00665\n",
      "2025-01-16 11:00:48.166462: train_loss -0.7443\n",
      "2025-01-16 11:00:48.166588: val_loss -0.6328\n",
      "2025-01-16 11:00:48.166622: Pseudo dice [np.float32(0.6752)]\n",
      "2025-01-16 11:00:48.166654: Epoch time: 110.55 s\n",
      "2025-01-16 11:00:48.743305: \n",
      "2025-01-16 11:00:48.743470: Epoch 365\n",
      "2025-01-16 11:00:48.743544: Current learning rate: 0.00665\n",
      "2025-01-16 11:02:39.414830: train_loss -0.7477\n",
      "2025-01-16 11:02:39.414954: val_loss -0.629\n",
      "2025-01-16 11:02:39.414987: Pseudo dice [np.float32(0.5839)]\n",
      "2025-01-16 11:02:39.415032: Epoch time: 110.67 s\n",
      "2025-01-16 11:02:40.260449: \n",
      "2025-01-16 11:02:40.260550: Epoch 366\n",
      "2025-01-16 11:02:40.260612: Current learning rate: 0.00664\n",
      "2025-01-16 11:04:30.971073: train_loss -0.7222\n",
      "2025-01-16 11:04:30.971201: val_loss -0.7083\n",
      "2025-01-16 11:04:30.971238: Pseudo dice [np.float32(0.7149)]\n",
      "2025-01-16 11:04:30.971279: Epoch time: 110.71 s\n",
      "2025-01-16 11:04:31.548151: \n",
      "2025-01-16 11:04:31.548416: Epoch 367\n",
      "2025-01-16 11:04:31.548503: Current learning rate: 0.00663\n",
      "2025-01-16 11:06:22.046439: train_loss -0.7532\n",
      "2025-01-16 11:06:22.046565: val_loss -0.7056\n",
      "2025-01-16 11:06:22.046602: Pseudo dice [np.float32(0.6976)]\n",
      "2025-01-16 11:06:22.046637: Epoch time: 110.5 s\n",
      "2025-01-16 11:06:22.629211: \n",
      "2025-01-16 11:06:22.629404: Epoch 368\n",
      "2025-01-16 11:06:22.629596: Current learning rate: 0.00662\n",
      "2025-01-16 11:08:13.156975: train_loss -0.7706\n",
      "2025-01-16 11:08:13.157140: val_loss -0.6812\n",
      "2025-01-16 11:08:13.157215: Pseudo dice [np.float32(0.7101)]\n",
      "2025-01-16 11:08:13.157258: Epoch time: 110.53 s\n",
      "2025-01-16 11:08:13.734168: \n",
      "2025-01-16 11:08:13.734510: Epoch 369\n",
      "2025-01-16 11:08:13.734754: Current learning rate: 0.00661\n",
      "2025-01-16 11:10:04.269860: train_loss -0.7534\n",
      "2025-01-16 11:10:04.269998: val_loss -0.6812\n",
      "2025-01-16 11:10:04.270032: Pseudo dice [np.float32(0.6456)]\n",
      "2025-01-16 11:10:04.270066: Epoch time: 110.54 s\n",
      "2025-01-16 11:10:04.846568: \n",
      "2025-01-16 11:10:04.846863: Epoch 370\n",
      "2025-01-16 11:10:04.847017: Current learning rate: 0.0066\n",
      "2025-01-16 11:11:55.525635: train_loss -0.7385\n",
      "2025-01-16 11:11:55.525763: val_loss -0.6705\n",
      "2025-01-16 11:11:55.525797: Pseudo dice [np.float32(0.6803)]\n",
      "2025-01-16 11:11:55.525831: Epoch time: 110.68 s\n",
      "2025-01-16 11:11:56.096226: \n",
      "2025-01-16 11:11:56.096427: Epoch 371\n",
      "2025-01-16 11:11:56.096498: Current learning rate: 0.00659\n",
      "2025-01-16 11:13:46.568981: train_loss -0.7339\n",
      "2025-01-16 11:13:46.569209: val_loss -0.5414\n",
      "2025-01-16 11:13:46.569252: Pseudo dice [np.float32(0.6481)]\n",
      "2025-01-16 11:13:46.569286: Epoch time: 110.47 s\n",
      "2025-01-16 11:13:47.150843: \n",
      "2025-01-16 11:13:47.151023: Epoch 372\n",
      "2025-01-16 11:13:47.151165: Current learning rate: 0.00658\n",
      "2025-01-16 11:15:37.697337: train_loss -0.7601\n",
      "2025-01-16 11:15:37.697459: val_loss -0.5767\n",
      "2025-01-16 11:15:37.697491: Pseudo dice [np.float32(0.6175)]\n",
      "2025-01-16 11:15:37.697523: Epoch time: 110.55 s\n",
      "2025-01-16 11:15:38.275028: \n",
      "2025-01-16 11:15:38.275117: Epoch 373\n",
      "2025-01-16 11:15:38.275179: Current learning rate: 0.00657\n",
      "2025-01-16 11:17:28.780792: train_loss -0.7791\n",
      "2025-01-16 11:17:28.780923: val_loss -0.5386\n",
      "2025-01-16 11:17:28.780957: Pseudo dice [np.float32(0.6113)]\n",
      "2025-01-16 11:17:28.780990: Epoch time: 110.51 s\n",
      "2025-01-16 11:17:29.359915: \n",
      "2025-01-16 11:17:29.360089: Epoch 374\n",
      "2025-01-16 11:17:29.360165: Current learning rate: 0.00656\n",
      "2025-01-16 11:19:19.873062: train_loss -0.7797\n",
      "2025-01-16 11:19:19.873184: val_loss -0.6615\n",
      "2025-01-16 11:19:19.873217: Pseudo dice [np.float32(0.6693)]\n",
      "2025-01-16 11:19:19.873250: Epoch time: 110.51 s\n",
      "2025-01-16 11:19:20.447176: \n",
      "2025-01-16 11:19:20.447344: Epoch 375\n",
      "2025-01-16 11:19:20.447450: Current learning rate: 0.00655\n",
      "2025-01-16 11:21:10.991650: train_loss -0.7635\n",
      "2025-01-16 11:21:10.991856: val_loss -0.6299\n",
      "2025-01-16 11:21:10.991902: Pseudo dice [np.float32(0.6351)]\n",
      "2025-01-16 11:21:10.991937: Epoch time: 110.55 s\n",
      "2025-01-16 11:21:11.577374: \n",
      "2025-01-16 11:21:11.577460: Epoch 376\n",
      "2025-01-16 11:21:11.577522: Current learning rate: 0.00654\n",
      "2025-01-16 11:23:02.094276: train_loss -0.7732\n",
      "2025-01-16 11:23:02.094656: val_loss -0.6622\n",
      "2025-01-16 11:23:02.094700: Pseudo dice [np.float32(0.6799)]\n",
      "2025-01-16 11:23:02.094735: Epoch time: 110.52 s\n",
      "2025-01-16 11:23:02.661483: \n",
      "2025-01-16 11:23:02.661653: Epoch 377\n",
      "2025-01-16 11:23:02.661726: Current learning rate: 0.00653\n",
      "2025-01-16 11:24:53.158753: train_loss -0.7933\n",
      "2025-01-16 11:24:53.158888: val_loss -0.4645\n",
      "2025-01-16 11:24:53.158925: Pseudo dice [np.float32(0.6399)]\n",
      "2025-01-16 11:24:53.158958: Epoch time: 110.5 s\n",
      "2025-01-16 11:24:53.973494: \n",
      "2025-01-16 11:24:53.973818: Epoch 378\n",
      "2025-01-16 11:24:53.973932: Current learning rate: 0.00652\n",
      "2025-01-16 11:26:44.546432: train_loss -0.7796\n",
      "2025-01-16 11:26:44.546596: val_loss -0.6575\n",
      "2025-01-16 11:26:44.546668: Pseudo dice [np.float32(0.7032)]\n",
      "2025-01-16 11:26:44.546711: Epoch time: 110.57 s\n",
      "2025-01-16 11:26:45.127622: \n",
      "2025-01-16 11:26:45.127724: Epoch 379\n",
      "2025-01-16 11:26:45.127790: Current learning rate: 0.00651\n",
      "2025-01-16 11:28:35.599109: train_loss -0.7745\n",
      "2025-01-16 11:28:35.599465: val_loss -0.6995\n",
      "2025-01-16 11:28:35.599602: Pseudo dice [np.float32(0.6599)]\n",
      "2025-01-16 11:28:35.599653: Epoch time: 110.47 s\n",
      "2025-01-16 11:28:36.178292: \n",
      "2025-01-16 11:28:36.178390: Epoch 380\n",
      "2025-01-16 11:28:36.178452: Current learning rate: 0.0065\n",
      "2025-01-16 11:30:26.824830: train_loss -0.7862\n",
      "2025-01-16 11:30:26.824960: val_loss -0.7159\n",
      "2025-01-16 11:30:26.824993: Pseudo dice [np.float32(0.6862)]\n",
      "2025-01-16 11:30:26.825025: Epoch time: 110.65 s\n",
      "2025-01-16 11:30:27.401711: \n",
      "2025-01-16 11:30:27.401816: Epoch 381\n",
      "2025-01-16 11:30:27.401880: Current learning rate: 0.00649\n",
      "2025-01-16 11:32:17.972944: train_loss -0.7921\n",
      "2025-01-16 11:32:17.973082: val_loss -0.6676\n",
      "2025-01-16 11:32:17.973114: Pseudo dice [np.float32(0.6227)]\n",
      "2025-01-16 11:32:17.973148: Epoch time: 110.57 s\n",
      "2025-01-16 11:32:18.559205: \n",
      "2025-01-16 11:32:18.559304: Epoch 382\n",
      "2025-01-16 11:32:18.559364: Current learning rate: 0.00648\n",
      "2025-01-16 11:34:09.235937: train_loss -0.8001\n",
      "2025-01-16 11:34:09.236054: val_loss -0.5494\n",
      "2025-01-16 11:34:09.236086: Pseudo dice [np.float32(0.5913)]\n",
      "2025-01-16 11:34:09.236119: Epoch time: 110.68 s\n",
      "2025-01-16 11:34:09.828286: \n",
      "2025-01-16 11:34:09.828616: Epoch 383\n",
      "2025-01-16 11:34:09.828784: Current learning rate: 0.00648\n",
      "2025-01-16 11:36:00.359904: train_loss -0.7977\n",
      "2025-01-16 11:36:00.360026: val_loss -0.6976\n",
      "2025-01-16 11:36:00.360074: Pseudo dice [np.float32(0.6966)]\n",
      "2025-01-16 11:36:00.360132: Epoch time: 110.53 s\n",
      "2025-01-16 11:36:00.948301: \n",
      "2025-01-16 11:36:00.948399: Epoch 384\n",
      "2025-01-16 11:36:00.948545: Current learning rate: 0.00647\n",
      "2025-01-16 11:37:51.550199: train_loss -0.7935\n",
      "2025-01-16 11:37:51.550558: val_loss -0.5364\n",
      "2025-01-16 11:37:51.550606: Pseudo dice [np.float32(0.5982)]\n",
      "2025-01-16 11:37:51.550640: Epoch time: 110.6 s\n",
      "2025-01-16 11:37:52.130457: \n",
      "2025-01-16 11:37:52.130546: Epoch 385\n",
      "2025-01-16 11:37:52.130611: Current learning rate: 0.00646\n",
      "2025-01-16 11:39:42.647772: train_loss -0.8132\n",
      "2025-01-16 11:39:42.647908: val_loss -0.639\n",
      "2025-01-16 11:39:42.647941: Pseudo dice [np.float32(0.6254)]\n",
      "2025-01-16 11:39:42.647973: Epoch time: 110.52 s\n",
      "2025-01-16 11:39:43.236197: \n",
      "2025-01-16 11:39:43.236608: Epoch 386\n",
      "2025-01-16 11:39:43.236804: Current learning rate: 0.00645\n",
      "2025-01-16 11:41:33.879942: train_loss -0.8016\n",
      "2025-01-16 11:41:33.880113: val_loss -0.7356\n",
      "2025-01-16 11:41:33.880147: Pseudo dice [np.float32(0.7306)]\n",
      "2025-01-16 11:41:33.880180: Epoch time: 110.64 s\n",
      "2025-01-16 11:41:34.464473: \n",
      "2025-01-16 11:41:34.464940: Epoch 387\n",
      "2025-01-16 11:41:34.465113: Current learning rate: 0.00644\n",
      "2025-01-16 11:43:25.124720: train_loss -0.8025\n",
      "2025-01-16 11:43:25.124948: val_loss -0.6726\n",
      "2025-01-16 11:43:25.124992: Pseudo dice [np.float32(0.6313)]\n",
      "2025-01-16 11:43:25.125039: Epoch time: 110.66 s\n",
      "2025-01-16 11:43:25.716288: \n",
      "2025-01-16 11:43:25.716380: Epoch 388\n",
      "2025-01-16 11:43:25.716445: Current learning rate: 0.00643\n",
      "2025-01-16 11:45:16.231213: train_loss -0.8185\n",
      "2025-01-16 11:45:16.231352: val_loss -0.5721\n",
      "2025-01-16 11:45:16.231388: Pseudo dice [np.float32(0.6199)]\n",
      "2025-01-16 11:45:16.231421: Epoch time: 110.52 s\n",
      "2025-01-16 11:45:16.823523: \n",
      "2025-01-16 11:45:16.823701: Epoch 389\n",
      "2025-01-16 11:45:16.823834: Current learning rate: 0.00642\n",
      "2025-01-16 11:47:07.354262: train_loss -0.7994\n",
      "2025-01-16 11:47:07.354446: val_loss -0.6259\n",
      "2025-01-16 11:47:07.354482: Pseudo dice [np.float32(0.5938)]\n",
      "2025-01-16 11:47:07.354516: Epoch time: 110.53 s\n",
      "2025-01-16 11:47:08.172791: \n",
      "2025-01-16 11:47:08.172890: Epoch 390\n",
      "2025-01-16 11:47:08.172953: Current learning rate: 0.00641\n",
      "2025-01-16 11:48:58.682322: train_loss -0.8153\n",
      "2025-01-16 11:48:58.682470: val_loss -0.6672\n",
      "2025-01-16 11:48:58.682508: Pseudo dice [np.float32(0.6919)]\n",
      "2025-01-16 11:48:58.682543: Epoch time: 110.51 s\n",
      "2025-01-16 11:48:59.271730: \n",
      "2025-01-16 11:48:59.271833: Epoch 391\n",
      "2025-01-16 11:48:59.271900: Current learning rate: 0.0064\n",
      "2025-01-16 11:50:49.910069: train_loss -0.7819\n",
      "2025-01-16 11:50:49.910293: val_loss -0.5571\n",
      "2025-01-16 11:50:49.910335: Pseudo dice [np.float32(0.6065)]\n",
      "2025-01-16 11:50:49.910369: Epoch time: 110.64 s\n",
      "2025-01-16 11:50:50.497449: \n",
      "2025-01-16 11:50:50.497549: Epoch 392\n",
      "2025-01-16 11:50:50.497645: Current learning rate: 0.00639\n",
      "2025-01-16 11:52:41.046699: train_loss -0.7752\n",
      "2025-01-16 11:52:41.046874: val_loss -0.6798\n",
      "2025-01-16 11:52:41.046923: Pseudo dice [np.float32(0.6826)]\n",
      "2025-01-16 11:52:41.046957: Epoch time: 110.55 s\n",
      "2025-01-16 11:52:41.634000: \n",
      "2025-01-16 11:52:41.634304: Epoch 393\n",
      "2025-01-16 11:52:41.634472: Current learning rate: 0.00638\n",
      "2025-01-16 11:54:32.165710: train_loss -0.8121\n",
      "2025-01-16 11:54:32.165888: val_loss -0.5023\n",
      "2025-01-16 11:54:32.165919: Pseudo dice [np.float32(0.5912)]\n",
      "2025-01-16 11:54:32.165952: Epoch time: 110.53 s\n",
      "2025-01-16 11:54:32.752961: \n",
      "2025-01-16 11:54:32.753051: Epoch 394\n",
      "2025-01-16 11:54:32.753115: Current learning rate: 0.00637\n",
      "2025-01-16 11:56:23.275192: train_loss -0.7758\n",
      "2025-01-16 11:56:23.275392: val_loss -0.6446\n",
      "2025-01-16 11:56:23.275425: Pseudo dice [np.float32(0.6759)]\n",
      "2025-01-16 11:56:23.275459: Epoch time: 110.52 s\n",
      "2025-01-16 11:56:23.861749: \n",
      "2025-01-16 11:56:23.861843: Epoch 395\n",
      "2025-01-16 11:56:23.861905: Current learning rate: 0.00636\n",
      "2025-01-16 11:58:14.517226: train_loss -0.8191\n",
      "2025-01-16 11:58:14.517406: val_loss -0.6374\n",
      "2025-01-16 11:58:14.517441: Pseudo dice [np.float32(0.6791)]\n",
      "2025-01-16 11:58:14.517477: Epoch time: 110.66 s\n",
      "2025-01-16 11:58:15.105112: \n",
      "2025-01-16 11:58:15.105308: Epoch 396\n",
      "2025-01-16 11:58:15.105412: Current learning rate: 0.00635\n",
      "2025-01-16 12:00:05.734277: train_loss -0.7855\n",
      "2025-01-16 12:00:05.734460: val_loss -0.6761\n",
      "2025-01-16 12:00:05.734492: Pseudo dice [np.float32(0.6925)]\n",
      "2025-01-16 12:00:05.734529: Epoch time: 110.63 s\n",
      "2025-01-16 12:00:06.317806: \n",
      "2025-01-16 12:00:06.318156: Epoch 397\n",
      "2025-01-16 12:00:06.318268: Current learning rate: 0.00634\n",
      "2025-01-16 12:01:56.916449: train_loss -0.7597\n",
      "2025-01-16 12:01:56.916584: val_loss -0.6313\n",
      "2025-01-16 12:01:56.916615: Pseudo dice [np.float32(0.6417)]\n",
      "2025-01-16 12:01:56.916649: Epoch time: 110.6 s\n",
      "2025-01-16 12:01:57.502937: \n",
      "2025-01-16 12:01:57.503330: Epoch 398\n",
      "2025-01-16 12:01:57.503425: Current learning rate: 0.00633\n",
      "2025-01-16 12:03:48.030854: train_loss -0.7641\n",
      "2025-01-16 12:03:48.030976: val_loss -0.6848\n",
      "2025-01-16 12:03:48.031007: Pseudo dice [np.float32(0.6808)]\n",
      "2025-01-16 12:03:48.031039: Epoch time: 110.53 s\n",
      "2025-01-16 12:03:48.611387: \n",
      "2025-01-16 12:03:48.611571: Epoch 399\n",
      "2025-01-16 12:03:48.611751: Current learning rate: 0.00632\n",
      "2025-01-16 12:05:39.157680: train_loss -0.7717\n",
      "2025-01-16 12:05:39.158036: val_loss -0.6983\n",
      "2025-01-16 12:05:39.158192: Pseudo dice [np.float32(0.7135)]\n",
      "2025-01-16 12:05:39.158245: Epoch time: 110.55 s\n",
      "2025-01-16 12:05:39.988844: \n",
      "2025-01-16 12:05:39.988925: Epoch 400\n",
      "2025-01-16 12:05:39.988988: Current learning rate: 0.00631\n",
      "2025-01-16 12:07:30.503697: train_loss -0.7693\n",
      "2025-01-16 12:07:30.503826: val_loss -0.6555\n",
      "2025-01-16 12:07:30.503859: Pseudo dice [np.float32(0.6272)]\n",
      "2025-01-16 12:07:30.503891: Epoch time: 110.52 s\n",
      "2025-01-16 12:07:31.334694: \n",
      "2025-01-16 12:07:31.335078: Epoch 401\n",
      "2025-01-16 12:07:31.335245: Current learning rate: 0.0063\n",
      "2025-01-16 12:09:21.833353: train_loss -0.7764\n",
      "2025-01-16 12:09:21.833493: val_loss -0.5461\n",
      "2025-01-16 12:09:21.833528: Pseudo dice [np.float32(0.6022)]\n",
      "2025-01-16 12:09:21.833565: Epoch time: 110.5 s\n",
      "2025-01-16 12:09:22.426881: \n",
      "2025-01-16 12:09:22.426989: Epoch 402\n",
      "2025-01-16 12:09:22.427049: Current learning rate: 0.0063\n",
      "2025-01-16 12:11:13.043740: train_loss -0.7887\n",
      "2025-01-16 12:11:13.044117: val_loss -0.5397\n",
      "2025-01-16 12:11:13.044275: Pseudo dice [np.float32(0.5654)]\n",
      "2025-01-16 12:11:13.044322: Epoch time: 110.62 s\n",
      "2025-01-16 12:11:13.637849: \n",
      "2025-01-16 12:11:13.638180: Epoch 403\n",
      "2025-01-16 12:11:13.638391: Current learning rate: 0.00629\n",
      "2025-01-16 12:13:04.079683: train_loss -0.7748\n",
      "2025-01-16 12:13:04.079824: val_loss -0.6782\n",
      "2025-01-16 12:13:04.079859: Pseudo dice [np.float32(0.6666)]\n",
      "2025-01-16 12:13:04.079892: Epoch time: 110.44 s\n",
      "2025-01-16 12:13:04.671880: \n",
      "2025-01-16 12:13:04.672204: Epoch 404\n",
      "2025-01-16 12:13:04.672278: Current learning rate: 0.00628\n",
      "2025-01-16 12:14:55.228961: train_loss -0.7815\n",
      "2025-01-16 12:14:55.229122: val_loss -0.6505\n",
      "2025-01-16 12:14:55.229270: Pseudo dice [np.float32(0.692)]\n",
      "2025-01-16 12:14:55.229347: Epoch time: 110.56 s\n",
      "2025-01-16 12:14:55.812406: \n",
      "2025-01-16 12:14:55.812506: Epoch 405\n",
      "2025-01-16 12:14:55.812600: Current learning rate: 0.00627\n",
      "2025-01-16 12:16:46.377908: train_loss -0.7513\n",
      "2025-01-16 12:16:46.378040: val_loss -0.6667\n",
      "2025-01-16 12:16:46.378074: Pseudo dice [np.float32(0.7272)]\n",
      "2025-01-16 12:16:46.378107: Epoch time: 110.57 s\n",
      "2025-01-16 12:16:46.966990: \n",
      "2025-01-16 12:16:46.967257: Epoch 406\n",
      "2025-01-16 12:16:46.967491: Current learning rate: 0.00626\n",
      "2025-01-16 12:18:37.524475: train_loss -0.7917\n",
      "2025-01-16 12:18:37.524677: val_loss -0.6162\n",
      "2025-01-16 12:18:37.524724: Pseudo dice [np.float32(0.6943)]\n",
      "2025-01-16 12:18:37.524759: Epoch time: 110.56 s\n",
      "2025-01-16 12:18:38.104580: \n",
      "2025-01-16 12:18:38.104900: Epoch 407\n",
      "2025-01-16 12:18:38.105111: Current learning rate: 0.00625\n",
      "2025-01-16 12:20:28.558817: train_loss -0.7805\n",
      "2025-01-16 12:20:28.558940: val_loss -0.6327\n",
      "2025-01-16 12:20:28.558971: Pseudo dice [np.float32(0.6002)]\n",
      "2025-01-16 12:20:28.559020: Epoch time: 110.45 s\n",
      "2025-01-16 12:20:29.143686: \n",
      "2025-01-16 12:20:29.143780: Epoch 408\n",
      "2025-01-16 12:20:29.143842: Current learning rate: 0.00624\n",
      "2025-01-16 12:22:19.664023: train_loss -0.7518\n",
      "2025-01-16 12:22:19.664225: val_loss -0.4952\n",
      "2025-01-16 12:22:19.664585: Pseudo dice [np.float32(0.618)]\n",
      "2025-01-16 12:22:19.664663: Epoch time: 110.52 s\n",
      "2025-01-16 12:22:20.255080: \n",
      "2025-01-16 12:22:20.255453: Epoch 409\n",
      "2025-01-16 12:22:20.255643: Current learning rate: 0.00623\n",
      "2025-01-16 12:24:10.739237: train_loss -0.6892\n",
      "2025-01-16 12:24:10.739402: val_loss -0.6402\n",
      "2025-01-16 12:24:10.739716: Pseudo dice [np.float32(0.6527)]\n",
      "2025-01-16 12:24:10.739903: Epoch time: 110.48 s\n",
      "2025-01-16 12:24:11.325164: \n",
      "2025-01-16 12:24:11.325499: Epoch 410\n",
      "2025-01-16 12:24:11.325593: Current learning rate: 0.00622\n",
      "2025-01-16 12:26:01.865571: train_loss -0.7481\n",
      "2025-01-16 12:26:01.865773: val_loss -0.7061\n",
      "2025-01-16 12:26:01.865808: Pseudo dice [np.float32(0.7691)]\n",
      "2025-01-16 12:26:01.865841: Epoch time: 110.54 s\n",
      "2025-01-16 12:26:02.428524: \n",
      "2025-01-16 12:26:02.428615: Epoch 411\n",
      "2025-01-16 12:26:02.428676: Current learning rate: 0.00621\n",
      "2025-01-16 12:27:52.939095: train_loss -0.7421\n",
      "2025-01-16 12:27:52.939219: val_loss -0.5945\n",
      "2025-01-16 12:27:52.939252: Pseudo dice [np.float32(0.6139)]\n",
      "2025-01-16 12:27:52.939285: Epoch time: 110.51 s\n",
      "2025-01-16 12:27:53.500546: \n",
      "2025-01-16 12:27:53.500636: Epoch 412\n",
      "2025-01-16 12:27:53.500701: Current learning rate: 0.0062\n",
      "2025-01-16 12:29:44.015550: train_loss -0.7547\n",
      "2025-01-16 12:29:44.015707: val_loss -0.5981\n",
      "2025-01-16 12:29:44.015900: Pseudo dice [np.float32(0.6748)]\n",
      "2025-01-16 12:29:44.015944: Epoch time: 110.52 s\n",
      "2025-01-16 12:29:44.823788: \n",
      "2025-01-16 12:29:44.823894: Epoch 413\n",
      "2025-01-16 12:29:44.823957: Current learning rate: 0.00619\n",
      "2025-01-16 12:31:35.394471: train_loss -0.7303\n",
      "2025-01-16 12:31:35.394659: val_loss -0.6983\n",
      "2025-01-16 12:31:35.394705: Pseudo dice [np.float32(0.7157)]\n",
      "2025-01-16 12:31:35.394739: Epoch time: 110.57 s\n",
      "2025-01-16 12:31:35.955749: \n",
      "2025-01-16 12:31:35.956216: Epoch 414\n",
      "2025-01-16 12:31:35.956308: Current learning rate: 0.00618\n",
      "2025-01-16 12:33:26.441032: train_loss -0.7703\n",
      "2025-01-16 12:33:26.441231: val_loss -0.629\n",
      "2025-01-16 12:33:26.441303: Pseudo dice [np.float32(0.5792)]\n",
      "2025-01-16 12:33:26.441345: Epoch time: 110.49 s\n",
      "2025-01-16 12:33:27.010113: \n",
      "2025-01-16 12:33:27.010329: Epoch 415\n",
      "2025-01-16 12:33:27.010398: Current learning rate: 0.00617\n",
      "2025-01-16 12:35:17.542178: train_loss -0.7629\n",
      "2025-01-16 12:35:17.542333: val_loss -0.5967\n",
      "2025-01-16 12:35:17.542563: Pseudo dice [np.float32(0.6004)]\n",
      "2025-01-16 12:35:17.542633: Epoch time: 110.53 s\n",
      "2025-01-16 12:35:18.112013: \n",
      "2025-01-16 12:35:18.112361: Epoch 416\n",
      "2025-01-16 12:35:18.112552: Current learning rate: 0.00616\n",
      "2025-01-16 12:37:08.718814: train_loss -0.7582\n",
      "2025-01-16 12:37:08.719012: val_loss -0.6599\n",
      "2025-01-16 12:37:08.719056: Pseudo dice [np.float32(0.6235)]\n",
      "2025-01-16 12:37:08.719092: Epoch time: 110.61 s\n",
      "2025-01-16 12:37:09.288802: \n",
      "2025-01-16 12:37:09.288904: Epoch 417\n",
      "2025-01-16 12:37:09.288966: Current learning rate: 0.00615\n",
      "2025-01-16 12:38:59.820436: train_loss -0.7573\n",
      "2025-01-16 12:38:59.820566: val_loss -0.6151\n",
      "2025-01-16 12:38:59.820600: Pseudo dice [np.float32(0.5334)]\n",
      "2025-01-16 12:38:59.820632: Epoch time: 110.53 s\n",
      "2025-01-16 12:39:00.380258: \n",
      "2025-01-16 12:39:00.380560: Epoch 418\n",
      "2025-01-16 12:39:00.380712: Current learning rate: 0.00614\n",
      "2025-01-16 12:40:50.859065: train_loss -0.7812\n",
      "2025-01-16 12:40:50.859428: val_loss -0.5451\n",
      "2025-01-16 12:40:50.859552: Pseudo dice [np.float32(0.6407)]\n",
      "2025-01-16 12:40:50.859596: Epoch time: 110.48 s\n",
      "2025-01-16 12:40:51.431519: \n",
      "2025-01-16 12:40:51.431618: Epoch 419\n",
      "2025-01-16 12:40:51.431682: Current learning rate: 0.00613\n",
      "2025-01-16 12:42:41.898334: train_loss -0.8017\n",
      "2025-01-16 12:42:41.898495: val_loss -0.7065\n",
      "2025-01-16 12:42:41.898528: Pseudo dice [np.float32(0.6209)]\n",
      "2025-01-16 12:42:41.898561: Epoch time: 110.47 s\n",
      "2025-01-16 12:42:42.461219: \n",
      "2025-01-16 12:42:42.461571: Epoch 420\n",
      "2025-01-16 12:42:42.461645: Current learning rate: 0.00612\n",
      "2025-01-16 12:44:33.068182: train_loss -0.7781\n",
      "2025-01-16 12:44:33.068434: val_loss -0.6812\n",
      "2025-01-16 12:44:33.068672: Pseudo dice [np.float32(0.6862)]\n",
      "2025-01-16 12:44:33.068748: Epoch time: 110.61 s\n",
      "2025-01-16 12:44:33.628761: \n",
      "2025-01-16 12:44:33.629133: Epoch 421\n",
      "2025-01-16 12:44:33.629219: Current learning rate: 0.00612\n",
      "2025-01-16 12:46:24.140985: train_loss -0.7864\n",
      "2025-01-16 12:46:24.141106: val_loss -0.6106\n",
      "2025-01-16 12:46:24.141139: Pseudo dice [np.float32(0.615)]\n",
      "2025-01-16 12:46:24.141171: Epoch time: 110.51 s\n",
      "2025-01-16 12:46:24.702441: \n",
      "2025-01-16 12:46:24.702529: Epoch 422\n",
      "2025-01-16 12:46:24.702591: Current learning rate: 0.00611\n",
      "2025-01-16 12:48:15.321271: train_loss -0.7826\n",
      "2025-01-16 12:48:15.321518: val_loss -0.4833\n",
      "2025-01-16 12:48:15.321593: Pseudo dice [np.float32(0.4892)]\n",
      "2025-01-16 12:48:15.321637: Epoch time: 110.62 s\n",
      "2025-01-16 12:48:15.893617: \n",
      "2025-01-16 12:48:15.893711: Epoch 423\n",
      "2025-01-16 12:48:15.893774: Current learning rate: 0.0061\n",
      "2025-01-16 12:50:06.385447: train_loss -0.802\n",
      "2025-01-16 12:50:06.385862: val_loss -0.7413\n",
      "2025-01-16 12:50:06.385907: Pseudo dice [np.float32(0.7626)]\n",
      "2025-01-16 12:50:06.385943: Epoch time: 110.49 s\n",
      "2025-01-16 12:50:06.954389: \n",
      "2025-01-16 12:50:06.954474: Epoch 424\n",
      "2025-01-16 12:50:06.954536: Current learning rate: 0.00609\n",
      "2025-01-16 12:51:57.445996: train_loss -0.794\n",
      "2025-01-16 12:51:57.446213: val_loss -0.6855\n",
      "2025-01-16 12:51:57.446260: Pseudo dice [np.float32(0.6417)]\n",
      "2025-01-16 12:51:57.446298: Epoch time: 110.49 s\n",
      "2025-01-16 12:51:58.249911: \n",
      "2025-01-16 12:51:58.250224: Epoch 425\n",
      "2025-01-16 12:51:58.250387: Current learning rate: 0.00608\n",
      "2025-01-16 12:53:48.773153: train_loss -0.7447\n",
      "2025-01-16 12:53:48.773323: val_loss -0.5638\n",
      "2025-01-16 12:53:48.773394: Pseudo dice [np.float32(0.5947)]\n",
      "2025-01-16 12:53:48.773437: Epoch time: 110.52 s\n",
      "2025-01-16 12:53:49.346183: \n",
      "2025-01-16 12:53:49.346287: Epoch 426\n",
      "2025-01-16 12:53:49.346348: Current learning rate: 0.00607\n",
      "2025-01-16 12:55:39.834011: train_loss -0.7221\n",
      "2025-01-16 12:55:39.834138: val_loss -0.5564\n",
      "2025-01-16 12:55:39.834172: Pseudo dice [np.float32(0.6636)]\n",
      "2025-01-16 12:55:39.834235: Epoch time: 110.49 s\n",
      "2025-01-16 12:55:40.404270: \n",
      "2025-01-16 12:55:40.404370: Epoch 427\n",
      "2025-01-16 12:55:40.404434: Current learning rate: 0.00606\n",
      "2025-01-16 12:57:30.881275: train_loss -0.7461\n",
      "2025-01-16 12:57:30.881675: val_loss -0.6653\n",
      "2025-01-16 12:57:30.881721: Pseudo dice [np.float32(0.6466)]\n",
      "2025-01-16 12:57:30.881756: Epoch time: 110.48 s\n",
      "2025-01-16 12:57:31.459541: \n",
      "2025-01-16 12:57:31.459641: Epoch 428\n",
      "2025-01-16 12:57:31.459702: Current learning rate: 0.00605\n",
      "2025-01-16 12:59:21.958141: train_loss -0.7498\n",
      "2025-01-16 12:59:21.958264: val_loss -0.5696\n",
      "2025-01-16 12:59:21.958295: Pseudo dice [np.float32(0.6273)]\n",
      "2025-01-16 12:59:21.958355: Epoch time: 110.5 s\n",
      "2025-01-16 12:59:22.529948: \n",
      "2025-01-16 12:59:22.530090: Epoch 429\n",
      "2025-01-16 12:59:22.530168: Current learning rate: 0.00604\n",
      "2025-01-16 13:01:12.994278: train_loss -0.7853\n",
      "2025-01-16 13:01:12.994399: val_loss -0.6247\n",
      "2025-01-16 13:01:12.994431: Pseudo dice [np.float32(0.6648)]\n",
      "2025-01-16 13:01:12.994464: Epoch time: 110.46 s\n",
      "2025-01-16 13:01:13.560469: \n",
      "2025-01-16 13:01:13.560670: Epoch 430\n",
      "2025-01-16 13:01:13.560766: Current learning rate: 0.00603\n",
      "2025-01-16 13:03:04.038582: train_loss -0.7848\n",
      "2025-01-16 13:03:04.038709: val_loss -0.5623\n",
      "2025-01-16 13:03:04.038743: Pseudo dice [np.float32(0.5548)]\n",
      "2025-01-16 13:03:04.038776: Epoch time: 110.48 s\n",
      "2025-01-16 13:03:04.613271: \n",
      "2025-01-16 13:03:04.613490: Epoch 431\n",
      "2025-01-16 13:03:04.613769: Current learning rate: 0.00602\n",
      "2025-01-16 13:04:55.086577: train_loss -0.743\n",
      "2025-01-16 13:04:55.086905: val_loss -0.6297\n",
      "2025-01-16 13:04:55.086952: Pseudo dice [np.float32(0.6533)]\n",
      "2025-01-16 13:04:55.086994: Epoch time: 110.47 s\n",
      "2025-01-16 13:04:55.652774: \n",
      "2025-01-16 13:04:55.652863: Epoch 432\n",
      "2025-01-16 13:04:55.652923: Current learning rate: 0.00601\n",
      "2025-01-16 13:06:46.288806: train_loss -0.7563\n",
      "2025-01-16 13:06:46.288998: val_loss -0.5393\n",
      "2025-01-16 13:06:46.289036: Pseudo dice [np.float32(0.6043)]\n",
      "2025-01-16 13:06:46.289070: Epoch time: 110.64 s\n",
      "2025-01-16 13:06:46.850809: \n",
      "2025-01-16 13:06:46.850903: Epoch 433\n",
      "2025-01-16 13:06:46.850965: Current learning rate: 0.006\n",
      "2025-01-16 13:08:37.452592: train_loss -0.7574\n",
      "2025-01-16 13:08:37.453003: val_loss -0.5376\n",
      "2025-01-16 13:08:37.453147: Pseudo dice [np.float32(0.6194)]\n",
      "2025-01-16 13:08:37.453197: Epoch time: 110.6 s\n",
      "2025-01-16 13:08:38.020035: \n",
      "2025-01-16 13:08:38.020201: Epoch 434\n",
      "2025-01-16 13:08:38.020277: Current learning rate: 0.00599\n",
      "2025-01-16 13:10:28.567185: train_loss -0.7803\n",
      "2025-01-16 13:10:28.567379: val_loss -0.6342\n",
      "2025-01-16 13:10:28.567494: Pseudo dice [np.float32(0.6665)]\n",
      "2025-01-16 13:10:28.567645: Epoch time: 110.55 s\n",
      "2025-01-16 13:10:29.140614: \n",
      "2025-01-16 13:10:29.140705: Epoch 435\n",
      "2025-01-16 13:10:29.140765: Current learning rate: 0.00598\n",
      "2025-01-16 13:12:19.774160: train_loss -0.7871\n",
      "2025-01-16 13:12:19.774307: val_loss -0.6723\n",
      "2025-01-16 13:12:19.774340: Pseudo dice [np.float32(0.6122)]\n",
      "2025-01-16 13:12:19.774372: Epoch time: 110.63 s\n",
      "2025-01-16 13:12:20.349520: \n",
      "2025-01-16 13:12:20.349618: Epoch 436\n",
      "2025-01-16 13:12:20.349682: Current learning rate: 0.00597\n",
      "2025-01-16 13:14:10.907037: train_loss -0.7735\n",
      "2025-01-16 13:14:10.907420: val_loss -0.7277\n",
      "2025-01-16 13:14:10.907624: Pseudo dice [np.float32(0.7571)]\n",
      "2025-01-16 13:14:10.907675: Epoch time: 110.56 s\n",
      "2025-01-16 13:14:11.481729: \n",
      "2025-01-16 13:14:11.482108: Epoch 437\n",
      "2025-01-16 13:14:11.482203: Current learning rate: 0.00596\n",
      "2025-01-16 13:16:02.022988: train_loss -0.8003\n",
      "2025-01-16 13:16:02.023281: val_loss -0.5875\n",
      "2025-01-16 13:16:02.023329: Pseudo dice [np.float32(0.6341)]\n",
      "2025-01-16 13:16:02.023366: Epoch time: 110.54 s\n",
      "2025-01-16 13:16:02.850032: \n",
      "2025-01-16 13:16:02.850416: Epoch 438\n",
      "2025-01-16 13:16:02.850515: Current learning rate: 0.00595\n",
      "2025-01-16 13:17:53.299141: train_loss -0.8117\n",
      "2025-01-16 13:17:53.299560: val_loss -0.5607\n",
      "2025-01-16 13:17:53.299637: Pseudo dice [np.float32(0.5441)]\n",
      "2025-01-16 13:17:53.299681: Epoch time: 110.45 s\n",
      "2025-01-16 13:17:53.876613: \n",
      "2025-01-16 13:17:53.877011: Epoch 439\n",
      "2025-01-16 13:17:53.877115: Current learning rate: 0.00594\n",
      "2025-01-16 13:19:44.414327: train_loss -0.7477\n",
      "2025-01-16 13:19:44.414479: val_loss -0.6366\n",
      "2025-01-16 13:19:44.414514: Pseudo dice [np.float32(0.6421)]\n",
      "2025-01-16 13:19:44.414676: Epoch time: 110.54 s\n",
      "2025-01-16 13:19:44.988346: \n",
      "2025-01-16 13:19:44.988712: Epoch 440\n",
      "2025-01-16 13:19:44.988813: Current learning rate: 0.00593\n",
      "2025-01-16 13:21:35.503535: train_loss -0.7743\n",
      "2025-01-16 13:21:35.503654: val_loss -0.7204\n",
      "2025-01-16 13:21:35.503688: Pseudo dice [np.float32(0.7325)]\n",
      "2025-01-16 13:21:35.503721: Epoch time: 110.52 s\n",
      "2025-01-16 13:21:36.082339: \n",
      "2025-01-16 13:21:36.082632: Epoch 441\n",
      "2025-01-16 13:21:36.082773: Current learning rate: 0.00592\n",
      "2025-01-16 13:23:26.575098: train_loss -0.8016\n",
      "2025-01-16 13:23:26.575465: val_loss -0.6461\n",
      "2025-01-16 13:23:26.575509: Pseudo dice [np.float32(0.6761)]\n",
      "2025-01-16 13:23:26.575544: Epoch time: 110.49 s\n",
      "2025-01-16 13:23:27.152231: \n",
      "2025-01-16 13:23:27.152582: Epoch 442\n",
      "2025-01-16 13:23:27.152651: Current learning rate: 0.00592\n",
      "2025-01-16 13:25:17.828382: train_loss -0.8075\n",
      "2025-01-16 13:25:17.828530: val_loss -0.6843\n",
      "2025-01-16 13:25:17.828564: Pseudo dice [np.float32(0.7268)]\n",
      "2025-01-16 13:25:17.828598: Epoch time: 110.68 s\n",
      "2025-01-16 13:25:18.400811: \n",
      "2025-01-16 13:25:18.400978: Epoch 443\n",
      "2025-01-16 13:25:18.401143: Current learning rate: 0.00591\n",
      "2025-01-16 13:27:08.935413: train_loss -0.7645\n",
      "2025-01-16 13:27:08.935752: val_loss -0.6567\n",
      "2025-01-16 13:27:08.935813: Pseudo dice [np.float32(0.641)]\n",
      "2025-01-16 13:27:08.935872: Epoch time: 110.54 s\n",
      "2025-01-16 13:27:09.498062: \n",
      "2025-01-16 13:27:09.498236: Epoch 444\n",
      "2025-01-16 13:27:09.498301: Current learning rate: 0.0059\n",
      "2025-01-16 13:29:00.039366: train_loss -0.7816\n",
      "2025-01-16 13:29:00.039501: val_loss -0.6773\n",
      "2025-01-16 13:29:00.039533: Pseudo dice [np.float32(0.6841)]\n",
      "2025-01-16 13:29:00.039603: Epoch time: 110.54 s\n",
      "2025-01-16 13:29:00.611572: \n",
      "2025-01-16 13:29:00.611665: Epoch 445\n",
      "2025-01-16 13:29:00.611727: Current learning rate: 0.00589\n",
      "2025-01-16 13:30:51.135718: train_loss -0.7752\n",
      "2025-01-16 13:30:51.135880: val_loss -0.6996\n",
      "2025-01-16 13:30:51.135926: Pseudo dice [np.float32(0.7417)]\n",
      "2025-01-16 13:30:51.135973: Epoch time: 110.52 s\n",
      "2025-01-16 13:30:51.704610: \n",
      "2025-01-16 13:30:51.704708: Epoch 446\n",
      "2025-01-16 13:30:51.704772: Current learning rate: 0.00588\n",
      "2025-01-16 13:32:42.209191: train_loss -0.7963\n",
      "2025-01-16 13:32:42.209315: val_loss -0.6521\n",
      "2025-01-16 13:32:42.209347: Pseudo dice [np.float32(0.6571)]\n",
      "2025-01-16 13:32:42.209378: Epoch time: 110.51 s\n",
      "2025-01-16 13:32:42.769709: \n",
      "2025-01-16 13:32:42.770032: Epoch 447\n",
      "2025-01-16 13:32:42.770134: Current learning rate: 0.00587\n",
      "2025-01-16 13:34:33.411016: train_loss -0.7751\n",
      "2025-01-16 13:34:33.411150: val_loss -0.6555\n",
      "2025-01-16 13:34:33.411262: Pseudo dice [np.float32(0.6953)]\n",
      "2025-01-16 13:34:33.411466: Epoch time: 110.64 s\n",
      "2025-01-16 13:34:33.982409: \n",
      "2025-01-16 13:34:33.982500: Epoch 448\n",
      "2025-01-16 13:34:33.982563: Current learning rate: 0.00586\n",
      "2025-01-16 13:36:24.468546: train_loss -0.7713\n",
      "2025-01-16 13:36:24.468675: val_loss -0.6672\n",
      "2025-01-16 13:36:24.468708: Pseudo dice [np.float32(0.6592)]\n",
      "2025-01-16 13:36:24.468740: Epoch time: 110.49 s\n",
      "2025-01-16 13:36:25.034841: \n",
      "2025-01-16 13:36:25.035005: Epoch 449\n",
      "2025-01-16 13:36:25.035076: Current learning rate: 0.00585\n",
      "2025-01-16 13:38:15.550907: train_loss -0.784\n",
      "2025-01-16 13:38:15.551031: val_loss -0.7114\n",
      "2025-01-16 13:38:15.551066: Pseudo dice [np.float32(0.7096)]\n",
      "2025-01-16 13:38:15.551099: Epoch time: 110.52 s\n",
      "2025-01-16 13:38:16.335756: \n",
      "2025-01-16 13:38:16.336097: Epoch 450\n",
      "2025-01-16 13:38:16.336196: Current learning rate: 0.00584\n",
      "2025-01-16 13:40:07.018455: train_loss -0.7674\n",
      "2025-01-16 13:40:07.018585: val_loss -0.7211\n",
      "2025-01-16 13:40:07.018617: Pseudo dice [np.float32(0.6453)]\n",
      "2025-01-16 13:40:07.018648: Epoch time: 110.68 s\n",
      "2025-01-16 13:40:07.818684: \n",
      "2025-01-16 13:40:07.818845: Epoch 451\n",
      "2025-01-16 13:40:07.819068: Current learning rate: 0.00583\n",
      "2025-01-16 13:41:58.278720: train_loss -0.7855\n",
      "2025-01-16 13:41:58.279161: val_loss -0.6702\n",
      "2025-01-16 13:41:58.279232: Pseudo dice [np.float32(0.7235)]\n",
      "2025-01-16 13:41:58.279273: Epoch time: 110.46 s\n",
      "2025-01-16 13:41:58.847980: \n",
      "2025-01-16 13:41:58.848428: Epoch 452\n",
      "2025-01-16 13:41:58.848542: Current learning rate: 0.00582\n",
      "2025-01-16 13:43:49.347473: train_loss -0.7675\n",
      "2025-01-16 13:43:49.347726: val_loss -0.6237\n",
      "2025-01-16 13:43:49.347842: Pseudo dice [np.float32(0.6474)]\n",
      "2025-01-16 13:43:49.347895: Epoch time: 110.5 s\n",
      "2025-01-16 13:43:49.909721: \n",
      "2025-01-16 13:43:49.910038: Epoch 453\n",
      "2025-01-16 13:43:49.910168: Current learning rate: 0.00581\n",
      "2025-01-16 13:45:40.455261: train_loss -0.719\n",
      "2025-01-16 13:45:40.455686: val_loss -0.4959\n",
      "2025-01-16 13:45:40.455756: Pseudo dice [np.float32(0.6058)]\n",
      "2025-01-16 13:45:40.455795: Epoch time: 110.55 s\n",
      "2025-01-16 13:45:41.020659: \n",
      "2025-01-16 13:45:41.021066: Epoch 454\n",
      "2025-01-16 13:45:41.021158: Current learning rate: 0.0058\n",
      "2025-01-16 13:47:31.586962: train_loss -0.7285\n",
      "2025-01-16 13:47:31.587122: val_loss -0.6886\n",
      "2025-01-16 13:47:31.587195: Pseudo dice [np.float32(0.7196)]\n",
      "2025-01-16 13:47:31.587237: Epoch time: 110.57 s\n",
      "2025-01-16 13:47:32.161021: \n",
      "2025-01-16 13:47:32.161218: Epoch 455\n",
      "2025-01-16 13:47:32.161284: Current learning rate: 0.00579\n",
      "2025-01-16 13:49:22.800385: train_loss -0.7381\n",
      "2025-01-16 13:49:22.800746: val_loss -0.6172\n",
      "2025-01-16 13:49:22.800792: Pseudo dice [np.float32(0.7044)]\n",
      "2025-01-16 13:49:22.800828: Epoch time: 110.64 s\n",
      "2025-01-16 13:49:23.364474: \n",
      "2025-01-16 13:49:23.364839: Epoch 456\n",
      "2025-01-16 13:49:23.364912: Current learning rate: 0.00578\n",
      "2025-01-16 13:51:13.992026: train_loss -0.7917\n",
      "2025-01-16 13:51:13.992155: val_loss -0.6279\n",
      "2025-01-16 13:51:13.992189: Pseudo dice [np.float32(0.6567)]\n",
      "2025-01-16 13:51:13.992224: Epoch time: 110.63 s\n",
      "2025-01-16 13:51:14.560342: \n",
      "2025-01-16 13:51:14.560699: Epoch 457\n",
      "2025-01-16 13:51:14.560776: Current learning rate: 0.00577\n",
      "2025-01-16 13:53:05.083240: train_loss -0.7833\n",
      "2025-01-16 13:53:05.083365: val_loss -0.7187\n",
      "2025-01-16 13:53:05.083397: Pseudo dice [np.float32(0.6571)]\n",
      "2025-01-16 13:53:05.083430: Epoch time: 110.52 s\n",
      "2025-01-16 13:53:05.650172: \n",
      "2025-01-16 13:53:05.650330: Epoch 458\n",
      "2025-01-16 13:53:05.650402: Current learning rate: 0.00576\n",
      "2025-01-16 13:54:56.105469: train_loss -0.805\n",
      "2025-01-16 13:54:56.105918: val_loss -0.6369\n",
      "2025-01-16 13:54:56.105963: Pseudo dice [np.float32(0.6653)]\n",
      "2025-01-16 13:54:56.105997: Epoch time: 110.46 s\n",
      "2025-01-16 13:54:56.673163: \n",
      "2025-01-16 13:54:56.673316: Epoch 459\n",
      "2025-01-16 13:54:56.673388: Current learning rate: 0.00575\n",
      "2025-01-16 13:56:47.321460: train_loss -0.7679\n",
      "2025-01-16 13:56:47.321715: val_loss -0.6939\n",
      "2025-01-16 13:56:47.321758: Pseudo dice [np.float32(0.7227)]\n",
      "2025-01-16 13:56:47.321791: Epoch time: 110.65 s\n",
      "2025-01-16 13:56:47.887537: \n",
      "2025-01-16 13:56:47.887868: Epoch 460\n",
      "2025-01-16 13:56:47.887941: Current learning rate: 0.00574\n",
      "2025-01-16 13:58:38.421269: train_loss -0.7775\n",
      "2025-01-16 13:58:38.421411: val_loss -0.717\n",
      "2025-01-16 13:58:38.421605: Pseudo dice [np.float32(0.7124)]\n",
      "2025-01-16 13:58:38.421820: Epoch time: 110.53 s\n",
      "2025-01-16 13:58:38.975561: \n",
      "2025-01-16 13:58:38.975892: Epoch 461\n",
      "2025-01-16 13:58:38.975967: Current learning rate: 0.00573\n",
      "2025-01-16 14:00:29.473997: train_loss -0.7865\n",
      "2025-01-16 14:00:29.474119: val_loss -0.6393\n",
      "2025-01-16 14:00:29.474149: Pseudo dice [np.float32(0.6111)]\n",
      "2025-01-16 14:00:29.474357: Epoch time: 110.5 s\n",
      "2025-01-16 14:00:30.040719: \n",
      "2025-01-16 14:00:30.041013: Epoch 462\n",
      "2025-01-16 14:00:30.041094: Current learning rate: 0.00572\n",
      "2025-01-16 14:02:20.542749: train_loss -0.8085\n",
      "2025-01-16 14:02:20.542922: val_loss -0.7205\n",
      "2025-01-16 14:02:20.542967: Pseudo dice [np.float32(0.7193)]\n",
      "2025-01-16 14:02:20.543094: Epoch time: 110.5 s\n",
      "2025-01-16 14:02:21.119801: \n",
      "2025-01-16 14:02:21.120145: Epoch 463\n",
      "2025-01-16 14:02:21.120218: Current learning rate: 0.00571\n",
      "2025-01-16 14:04:11.790080: train_loss -0.7842\n",
      "2025-01-16 14:04:11.790190: val_loss -0.7164\n",
      "2025-01-16 14:04:11.790217: Pseudo dice [np.float32(0.6728)]\n",
      "2025-01-16 14:04:11.790250: Epoch time: 110.67 s\n",
      "2025-01-16 14:04:12.358073: \n",
      "2025-01-16 14:04:12.358218: Epoch 464\n",
      "2025-01-16 14:04:12.358290: Current learning rate: 0.0057\n",
      "2025-01-16 14:06:02.835032: train_loss -0.7975\n",
      "2025-01-16 14:06:02.835155: val_loss -0.7\n",
      "2025-01-16 14:06:02.835193: Pseudo dice [np.float32(0.7157)]\n",
      "2025-01-16 14:06:02.835227: Epoch time: 110.48 s\n",
      "2025-01-16 14:06:03.405634: \n",
      "2025-01-16 14:06:03.405821: Epoch 465\n",
      "2025-01-16 14:06:03.406100: Current learning rate: 0.0057\n",
      "2025-01-16 14:07:53.999899: train_loss -0.7447\n",
      "2025-01-16 14:07:54.000042: val_loss -0.5157\n",
      "2025-01-16 14:07:54.000080: Pseudo dice [np.float32(0.5408)]\n",
      "2025-01-16 14:07:54.000113: Epoch time: 110.59 s\n",
      "2025-01-16 14:07:54.565037: \n",
      "2025-01-16 14:07:54.565133: Epoch 466\n",
      "2025-01-16 14:07:54.565197: Current learning rate: 0.00569\n",
      "2025-01-16 14:09:45.065108: train_loss -0.7364\n",
      "2025-01-16 14:09:45.065228: val_loss -0.6961\n",
      "2025-01-16 14:09:45.065259: Pseudo dice [np.float32(0.6819)]\n",
      "2025-01-16 14:09:45.065294: Epoch time: 110.5 s\n",
      "2025-01-16 14:09:45.638145: \n",
      "2025-01-16 14:09:45.638237: Epoch 467\n",
      "2025-01-16 14:09:45.638298: Current learning rate: 0.00568\n",
      "2025-01-16 14:11:36.137914: train_loss -0.7957\n",
      "2025-01-16 14:11:36.138076: val_loss -0.7284\n",
      "2025-01-16 14:11:36.138375: Pseudo dice [np.float32(0.6847)]\n",
      "2025-01-16 14:11:36.138579: Epoch time: 110.5 s\n",
      "2025-01-16 14:11:36.708484: \n",
      "2025-01-16 14:11:36.708895: Epoch 468\n",
      "2025-01-16 14:11:36.708983: Current learning rate: 0.00567\n",
      "2025-01-16 14:13:27.171368: train_loss -0.7993\n",
      "2025-01-16 14:13:27.171607: val_loss -0.6941\n",
      "2025-01-16 14:13:27.171649: Pseudo dice [np.float32(0.744)]\n",
      "2025-01-16 14:13:27.171683: Epoch time: 110.46 s\n",
      "2025-01-16 14:13:27.732416: \n",
      "2025-01-16 14:13:27.732519: Epoch 469\n",
      "2025-01-16 14:13:27.732583: Current learning rate: 0.00566\n",
      "2025-01-16 14:15:18.257205: train_loss -0.8035\n",
      "2025-01-16 14:15:18.257332: val_loss -0.6351\n",
      "2025-01-16 14:15:18.257366: Pseudo dice [np.float32(0.6187)]\n",
      "2025-01-16 14:15:18.257400: Epoch time: 110.53 s\n",
      "2025-01-16 14:15:18.822343: \n",
      "2025-01-16 14:15:18.822429: Epoch 470\n",
      "2025-01-16 14:15:18.822489: Current learning rate: 0.00565\n",
      "2025-01-16 14:17:09.409640: train_loss -0.7934\n",
      "2025-01-16 14:17:09.409778: val_loss -0.6417\n",
      "2025-01-16 14:17:09.409814: Pseudo dice [np.float32(0.6231)]\n",
      "2025-01-16 14:17:09.409847: Epoch time: 110.59 s\n",
      "2025-01-16 14:17:09.976893: \n",
      "2025-01-16 14:17:09.976987: Epoch 471\n",
      "2025-01-16 14:17:09.977051: Current learning rate: 0.00564\n",
      "2025-01-16 14:19:00.497247: train_loss -0.7872\n",
      "2025-01-16 14:19:00.497453: val_loss -0.5962\n",
      "2025-01-16 14:19:00.497486: Pseudo dice [np.float32(0.6888)]\n",
      "2025-01-16 14:19:00.497519: Epoch time: 110.52 s\n",
      "2025-01-16 14:19:01.048969: \n",
      "2025-01-16 14:19:01.049133: Epoch 472\n",
      "2025-01-16 14:19:01.049205: Current learning rate: 0.00563\n",
      "2025-01-16 14:20:51.639309: train_loss -0.7956\n",
      "2025-01-16 14:20:51.639442: val_loss -0.6433\n",
      "2025-01-16 14:20:51.639475: Pseudo dice [np.float32(0.6775)]\n",
      "2025-01-16 14:20:51.639509: Epoch time: 110.59 s\n",
      "2025-01-16 14:20:52.212414: \n",
      "2025-01-16 14:20:52.212851: Epoch 473\n",
      "2025-01-16 14:20:52.213110: Current learning rate: 0.00562\n",
      "2025-01-16 14:22:42.722928: train_loss -0.8023\n",
      "2025-01-16 14:22:42.723059: val_loss -0.7175\n",
      "2025-01-16 14:22:42.723091: Pseudo dice [np.float32(0.7062)]\n",
      "2025-01-16 14:22:42.723124: Epoch time: 110.51 s\n",
      "2025-01-16 14:22:43.288135: \n",
      "2025-01-16 14:22:43.288474: Epoch 474\n",
      "2025-01-16 14:22:43.288549: Current learning rate: 0.00561\n",
      "2025-01-16 14:24:33.819531: train_loss -0.7844\n",
      "2025-01-16 14:24:33.819659: val_loss -0.7064\n",
      "2025-01-16 14:24:33.819691: Pseudo dice [np.float32(0.6766)]\n",
      "2025-01-16 14:24:33.819722: Epoch time: 110.53 s\n",
      "2025-01-16 14:24:34.384087: \n",
      "2025-01-16 14:24:34.384184: Epoch 475\n",
      "2025-01-16 14:24:34.384257: Current learning rate: 0.0056\n",
      "2025-01-16 14:26:24.850392: train_loss -0.7947\n",
      "2025-01-16 14:26:24.850587: val_loss -0.6491\n",
      "2025-01-16 14:26:24.850620: Pseudo dice [np.float32(0.6479)]\n",
      "2025-01-16 14:26:24.850651: Epoch time: 110.47 s\n",
      "2025-01-16 14:26:25.674330: \n",
      "2025-01-16 14:26:25.674429: Epoch 476\n",
      "2025-01-16 14:26:25.674496: Current learning rate: 0.00559\n",
      "2025-01-16 14:28:16.252124: train_loss -0.7634\n",
      "2025-01-16 14:28:16.252263: val_loss -0.626\n",
      "2025-01-16 14:28:16.252297: Pseudo dice [np.float32(0.6167)]\n",
      "2025-01-16 14:28:16.252352: Epoch time: 110.58 s\n",
      "2025-01-16 14:28:16.850449: \n",
      "2025-01-16 14:28:16.850628: Epoch 477\n",
      "2025-01-16 14:28:16.850703: Current learning rate: 0.00558\n",
      "2025-01-16 14:30:07.376694: train_loss -0.7774\n",
      "2025-01-16 14:30:07.376827: val_loss -0.6986\n",
      "2025-01-16 14:30:07.376863: Pseudo dice [np.float32(0.6919)]\n",
      "2025-01-16 14:30:07.376901: Epoch time: 110.53 s\n",
      "2025-01-16 14:30:07.953958: \n",
      "2025-01-16 14:30:07.954302: Epoch 478\n",
      "2025-01-16 14:30:07.954379: Current learning rate: 0.00557\n",
      "2025-01-16 14:31:58.585381: train_loss -0.7836\n",
      "2025-01-16 14:31:58.585530: val_loss -0.707\n",
      "2025-01-16 14:31:58.585671: Pseudo dice [np.float32(0.6859)]\n",
      "2025-01-16 14:31:58.585751: Epoch time: 110.63 s\n",
      "2025-01-16 14:31:59.157962: \n",
      "2025-01-16 14:31:59.158351: Epoch 479\n",
      "2025-01-16 14:31:59.158442: Current learning rate: 0.00556\n",
      "2025-01-16 14:33:49.666507: train_loss -0.7935\n",
      "2025-01-16 14:33:49.666663: val_loss -0.707\n",
      "2025-01-16 14:33:49.666698: Pseudo dice [np.float32(0.6793)]\n",
      "2025-01-16 14:33:49.666731: Epoch time: 110.51 s\n",
      "2025-01-16 14:33:50.244935: \n",
      "2025-01-16 14:33:50.245218: Epoch 480\n",
      "2025-01-16 14:33:50.245294: Current learning rate: 0.00555\n",
      "2025-01-16 14:35:40.771763: train_loss -0.8063\n",
      "2025-01-16 14:35:40.771938: val_loss -0.6747\n",
      "2025-01-16 14:35:40.771972: Pseudo dice [np.float32(0.6965)]\n",
      "2025-01-16 14:35:40.772005: Epoch time: 110.53 s\n",
      "2025-01-16 14:35:41.341561: \n",
      "2025-01-16 14:35:41.341999: Epoch 481\n",
      "2025-01-16 14:35:41.342089: Current learning rate: 0.00554\n",
      "2025-01-16 14:37:31.829816: train_loss -0.7971\n",
      "2025-01-16 14:37:31.830230: val_loss -0.6825\n",
      "2025-01-16 14:37:31.830335: Pseudo dice [np.float32(0.7054)]\n",
      "2025-01-16 14:37:31.830378: Epoch time: 110.49 s\n",
      "2025-01-16 14:37:32.404588: \n",
      "2025-01-16 14:37:32.404962: Epoch 482\n",
      "2025-01-16 14:37:32.405080: Current learning rate: 0.00553\n",
      "2025-01-16 14:39:22.949155: train_loss -0.802\n",
      "2025-01-16 14:39:22.949285: val_loss -0.638\n",
      "2025-01-16 14:39:22.949320: Pseudo dice [np.float32(0.7299)]\n",
      "2025-01-16 14:39:22.949354: Epoch time: 110.55 s\n",
      "2025-01-16 14:39:23.525012: \n",
      "2025-01-16 14:39:23.525456: Epoch 483\n",
      "2025-01-16 14:39:23.525550: Current learning rate: 0.00552\n",
      "2025-01-16 14:41:14.015585: train_loss -0.8173\n",
      "2025-01-16 14:41:14.015747: val_loss -0.5725\n",
      "2025-01-16 14:41:14.015802: Pseudo dice [np.float32(0.6239)]\n",
      "2025-01-16 14:41:14.015839: Epoch time: 110.49 s\n",
      "2025-01-16 14:41:14.591656: \n",
      "2025-01-16 14:41:14.592013: Epoch 484\n",
      "2025-01-16 14:41:14.592283: Current learning rate: 0.00551\n",
      "2025-01-16 14:43:05.092342: train_loss -0.7958\n",
      "2025-01-16 14:43:05.092524: val_loss -0.5358\n",
      "2025-01-16 14:43:05.092575: Pseudo dice [np.float32(0.5795)]\n",
      "2025-01-16 14:43:05.092613: Epoch time: 110.5 s\n",
      "2025-01-16 14:43:05.662560: \n",
      "2025-01-16 14:43:05.662943: Epoch 485\n",
      "2025-01-16 14:43:05.663036: Current learning rate: 0.0055\n",
      "2025-01-16 14:44:56.210980: train_loss -0.7978\n",
      "2025-01-16 14:44:56.211215: val_loss -0.653\n",
      "2025-01-16 14:44:56.211251: Pseudo dice [np.float32(0.5757)]\n",
      "2025-01-16 14:44:56.211283: Epoch time: 110.55 s\n",
      "2025-01-16 14:44:56.775616: \n",
      "2025-01-16 14:44:56.775834: Epoch 486\n",
      "2025-01-16 14:44:56.775965: Current learning rate: 0.00549\n",
      "2025-01-16 14:46:47.323557: train_loss -0.7993\n",
      "2025-01-16 14:46:47.323676: val_loss -0.7458\n",
      "2025-01-16 14:46:47.323711: Pseudo dice [np.float32(0.7716)]\n",
      "2025-01-16 14:46:47.323745: Epoch time: 110.55 s\n",
      "2025-01-16 14:46:47.902764: \n",
      "2025-01-16 14:46:47.902856: Epoch 487\n",
      "2025-01-16 14:46:47.902919: Current learning rate: 0.00548\n",
      "2025-01-16 14:48:38.415472: train_loss -0.8073\n",
      "2025-01-16 14:48:38.415983: val_loss -0.7367\n",
      "2025-01-16 14:48:38.416046: Pseudo dice [np.float32(0.7696)]\n",
      "2025-01-16 14:48:38.416086: Epoch time: 110.51 s\n",
      "2025-01-16 14:48:38.975689: \n",
      "2025-01-16 14:48:38.975840: Epoch 488\n",
      "2025-01-16 14:48:38.975908: Current learning rate: 0.00547\n",
      "2025-01-16 14:50:29.601043: train_loss -0.8043\n",
      "2025-01-16 14:50:29.601228: val_loss -0.6218\n",
      "2025-01-16 14:50:29.601495: Pseudo dice [np.float32(0.715)]\n",
      "2025-01-16 14:50:29.601600: Epoch time: 110.63 s\n",
      "2025-01-16 14:50:29.601654: Yayy! New best EMA pseudo Dice: 0.6825000047683716\n",
      "2025-01-16 14:50:30.647583: \n",
      "2025-01-16 14:50:30.647893: Epoch 489\n",
      "2025-01-16 14:50:30.647966: Current learning rate: 0.00546\n",
      "2025-01-16 14:52:21.274696: train_loss -0.783\n",
      "2025-01-16 14:52:21.274817: val_loss -0.6883\n",
      "2025-01-16 14:52:21.274851: Pseudo dice [np.float32(0.6979)]\n",
      "2025-01-16 14:52:21.274884: Epoch time: 110.63 s\n",
      "2025-01-16 14:52:21.274905: Yayy! New best EMA pseudo Dice: 0.6840999722480774\n",
      "2025-01-16 14:52:22.077008: \n",
      "2025-01-16 14:52:22.077111: Epoch 490\n",
      "2025-01-16 14:52:22.077174: Current learning rate: 0.00546\n",
      "2025-01-16 14:54:12.730072: train_loss -0.8151\n",
      "2025-01-16 14:54:12.730271: val_loss -0.599\n",
      "2025-01-16 14:54:12.730303: Pseudo dice [np.float32(0.6967)]\n",
      "2025-01-16 14:54:12.730334: Epoch time: 110.65 s\n",
      "2025-01-16 14:54:12.730355: Yayy! New best EMA pseudo Dice: 0.6852999925613403\n",
      "2025-01-16 14:54:13.538635: \n",
      "2025-01-16 14:54:13.538735: Epoch 491\n",
      "2025-01-16 14:54:13.538797: Current learning rate: 0.00545\n",
      "2025-01-16 14:56:04.018355: train_loss -0.8241\n",
      "2025-01-16 14:56:04.018642: val_loss -0.6706\n",
      "2025-01-16 14:56:04.018687: Pseudo dice [np.float32(0.7387)]\n",
      "2025-01-16 14:56:04.018723: Epoch time: 110.48 s\n",
      "2025-01-16 14:56:04.018748: Yayy! New best EMA pseudo Dice: 0.6906999945640564\n",
      "2025-01-16 14:56:04.826912: \n",
      "2025-01-16 14:56:04.827296: Epoch 492\n",
      "2025-01-16 14:56:04.827422: Current learning rate: 0.00544\n",
      "2025-01-16 14:57:55.471952: train_loss -0.795\n",
      "2025-01-16 14:57:55.472080: val_loss -0.6383\n",
      "2025-01-16 14:57:55.472112: Pseudo dice [np.float32(0.737)]\n",
      "2025-01-16 14:57:55.472145: Epoch time: 110.65 s\n",
      "2025-01-16 14:57:55.472166: Yayy! New best EMA pseudo Dice: 0.6952999830245972\n",
      "2025-01-16 14:57:56.266961: \n",
      "2025-01-16 14:57:56.267357: Epoch 493\n",
      "2025-01-16 14:57:56.267487: Current learning rate: 0.00543\n",
      "2025-01-16 14:59:46.706351: train_loss -0.8074\n",
      "2025-01-16 14:59:46.706480: val_loss -0.6522\n",
      "2025-01-16 14:59:46.706513: Pseudo dice [np.float32(0.7289)]\n",
      "2025-01-16 14:59:46.706547: Epoch time: 110.44 s\n",
      "2025-01-16 14:59:46.706567: Yayy! New best EMA pseudo Dice: 0.6985999941825867\n",
      "2025-01-16 14:59:47.514029: \n",
      "2025-01-16 14:59:47.514132: Epoch 494\n",
      "2025-01-16 14:59:47.514217: Current learning rate: 0.00542\n",
      "2025-01-16 15:01:38.029698: train_loss -0.816\n",
      "2025-01-16 15:01:38.029830: val_loss -0.686\n",
      "2025-01-16 15:01:38.029864: Pseudo dice [np.float32(0.7155)]\n",
      "2025-01-16 15:01:38.029897: Epoch time: 110.52 s\n",
      "2025-01-16 15:01:38.029917: Yayy! New best EMA pseudo Dice: 0.7002999782562256\n",
      "2025-01-16 15:01:38.835587: \n",
      "2025-01-16 15:01:38.835863: Epoch 495\n",
      "2025-01-16 15:01:38.835936: Current learning rate: 0.00541\n",
      "2025-01-16 15:03:29.335564: train_loss -0.816\n",
      "2025-01-16 15:03:29.335701: val_loss -0.652\n",
      "2025-01-16 15:03:29.335735: Pseudo dice [np.float32(0.6733)]\n",
      "2025-01-16 15:03:29.335768: Epoch time: 110.5 s\n",
      "2025-01-16 15:03:29.918938: \n",
      "2025-01-16 15:03:29.919028: Epoch 496\n",
      "2025-01-16 15:03:29.919088: Current learning rate: 0.0054\n",
      "2025-01-16 15:05:20.473441: train_loss -0.8247\n",
      "2025-01-16 15:05:20.473572: val_loss -0.6592\n",
      "2025-01-16 15:05:20.473605: Pseudo dice [np.float32(0.7068)]\n",
      "2025-01-16 15:05:20.473637: Epoch time: 110.56 s\n",
      "2025-01-16 15:05:21.039271: \n",
      "2025-01-16 15:05:21.039652: Epoch 497\n",
      "2025-01-16 15:05:21.039748: Current learning rate: 0.00539\n",
      "2025-01-16 15:07:11.558660: train_loss -0.8041\n",
      "2025-01-16 15:07:11.558793: val_loss -0.5417\n",
      "2025-01-16 15:07:11.558825: Pseudo dice [np.float32(0.3714)]\n",
      "2025-01-16 15:07:11.558857: Epoch time: 110.52 s\n",
      "2025-01-16 15:07:12.128296: \n",
      "2025-01-16 15:07:12.128637: Epoch 498\n",
      "2025-01-16 15:07:12.128774: Current learning rate: 0.00538\n",
      "2025-01-16 15:09:02.607018: train_loss -0.7533\n",
      "2025-01-16 15:09:02.607207: val_loss -0.6737\n",
      "2025-01-16 15:09:02.607253: Pseudo dice [np.float32(0.7183)]\n",
      "2025-01-16 15:09:02.607288: Epoch time: 110.48 s\n",
      "2025-01-16 15:09:03.183779: \n",
      "2025-01-16 15:09:03.184002: Epoch 499\n",
      "2025-01-16 15:09:03.184135: Current learning rate: 0.00537\n",
      "2025-01-16 15:10:53.722957: train_loss -0.76\n",
      "2025-01-16 15:10:53.723167: val_loss -0.602\n",
      "2025-01-16 15:10:53.723210: Pseudo dice [np.float32(0.6967)]\n",
      "2025-01-16 15:10:53.723250: Epoch time: 110.54 s\n",
      "2025-01-16 15:10:54.520145: \n",
      "2025-01-16 15:10:54.520579: Epoch 500\n",
      "2025-01-16 15:10:54.520729: Current learning rate: 0.00536\n",
      "2025-01-16 15:12:45.047286: train_loss -0.7592\n",
      "2025-01-16 15:12:45.047513: val_loss -0.7014\n",
      "2025-01-16 15:12:45.047549: Pseudo dice [np.float32(0.7437)]\n",
      "2025-01-16 15:12:45.047581: Epoch time: 110.53 s\n",
      "2025-01-16 15:12:45.873672: \n",
      "2025-01-16 15:12:45.874045: Epoch 501\n",
      "2025-01-16 15:12:45.874146: Current learning rate: 0.00535\n",
      "2025-01-16 15:14:36.466200: train_loss -0.7815\n",
      "2025-01-16 15:14:36.466320: val_loss -0.5835\n",
      "2025-01-16 15:14:36.466353: Pseudo dice [np.float32(0.7078)]\n",
      "2025-01-16 15:14:36.466387: Epoch time: 110.59 s\n",
      "2025-01-16 15:14:37.041995: \n",
      "2025-01-16 15:14:37.042094: Epoch 502\n",
      "2025-01-16 15:14:37.042157: Current learning rate: 0.00534\n",
      "2025-01-16 15:16:27.527880: train_loss -0.7824\n",
      "2025-01-16 15:16:27.528059: val_loss -0.4713\n",
      "2025-01-16 15:16:27.528093: Pseudo dice [np.float32(0.5336)]\n",
      "2025-01-16 15:16:27.528178: Epoch time: 110.49 s\n",
      "2025-01-16 15:16:28.103334: \n",
      "2025-01-16 15:16:28.103483: Epoch 503\n",
      "2025-01-16 15:16:28.103556: Current learning rate: 0.00533\n",
      "2025-01-16 15:18:18.726374: train_loss -0.7767\n",
      "2025-01-16 15:18:18.726501: val_loss -0.6398\n",
      "2025-01-16 15:18:18.726533: Pseudo dice [np.float32(0.6845)]\n",
      "2025-01-16 15:18:18.726566: Epoch time: 110.62 s\n",
      "2025-01-16 15:18:19.301566: \n",
      "2025-01-16 15:18:19.301657: Epoch 504\n",
      "2025-01-16 15:18:19.301720: Current learning rate: 0.00532\n",
      "2025-01-16 15:20:09.958110: train_loss -0.7761\n",
      "2025-01-16 15:20:09.958241: val_loss -0.7059\n",
      "2025-01-16 15:20:09.958273: Pseudo dice [np.float32(0.6973)]\n",
      "2025-01-16 15:20:09.958307: Epoch time: 110.66 s\n",
      "2025-01-16 15:20:10.545541: \n",
      "2025-01-16 15:20:10.545900: Epoch 505\n",
      "2025-01-16 15:20:10.545976: Current learning rate: 0.00531\n",
      "2025-01-16 15:22:01.235842: train_loss -0.8203\n",
      "2025-01-16 15:22:01.236235: val_loss -0.6451\n",
      "2025-01-16 15:22:01.236383: Pseudo dice [np.float32(0.6806)]\n",
      "2025-01-16 15:22:01.236513: Epoch time: 110.69 s\n",
      "2025-01-16 15:22:01.820443: \n",
      "2025-01-16 15:22:01.820746: Epoch 506\n",
      "2025-01-16 15:22:01.820876: Current learning rate: 0.0053\n",
      "2025-01-16 15:23:52.305342: train_loss -0.8057\n",
      "2025-01-16 15:23:52.305710: val_loss -0.6656\n",
      "2025-01-16 15:23:52.305877: Pseudo dice [np.float32(0.6698)]\n",
      "2025-01-16 15:23:52.305924: Epoch time: 110.49 s\n",
      "2025-01-16 15:23:52.881665: \n",
      "2025-01-16 15:23:52.881757: Epoch 507\n",
      "2025-01-16 15:23:52.881820: Current learning rate: 0.00529\n",
      "2025-01-16 15:25:43.398984: train_loss -0.7581\n",
      "2025-01-16 15:25:43.399112: val_loss -0.6518\n",
      "2025-01-16 15:25:43.399145: Pseudo dice [np.float32(0.665)]\n",
      "2025-01-16 15:25:43.399238: Epoch time: 110.52 s\n",
      "2025-01-16 15:25:43.982642: \n",
      "2025-01-16 15:25:43.982734: Epoch 508\n",
      "2025-01-16 15:25:43.982798: Current learning rate: 0.00528\n",
      "2025-01-16 15:27:34.518829: train_loss -0.7756\n",
      "2025-01-16 15:27:34.519133: val_loss -0.6165\n",
      "2025-01-16 15:27:34.519177: Pseudo dice [np.float32(0.6478)]\n",
      "2025-01-16 15:27:34.519212: Epoch time: 110.54 s\n",
      "2025-01-16 15:27:35.091535: \n",
      "2025-01-16 15:27:35.091813: Epoch 509\n",
      "2025-01-16 15:27:35.091956: Current learning rate: 0.00527\n",
      "2025-01-16 15:29:25.620368: train_loss -0.7614\n",
      "2025-01-16 15:29:25.620498: val_loss -0.713\n",
      "2025-01-16 15:29:25.620636: Pseudo dice [np.float32(0.646)]\n",
      "2025-01-16 15:29:25.620677: Epoch time: 110.53 s\n",
      "2025-01-16 15:29:26.194431: \n",
      "2025-01-16 15:29:26.194803: Epoch 510\n",
      "2025-01-16 15:29:26.194905: Current learning rate: 0.00526\n",
      "2025-01-16 15:31:16.817345: train_loss -0.7722\n",
      "2025-01-16 15:31:16.817482: val_loss -0.7434\n",
      "2025-01-16 15:31:16.817517: Pseudo dice [np.float32(0.775)]\n",
      "2025-01-16 15:31:16.817550: Epoch time: 110.62 s\n",
      "2025-01-16 15:31:17.387232: \n",
      "2025-01-16 15:31:17.387505: Epoch 511\n",
      "2025-01-16 15:31:17.387625: Current learning rate: 0.00525\n",
      "2025-01-16 15:33:07.995045: train_loss -0.7848\n",
      "2025-01-16 15:33:07.995185: val_loss -0.6798\n",
      "2025-01-16 15:33:07.995230: Pseudo dice [np.float32(0.7184)]\n",
      "2025-01-16 15:33:07.995268: Epoch time: 110.61 s\n",
      "2025-01-16 15:33:08.576658: \n",
      "2025-01-16 15:33:08.577046: Epoch 512\n",
      "2025-01-16 15:33:08.577141: Current learning rate: 0.00524\n",
      "2025-01-16 15:34:59.154266: train_loss -0.7767\n",
      "2025-01-16 15:34:59.154382: val_loss -0.7447\n",
      "2025-01-16 15:34:59.154412: Pseudo dice [np.float32(0.776)]\n",
      "2025-01-16 15:34:59.154444: Epoch time: 110.58 s\n",
      "2025-01-16 15:34:59.727200: \n",
      "2025-01-16 15:34:59.727279: Epoch 513\n",
      "2025-01-16 15:34:59.727339: Current learning rate: 0.00523\n",
      "2025-01-16 15:36:50.253600: train_loss -0.7971\n",
      "2025-01-16 15:36:50.253928: val_loss -0.715\n",
      "2025-01-16 15:36:50.254038: Pseudo dice [np.float32(0.6834)]\n",
      "2025-01-16 15:36:50.254079: Epoch time: 110.53 s\n",
      "2025-01-16 15:36:51.066195: \n",
      "2025-01-16 15:36:51.066301: Epoch 514\n",
      "2025-01-16 15:36:51.066371: Current learning rate: 0.00522\n",
      "2025-01-16 15:38:41.642275: train_loss -0.8101\n",
      "2025-01-16 15:38:41.642405: val_loss -0.6409\n",
      "2025-01-16 15:38:41.642440: Pseudo dice [np.float32(0.6383)]\n",
      "2025-01-16 15:38:41.642471: Epoch time: 110.58 s\n",
      "2025-01-16 15:38:42.229154: \n",
      "2025-01-16 15:38:42.229253: Epoch 515\n",
      "2025-01-16 15:38:42.229316: Current learning rate: 0.00521\n",
      "2025-01-16 15:40:32.855580: train_loss -0.805\n",
      "2025-01-16 15:40:32.855703: val_loss -0.6757\n",
      "2025-01-16 15:40:32.855736: Pseudo dice [np.float32(0.7071)]\n",
      "2025-01-16 15:40:32.855769: Epoch time: 110.63 s\n",
      "2025-01-16 15:40:33.435492: \n",
      "2025-01-16 15:40:33.435588: Epoch 516\n",
      "2025-01-16 15:40:33.435650: Current learning rate: 0.0052\n",
      "2025-01-16 15:42:23.910477: train_loss -0.798\n",
      "2025-01-16 15:42:23.910625: val_loss -0.7056\n",
      "2025-01-16 15:42:23.910695: Pseudo dice [np.float32(0.7176)]\n",
      "2025-01-16 15:42:23.910738: Epoch time: 110.48 s\n",
      "2025-01-16 15:42:24.488688: \n",
      "2025-01-16 15:42:24.488991: Epoch 517\n",
      "2025-01-16 15:42:24.489134: Current learning rate: 0.00519\n",
      "2025-01-16 15:44:15.152137: train_loss -0.7636\n",
      "2025-01-16 15:44:15.152267: val_loss -0.7175\n",
      "2025-01-16 15:44:15.152300: Pseudo dice [np.float32(0.7177)]\n",
      "2025-01-16 15:44:15.152333: Epoch time: 110.66 s\n",
      "2025-01-16 15:44:15.733153: \n",
      "2025-01-16 15:44:15.733493: Epoch 518\n",
      "2025-01-16 15:44:15.733703: Current learning rate: 0.00518\n",
      "2025-01-16 15:46:06.253925: train_loss -0.7911\n",
      "2025-01-16 15:46:06.254040: val_loss -0.6703\n",
      "2025-01-16 15:46:06.254081: Pseudo dice [np.float32(0.6295)]\n",
      "2025-01-16 15:46:06.254114: Epoch time: 110.52 s\n",
      "2025-01-16 15:46:06.836325: \n",
      "2025-01-16 15:46:06.836419: Epoch 519\n",
      "2025-01-16 15:46:06.836484: Current learning rate: 0.00518\n",
      "2025-01-16 15:47:57.348665: train_loss -0.7734\n",
      "2025-01-16 15:47:57.348790: val_loss -0.7014\n",
      "2025-01-16 15:47:57.348824: Pseudo dice [np.float32(0.7282)]\n",
      "2025-01-16 15:47:57.348858: Epoch time: 110.51 s\n",
      "2025-01-16 15:47:57.931586: \n",
      "2025-01-16 15:47:57.931673: Epoch 520\n",
      "2025-01-16 15:47:57.931737: Current learning rate: 0.00517\n",
      "2025-01-16 15:49:48.471383: train_loss -0.7777\n",
      "2025-01-16 15:49:48.471744: val_loss -0.6569\n",
      "2025-01-16 15:49:48.471907: Pseudo dice [np.float32(0.6715)]\n",
      "2025-01-16 15:49:48.471972: Epoch time: 110.54 s\n",
      "2025-01-16 15:49:49.047081: \n",
      "2025-01-16 15:49:49.047234: Epoch 521\n",
      "2025-01-16 15:49:49.047307: Current learning rate: 0.00516\n",
      "2025-01-16 15:51:39.658407: train_loss -0.8121\n",
      "2025-01-16 15:51:39.658893: val_loss -0.6598\n",
      "2025-01-16 15:51:39.658960: Pseudo dice [np.float32(0.5758)]\n",
      "2025-01-16 15:51:39.659000: Epoch time: 110.61 s\n",
      "2025-01-16 15:51:40.239056: \n",
      "2025-01-16 15:51:40.239149: Epoch 522\n",
      "2025-01-16 15:51:40.239210: Current learning rate: 0.00515\n",
      "2025-01-16 15:53:30.766704: train_loss -0.7878\n",
      "2025-01-16 15:53:30.766870: val_loss -0.5964\n",
      "2025-01-16 15:53:30.766977: Pseudo dice [np.float32(0.5809)]\n",
      "2025-01-16 15:53:30.767111: Epoch time: 110.53 s\n",
      "2025-01-16 15:53:31.351886: \n",
      "2025-01-16 15:53:31.352120: Epoch 523\n",
      "2025-01-16 15:53:31.352304: Current learning rate: 0.00514\n",
      "2025-01-16 15:55:21.875526: train_loss -0.814\n",
      "2025-01-16 15:55:21.875648: val_loss -0.6972\n",
      "2025-01-16 15:55:21.875681: Pseudo dice [np.float32(0.7121)]\n",
      "2025-01-16 15:55:21.875714: Epoch time: 110.52 s\n",
      "2025-01-16 15:55:22.446173: \n",
      "2025-01-16 15:55:22.446264: Epoch 524\n",
      "2025-01-16 15:55:22.446332: Current learning rate: 0.00513\n",
      "2025-01-16 15:57:13.020027: train_loss -0.8151\n",
      "2025-01-16 15:57:13.020442: val_loss -0.6759\n",
      "2025-01-16 15:57:13.020643: Pseudo dice [np.float32(0.63)]\n",
      "2025-01-16 15:57:13.020695: Epoch time: 110.57 s\n",
      "2025-01-16 15:57:13.604388: \n",
      "2025-01-16 15:57:13.604477: Epoch 525\n",
      "2025-01-16 15:57:13.604539: Current learning rate: 0.00512\n",
      "2025-01-16 15:59:04.283250: train_loss -0.8194\n",
      "2025-01-16 15:59:04.283454: val_loss -0.5785\n",
      "2025-01-16 15:59:04.283491: Pseudo dice [np.float32(0.5484)]\n",
      "2025-01-16 15:59:04.283524: Epoch time: 110.68 s\n",
      "2025-01-16 15:59:05.090412: \n",
      "2025-01-16 15:59:05.090809: Epoch 526\n",
      "2025-01-16 15:59:05.090888: Current learning rate: 0.00511\n",
      "2025-01-16 16:00:55.592703: train_loss -0.8136\n",
      "2025-01-16 16:00:55.592901: val_loss -0.6971\n",
      "2025-01-16 16:00:55.592943: Pseudo dice [np.float32(0.6852)]\n",
      "2025-01-16 16:00:55.593018: Epoch time: 110.5 s\n",
      "2025-01-16 16:00:56.167913: \n",
      "2025-01-16 16:00:56.168316: Epoch 527\n",
      "2025-01-16 16:00:56.168455: Current learning rate: 0.0051\n",
      "2025-01-16 16:02:46.685430: train_loss -0.7988\n",
      "2025-01-16 16:02:46.685551: val_loss -0.6475\n",
      "2025-01-16 16:02:46.685586: Pseudo dice [np.float32(0.6491)]\n",
      "2025-01-16 16:02:46.685618: Epoch time: 110.52 s\n",
      "2025-01-16 16:02:47.260921: \n",
      "2025-01-16 16:02:47.261339: Epoch 528\n",
      "2025-01-16 16:02:47.261437: Current learning rate: 0.00509\n",
      "2025-01-16 16:04:37.881835: train_loss -0.8158\n",
      "2025-01-16 16:04:37.881999: val_loss -0.7089\n",
      "2025-01-16 16:04:37.882048: Pseudo dice [np.float32(0.7558)]\n",
      "2025-01-16 16:04:37.882081: Epoch time: 110.62 s\n",
      "2025-01-16 16:04:38.462545: \n",
      "2025-01-16 16:04:38.462937: Epoch 529\n",
      "2025-01-16 16:04:38.463059: Current learning rate: 0.00508\n",
      "2025-01-16 16:06:28.935195: train_loss -0.7793\n",
      "2025-01-16 16:06:28.935392: val_loss -0.7116\n",
      "2025-01-16 16:06:28.935432: Pseudo dice [np.float32(0.6991)]\n",
      "2025-01-16 16:06:28.935464: Epoch time: 110.47 s\n",
      "2025-01-16 16:06:29.512508: \n",
      "2025-01-16 16:06:29.512872: Epoch 530\n",
      "2025-01-16 16:06:29.512947: Current learning rate: 0.00507\n",
      "2025-01-16 16:08:20.042406: train_loss -0.7748\n",
      "2025-01-16 16:08:20.042536: val_loss -0.6127\n",
      "2025-01-16 16:08:20.042569: Pseudo dice [np.float32(0.6006)]\n",
      "2025-01-16 16:08:20.042602: Epoch time: 110.53 s\n",
      "2025-01-16 16:08:20.633933: \n",
      "2025-01-16 16:08:20.634022: Epoch 531\n",
      "2025-01-16 16:08:20.634083: Current learning rate: 0.00506\n",
      "2025-01-16 16:10:11.324876: train_loss -0.7459\n",
      "2025-01-16 16:10:11.325077: val_loss -0.7134\n",
      "2025-01-16 16:10:11.325123: Pseudo dice [np.float32(0.745)]\n",
      "2025-01-16 16:10:11.325156: Epoch time: 110.69 s\n",
      "2025-01-16 16:10:11.909753: \n",
      "2025-01-16 16:10:11.909848: Epoch 532\n",
      "2025-01-16 16:10:11.909913: Current learning rate: 0.00505\n",
      "2025-01-16 16:12:02.572199: train_loss -0.7658\n",
      "2025-01-16 16:12:02.572387: val_loss -0.5961\n",
      "2025-01-16 16:12:02.572422: Pseudo dice [np.float32(0.6266)]\n",
      "2025-01-16 16:12:02.572514: Epoch time: 110.66 s\n",
      "2025-01-16 16:12:03.158254: \n",
      "2025-01-16 16:12:03.158345: Epoch 533\n",
      "2025-01-16 16:12:03.158406: Current learning rate: 0.00504\n",
      "2025-01-16 16:13:53.661531: train_loss -0.7832\n",
      "2025-01-16 16:13:53.661893: val_loss -0.5384\n",
      "2025-01-16 16:13:53.661939: Pseudo dice [np.float32(0.5369)]\n",
      "2025-01-16 16:13:53.661973: Epoch time: 110.5 s\n",
      "2025-01-16 16:13:54.243427: \n",
      "2025-01-16 16:13:54.243808: Epoch 534\n",
      "2025-01-16 16:13:54.243899: Current learning rate: 0.00503\n",
      "2025-01-16 16:15:44.761782: train_loss -0.7916\n",
      "2025-01-16 16:15:44.761932: val_loss -0.6761\n",
      "2025-01-16 16:15:44.761968: Pseudo dice [np.float32(0.6558)]\n",
      "2025-01-16 16:15:44.762001: Epoch time: 110.52 s\n",
      "2025-01-16 16:15:45.345760: \n",
      "2025-01-16 16:15:45.346096: Epoch 535\n",
      "2025-01-16 16:15:45.346210: Current learning rate: 0.00502\n",
      "2025-01-16 16:17:35.839637: train_loss -0.7755\n",
      "2025-01-16 16:17:35.839857: val_loss -0.6453\n",
      "2025-01-16 16:17:35.839899: Pseudo dice [np.float32(0.6812)]\n",
      "2025-01-16 16:17:35.839934: Epoch time: 110.49 s\n",
      "2025-01-16 16:17:36.422875: \n",
      "2025-01-16 16:17:36.422967: Epoch 536\n",
      "2025-01-16 16:17:36.423029: Current learning rate: 0.00501\n",
      "2025-01-16 16:19:26.913606: train_loss -0.7708\n",
      "2025-01-16 16:19:26.913749: val_loss -0.6795\n",
      "2025-01-16 16:19:26.913783: Pseudo dice [np.float32(0.6603)]\n",
      "2025-01-16 16:19:26.913817: Epoch time: 110.49 s\n",
      "2025-01-16 16:19:27.485488: \n",
      "2025-01-16 16:19:27.485575: Epoch 537\n",
      "2025-01-16 16:19:27.485757: Current learning rate: 0.005\n",
      "2025-01-16 16:21:18.037212: train_loss -0.8046\n",
      "2025-01-16 16:21:18.037423: val_loss -0.7084\n",
      "2025-01-16 16:21:18.037466: Pseudo dice [np.float32(0.7416)]\n",
      "2025-01-16 16:21:18.037501: Epoch time: 110.55 s\n",
      "2025-01-16 16:21:18.617413: \n",
      "2025-01-16 16:21:18.617711: Epoch 538\n",
      "2025-01-16 16:21:18.617799: Current learning rate: 0.00499\n",
      "2025-01-16 16:23:09.243752: train_loss -0.8201\n",
      "2025-01-16 16:23:09.243877: val_loss -0.6453\n",
      "2025-01-16 16:23:09.243911: Pseudo dice [np.float32(0.7666)]\n",
      "2025-01-16 16:23:09.243943: Epoch time: 110.63 s\n",
      "2025-01-16 16:23:10.109426: \n",
      "2025-01-16 16:23:10.109582: Epoch 539\n",
      "2025-01-16 16:23:10.109720: Current learning rate: 0.00498\n",
      "2025-01-16 16:25:00.684322: train_loss -0.7933\n",
      "2025-01-16 16:25:00.684457: val_loss -0.5467\n",
      "2025-01-16 16:25:00.684496: Pseudo dice [np.float32(0.6621)]\n",
      "2025-01-16 16:25:00.684529: Epoch time: 110.58 s\n",
      "2025-01-16 16:25:01.265722: \n",
      "2025-01-16 16:25:01.265827: Epoch 540\n",
      "2025-01-16 16:25:01.265889: Current learning rate: 0.00497\n",
      "2025-01-16 16:26:51.763790: train_loss -0.8227\n",
      "2025-01-16 16:26:51.763979: val_loss -0.5713\n",
      "2025-01-16 16:26:51.764056: Pseudo dice [np.float32(0.5931)]\n",
      "2025-01-16 16:26:51.764097: Epoch time: 110.5 s\n",
      "2025-01-16 16:26:52.338144: \n",
      "2025-01-16 16:26:52.338245: Epoch 541\n",
      "2025-01-16 16:26:52.338308: Current learning rate: 0.00496\n",
      "2025-01-16 16:28:42.804621: train_loss -0.7986\n",
      "2025-01-16 16:28:42.804756: val_loss -0.6738\n",
      "2025-01-16 16:28:42.804789: Pseudo dice [np.float32(0.7112)]\n",
      "2025-01-16 16:28:42.804825: Epoch time: 110.47 s\n",
      "2025-01-16 16:28:43.390619: \n",
      "2025-01-16 16:28:43.391018: Epoch 542\n",
      "2025-01-16 16:28:43.391108: Current learning rate: 0.00495\n",
      "2025-01-16 16:30:33.920582: train_loss -0.7728\n",
      "2025-01-16 16:30:33.921001: val_loss -0.4979\n",
      "2025-01-16 16:30:33.921075: Pseudo dice [np.float32(0.6381)]\n",
      "2025-01-16 16:30:33.921116: Epoch time: 110.53 s\n",
      "2025-01-16 16:30:34.498759: \n",
      "2025-01-16 16:30:34.499184: Epoch 543\n",
      "2025-01-16 16:30:34.499271: Current learning rate: 0.00494\n",
      "2025-01-16 16:32:25.160803: train_loss -0.7592\n",
      "2025-01-16 16:32:25.161247: val_loss -0.6743\n",
      "2025-01-16 16:32:25.161307: Pseudo dice [np.float32(0.6747)]\n",
      "2025-01-16 16:32:25.161348: Epoch time: 110.66 s\n",
      "2025-01-16 16:32:25.742364: \n",
      "2025-01-16 16:32:25.742778: Epoch 544\n",
      "2025-01-16 16:32:25.742876: Current learning rate: 0.00493\n",
      "2025-01-16 16:34:16.255954: train_loss -0.7907\n",
      "2025-01-16 16:34:16.256123: val_loss -0.7189\n",
      "2025-01-16 16:34:16.256260: Pseudo dice [np.float32(0.7749)]\n",
      "2025-01-16 16:34:16.256460: Epoch time: 110.51 s\n",
      "2025-01-16 16:34:16.839231: \n",
      "2025-01-16 16:34:16.839707: Epoch 545\n",
      "2025-01-16 16:34:16.839906: Current learning rate: 0.00492\n",
      "2025-01-16 16:36:07.372677: train_loss -0.7671\n",
      "2025-01-16 16:36:07.372803: val_loss -0.6351\n",
      "2025-01-16 16:36:07.372837: Pseudo dice [np.float32(0.6037)]\n",
      "2025-01-16 16:36:07.372872: Epoch time: 110.53 s\n",
      "2025-01-16 16:36:07.958129: \n",
      "2025-01-16 16:36:07.958506: Epoch 546\n",
      "2025-01-16 16:36:07.958595: Current learning rate: 0.00491\n",
      "2025-01-16 16:37:58.551018: train_loss -0.7854\n",
      "2025-01-16 16:37:58.551431: val_loss -0.5745\n",
      "2025-01-16 16:37:58.551550: Pseudo dice [np.float32(0.6298)]\n",
      "2025-01-16 16:37:58.551594: Epoch time: 110.59 s\n",
      "2025-01-16 16:37:59.174534: \n",
      "2025-01-16 16:37:59.175034: Epoch 547\n",
      "2025-01-16 16:37:59.175140: Current learning rate: 0.0049\n",
      "2025-01-16 16:39:49.845997: train_loss -0.7534\n",
      "2025-01-16 16:39:49.846151: val_loss -0.6754\n",
      "2025-01-16 16:39:49.846184: Pseudo dice [np.float32(0.6882)]\n",
      "2025-01-16 16:39:49.846217: Epoch time: 110.67 s\n",
      "2025-01-16 16:39:50.443123: \n",
      "2025-01-16 16:39:50.443507: Epoch 548\n",
      "2025-01-16 16:39:50.443636: Current learning rate: 0.00489\n",
      "2025-01-16 16:41:41.007250: train_loss -0.7981\n",
      "2025-01-16 16:41:41.007661: val_loss -0.6966\n",
      "2025-01-16 16:41:41.007705: Pseudo dice [np.float32(0.6996)]\n",
      "2025-01-16 16:41:41.007746: Epoch time: 110.56 s\n",
      "2025-01-16 16:41:41.607678: \n",
      "2025-01-16 16:41:41.607777: Epoch 549\n",
      "2025-01-16 16:41:41.607844: Current learning rate: 0.00488\n",
      "2025-01-16 16:43:32.191558: train_loss -0.7751\n",
      "2025-01-16 16:43:32.191682: val_loss -0.6909\n",
      "2025-01-16 16:43:32.191714: Pseudo dice [np.float32(0.6623)]\n",
      "2025-01-16 16:43:32.191746: Epoch time: 110.58 s\n",
      "2025-01-16 16:43:32.984799: \n",
      "2025-01-16 16:43:32.984972: Epoch 550\n",
      "2025-01-16 16:43:32.985049: Current learning rate: 0.00487\n",
      "2025-01-16 16:45:23.485442: train_loss -0.7862\n",
      "2025-01-16 16:45:23.485635: val_loss -0.7143\n",
      "2025-01-16 16:45:23.485679: Pseudo dice [np.float32(0.7158)]\n",
      "2025-01-16 16:45:23.485714: Epoch time: 110.5 s\n",
      "2025-01-16 16:45:24.055974: \n",
      "2025-01-16 16:45:24.056127: Epoch 551\n",
      "2025-01-16 16:45:24.056201: Current learning rate: 0.00486\n",
      "2025-01-16 16:47:14.719336: train_loss -0.8141\n",
      "2025-01-16 16:47:14.719465: val_loss -0.7508\n",
      "2025-01-16 16:47:14.719612: Pseudo dice [np.float32(0.7442)]\n",
      "2025-01-16 16:47:14.719751: Epoch time: 110.66 s\n",
      "2025-01-16 16:47:15.297529: \n",
      "2025-01-16 16:47:15.297639: Epoch 552\n",
      "2025-01-16 16:47:15.297702: Current learning rate: 0.00485\n",
      "2025-01-16 16:49:05.892821: train_loss -0.7896\n",
      "2025-01-16 16:49:05.893065: val_loss -0.7346\n",
      "2025-01-16 16:49:05.893151: Pseudo dice [np.float32(0.7371)]\n",
      "2025-01-16 16:49:05.893193: Epoch time: 110.6 s\n",
      "2025-01-16 16:49:06.468405: \n",
      "2025-01-16 16:49:06.468644: Epoch 553\n",
      "2025-01-16 16:49:06.468821: Current learning rate: 0.00484\n",
      "2025-01-16 16:50:58.371163: train_loss -0.7859\n",
      "2025-01-16 16:50:58.371378: val_loss -0.6507\n",
      "2025-01-16 16:50:58.371421: Pseudo dice [np.float32(0.6888)]\n",
      "2025-01-16 16:50:58.371454: Epoch time: 111.9 s\n",
      "2025-01-16 16:50:58.963505: \n",
      "2025-01-16 16:50:58.963934: Epoch 554\n",
      "2025-01-16 16:50:58.964087: Current learning rate: 0.00484\n",
      "2025-01-16 16:52:51.132967: train_loss -0.7965\n",
      "2025-01-16 16:52:51.133180: val_loss -0.6704\n",
      "2025-01-16 16:52:51.133228: Pseudo dice [np.float32(0.6719)]\n",
      "2025-01-16 16:52:51.133265: Epoch time: 112.17 s\n",
      "2025-01-16 16:52:51.724266: \n",
      "2025-01-16 16:52:51.724687: Epoch 555\n",
      "2025-01-16 16:52:51.724764: Current learning rate: 0.00483\n",
      "2025-01-16 16:54:42.748801: train_loss -0.821\n",
      "2025-01-16 16:54:42.748980: val_loss -0.7085\n",
      "2025-01-16 16:54:42.749013: Pseudo dice [np.float32(0.6859)]\n",
      "2025-01-16 16:54:42.749047: Epoch time: 111.03 s\n",
      "2025-01-16 16:54:43.337676: \n",
      "2025-01-16 16:54:43.337810: Epoch 556\n",
      "2025-01-16 16:54:43.337904: Current learning rate: 0.00482\n",
      "2025-01-16 16:56:34.451017: train_loss -0.822\n",
      "2025-01-16 16:56:34.451149: val_loss -0.6669\n",
      "2025-01-16 16:56:34.451183: Pseudo dice [np.float32(0.7078)]\n",
      "2025-01-16 16:56:34.451217: Epoch time: 111.11 s\n",
      "2025-01-16 16:56:35.031081: \n",
      "2025-01-16 16:56:35.031180: Epoch 557\n",
      "2025-01-16 16:56:35.031243: Current learning rate: 0.00481\n",
      "2025-01-16 16:58:26.570739: train_loss -0.828\n",
      "2025-01-16 16:58:26.571007: val_loss -0.6727\n",
      "2025-01-16 16:58:26.571069: Pseudo dice [np.float32(0.7083)]\n",
      "2025-01-16 16:58:26.571211: Epoch time: 111.54 s\n",
      "2025-01-16 16:58:27.163862: \n",
      "2025-01-16 16:58:27.164232: Epoch 558\n",
      "2025-01-16 16:58:27.164368: Current learning rate: 0.0048\n",
      "2025-01-16 17:00:18.490810: train_loss -0.8121\n",
      "2025-01-16 17:00:18.490958: val_loss -0.4946\n",
      "2025-01-16 17:00:18.490993: Pseudo dice [np.float32(0.6456)]\n",
      "2025-01-16 17:00:18.491027: Epoch time: 111.33 s\n",
      "2025-01-16 17:00:19.073588: \n",
      "2025-01-16 17:00:19.073768: Epoch 559\n",
      "2025-01-16 17:00:19.073844: Current learning rate: 0.00479\n",
      "2025-01-16 17:02:09.999242: train_loss -0.8049\n",
      "2025-01-16 17:02:09.999367: val_loss -0.459\n",
      "2025-01-16 17:02:09.999401: Pseudo dice [np.float32(0.67)]\n",
      "2025-01-16 17:02:09.999434: Epoch time: 110.93 s\n",
      "2025-01-16 17:02:10.585561: \n",
      "2025-01-16 17:02:10.585675: Epoch 560\n",
      "2025-01-16 17:02:10.585755: Current learning rate: 0.00478\n",
      "2025-01-16 17:04:01.506659: train_loss -0.7928\n",
      "2025-01-16 17:04:01.506785: val_loss -0.7003\n",
      "2025-01-16 17:04:01.506817: Pseudo dice [np.float32(0.7374)]\n",
      "2025-01-16 17:04:01.506849: Epoch time: 110.92 s\n",
      "2025-01-16 17:04:02.094169: \n",
      "2025-01-16 17:04:02.094511: Epoch 561\n",
      "2025-01-16 17:04:02.094611: Current learning rate: 0.00477\n",
      "2025-01-16 17:05:52.996821: train_loss -0.7594\n",
      "2025-01-16 17:05:52.996957: val_loss -0.7063\n",
      "2025-01-16 17:05:52.996991: Pseudo dice [np.float32(0.6574)]\n",
      "2025-01-16 17:05:52.997025: Epoch time: 110.9 s\n",
      "2025-01-16 17:05:53.576131: \n",
      "2025-01-16 17:05:53.576572: Epoch 562\n",
      "2025-01-16 17:05:53.576674: Current learning rate: 0.00476\n",
      "2025-01-16 17:07:44.477968: train_loss -0.7977\n",
      "2025-01-16 17:07:44.478080: val_loss -0.7119\n",
      "2025-01-16 17:07:44.478113: Pseudo dice [np.float32(0.7481)]\n",
      "2025-01-16 17:07:44.478145: Epoch time: 110.9 s\n",
      "2025-01-16 17:07:45.061088: \n",
      "2025-01-16 17:07:45.061452: Epoch 563\n",
      "2025-01-16 17:07:45.061568: Current learning rate: 0.00475\n",
      "2025-01-16 17:09:35.959886: train_loss -0.805\n",
      "2025-01-16 17:09:35.960021: val_loss -0.5812\n",
      "2025-01-16 17:09:35.960057: Pseudo dice [np.float32(0.6221)]\n",
      "2025-01-16 17:09:35.960090: Epoch time: 110.9 s\n",
      "2025-01-16 17:09:36.779601: \n",
      "2025-01-16 17:09:36.779705: Epoch 564\n",
      "2025-01-16 17:09:36.779774: Current learning rate: 0.00474\n",
      "2025-01-16 17:11:27.345793: train_loss -0.8009\n",
      "2025-01-16 17:11:27.345975: val_loss -0.6924\n",
      "2025-01-16 17:11:27.346035: Pseudo dice [np.float32(0.7456)]\n",
      "2025-01-16 17:11:27.346078: Epoch time: 110.57 s\n",
      "2025-01-16 17:11:27.929594: \n",
      "2025-01-16 17:11:27.929690: Epoch 565\n",
      "2025-01-16 17:11:27.929751: Current learning rate: 0.00473\n",
      "2025-01-16 17:13:18.385326: train_loss -0.7777\n",
      "2025-01-16 17:13:18.385653: val_loss -0.5258\n",
      "2025-01-16 17:13:18.385830: Pseudo dice [np.float32(0.6835)]\n",
      "2025-01-16 17:13:18.385903: Epoch time: 110.46 s\n",
      "2025-01-16 17:13:18.963433: \n",
      "2025-01-16 17:13:18.963609: Epoch 566\n",
      "2025-01-16 17:13:18.963682: Current learning rate: 0.00472\n",
      "2025-01-16 17:15:09.449265: train_loss -0.7986\n",
      "2025-01-16 17:15:09.449614: val_loss -0.5072\n",
      "2025-01-16 17:15:09.449675: Pseudo dice [np.float32(0.5242)]\n",
      "2025-01-16 17:15:09.449716: Epoch time: 110.49 s\n",
      "2025-01-16 17:15:10.035103: \n",
      "2025-01-16 17:15:10.035201: Epoch 567\n",
      "2025-01-16 17:15:10.035263: Current learning rate: 0.00471\n",
      "2025-01-16 17:17:00.709939: train_loss -0.8164\n",
      "2025-01-16 17:17:00.710166: val_loss -0.5612\n",
      "2025-01-16 17:17:00.710290: Pseudo dice [np.float32(0.6212)]\n",
      "2025-01-16 17:17:00.710392: Epoch time: 110.68 s\n",
      "2025-01-16 17:17:01.295300: \n",
      "2025-01-16 17:17:01.295427: Epoch 568\n",
      "2025-01-16 17:17:01.295495: Current learning rate: 0.0047\n",
      "2025-01-16 17:18:51.823626: train_loss -0.8227\n",
      "2025-01-16 17:18:51.823759: val_loss -0.6324\n",
      "2025-01-16 17:18:51.823794: Pseudo dice [np.float32(0.6411)]\n",
      "2025-01-16 17:18:51.823828: Epoch time: 110.53 s\n",
      "2025-01-16 17:18:52.407475: \n",
      "2025-01-16 17:18:52.407792: Epoch 569\n",
      "2025-01-16 17:18:52.407869: Current learning rate: 0.00469\n",
      "2025-01-16 17:20:43.050651: train_loss -0.8221\n",
      "2025-01-16 17:20:43.050784: val_loss -0.6815\n",
      "2025-01-16 17:20:43.050817: Pseudo dice [np.float32(0.7017)]\n",
      "2025-01-16 17:20:43.050850: Epoch time: 110.64 s\n",
      "2025-01-16 17:20:43.633839: \n",
      "2025-01-16 17:20:43.633932: Epoch 570\n",
      "2025-01-16 17:20:43.633995: Current learning rate: 0.00468\n",
      "2025-01-16 17:22:34.082541: train_loss -0.822\n",
      "2025-01-16 17:22:34.082953: val_loss -0.7234\n",
      "2025-01-16 17:22:34.083016: Pseudo dice [np.float32(0.6869)]\n",
      "2025-01-16 17:22:34.083055: Epoch time: 110.45 s\n",
      "2025-01-16 17:22:34.664940: \n",
      "2025-01-16 17:22:34.665323: Epoch 571\n",
      "2025-01-16 17:22:34.665462: Current learning rate: 0.00467\n",
      "2025-01-16 17:24:25.122093: train_loss -0.8349\n",
      "2025-01-16 17:24:25.122229: val_loss -0.6607\n",
      "2025-01-16 17:24:25.122265: Pseudo dice [np.float32(0.6446)]\n",
      "2025-01-16 17:24:25.122300: Epoch time: 110.46 s\n",
      "2025-01-16 17:24:25.701057: \n",
      "2025-01-16 17:24:25.701157: Epoch 572\n",
      "2025-01-16 17:24:25.701222: Current learning rate: 0.00466\n",
      "2025-01-16 17:26:16.196864: train_loss -0.7688\n",
      "2025-01-16 17:26:16.196992: val_loss -0.6094\n",
      "2025-01-16 17:26:16.197026: Pseudo dice [np.float32(0.6965)]\n",
      "2025-01-16 17:26:16.197060: Epoch time: 110.5 s\n",
      "2025-01-16 17:26:16.780693: \n",
      "2025-01-16 17:26:16.781020: Epoch 573\n",
      "2025-01-16 17:26:16.781172: Current learning rate: 0.00465\n",
      "2025-01-16 17:28:07.322559: train_loss -0.8237\n",
      "2025-01-16 17:28:07.322694: val_loss -0.6904\n",
      "2025-01-16 17:28:07.322726: Pseudo dice [np.float32(0.7356)]\n",
      "2025-01-16 17:28:07.322900: Epoch time: 110.54 s\n",
      "2025-01-16 17:28:07.913190: \n",
      "2025-01-16 17:28:07.913571: Epoch 574\n",
      "2025-01-16 17:28:07.913673: Current learning rate: 0.00464\n",
      "2025-01-16 17:29:58.569418: train_loss -0.8155\n",
      "2025-01-16 17:29:58.569654: val_loss -0.692\n",
      "2025-01-16 17:29:58.569690: Pseudo dice [np.float32(0.8009)]\n",
      "2025-01-16 17:29:58.569724: Epoch time: 110.66 s\n",
      "2025-01-16 17:29:59.158598: \n",
      "2025-01-16 17:29:59.158685: Epoch 575\n",
      "2025-01-16 17:29:59.158748: Current learning rate: 0.00463\n",
      "2025-01-16 17:31:49.683029: train_loss -0.8364\n",
      "2025-01-16 17:31:49.683160: val_loss -0.7081\n",
      "2025-01-16 17:31:49.683192: Pseudo dice [np.float32(0.7037)]\n",
      "2025-01-16 17:31:49.683223: Epoch time: 110.52 s\n",
      "2025-01-16 17:31:50.508963: \n",
      "2025-01-16 17:31:50.509320: Epoch 576\n",
      "2025-01-16 17:31:50.509485: Current learning rate: 0.00462\n",
      "2025-01-16 17:33:41.103476: train_loss -0.8094\n",
      "2025-01-16 17:33:41.103675: val_loss -0.6206\n",
      "2025-01-16 17:33:41.103723: Pseudo dice [np.float32(0.6415)]\n",
      "2025-01-16 17:33:41.103757: Epoch time: 110.6 s\n",
      "2025-01-16 17:33:41.693525: \n",
      "2025-01-16 17:33:41.693922: Epoch 577\n",
      "2025-01-16 17:33:41.694010: Current learning rate: 0.00461\n",
      "2025-01-16 17:35:32.146626: train_loss -0.8055\n",
      "2025-01-16 17:35:32.146873: val_loss -0.5773\n",
      "2025-01-16 17:35:32.146917: Pseudo dice [np.float32(0.5079)]\n",
      "2025-01-16 17:35:32.146951: Epoch time: 110.45 s\n",
      "2025-01-16 17:35:32.731260: \n",
      "2025-01-16 17:35:32.731420: Epoch 578\n",
      "2025-01-16 17:35:32.731494: Current learning rate: 0.0046\n",
      "2025-01-16 17:37:23.219733: train_loss -0.8115\n",
      "2025-01-16 17:37:23.220191: val_loss -0.7133\n",
      "2025-01-16 17:37:23.220300: Pseudo dice [np.float32(0.7137)]\n",
      "2025-01-16 17:37:23.220351: Epoch time: 110.49 s\n",
      "2025-01-16 17:37:23.808342: \n",
      "2025-01-16 17:37:23.808571: Epoch 579\n",
      "2025-01-16 17:37:23.808678: Current learning rate: 0.00459\n",
      "2025-01-16 17:39:14.483567: train_loss -0.7709\n",
      "2025-01-16 17:39:14.483703: val_loss -0.6721\n",
      "2025-01-16 17:39:14.483739: Pseudo dice [np.float32(0.6928)]\n",
      "2025-01-16 17:39:14.483772: Epoch time: 110.68 s\n",
      "2025-01-16 17:39:15.068132: \n",
      "2025-01-16 17:39:15.068479: Epoch 580\n",
      "2025-01-16 17:39:15.068554: Current learning rate: 0.00458\n",
      "2025-01-16 17:41:05.750355: train_loss -0.7873\n",
      "2025-01-16 17:41:05.750584: val_loss -0.5\n",
      "2025-01-16 17:41:05.750663: Pseudo dice [np.float32(0.5472)]\n",
      "2025-01-16 17:41:05.750705: Epoch time: 110.68 s\n",
      "2025-01-16 17:41:06.348946: \n",
      "2025-01-16 17:41:06.349294: Epoch 581\n",
      "2025-01-16 17:41:06.349371: Current learning rate: 0.00457\n",
      "2025-01-16 17:42:56.978413: train_loss -0.8208\n",
      "2025-01-16 17:42:56.978552: val_loss -0.7305\n",
      "2025-01-16 17:42:56.978585: Pseudo dice [np.float32(0.7893)]\n",
      "2025-01-16 17:42:56.978635: Epoch time: 110.63 s\n",
      "2025-01-16 17:42:57.568763: \n",
      "2025-01-16 17:42:57.569037: Epoch 582\n",
      "2025-01-16 17:42:57.569113: Current learning rate: 0.00456\n",
      "2025-01-16 17:44:48.252260: train_loss -0.7745\n",
      "2025-01-16 17:44:48.252382: val_loss -0.5796\n",
      "2025-01-16 17:44:48.252415: Pseudo dice [np.float32(0.5619)]\n",
      "2025-01-16 17:44:48.252523: Epoch time: 110.68 s\n",
      "2025-01-16 17:44:48.839036: \n",
      "2025-01-16 17:44:48.839385: Epoch 583\n",
      "2025-01-16 17:44:48.839508: Current learning rate: 0.00455\n",
      "2025-01-16 17:46:39.466144: train_loss -0.7919\n",
      "2025-01-16 17:46:39.466326: val_loss -0.6999\n",
      "2025-01-16 17:46:39.466368: Pseudo dice [np.float32(0.7256)]\n",
      "2025-01-16 17:46:39.466402: Epoch time: 110.63 s\n",
      "2025-01-16 17:46:40.063936: \n",
      "2025-01-16 17:46:40.064037: Epoch 584\n",
      "2025-01-16 17:46:40.064278: Current learning rate: 0.00454\n",
      "2025-01-16 17:48:30.704648: train_loss -0.8026\n",
      "2025-01-16 17:48:30.705031: val_loss -0.7245\n",
      "2025-01-16 17:48:30.705177: Pseudo dice [np.float32(0.7334)]\n",
      "2025-01-16 17:48:30.705259: Epoch time: 110.64 s\n",
      "2025-01-16 17:48:31.297537: \n",
      "2025-01-16 17:48:31.297771: Epoch 585\n",
      "2025-01-16 17:48:31.297902: Current learning rate: 0.00453\n",
      "2025-01-16 17:50:21.757461: train_loss -0.8223\n",
      "2025-01-16 17:50:21.757588: val_loss -0.5862\n",
      "2025-01-16 17:50:21.757619: Pseudo dice [np.float32(0.5382)]\n",
      "2025-01-16 17:50:21.757652: Epoch time: 110.46 s\n",
      "2025-01-16 17:50:22.349530: \n",
      "2025-01-16 17:50:22.349896: Epoch 586\n",
      "2025-01-16 17:50:22.349991: Current learning rate: 0.00452\n",
      "2025-01-16 17:52:13.051808: train_loss -0.8163\n",
      "2025-01-16 17:52:13.051996: val_loss -0.6994\n",
      "2025-01-16 17:52:13.052032: Pseudo dice [np.float32(0.7176)]\n",
      "2025-01-16 17:52:13.052066: Epoch time: 110.7 s\n",
      "2025-01-16 17:52:13.641843: \n",
      "2025-01-16 17:52:13.642068: Epoch 587\n",
      "2025-01-16 17:52:13.642269: Current learning rate: 0.00451\n",
      "2025-01-16 17:54:04.177697: train_loss -0.7967\n",
      "2025-01-16 17:54:04.177865: val_loss -0.6025\n",
      "2025-01-16 17:54:04.177906: Pseudo dice [np.float32(0.6197)]\n",
      "2025-01-16 17:54:04.177943: Epoch time: 110.54 s\n",
      "2025-01-16 17:54:04.763101: \n",
      "2025-01-16 17:54:04.763277: Epoch 588\n",
      "2025-01-16 17:54:04.763351: Current learning rate: 0.0045\n",
      "2025-01-16 17:55:55.286286: train_loss -0.8274\n",
      "2025-01-16 17:55:55.286589: val_loss -0.5424\n",
      "2025-01-16 17:55:55.286680: Pseudo dice [np.float32(0.5629)]\n",
      "2025-01-16 17:55:55.286744: Epoch time: 110.52 s\n",
      "2025-01-16 17:55:56.106125: \n",
      "2025-01-16 17:55:56.106472: Epoch 589\n",
      "2025-01-16 17:55:56.106568: Current learning rate: 0.00449\n",
      "2025-01-16 17:57:46.602223: train_loss -0.819\n",
      "2025-01-16 17:57:46.602358: val_loss -0.645\n",
      "2025-01-16 17:57:46.602392: Pseudo dice [np.float32(0.5872)]\n",
      "2025-01-16 17:57:46.602425: Epoch time: 110.5 s\n",
      "2025-01-16 17:57:47.187150: \n",
      "2025-01-16 17:57:47.187343: Epoch 590\n",
      "2025-01-16 17:57:47.187425: Current learning rate: 0.00448\n",
      "2025-01-16 17:59:37.712257: train_loss -0.7773\n",
      "2025-01-16 17:59:37.712388: val_loss -0.7313\n",
      "2025-01-16 17:59:37.712422: Pseudo dice [np.float32(0.7398)]\n",
      "2025-01-16 17:59:37.712454: Epoch time: 110.53 s\n",
      "2025-01-16 17:59:38.307427: \n",
      "2025-01-16 17:59:38.307735: Epoch 591\n",
      "2025-01-16 17:59:38.308000: Current learning rate: 0.00447\n",
      "2025-01-16 18:01:28.818471: train_loss -0.7476\n",
      "2025-01-16 18:01:28.818686: val_loss -0.6685\n",
      "2025-01-16 18:01:28.818760: Pseudo dice [np.float32(0.6846)]\n",
      "2025-01-16 18:01:28.818802: Epoch time: 110.51 s\n",
      "2025-01-16 18:01:29.402371: \n",
      "2025-01-16 18:01:29.402460: Epoch 592\n",
      "2025-01-16 18:01:29.402526: Current learning rate: 0.00446\n",
      "2025-01-16 18:03:20.016241: train_loss -0.8037\n",
      "2025-01-16 18:03:20.016364: val_loss -0.6089\n",
      "2025-01-16 18:03:20.016397: Pseudo dice [np.float32(0.5436)]\n",
      "2025-01-16 18:03:20.016431: Epoch time: 110.61 s\n",
      "2025-01-16 18:03:20.606008: \n",
      "2025-01-16 18:03:20.606310: Epoch 593\n",
      "2025-01-16 18:03:20.606411: Current learning rate: 0.00445\n",
      "2025-01-16 18:05:11.132724: train_loss -0.7943\n",
      "2025-01-16 18:05:11.132848: val_loss -0.6512\n",
      "2025-01-16 18:05:11.132878: Pseudo dice [np.float32(0.6381)]\n",
      "2025-01-16 18:05:11.132910: Epoch time: 110.53 s\n",
      "2025-01-16 18:05:11.723150: \n",
      "2025-01-16 18:05:11.723514: Epoch 594\n",
      "2025-01-16 18:05:11.723609: Current learning rate: 0.00444\n",
      "2025-01-16 18:07:02.235345: train_loss -0.8152\n",
      "2025-01-16 18:07:02.235464: val_loss -0.6462\n",
      "2025-01-16 18:07:02.235495: Pseudo dice [np.float32(0.5962)]\n",
      "2025-01-16 18:07:02.235528: Epoch time: 110.51 s\n",
      "2025-01-16 18:07:02.827120: \n",
      "2025-01-16 18:07:02.827495: Epoch 595\n",
      "2025-01-16 18:07:02.827590: Current learning rate: 0.00443\n",
      "2025-01-16 18:08:53.379801: train_loss -0.8197\n",
      "2025-01-16 18:08:53.379927: val_loss -0.6218\n",
      "2025-01-16 18:08:53.379959: Pseudo dice [np.float32(0.6966)]\n",
      "2025-01-16 18:08:53.379990: Epoch time: 110.55 s\n",
      "2025-01-16 18:08:53.961569: \n",
      "2025-01-16 18:08:53.961969: Epoch 596\n",
      "2025-01-16 18:08:53.962092: Current learning rate: 0.00442\n",
      "2025-01-16 18:10:44.579399: train_loss -0.816\n",
      "2025-01-16 18:10:44.579534: val_loss -0.5761\n",
      "2025-01-16 18:10:44.579568: Pseudo dice [np.float32(0.6604)]\n",
      "2025-01-16 18:10:44.579602: Epoch time: 110.62 s\n",
      "2025-01-16 18:10:45.165655: \n",
      "2025-01-16 18:10:45.165797: Epoch 597\n",
      "2025-01-16 18:10:45.165861: Current learning rate: 0.00441\n",
      "2025-01-16 18:12:35.802521: train_loss -0.8228\n",
      "2025-01-16 18:12:35.802656: val_loss -0.5049\n",
      "2025-01-16 18:12:35.802690: Pseudo dice [np.float32(0.5453)]\n",
      "2025-01-16 18:12:35.802731: Epoch time: 110.64 s\n",
      "2025-01-16 18:12:36.387495: \n",
      "2025-01-16 18:12:36.387854: Epoch 598\n",
      "2025-01-16 18:12:36.387980: Current learning rate: 0.0044\n",
      "2025-01-16 18:14:26.921282: train_loss -0.7955\n",
      "2025-01-16 18:14:26.921448: val_loss -0.6638\n",
      "2025-01-16 18:14:26.921520: Pseudo dice [np.float32(0.595)]\n",
      "2025-01-16 18:14:26.921563: Epoch time: 110.53 s\n",
      "2025-01-16 18:14:27.502931: \n",
      "2025-01-16 18:14:27.503266: Epoch 599\n",
      "2025-01-16 18:14:27.503408: Current learning rate: 0.00439\n",
      "2025-01-16 18:16:18.029709: train_loss -0.8138\n",
      "2025-01-16 18:16:18.029834: val_loss -0.6982\n",
      "2025-01-16 18:16:18.029866: Pseudo dice [np.float32(0.6934)]\n",
      "2025-01-16 18:16:18.029903: Epoch time: 110.53 s\n",
      "2025-01-16 18:16:18.841818: \n",
      "2025-01-16 18:16:18.841962: Epoch 600\n",
      "2025-01-16 18:16:18.842036: Current learning rate: 0.00438\n",
      "2025-01-16 18:18:09.354121: train_loss -0.8072\n",
      "2025-01-16 18:18:09.354479: val_loss -0.6957\n",
      "2025-01-16 18:18:09.354590: Pseudo dice [np.float32(0.6952)]\n",
      "2025-01-16 18:18:09.354635: Epoch time: 110.51 s\n",
      "2025-01-16 18:18:10.172540: \n",
      "2025-01-16 18:18:10.172657: Epoch 601\n",
      "2025-01-16 18:18:10.172717: Current learning rate: 0.00437\n",
      "2025-01-16 18:20:00.665469: train_loss -0.7859\n",
      "2025-01-16 18:20:00.665593: val_loss -0.6556\n",
      "2025-01-16 18:20:00.665627: Pseudo dice [np.float32(0.616)]\n",
      "2025-01-16 18:20:00.665661: Epoch time: 110.49 s\n",
      "2025-01-16 18:20:01.246366: \n",
      "2025-01-16 18:20:01.246517: Epoch 602\n",
      "2025-01-16 18:20:01.246588: Current learning rate: 0.00436\n",
      "2025-01-16 18:21:51.803499: train_loss -0.792\n",
      "2025-01-16 18:21:51.803638: val_loss -0.691\n",
      "2025-01-16 18:21:51.803677: Pseudo dice [np.float32(0.6885)]\n",
      "2025-01-16 18:21:51.803715: Epoch time: 110.56 s\n",
      "2025-01-16 18:21:52.392392: \n",
      "2025-01-16 18:21:52.392594: Epoch 603\n",
      "2025-01-16 18:21:52.392693: Current learning rate: 0.00435\n",
      "2025-01-16 18:23:42.900033: train_loss -0.817\n",
      "2025-01-16 18:23:42.900151: val_loss -0.659\n",
      "2025-01-16 18:23:42.900180: Pseudo dice [np.float32(0.7124)]\n",
      "2025-01-16 18:23:42.900211: Epoch time: 110.51 s\n",
      "2025-01-16 18:23:43.489724: \n",
      "2025-01-16 18:23:43.490112: Epoch 604\n",
      "2025-01-16 18:23:43.490193: Current learning rate: 0.00434\n",
      "2025-01-16 18:25:34.020292: train_loss -0.7849\n",
      "2025-01-16 18:25:34.020436: val_loss -0.6722\n",
      "2025-01-16 18:25:34.020469: Pseudo dice [np.float32(0.6894)]\n",
      "2025-01-16 18:25:34.020503: Epoch time: 110.53 s\n",
      "2025-01-16 18:25:34.613734: \n",
      "2025-01-16 18:25:34.614036: Epoch 605\n",
      "2025-01-16 18:25:34.614124: Current learning rate: 0.00433\n",
      "2025-01-16 18:27:25.261380: train_loss -0.8013\n",
      "2025-01-16 18:27:25.261698: val_loss -0.6932\n",
      "2025-01-16 18:27:25.261744: Pseudo dice [np.float32(0.726)]\n",
      "2025-01-16 18:27:25.261778: Epoch time: 110.65 s\n",
      "2025-01-16 18:27:25.849065: \n",
      "2025-01-16 18:27:25.849474: Epoch 606\n",
      "2025-01-16 18:27:25.849570: Current learning rate: 0.00432\n",
      "2025-01-16 18:29:16.445763: train_loss -0.8316\n",
      "2025-01-16 18:29:16.446154: val_loss -0.6621\n",
      "2025-01-16 18:29:16.446216: Pseudo dice [np.float32(0.6962)]\n",
      "2025-01-16 18:29:16.446256: Epoch time: 110.6 s\n",
      "2025-01-16 18:29:17.035899: \n",
      "2025-01-16 18:29:17.036105: Epoch 607\n",
      "2025-01-16 18:29:17.036177: Current learning rate: 0.00431\n",
      "2025-01-16 18:31:07.522510: train_loss -0.8134\n",
      "2025-01-16 18:31:07.522628: val_loss -0.5489\n",
      "2025-01-16 18:31:07.522660: Pseudo dice [np.float32(0.532)]\n",
      "2025-01-16 18:31:07.522756: Epoch time: 110.49 s\n",
      "2025-01-16 18:31:08.109820: \n",
      "2025-01-16 18:31:08.110128: Epoch 608\n",
      "2025-01-16 18:31:08.110320: Current learning rate: 0.0043\n",
      "2025-01-16 18:32:58.640868: train_loss -0.773\n",
      "2025-01-16 18:32:58.641111: val_loss -0.6234\n",
      "2025-01-16 18:32:58.641155: Pseudo dice [np.float32(0.6265)]\n",
      "2025-01-16 18:32:58.641190: Epoch time: 110.53 s\n",
      "2025-01-16 18:32:59.228435: \n",
      "2025-01-16 18:32:59.228528: Epoch 609\n",
      "2025-01-16 18:32:59.228590: Current learning rate: 0.00429\n",
      "2025-01-16 18:34:49.765887: train_loss -0.8293\n",
      "2025-01-16 18:34:49.766078: val_loss -0.6478\n",
      "2025-01-16 18:34:49.766114: Pseudo dice [np.float32(0.6726)]\n",
      "2025-01-16 18:34:49.766148: Epoch time: 110.54 s\n",
      "2025-01-16 18:34:50.356891: \n",
      "2025-01-16 18:34:50.356998: Epoch 610\n",
      "2025-01-16 18:34:50.357059: Current learning rate: 0.00429\n",
      "2025-01-16 18:36:40.874348: train_loss -0.7863\n",
      "2025-01-16 18:36:40.874482: val_loss -0.672\n",
      "2025-01-16 18:36:40.874516: Pseudo dice [np.float32(0.686)]\n",
      "2025-01-16 18:36:40.874550: Epoch time: 110.52 s\n",
      "2025-01-16 18:36:41.465324: \n",
      "2025-01-16 18:36:41.465691: Epoch 611\n",
      "2025-01-16 18:36:41.465765: Current learning rate: 0.00428\n",
      "2025-01-16 18:38:31.989814: train_loss -0.8092\n",
      "2025-01-16 18:38:31.990019: val_loss -0.5613\n",
      "2025-01-16 18:38:31.990089: Pseudo dice [np.float32(0.5736)]\n",
      "2025-01-16 18:38:31.990130: Epoch time: 110.53 s\n",
      "2025-01-16 18:38:32.572470: \n",
      "2025-01-16 18:38:32.572634: Epoch 612\n",
      "2025-01-16 18:38:32.572737: Current learning rate: 0.00427\n",
      "2025-01-16 18:40:23.085749: train_loss -0.8193\n",
      "2025-01-16 18:40:23.085883: val_loss -0.6199\n",
      "2025-01-16 18:40:23.085917: Pseudo dice [np.float32(0.6577)]\n",
      "2025-01-16 18:40:23.085950: Epoch time: 110.51 s\n",
      "2025-01-16 18:40:23.933190: \n",
      "2025-01-16 18:40:23.933294: Epoch 613\n",
      "2025-01-16 18:40:23.933368: Current learning rate: 0.00426\n",
      "2025-01-16 18:42:14.632167: train_loss -0.8302\n",
      "2025-01-16 18:42:14.632297: val_loss -0.7135\n",
      "2025-01-16 18:42:14.632334: Pseudo dice [np.float32(0.7787)]\n",
      "2025-01-16 18:42:14.632368: Epoch time: 110.7 s\n",
      "2025-01-16 18:42:15.230470: \n",
      "2025-01-16 18:42:15.230726: Epoch 614\n",
      "2025-01-16 18:42:15.230874: Current learning rate: 0.00425\n",
      "2025-01-16 18:44:05.857240: train_loss -0.8005\n",
      "2025-01-16 18:44:05.857390: val_loss -0.6837\n",
      "2025-01-16 18:44:05.857430: Pseudo dice [np.float32(0.6668)]\n",
      "2025-01-16 18:44:05.857466: Epoch time: 110.63 s\n",
      "2025-01-16 18:44:06.449461: \n",
      "2025-01-16 18:44:06.449555: Epoch 615\n",
      "2025-01-16 18:44:06.449621: Current learning rate: 0.00424\n",
      "2025-01-16 18:45:57.002886: train_loss -0.7708\n",
      "2025-01-16 18:45:57.003021: val_loss -0.6512\n",
      "2025-01-16 18:45:57.003061: Pseudo dice [np.float32(0.6043)]\n",
      "2025-01-16 18:45:57.003093: Epoch time: 110.55 s\n",
      "2025-01-16 18:45:57.599076: \n",
      "2025-01-16 18:45:57.599450: Epoch 616\n",
      "2025-01-16 18:45:57.599533: Current learning rate: 0.00423\n",
      "2025-01-16 18:47:48.233633: train_loss -0.8055\n",
      "2025-01-16 18:47:48.233761: val_loss -0.7246\n",
      "2025-01-16 18:47:48.233794: Pseudo dice [np.float32(0.7713)]\n",
      "2025-01-16 18:47:48.233829: Epoch time: 110.64 s\n",
      "2025-01-16 18:47:48.830753: \n",
      "2025-01-16 18:47:48.830973: Epoch 617\n",
      "2025-01-16 18:47:48.831114: Current learning rate: 0.00422\n",
      "2025-01-16 18:49:39.323277: train_loss -0.8154\n",
      "2025-01-16 18:49:39.323407: val_loss -0.6833\n",
      "2025-01-16 18:49:39.323437: Pseudo dice [np.float32(0.6977)]\n",
      "2025-01-16 18:49:39.323470: Epoch time: 110.49 s\n",
      "2025-01-16 18:49:39.914766: \n",
      "2025-01-16 18:49:39.915112: Epoch 618\n",
      "2025-01-16 18:49:39.915333: Current learning rate: 0.00421\n",
      "2025-01-16 18:51:30.442051: train_loss -0.8153\n",
      "2025-01-16 18:51:30.442174: val_loss -0.6195\n",
      "2025-01-16 18:51:30.442566: Pseudo dice [np.float32(0.6321)]\n",
      "2025-01-16 18:51:30.442717: Epoch time: 110.53 s\n",
      "2025-01-16 18:51:31.039032: \n",
      "2025-01-16 18:51:31.039193: Epoch 619\n",
      "2025-01-16 18:51:31.039268: Current learning rate: 0.0042\n",
      "2025-01-16 18:53:21.675876: train_loss -0.7834\n",
      "2025-01-16 18:53:21.676000: val_loss -0.5571\n",
      "2025-01-16 18:53:21.676035: Pseudo dice [np.float32(0.55)]\n",
      "2025-01-16 18:53:21.676068: Epoch time: 110.64 s\n",
      "2025-01-16 18:53:22.267608: \n",
      "2025-01-16 18:53:22.267795: Epoch 620\n",
      "2025-01-16 18:53:22.267868: Current learning rate: 0.00419\n",
      "2025-01-16 18:55:12.736829: train_loss -0.773\n",
      "2025-01-16 18:55:12.736951: val_loss -0.6433\n",
      "2025-01-16 18:55:12.736985: Pseudo dice [np.float32(0.6871)]\n",
      "2025-01-16 18:55:12.737017: Epoch time: 110.47 s\n",
      "2025-01-16 18:55:13.335470: \n",
      "2025-01-16 18:55:13.335810: Epoch 621\n",
      "2025-01-16 18:55:13.336116: Current learning rate: 0.00418\n",
      "2025-01-16 18:57:03.976476: train_loss -0.815\n",
      "2025-01-16 18:57:03.976610: val_loss -0.7026\n",
      "2025-01-16 18:57:03.976643: Pseudo dice [np.float32(0.7257)]\n",
      "2025-01-16 18:57:03.976837: Epoch time: 110.64 s\n",
      "2025-01-16 18:57:04.562649: \n",
      "2025-01-16 18:57:04.562856: Epoch 622\n",
      "2025-01-16 18:57:04.562987: Current learning rate: 0.00417\n",
      "2025-01-16 18:58:55.090617: train_loss -0.8014\n",
      "2025-01-16 18:58:55.090737: val_loss -0.6379\n",
      "2025-01-16 18:58:55.090770: Pseudo dice [np.float32(0.6473)]\n",
      "2025-01-16 18:58:55.090803: Epoch time: 110.53 s\n",
      "2025-01-16 18:58:55.676967: \n",
      "2025-01-16 18:58:55.677131: Epoch 623\n",
      "2025-01-16 18:58:55.677233: Current learning rate: 0.00416\n",
      "2025-01-16 19:00:46.157155: train_loss -0.791\n",
      "2025-01-16 19:00:46.157356: val_loss -0.7613\n",
      "2025-01-16 19:00:46.157401: Pseudo dice [np.float32(0.7817)]\n",
      "2025-01-16 19:00:46.157436: Epoch time: 110.48 s\n",
      "2025-01-16 19:00:46.738999: \n",
      "2025-01-16 19:00:46.739393: Epoch 624\n",
      "2025-01-16 19:00:46.739498: Current learning rate: 0.00415\n",
      "2025-01-16 19:02:37.240993: train_loss -0.795\n",
      "2025-01-16 19:02:37.241179: val_loss -0.6912\n",
      "2025-01-16 19:02:37.241212: Pseudo dice [np.float32(0.6873)]\n",
      "2025-01-16 19:02:37.241246: Epoch time: 110.5 s\n",
      "2025-01-16 19:02:38.084635: \n",
      "2025-01-16 19:02:38.085082: Epoch 625\n",
      "2025-01-16 19:02:38.085183: Current learning rate: 0.00414\n",
      "2025-01-16 19:04:28.591847: train_loss -0.8095\n",
      "2025-01-16 19:04:28.591985: val_loss -0.7091\n",
      "2025-01-16 19:04:28.592021: Pseudo dice [np.float32(0.7369)]\n",
      "2025-01-16 19:04:28.592053: Epoch time: 110.51 s\n",
      "2025-01-16 19:04:29.182609: \n",
      "2025-01-16 19:04:29.182708: Epoch 626\n",
      "2025-01-16 19:04:29.182769: Current learning rate: 0.00413\n",
      "2025-01-16 19:06:19.760495: train_loss -0.8009\n",
      "2025-01-16 19:06:19.760712: val_loss -0.6973\n",
      "2025-01-16 19:06:19.760761: Pseudo dice [np.float32(0.6566)]\n",
      "2025-01-16 19:06:19.760796: Epoch time: 110.58 s\n",
      "2025-01-16 19:06:20.359293: \n",
      "2025-01-16 19:06:20.359389: Epoch 627\n",
      "2025-01-16 19:06:20.359450: Current learning rate: 0.00412\n",
      "2025-01-16 19:08:10.854690: train_loss -0.8065\n",
      "2025-01-16 19:08:10.854912: val_loss -0.6378\n",
      "2025-01-16 19:08:10.854960: Pseudo dice [np.float32(0.6682)]\n",
      "2025-01-16 19:08:10.854995: Epoch time: 110.5 s\n",
      "2025-01-16 19:08:11.457974: \n",
      "2025-01-16 19:08:11.458076: Epoch 628\n",
      "2025-01-16 19:08:11.458140: Current learning rate: 0.00411\n",
      "2025-01-16 19:10:01.918382: train_loss -0.8077\n",
      "2025-01-16 19:10:01.918504: val_loss -0.531\n",
      "2025-01-16 19:10:01.918536: Pseudo dice [np.float32(0.5653)]\n",
      "2025-01-16 19:10:01.918569: Epoch time: 110.46 s\n",
      "2025-01-16 19:10:02.508286: \n",
      "2025-01-16 19:10:02.508501: Epoch 629\n",
      "2025-01-16 19:10:02.508688: Current learning rate: 0.0041\n",
      "2025-01-16 19:11:53.052428: train_loss -0.821\n",
      "2025-01-16 19:11:53.052619: val_loss -0.6668\n",
      "2025-01-16 19:11:53.052651: Pseudo dice [np.float32(0.664)]\n",
      "2025-01-16 19:11:53.052685: Epoch time: 110.54 s\n",
      "2025-01-16 19:11:53.640486: \n",
      "2025-01-16 19:11:53.640587: Epoch 630\n",
      "2025-01-16 19:11:53.640647: Current learning rate: 0.00409\n",
      "2025-01-16 19:13:44.160927: train_loss -0.8027\n",
      "2025-01-16 19:13:44.161114: val_loss -0.5581\n",
      "2025-01-16 19:13:44.161150: Pseudo dice [np.float32(0.7099)]\n",
      "2025-01-16 19:13:44.161182: Epoch time: 110.52 s\n",
      "2025-01-16 19:13:44.749927: \n",
      "2025-01-16 19:13:44.750026: Epoch 631\n",
      "2025-01-16 19:13:44.750092: Current learning rate: 0.00408\n",
      "2025-01-16 19:15:35.260287: train_loss -0.7913\n",
      "2025-01-16 19:15:35.260481: val_loss -0.6756\n",
      "2025-01-16 19:15:35.260516: Pseudo dice [np.float32(0.7363)]\n",
      "2025-01-16 19:15:35.260550: Epoch time: 110.51 s\n",
      "2025-01-16 19:15:35.848646: \n",
      "2025-01-16 19:15:35.848832: Epoch 632\n",
      "2025-01-16 19:15:35.848914: Current learning rate: 0.00407\n",
      "2025-01-16 19:17:26.441370: train_loss -0.8246\n",
      "2025-01-16 19:17:26.441494: val_loss -0.6557\n",
      "2025-01-16 19:17:26.441525: Pseudo dice [np.float32(0.5818)]\n",
      "2025-01-16 19:17:26.441617: Epoch time: 110.59 s\n",
      "2025-01-16 19:17:27.038350: \n",
      "2025-01-16 19:17:27.038684: Epoch 633\n",
      "2025-01-16 19:17:27.038802: Current learning rate: 0.00406\n",
      "2025-01-16 19:19:17.639757: train_loss -0.822\n",
      "2025-01-16 19:19:17.639884: val_loss -0.6449\n",
      "2025-01-16 19:19:17.639918: Pseudo dice [np.float32(0.644)]\n",
      "2025-01-16 19:19:17.639952: Epoch time: 110.6 s\n",
      "2025-01-16 19:19:18.230727: \n",
      "2025-01-16 19:19:18.231037: Epoch 634\n",
      "2025-01-16 19:19:18.231106: Current learning rate: 0.00405\n",
      "2025-01-16 19:21:08.690898: train_loss -0.8195\n",
      "2025-01-16 19:21:08.691316: val_loss -0.6295\n",
      "2025-01-16 19:21:08.691361: Pseudo dice [np.float32(0.5911)]\n",
      "2025-01-16 19:21:08.691395: Epoch time: 110.46 s\n",
      "2025-01-16 19:21:09.300288: \n",
      "2025-01-16 19:21:09.300571: Epoch 635\n",
      "2025-01-16 19:21:09.300832: Current learning rate: 0.00404\n",
      "2025-01-16 19:22:59.987535: train_loss -0.8244\n",
      "2025-01-16 19:22:59.987663: val_loss -0.6906\n",
      "2025-01-16 19:22:59.987696: Pseudo dice [np.float32(0.6865)]\n",
      "2025-01-16 19:22:59.987728: Epoch time: 110.69 s\n",
      "2025-01-16 19:23:00.577120: \n",
      "2025-01-16 19:23:00.577211: Epoch 636\n",
      "2025-01-16 19:23:00.577276: Current learning rate: 0.00403\n",
      "2025-01-16 19:24:51.103904: train_loss -0.8148\n",
      "2025-01-16 19:24:51.104085: val_loss -0.531\n",
      "2025-01-16 19:24:51.104118: Pseudo dice [np.float32(0.596)]\n",
      "2025-01-16 19:24:51.104151: Epoch time: 110.53 s\n",
      "2025-01-16 19:24:51.696245: \n",
      "2025-01-16 19:24:51.696571: Epoch 637\n",
      "2025-01-16 19:24:51.696646: Current learning rate: 0.00402\n",
      "2025-01-16 19:26:42.259347: train_loss -0.7842\n",
      "2025-01-16 19:26:42.259463: val_loss -0.5872\n",
      "2025-01-16 19:26:42.259496: Pseudo dice [np.float32(0.6609)]\n",
      "2025-01-16 19:26:42.259584: Epoch time: 110.56 s\n",
      "2025-01-16 19:26:43.090905: \n",
      "2025-01-16 19:26:43.091259: Epoch 638\n",
      "2025-01-16 19:26:43.091330: Current learning rate: 0.00401\n",
      "2025-01-16 19:28:33.640542: train_loss -0.8098\n",
      "2025-01-16 19:28:33.640669: val_loss -0.6934\n",
      "2025-01-16 19:28:33.640702: Pseudo dice [np.float32(0.64)]\n",
      "2025-01-16 19:28:33.640735: Epoch time: 110.55 s\n",
      "2025-01-16 19:28:34.229506: \n",
      "2025-01-16 19:28:34.229849: Epoch 639\n",
      "2025-01-16 19:28:34.230018: Current learning rate: 0.004\n",
      "2025-01-16 19:30:24.734514: train_loss -0.8231\n",
      "2025-01-16 19:30:24.734739: val_loss -0.7183\n",
      "2025-01-16 19:30:24.734786: Pseudo dice [np.float32(0.6968)]\n",
      "2025-01-16 19:30:24.734867: Epoch time: 110.51 s\n",
      "2025-01-16 19:30:25.326133: \n",
      "2025-01-16 19:30:25.326247: Epoch 640\n",
      "2025-01-16 19:30:25.326316: Current learning rate: 0.00399\n",
      "2025-01-16 19:32:15.924903: train_loss -0.7981\n",
      "2025-01-16 19:32:15.925034: val_loss -0.7129\n",
      "2025-01-16 19:32:15.925069: Pseudo dice [np.float32(0.6705)]\n",
      "2025-01-16 19:32:15.925102: Epoch time: 110.6 s\n",
      "2025-01-16 19:32:16.516913: \n",
      "2025-01-16 19:32:16.517014: Epoch 641\n",
      "2025-01-16 19:32:16.517076: Current learning rate: 0.00398\n",
      "2025-01-16 19:34:07.037228: train_loss -0.8117\n",
      "2025-01-16 19:34:07.037349: val_loss -0.6426\n",
      "2025-01-16 19:34:07.037382: Pseudo dice [np.float32(0.6816)]\n",
      "2025-01-16 19:34:07.037415: Epoch time: 110.52 s\n",
      "2025-01-16 19:34:07.636502: \n",
      "2025-01-16 19:34:07.636671: Epoch 642\n",
      "2025-01-16 19:34:07.636743: Current learning rate: 0.00397\n",
      "2025-01-16 19:35:58.247241: train_loss -0.8359\n",
      "2025-01-16 19:35:58.247469: val_loss -0.6314\n",
      "2025-01-16 19:35:58.247508: Pseudo dice [np.float32(0.6932)]\n",
      "2025-01-16 19:35:58.247546: Epoch time: 110.61 s\n",
      "2025-01-16 19:35:58.839680: \n",
      "2025-01-16 19:35:58.839972: Epoch 643\n",
      "2025-01-16 19:35:58.840099: Current learning rate: 0.00396\n",
      "2025-01-16 19:37:49.375043: train_loss -0.833\n",
      "2025-01-16 19:37:49.375273: val_loss -0.686\n",
      "2025-01-16 19:37:49.375318: Pseudo dice [np.float32(0.6925)]\n",
      "2025-01-16 19:37:49.375354: Epoch time: 110.54 s\n",
      "2025-01-16 19:37:49.963787: \n",
      "2025-01-16 19:37:49.963933: Epoch 644\n",
      "2025-01-16 19:37:49.964005: Current learning rate: 0.00395\n",
      "2025-01-16 19:39:40.681386: train_loss -0.7809\n",
      "2025-01-16 19:39:40.681520: val_loss -0.6185\n",
      "2025-01-16 19:39:40.681553: Pseudo dice [np.float32(0.5927)]\n",
      "2025-01-16 19:39:40.681585: Epoch time: 110.72 s\n",
      "2025-01-16 19:39:41.281114: \n",
      "2025-01-16 19:39:41.281212: Epoch 645\n",
      "2025-01-16 19:39:41.281273: Current learning rate: 0.00394\n",
      "2025-01-16 19:41:32.020592: train_loss -0.7996\n",
      "2025-01-16 19:41:32.020719: val_loss -0.5946\n",
      "2025-01-16 19:41:32.020751: Pseudo dice [np.float32(0.6118)]\n",
      "2025-01-16 19:41:32.020783: Epoch time: 110.74 s\n",
      "2025-01-16 19:41:32.607699: \n",
      "2025-01-16 19:41:32.607795: Epoch 646\n",
      "2025-01-16 19:41:32.607856: Current learning rate: 0.00393\n",
      "2025-01-16 19:43:23.191794: train_loss -0.7921\n",
      "2025-01-16 19:43:23.191917: val_loss -0.7039\n",
      "2025-01-16 19:43:23.191950: Pseudo dice [np.float32(0.651)]\n",
      "2025-01-16 19:43:23.191991: Epoch time: 110.58 s\n",
      "2025-01-16 19:43:23.784597: \n",
      "2025-01-16 19:43:23.784693: Epoch 647\n",
      "2025-01-16 19:43:23.784756: Current learning rate: 0.00392\n",
      "2025-01-16 19:45:14.529658: train_loss -0.8288\n",
      "2025-01-16 19:45:14.529785: val_loss -0.718\n",
      "2025-01-16 19:45:14.529817: Pseudo dice [np.float32(0.7396)]\n",
      "2025-01-16 19:45:14.529850: Epoch time: 110.75 s\n",
      "2025-01-16 19:45:15.126990: \n",
      "2025-01-16 19:45:15.127411: Epoch 648\n",
      "2025-01-16 19:45:15.127555: Current learning rate: 0.00391\n",
      "2025-01-16 19:47:05.753597: train_loss -0.824\n",
      "2025-01-16 19:47:05.753723: val_loss -0.6433\n",
      "2025-01-16 19:47:05.753753: Pseudo dice [np.float32(0.6722)]\n",
      "2025-01-16 19:47:05.753813: Epoch time: 110.63 s\n",
      "2025-01-16 19:47:06.345391: \n",
      "2025-01-16 19:47:06.345613: Epoch 649\n",
      "2025-01-16 19:47:06.345757: Current learning rate: 0.0039\n",
      "2025-01-16 19:48:56.885629: train_loss -0.8085\n",
      "2025-01-16 19:48:56.885849: val_loss -0.6933\n",
      "2025-01-16 19:48:56.885887: Pseudo dice [np.float32(0.6629)]\n",
      "2025-01-16 19:48:56.885919: Epoch time: 110.54 s\n",
      "2025-01-16 19:48:57.934465: \n",
      "2025-01-16 19:48:57.934570: Epoch 650\n",
      "2025-01-16 19:48:57.934635: Current learning rate: 0.00389\n",
      "2025-01-16 19:50:48.477136: train_loss -0.8132\n",
      "2025-01-16 19:50:48.477316: val_loss -0.7039\n",
      "2025-01-16 19:50:48.477389: Pseudo dice [np.float32(0.713)]\n",
      "2025-01-16 19:50:48.477558: Epoch time: 110.54 s\n",
      "2025-01-16 19:50:49.076378: \n",
      "2025-01-16 19:50:49.076682: Epoch 651\n",
      "2025-01-16 19:50:49.076753: Current learning rate: 0.00388\n",
      "2025-01-16 19:52:39.601588: train_loss -0.7861\n",
      "2025-01-16 19:52:39.602025: val_loss -0.4671\n",
      "2025-01-16 19:52:39.602189: Pseudo dice [np.float32(0.3363)]\n",
      "2025-01-16 19:52:39.602248: Epoch time: 110.53 s\n",
      "2025-01-16 19:52:40.203832: \n",
      "2025-01-16 19:52:40.204237: Epoch 652\n",
      "2025-01-16 19:52:40.204336: Current learning rate: 0.00387\n",
      "2025-01-16 19:54:30.838561: train_loss -0.8139\n",
      "2025-01-16 19:54:30.838694: val_loss -0.6816\n",
      "2025-01-16 19:54:30.838729: Pseudo dice [np.float32(0.617)]\n",
      "2025-01-16 19:54:30.838762: Epoch time: 110.64 s\n",
      "2025-01-16 19:54:31.430318: \n",
      "2025-01-16 19:54:31.430808: Epoch 653\n",
      "2025-01-16 19:54:31.430903: Current learning rate: 0.00386\n",
      "2025-01-16 19:56:22.107305: train_loss -0.794\n",
      "2025-01-16 19:56:22.107431: val_loss -0.6988\n",
      "2025-01-16 19:56:22.107480: Pseudo dice [np.float32(0.7072)]\n",
      "2025-01-16 19:56:22.108252: Epoch time: 110.68 s\n",
      "2025-01-16 19:56:22.704918: \n",
      "2025-01-16 19:56:22.705232: Epoch 654\n",
      "2025-01-16 19:56:22.705305: Current learning rate: 0.00385\n",
      "2025-01-16 19:58:13.465371: train_loss -0.7844\n",
      "2025-01-16 19:58:13.465511: val_loss -0.6933\n",
      "2025-01-16 19:58:13.465548: Pseudo dice [np.float32(0.6676)]\n",
      "2025-01-16 19:58:13.465584: Epoch time: 110.76 s\n",
      "2025-01-16 19:58:14.070400: \n",
      "2025-01-16 19:58:14.070731: Epoch 655\n",
      "2025-01-16 19:58:14.070890: Current learning rate: 0.00384\n",
      "2025-01-16 20:00:04.644810: train_loss -0.8257\n",
      "2025-01-16 20:00:04.644948: val_loss -0.7486\n",
      "2025-01-16 20:00:04.645182: Pseudo dice [np.float32(0.7396)]\n",
      "2025-01-16 20:00:04.645344: Epoch time: 110.57 s\n",
      "2025-01-16 20:00:05.244093: \n",
      "2025-01-16 20:00:05.244501: Epoch 656\n",
      "2025-01-16 20:00:05.244582: Current learning rate: 0.00383\n",
      "2025-01-16 20:01:55.978073: train_loss -0.8149\n",
      "2025-01-16 20:01:55.978213: val_loss -0.6352\n",
      "2025-01-16 20:01:55.978307: Pseudo dice [np.float32(0.7377)]\n",
      "2025-01-16 20:01:55.978438: Epoch time: 110.73 s\n",
      "2025-01-16 20:01:56.577499: \n",
      "2025-01-16 20:01:56.577903: Epoch 657\n",
      "2025-01-16 20:01:56.577999: Current learning rate: 0.00382\n",
      "2025-01-16 20:03:47.319074: train_loss -0.8186\n",
      "2025-01-16 20:03:47.319254: val_loss -0.6767\n",
      "2025-01-16 20:03:47.319287: Pseudo dice [np.float32(0.6462)]\n",
      "2025-01-16 20:03:47.319374: Epoch time: 110.74 s\n",
      "2025-01-16 20:03:47.918290: \n",
      "2025-01-16 20:03:47.918449: Epoch 658\n",
      "2025-01-16 20:03:47.918520: Current learning rate: 0.00381\n",
      "2025-01-16 20:05:38.522470: train_loss -0.8071\n",
      "2025-01-16 20:05:38.522668: val_loss -0.7242\n",
      "2025-01-16 20:05:38.522711: Pseudo dice [np.float32(0.7546)]\n",
      "2025-01-16 20:05:38.522745: Epoch time: 110.6 s\n",
      "2025-01-16 20:05:39.120259: \n",
      "2025-01-16 20:05:39.120637: Epoch 659\n",
      "2025-01-16 20:05:39.120817: Current learning rate: 0.0038\n",
      "2025-01-16 20:07:29.848661: train_loss -0.7972\n",
      "2025-01-16 20:07:29.848788: val_loss -0.6855\n",
      "2025-01-16 20:07:29.848820: Pseudo dice [np.float32(0.7269)]\n",
      "2025-01-16 20:07:29.848853: Epoch time: 110.73 s\n",
      "2025-01-16 20:07:30.449884: \n",
      "2025-01-16 20:07:30.449968: Epoch 660\n",
      "2025-01-16 20:07:30.450032: Current learning rate: 0.00379\n",
      "2025-01-16 20:09:21.179204: train_loss -0.8357\n",
      "2025-01-16 20:09:21.179340: val_loss -0.7095\n",
      "2025-01-16 20:09:21.179372: Pseudo dice [np.float32(0.7147)]\n",
      "2025-01-16 20:09:21.179405: Epoch time: 110.73 s\n",
      "2025-01-16 20:09:21.774923: \n",
      "2025-01-16 20:09:21.775008: Epoch 661\n",
      "2025-01-16 20:09:21.775074: Current learning rate: 0.00378\n",
      "2025-01-16 20:11:12.326771: train_loss -0.8165\n",
      "2025-01-16 20:11:12.326900: val_loss -0.7406\n",
      "2025-01-16 20:11:12.326934: Pseudo dice [np.float32(0.715)]\n",
      "2025-01-16 20:11:12.326967: Epoch time: 110.55 s\n",
      "2025-01-16 20:11:13.181645: \n",
      "2025-01-16 20:11:13.181745: Epoch 662\n",
      "2025-01-16 20:11:13.181808: Current learning rate: 0.00377\n",
      "2025-01-16 20:13:03.934898: train_loss -0.8235\n",
      "2025-01-16 20:13:03.935011: val_loss -0.7453\n",
      "2025-01-16 20:13:03.935042: Pseudo dice [np.float32(0.7991)]\n",
      "2025-01-16 20:13:03.935076: Epoch time: 110.75 s\n",
      "2025-01-16 20:13:04.528050: \n",
      "2025-01-16 20:13:04.528291: Epoch 663\n",
      "2025-01-16 20:13:04.528362: Current learning rate: 0.00376\n",
      "2025-01-16 20:14:55.096289: train_loss -0.8084\n",
      "2025-01-16 20:14:55.096446: val_loss -0.6199\n",
      "2025-01-16 20:14:55.096602: Pseudo dice [np.float32(0.623)]\n",
      "2025-01-16 20:14:55.096742: Epoch time: 110.57 s\n",
      "2025-01-16 20:14:55.687202: \n",
      "2025-01-16 20:14:55.687294: Epoch 664\n",
      "2025-01-16 20:14:55.687356: Current learning rate: 0.00375\n",
      "2025-01-16 20:16:46.403150: train_loss -0.8234\n",
      "2025-01-16 20:16:46.403368: val_loss -0.7159\n",
      "2025-01-16 20:16:46.403415: Pseudo dice [np.float32(0.7236)]\n",
      "2025-01-16 20:16:46.403466: Epoch time: 110.72 s\n",
      "2025-01-16 20:16:46.996830: \n",
      "2025-01-16 20:16:46.997220: Epoch 665\n",
      "2025-01-16 20:16:46.997308: Current learning rate: 0.00374\n",
      "2025-01-16 20:18:37.552294: train_loss -0.8225\n",
      "2025-01-16 20:18:37.552426: val_loss -0.7047\n",
      "2025-01-16 20:18:37.552461: Pseudo dice [np.float32(0.7252)]\n",
      "2025-01-16 20:18:37.552495: Epoch time: 110.56 s\n",
      "2025-01-16 20:18:38.153100: \n",
      "2025-01-16 20:18:38.153198: Epoch 666\n",
      "2025-01-16 20:18:38.153262: Current learning rate: 0.00373\n",
      "2025-01-16 20:20:28.909569: train_loss -0.8273\n",
      "2025-01-16 20:20:28.909716: val_loss -0.6894\n",
      "2025-01-16 20:20:28.909751: Pseudo dice [np.float32(0.6361)]\n",
      "2025-01-16 20:20:28.909785: Epoch time: 110.76 s\n",
      "2025-01-16 20:20:29.508032: \n",
      "2025-01-16 20:20:29.508372: Epoch 667\n",
      "2025-01-16 20:20:29.508446: Current learning rate: 0.00372\n",
      "2025-01-16 20:22:20.087659: train_loss -0.8099\n",
      "2025-01-16 20:22:20.087778: val_loss -0.7015\n",
      "2025-01-16 20:22:20.087811: Pseudo dice [np.float32(0.6885)]\n",
      "2025-01-16 20:22:20.087845: Epoch time: 110.58 s\n",
      "2025-01-16 20:22:20.689044: \n",
      "2025-01-16 20:22:20.689376: Epoch 668\n",
      "2025-01-16 20:22:20.689450: Current learning rate: 0.00371\n",
      "2025-01-16 20:24:11.386804: train_loss -0.8362\n",
      "2025-01-16 20:24:11.386967: val_loss -0.7171\n",
      "2025-01-16 20:24:11.386999: Pseudo dice [np.float32(0.7112)]\n",
      "2025-01-16 20:24:11.387032: Epoch time: 110.7 s\n",
      "2025-01-16 20:24:11.990139: \n",
      "2025-01-16 20:24:11.990389: Epoch 669\n",
      "2025-01-16 20:24:11.990572: Current learning rate: 0.0037\n",
      "2025-01-16 20:26:02.740768: train_loss -0.8303\n",
      "2025-01-16 20:26:02.740893: val_loss -0.6789\n",
      "2025-01-16 20:26:02.740926: Pseudo dice [np.float32(0.7088)]\n",
      "2025-01-16 20:26:02.740956: Epoch time: 110.75 s\n",
      "2025-01-16 20:26:03.342579: \n",
      "2025-01-16 20:26:03.342720: Epoch 670\n",
      "2025-01-16 20:26:03.342793: Current learning rate: 0.00369\n",
      "2025-01-16 20:27:53.920509: train_loss -0.8328\n",
      "2025-01-16 20:27:53.920689: val_loss -0.6859\n",
      "2025-01-16 20:27:53.920722: Pseudo dice [np.float32(0.6704)]\n",
      "2025-01-16 20:27:53.920754: Epoch time: 110.58 s\n",
      "2025-01-16 20:27:54.519009: \n",
      "2025-01-16 20:27:54.519104: Epoch 671\n",
      "2025-01-16 20:27:54.519173: Current learning rate: 0.00368\n",
      "2025-01-16 20:29:45.279138: train_loss -0.8047\n",
      "2025-01-16 20:29:45.279426: val_loss -0.578\n",
      "2025-01-16 20:29:45.279499: Pseudo dice [np.float32(0.6676)]\n",
      "2025-01-16 20:29:45.279542: Epoch time: 110.76 s\n",
      "2025-01-16 20:29:45.875235: \n",
      "2025-01-16 20:29:45.875571: Epoch 672\n",
      "2025-01-16 20:29:45.875720: Current learning rate: 0.00367\n",
      "2025-01-16 20:31:36.385738: train_loss -0.8215\n",
      "2025-01-16 20:31:36.385875: val_loss -0.7034\n",
      "2025-01-16 20:31:36.385910: Pseudo dice [np.float32(0.7484)]\n",
      "2025-01-16 20:31:36.385942: Epoch time: 110.51 s\n",
      "2025-01-16 20:31:36.981712: \n",
      "2025-01-16 20:31:36.981799: Epoch 673\n",
      "2025-01-16 20:31:36.981861: Current learning rate: 0.00366\n",
      "2025-01-16 20:33:27.713736: train_loss -0.8458\n",
      "2025-01-16 20:33:27.713978: val_loss -0.6464\n",
      "2025-01-16 20:33:27.714104: Pseudo dice [np.float32(0.6803)]\n",
      "2025-01-16 20:33:27.714166: Epoch time: 110.73 s\n",
      "2025-01-16 20:33:28.535760: \n",
      "2025-01-16 20:33:28.535964: Epoch 674\n",
      "2025-01-16 20:33:28.536030: Current learning rate: 0.00365\n",
      "2025-01-16 20:35:19.212240: train_loss -0.8532\n",
      "2025-01-16 20:35:19.212440: val_loss -0.6277\n",
      "2025-01-16 20:35:19.212474: Pseudo dice [np.float32(0.6679)]\n",
      "2025-01-16 20:35:19.212507: Epoch time: 110.68 s\n",
      "2025-01-16 20:35:19.816129: \n",
      "2025-01-16 20:35:19.816230: Epoch 675\n",
      "2025-01-16 20:35:19.816295: Current learning rate: 0.00364\n",
      "2025-01-16 20:37:10.478274: train_loss -0.8262\n",
      "2025-01-16 20:37:10.478756: val_loss -0.6508\n",
      "2025-01-16 20:37:10.478842: Pseudo dice [np.float32(0.7027)]\n",
      "2025-01-16 20:37:10.478941: Epoch time: 110.66 s\n",
      "2025-01-16 20:37:11.076729: \n",
      "2025-01-16 20:37:11.077088: Epoch 676\n",
      "2025-01-16 20:37:11.077206: Current learning rate: 0.00363\n",
      "2025-01-16 20:39:01.780753: train_loss -0.8153\n",
      "2025-01-16 20:39:01.780967: val_loss -0.7266\n",
      "2025-01-16 20:39:01.781034: Pseudo dice [np.float32(0.7419)]\n",
      "2025-01-16 20:39:01.781075: Epoch time: 110.7 s\n",
      "2025-01-16 20:39:02.381387: \n",
      "2025-01-16 20:39:02.381580: Epoch 677\n",
      "2025-01-16 20:39:02.381651: Current learning rate: 0.00362\n",
      "2025-01-16 20:40:52.958331: train_loss -0.8173\n",
      "2025-01-16 20:40:52.958469: val_loss -0.6913\n",
      "2025-01-16 20:40:52.958508: Pseudo dice [np.float32(0.6758)]\n",
      "2025-01-16 20:40:52.958544: Epoch time: 110.58 s\n",
      "2025-01-16 20:40:53.565299: \n",
      "2025-01-16 20:40:53.565499: Epoch 678\n",
      "2025-01-16 20:40:53.565598: Current learning rate: 0.00361\n",
      "2025-01-16 20:42:44.134043: train_loss -0.8382\n",
      "2025-01-16 20:42:44.134168: val_loss -0.7362\n",
      "2025-01-16 20:42:44.134304: Pseudo dice [np.float32(0.6838)]\n",
      "2025-01-16 20:42:44.134363: Epoch time: 110.57 s\n",
      "2025-01-16 20:42:44.735949: \n",
      "2025-01-16 20:42:44.736182: Epoch 679\n",
      "2025-01-16 20:42:44.736315: Current learning rate: 0.0036\n",
      "2025-01-16 20:44:35.305133: train_loss -0.8396\n",
      "2025-01-16 20:44:35.305259: val_loss -0.5914\n",
      "2025-01-16 20:44:35.305293: Pseudo dice [np.float32(0.6667)]\n",
      "2025-01-16 20:44:35.305326: Epoch time: 110.57 s\n",
      "2025-01-16 20:44:35.904688: \n",
      "2025-01-16 20:44:35.904836: Epoch 680\n",
      "2025-01-16 20:44:35.904902: Current learning rate: 0.00359\n",
      "2025-01-16 20:46:26.545991: train_loss -0.8169\n",
      "2025-01-16 20:46:26.546164: val_loss -0.6207\n",
      "2025-01-16 20:46:26.546262: Pseudo dice [np.float32(0.6349)]\n",
      "2025-01-16 20:46:26.546394: Epoch time: 110.64 s\n",
      "2025-01-16 20:46:27.142470: \n",
      "2025-01-16 20:46:27.142626: Epoch 681\n",
      "2025-01-16 20:46:27.142699: Current learning rate: 0.00358\n",
      "2025-01-16 20:48:17.736345: train_loss -0.8302\n",
      "2025-01-16 20:48:17.736465: val_loss -0.6774\n",
      "2025-01-16 20:48:17.736498: Pseudo dice [np.float32(0.6513)]\n",
      "2025-01-16 20:48:17.736531: Epoch time: 110.59 s\n",
      "2025-01-16 20:48:18.335562: \n",
      "2025-01-16 20:48:18.335979: Epoch 682\n",
      "2025-01-16 20:48:18.336075: Current learning rate: 0.00357\n",
      "2025-01-16 20:50:09.065462: train_loss -0.8333\n",
      "2025-01-16 20:50:09.065609: val_loss -0.6128\n",
      "2025-01-16 20:50:09.065643: Pseudo dice [np.float32(0.5668)]\n",
      "2025-01-16 20:50:09.065734: Epoch time: 110.73 s\n",
      "2025-01-16 20:50:09.670794: \n",
      "2025-01-16 20:50:09.671086: Epoch 683\n",
      "2025-01-16 20:50:09.671159: Current learning rate: 0.00356\n",
      "2025-01-16 20:52:00.373847: train_loss -0.8373\n",
      "2025-01-16 20:52:00.374079: val_loss -0.7611\n",
      "2025-01-16 20:52:00.374114: Pseudo dice [np.float32(0.7549)]\n",
      "2025-01-16 20:52:00.374146: Epoch time: 110.7 s\n",
      "2025-01-16 20:52:00.969482: \n",
      "2025-01-16 20:52:00.969785: Epoch 684\n",
      "2025-01-16 20:52:00.969954: Current learning rate: 0.00355\n",
      "2025-01-16 20:53:51.648620: train_loss -0.8293\n",
      "2025-01-16 20:53:51.648786: val_loss -0.6205\n",
      "2025-01-16 20:53:51.648832: Pseudo dice [np.float32(0.6796)]\n",
      "2025-01-16 20:53:51.648867: Epoch time: 110.68 s\n",
      "2025-01-16 20:53:52.246628: \n",
      "2025-01-16 20:53:52.246792: Epoch 685\n",
      "2025-01-16 20:53:52.246858: Current learning rate: 0.00354\n",
      "2025-01-16 20:55:43.020683: train_loss -0.8245\n",
      "2025-01-16 20:55:43.020916: val_loss -0.6799\n",
      "2025-01-16 20:55:43.020957: Pseudo dice [np.float32(0.7129)]\n",
      "2025-01-16 20:55:43.020996: Epoch time: 110.77 s\n",
      "2025-01-16 20:55:43.905783: \n",
      "2025-01-16 20:55:43.905944: Epoch 686\n",
      "2025-01-16 20:55:43.906013: Current learning rate: 0.00353\n",
      "2025-01-16 20:57:34.661069: train_loss -0.8148\n",
      "2025-01-16 20:57:34.661198: val_loss -0.7099\n",
      "2025-01-16 20:57:34.661230: Pseudo dice [np.float32(0.7332)]\n",
      "2025-01-16 20:57:34.661263: Epoch time: 110.76 s\n",
      "2025-01-16 20:57:35.258155: \n",
      "2025-01-16 20:57:35.258477: Epoch 687\n",
      "2025-01-16 20:57:35.258710: Current learning rate: 0.00352\n",
      "2025-01-16 20:59:25.834409: train_loss -0.8207\n",
      "2025-01-16 20:59:25.834789: val_loss -0.676\n",
      "2025-01-16 20:59:25.834830: Pseudo dice [np.float32(0.7126)]\n",
      "2025-01-16 20:59:25.834864: Epoch time: 110.58 s\n",
      "2025-01-16 20:59:26.424153: \n",
      "2025-01-16 20:59:26.424277: Epoch 688\n",
      "2025-01-16 20:59:26.424339: Current learning rate: 0.00351\n",
      "2025-01-16 21:01:16.960072: train_loss -0.8136\n",
      "2025-01-16 21:01:16.960199: val_loss -0.5791\n",
      "2025-01-16 21:01:16.960234: Pseudo dice [np.float32(0.6301)]\n",
      "2025-01-16 21:01:16.960269: Epoch time: 110.54 s\n",
      "2025-01-16 21:01:17.569152: \n",
      "2025-01-16 21:01:17.569309: Epoch 689\n",
      "2025-01-16 21:01:17.569383: Current learning rate: 0.0035\n",
      "2025-01-16 21:03:08.243425: train_loss -0.8113\n",
      "2025-01-16 21:03:08.243555: val_loss -0.5394\n",
      "2025-01-16 21:03:08.243589: Pseudo dice [np.float32(0.5697)]\n",
      "2025-01-16 21:03:08.243624: Epoch time: 110.67 s\n",
      "2025-01-16 21:03:08.835732: \n",
      "2025-01-16 21:03:08.835952: Epoch 690\n",
      "2025-01-16 21:03:08.836023: Current learning rate: 0.00349\n",
      "2025-01-16 21:04:59.445515: train_loss -0.7953\n",
      "2025-01-16 21:04:59.445780: val_loss -0.6344\n",
      "2025-01-16 21:04:59.445935: Pseudo dice [np.float32(0.7041)]\n",
      "2025-01-16 21:04:59.445986: Epoch time: 110.61 s\n",
      "2025-01-16 21:05:00.049815: \n",
      "2025-01-16 21:05:00.049986: Epoch 691\n",
      "2025-01-16 21:05:00.050059: Current learning rate: 0.00348\n",
      "2025-01-16 21:06:50.623198: train_loss -0.8018\n",
      "2025-01-16 21:06:50.623328: val_loss -0.6409\n",
      "2025-01-16 21:06:50.623362: Pseudo dice [np.float32(0.6747)]\n",
      "2025-01-16 21:06:50.623399: Epoch time: 110.57 s\n",
      "2025-01-16 21:06:51.234867: \n",
      "2025-01-16 21:06:51.234960: Epoch 692\n",
      "2025-01-16 21:06:51.235027: Current learning rate: 0.00346\n",
      "2025-01-16 21:08:41.967632: train_loss -0.7652\n",
      "2025-01-16 21:08:41.967803: val_loss -0.6611\n",
      "2025-01-16 21:08:41.967837: Pseudo dice [np.float32(0.6804)]\n",
      "2025-01-16 21:08:41.967871: Epoch time: 110.73 s\n",
      "2025-01-16 21:08:42.572288: \n",
      "2025-01-16 21:08:42.572463: Epoch 693\n",
      "2025-01-16 21:08:42.572536: Current learning rate: 0.00345\n",
      "2025-01-16 21:10:33.310924: train_loss -0.7994\n",
      "2025-01-16 21:10:33.311059: val_loss -0.6335\n",
      "2025-01-16 21:10:33.311091: Pseudo dice [np.float32(0.6176)]\n",
      "2025-01-16 21:10:33.311123: Epoch time: 110.74 s\n",
      "2025-01-16 21:10:33.909537: \n",
      "2025-01-16 21:10:33.909705: Epoch 694\n",
      "2025-01-16 21:10:33.909780: Current learning rate: 0.00344\n",
      "2025-01-16 21:12:24.477221: train_loss -0.8208\n",
      "2025-01-16 21:12:24.477418: val_loss -0.6278\n",
      "2025-01-16 21:12:24.477460: Pseudo dice [np.float32(0.7053)]\n",
      "2025-01-16 21:12:24.477493: Epoch time: 110.57 s\n",
      "2025-01-16 21:12:25.086406: \n",
      "2025-01-16 21:12:25.086716: Epoch 695\n",
      "2025-01-16 21:12:25.086788: Current learning rate: 0.00343\n",
      "2025-01-16 21:14:15.798710: train_loss -0.833\n",
      "2025-01-16 21:14:15.798833: val_loss -0.6387\n",
      "2025-01-16 21:14:15.798867: Pseudo dice [np.float32(0.6958)]\n",
      "2025-01-16 21:14:15.798906: Epoch time: 110.71 s\n",
      "2025-01-16 21:14:16.396395: \n",
      "2025-01-16 21:14:16.396731: Epoch 696\n",
      "2025-01-16 21:14:16.396813: Current learning rate: 0.00342\n",
      "2025-01-16 21:16:07.139024: train_loss -0.8205\n",
      "2025-01-16 21:16:07.139435: val_loss -0.6827\n",
      "2025-01-16 21:16:07.139588: Pseudo dice [np.float32(0.6264)]\n",
      "2025-01-16 21:16:07.139640: Epoch time: 110.74 s\n",
      "2025-01-16 21:16:07.746941: \n",
      "2025-01-16 21:16:07.747031: Epoch 697\n",
      "2025-01-16 21:16:07.747096: Current learning rate: 0.00341\n",
      "2025-01-16 21:17:58.580992: train_loss -0.827\n",
      "2025-01-16 21:17:58.581113: val_loss -0.6326\n",
      "2025-01-16 21:17:58.581145: Pseudo dice [np.float32(0.6256)]\n",
      "2025-01-16 21:17:58.581176: Epoch time: 110.83 s\n",
      "2025-01-16 21:17:59.186322: \n",
      "2025-01-16 21:17:59.186693: Epoch 698\n",
      "2025-01-16 21:17:59.186830: Current learning rate: 0.0034\n",
      "2025-01-16 21:19:49.839900: train_loss -0.8339\n",
      "2025-01-16 21:19:49.840030: val_loss -0.6888\n",
      "2025-01-16 21:19:49.840065: Pseudo dice [np.float32(0.642)]\n",
      "2025-01-16 21:19:49.840097: Epoch time: 110.65 s\n",
      "2025-01-16 21:19:50.433198: \n",
      "2025-01-16 21:19:50.433564: Epoch 699\n",
      "2025-01-16 21:19:50.433701: Current learning rate: 0.00339\n",
      "2025-01-16 21:21:41.033610: train_loss -0.8282\n",
      "2025-01-16 21:21:41.033975: val_loss -0.6552\n",
      "2025-01-16 21:21:41.034096: Pseudo dice [np.float32(0.6055)]\n",
      "2025-01-16 21:21:41.034138: Epoch time: 110.6 s\n",
      "2025-01-16 21:21:41.869523: \n",
      "2025-01-16 21:21:41.869828: Epoch 700\n",
      "2025-01-16 21:21:41.869907: Current learning rate: 0.00338\n",
      "2025-01-16 21:23:32.587210: train_loss -0.8108\n",
      "2025-01-16 21:23:32.587339: val_loss -0.7244\n",
      "2025-01-16 21:23:32.587372: Pseudo dice [np.float32(0.7591)]\n",
      "2025-01-16 21:23:32.587405: Epoch time: 110.72 s\n",
      "2025-01-16 21:23:33.187855: \n",
      "2025-01-16 21:23:33.187964: Epoch 701\n",
      "2025-01-16 21:23:33.188041: Current learning rate: 0.00337\n",
      "2025-01-16 21:25:23.932850: train_loss -0.8324\n",
      "2025-01-16 21:25:23.933052: val_loss -0.6708\n",
      "2025-01-16 21:25:23.933096: Pseudo dice [np.float32(0.6431)]\n",
      "2025-01-16 21:25:23.933140: Epoch time: 110.75 s\n",
      "2025-01-16 21:25:24.538466: \n",
      "2025-01-16 21:25:24.538885: Epoch 702\n",
      "2025-01-16 21:25:24.538972: Current learning rate: 0.00336\n",
      "2025-01-16 21:27:15.267945: train_loss -0.8142\n",
      "2025-01-16 21:27:15.268120: val_loss -0.6279\n",
      "2025-01-16 21:27:15.268263: Pseudo dice [np.float32(0.6085)]\n",
      "2025-01-16 21:27:15.268348: Epoch time: 110.73 s\n",
      "2025-01-16 21:27:15.874343: \n",
      "2025-01-16 21:27:15.874531: Epoch 703\n",
      "2025-01-16 21:27:15.874603: Current learning rate: 0.00335\n",
      "2025-01-16 21:29:06.593734: train_loss -0.8255\n",
      "2025-01-16 21:29:06.593861: val_loss -0.6673\n",
      "2025-01-16 21:29:06.593895: Pseudo dice [np.float32(0.7296)]\n",
      "2025-01-16 21:29:06.593929: Epoch time: 110.72 s\n",
      "2025-01-16 21:29:07.194272: \n",
      "2025-01-16 21:29:07.194364: Epoch 704\n",
      "2025-01-16 21:29:07.194426: Current learning rate: 0.00334\n",
      "2025-01-16 21:30:57.930589: train_loss -0.7934\n",
      "2025-01-16 21:30:57.930756: val_loss -0.6337\n",
      "2025-01-16 21:30:57.930830: Pseudo dice [np.float32(0.6473)]\n",
      "2025-01-16 21:30:57.930872: Epoch time: 110.74 s\n",
      "2025-01-16 21:30:58.519319: \n",
      "2025-01-16 21:30:58.519637: Epoch 705\n",
      "2025-01-16 21:30:58.519768: Current learning rate: 0.00333\n",
      "2025-01-16 21:32:49.290334: train_loss -0.8152\n",
      "2025-01-16 21:32:49.290463: val_loss -0.6992\n",
      "2025-01-16 21:32:49.290496: Pseudo dice [np.float32(0.6631)]\n",
      "2025-01-16 21:32:49.290529: Epoch time: 110.77 s\n",
      "2025-01-16 21:32:49.899203: \n",
      "2025-01-16 21:32:49.899294: Epoch 706\n",
      "2025-01-16 21:32:49.899359: Current learning rate: 0.00332\n",
      "2025-01-16 21:34:40.699762: train_loss -0.8235\n",
      "2025-01-16 21:34:40.699885: val_loss -0.67\n",
      "2025-01-16 21:34:40.699915: Pseudo dice [np.float32(0.6877)]\n",
      "2025-01-16 21:34:40.699948: Epoch time: 110.8 s\n",
      "2025-01-16 21:34:41.294271: \n",
      "2025-01-16 21:34:41.294628: Epoch 707\n",
      "2025-01-16 21:34:41.294726: Current learning rate: 0.00331\n",
      "2025-01-16 21:36:31.876246: train_loss -0.8275\n",
      "2025-01-16 21:36:31.876391: val_loss -0.6781\n",
      "2025-01-16 21:36:31.876441: Pseudo dice [np.float32(0.6798)]\n",
      "2025-01-16 21:36:31.876652: Epoch time: 110.58 s\n",
      "2025-01-16 21:36:32.475903: \n",
      "2025-01-16 21:36:32.476181: Epoch 708\n",
      "2025-01-16 21:36:32.476329: Current learning rate: 0.0033\n",
      "2025-01-16 21:38:23.228286: train_loss -0.8357\n",
      "2025-01-16 21:38:23.228498: val_loss -0.7372\n",
      "2025-01-16 21:38:23.228532: Pseudo dice [np.float32(0.7491)]\n",
      "2025-01-16 21:38:23.228564: Epoch time: 110.75 s\n",
      "2025-01-16 21:38:24.057565: \n",
      "2025-01-16 21:38:24.057759: Epoch 709\n",
      "2025-01-16 21:38:24.057840: Current learning rate: 0.00329\n",
      "2025-01-16 21:40:14.807271: train_loss -0.8186\n",
      "2025-01-16 21:40:14.807438: val_loss -0.6674\n",
      "2025-01-16 21:40:14.807747: Pseudo dice [np.float32(0.7035)]\n",
      "2025-01-16 21:40:14.807859: Epoch time: 110.75 s\n",
      "2025-01-16 21:40:15.406749: \n",
      "2025-01-16 21:40:15.406854: Epoch 710\n",
      "2025-01-16 21:40:15.406919: Current learning rate: 0.00328\n",
      "2025-01-16 21:42:06.172697: train_loss -0.8186\n",
      "2025-01-16 21:42:06.172881: val_loss -0.7116\n",
      "2025-01-16 21:42:06.172925: Pseudo dice [np.float32(0.7223)]\n",
      "2025-01-16 21:42:06.172958: Epoch time: 110.77 s\n",
      "2025-01-16 21:42:06.779549: \n",
      "2025-01-16 21:42:06.779858: Epoch 711\n",
      "2025-01-16 21:42:06.780090: Current learning rate: 0.00327\n",
      "2025-01-16 21:43:57.500894: train_loss -0.8284\n",
      "2025-01-16 21:43:57.501055: val_loss -0.6928\n",
      "2025-01-16 21:43:57.501190: Pseudo dice [np.float32(0.6804)]\n",
      "2025-01-16 21:43:57.501244: Epoch time: 110.72 s\n",
      "2025-01-16 21:43:58.101645: \n",
      "2025-01-16 21:43:58.101836: Epoch 712\n",
      "2025-01-16 21:43:58.102001: Current learning rate: 0.00326\n",
      "2025-01-16 21:45:48.850981: train_loss -0.8401\n",
      "2025-01-16 21:45:48.851326: val_loss -0.6232\n",
      "2025-01-16 21:45:48.851372: Pseudo dice [np.float32(0.6348)]\n",
      "2025-01-16 21:45:48.851411: Epoch time: 110.75 s\n",
      "2025-01-16 21:45:49.450117: \n",
      "2025-01-16 21:45:49.450321: Epoch 713\n",
      "2025-01-16 21:45:49.450410: Current learning rate: 0.00325\n",
      "2025-01-16 21:47:40.034590: train_loss -0.836\n",
      "2025-01-16 21:47:40.034708: val_loss -0.7029\n",
      "2025-01-16 21:47:40.034742: Pseudo dice [np.float32(0.7231)]\n",
      "2025-01-16 21:47:40.034777: Epoch time: 110.59 s\n",
      "2025-01-16 21:47:40.643673: \n",
      "2025-01-16 21:47:40.643766: Epoch 714\n",
      "2025-01-16 21:47:40.643831: Current learning rate: 0.00324\n",
      "2025-01-16 21:49:31.400160: train_loss -0.8195\n",
      "2025-01-16 21:49:31.400297: val_loss -0.7109\n",
      "2025-01-16 21:49:31.400333: Pseudo dice [np.float32(0.7259)]\n",
      "2025-01-16 21:49:31.400367: Epoch time: 110.76 s\n",
      "2025-01-16 21:49:32.002549: \n",
      "2025-01-16 21:49:32.002878: Epoch 715\n",
      "2025-01-16 21:49:32.003075: Current learning rate: 0.00323\n",
      "2025-01-16 21:51:22.799240: train_loss -0.8286\n",
      "2025-01-16 21:51:22.799376: val_loss -0.5577\n",
      "2025-01-16 21:51:22.799410: Pseudo dice [np.float32(0.5743)]\n",
      "2025-01-16 21:51:22.799444: Epoch time: 110.8 s\n",
      "2025-01-16 21:51:23.400689: \n",
      "2025-01-16 21:51:23.401029: Epoch 716\n",
      "2025-01-16 21:51:23.401105: Current learning rate: 0.00322\n",
      "2025-01-16 21:53:14.160861: train_loss -0.8137\n",
      "2025-01-16 21:53:14.160986: val_loss -0.7649\n",
      "2025-01-16 21:53:14.161020: Pseudo dice [np.float32(0.7385)]\n",
      "2025-01-16 21:53:14.161054: Epoch time: 110.76 s\n",
      "2025-01-16 21:53:14.767470: \n",
      "2025-01-16 21:53:14.767826: Epoch 717\n",
      "2025-01-16 21:53:14.767957: Current learning rate: 0.00321\n",
      "2025-01-16 21:55:05.545296: train_loss -0.8378\n",
      "2025-01-16 21:55:05.545415: val_loss -0.7047\n",
      "2025-01-16 21:55:05.545446: Pseudo dice [np.float32(0.6879)]\n",
      "2025-01-16 21:55:05.545477: Epoch time: 110.78 s\n",
      "2025-01-16 21:55:06.148167: \n",
      "2025-01-16 21:55:06.148449: Epoch 718\n",
      "2025-01-16 21:55:06.148560: Current learning rate: 0.0032\n",
      "2025-01-16 21:56:56.750103: train_loss -0.8389\n",
      "2025-01-16 21:56:56.750336: val_loss -0.7048\n",
      "2025-01-16 21:56:56.750382: Pseudo dice [np.float32(0.6822)]\n",
      "2025-01-16 21:56:56.750418: Epoch time: 110.6 s\n",
      "2025-01-16 21:56:57.353563: \n",
      "2025-01-16 21:56:57.353730: Epoch 719\n",
      "2025-01-16 21:56:57.353792: Current learning rate: 0.00319\n",
      "2025-01-16 21:58:48.142563: train_loss -0.8243\n",
      "2025-01-16 21:58:48.142802: val_loss -0.697\n",
      "2025-01-16 21:58:48.142862: Pseudo dice [np.float32(0.702)]\n",
      "2025-01-16 21:58:48.142902: Epoch time: 110.79 s\n",
      "2025-01-16 21:58:48.753782: \n",
      "2025-01-16 21:58:48.754112: Epoch 720\n",
      "2025-01-16 21:58:48.754218: Current learning rate: 0.00318\n",
      "2025-01-16 22:00:39.483815: train_loss -0.8156\n",
      "2025-01-16 22:00:39.484041: val_loss -0.689\n",
      "2025-01-16 22:00:39.484086: Pseudo dice [np.float32(0.6725)]\n",
      "2025-01-16 22:00:39.484120: Epoch time: 110.73 s\n",
      "2025-01-16 22:00:40.324973: \n",
      "2025-01-16 22:00:40.325358: Epoch 721\n",
      "2025-01-16 22:00:40.325434: Current learning rate: 0.00317\n",
      "2025-01-16 22:02:31.046566: train_loss -0.8466\n",
      "2025-01-16 22:02:31.046696: val_loss -0.6782\n",
      "2025-01-16 22:02:31.046732: Pseudo dice [np.float32(0.6863)]\n",
      "2025-01-16 22:02:31.046765: Epoch time: 110.72 s\n",
      "2025-01-16 22:02:31.672161: \n",
      "2025-01-16 22:02:31.672487: Epoch 722\n",
      "2025-01-16 22:02:31.672574: Current learning rate: 0.00316\n",
      "2025-01-16 22:04:22.213253: train_loss -0.8364\n",
      "2025-01-16 22:04:22.213380: val_loss -0.7169\n",
      "2025-01-16 22:04:22.213413: Pseudo dice [np.float32(0.7172)]\n",
      "2025-01-16 22:04:22.213445: Epoch time: 110.54 s\n",
      "2025-01-16 22:04:22.817028: \n",
      "2025-01-16 22:04:22.817275: Epoch 723\n",
      "2025-01-16 22:04:22.817451: Current learning rate: 0.00315\n",
      "2025-01-16 22:06:13.439220: train_loss -0.8417\n",
      "2025-01-16 22:06:13.439354: val_loss -0.7086\n",
      "2025-01-16 22:06:13.439389: Pseudo dice [np.float32(0.6995)]\n",
      "2025-01-16 22:06:13.439421: Epoch time: 110.62 s\n",
      "2025-01-16 22:06:14.036703: \n",
      "2025-01-16 22:06:14.037056: Epoch 724\n",
      "2025-01-16 22:06:14.037164: Current learning rate: 0.00314\n",
      "2025-01-16 22:08:04.518464: train_loss -0.8483\n",
      "2025-01-16 22:08:04.518660: val_loss -0.7193\n",
      "2025-01-16 22:08:04.518692: Pseudo dice [np.float32(0.6831)]\n",
      "2025-01-16 22:08:04.518727: Epoch time: 110.48 s\n",
      "2025-01-16 22:08:05.134120: \n",
      "2025-01-16 22:08:05.134475: Epoch 725\n",
      "2025-01-16 22:08:05.134617: Current learning rate: 0.00313\n",
      "2025-01-16 22:09:55.873575: train_loss -0.853\n",
      "2025-01-16 22:09:55.873698: val_loss -0.6865\n",
      "2025-01-16 22:09:55.873732: Pseudo dice [np.float32(0.6775)]\n",
      "2025-01-16 22:09:55.873765: Epoch time: 110.74 s\n",
      "2025-01-16 22:09:56.474127: \n",
      "2025-01-16 22:09:56.474541: Epoch 726\n",
      "2025-01-16 22:09:56.474679: Current learning rate: 0.00312\n",
      "2025-01-16 22:11:47.100971: train_loss -0.8305\n",
      "2025-01-16 22:11:47.101093: val_loss -0.5371\n",
      "2025-01-16 22:11:47.101126: Pseudo dice [np.float32(0.5083)]\n",
      "2025-01-16 22:11:47.101158: Epoch time: 110.63 s\n",
      "2025-01-16 22:11:47.704962: \n",
      "2025-01-16 22:11:47.705112: Epoch 727\n",
      "2025-01-16 22:11:47.705182: Current learning rate: 0.00311\n",
      "2025-01-16 22:13:38.397316: train_loss -0.8151\n",
      "2025-01-16 22:13:38.397438: val_loss -0.5494\n",
      "2025-01-16 22:13:38.397471: Pseudo dice [np.float32(0.5952)]\n",
      "2025-01-16 22:13:38.397504: Epoch time: 110.69 s\n",
      "2025-01-16 22:13:39.106091: \n",
      "2025-01-16 22:13:39.106447: Epoch 728\n",
      "2025-01-16 22:13:39.106550: Current learning rate: 0.0031\n",
      "2025-01-16 22:15:29.695289: train_loss -0.7872\n",
      "2025-01-16 22:15:29.695578: val_loss -0.6518\n",
      "2025-01-16 22:15:29.695625: Pseudo dice [np.float32(0.7125)]\n",
      "2025-01-16 22:15:29.695661: Epoch time: 110.59 s\n",
      "2025-01-16 22:15:30.298825: \n",
      "2025-01-16 22:15:30.298920: Epoch 729\n",
      "2025-01-16 22:15:30.298987: Current learning rate: 0.00309\n",
      "2025-01-16 22:17:20.908028: train_loss -0.8035\n",
      "2025-01-16 22:17:20.908170: val_loss -0.6988\n",
      "2025-01-16 22:17:20.908205: Pseudo dice [np.float32(0.699)]\n",
      "2025-01-16 22:17:20.908248: Epoch time: 110.61 s\n",
      "2025-01-16 22:17:21.514011: \n",
      "2025-01-16 22:17:21.514399: Epoch 730\n",
      "2025-01-16 22:17:21.514705: Current learning rate: 0.00308\n",
      "2025-01-16 22:19:12.256349: train_loss -0.8283\n",
      "2025-01-16 22:19:12.256548: val_loss -0.6202\n",
      "2025-01-16 22:19:12.257154: Pseudo dice [np.float32(0.6773)]\n",
      "2025-01-16 22:19:12.257307: Epoch time: 110.74 s\n",
      "2025-01-16 22:19:12.858821: \n",
      "2025-01-16 22:19:12.859185: Epoch 731\n",
      "2025-01-16 22:19:12.859262: Current learning rate: 0.00307\n",
      "2025-01-16 22:21:03.588244: train_loss -0.8066\n",
      "2025-01-16 22:21:03.588449: val_loss -0.538\n",
      "2025-01-16 22:21:03.588483: Pseudo dice [np.float32(0.4532)]\n",
      "2025-01-16 22:21:03.588516: Epoch time: 110.73 s\n",
      "2025-01-16 22:21:04.188633: \n",
      "2025-01-16 22:21:04.188829: Epoch 732\n",
      "2025-01-16 22:21:04.188930: Current learning rate: 0.00306\n",
      "2025-01-16 22:22:54.868580: train_loss -0.7977\n",
      "2025-01-16 22:22:54.868745: val_loss -0.5692\n",
      "2025-01-16 22:22:54.868865: Pseudo dice [np.float32(0.5832)]\n",
      "2025-01-16 22:22:54.869056: Epoch time: 110.68 s\n",
      "2025-01-16 22:22:55.471971: \n",
      "2025-01-16 22:22:55.472313: Epoch 733\n",
      "2025-01-16 22:22:55.472416: Current learning rate: 0.00305\n",
      "2025-01-16 22:24:46.206519: train_loss -0.8312\n",
      "2025-01-16 22:24:46.206643: val_loss -0.6433\n",
      "2025-01-16 22:24:46.206679: Pseudo dice [np.float32(0.6481)]\n",
      "2025-01-16 22:24:46.206748: Epoch time: 110.74 s\n",
      "2025-01-16 22:24:47.069697: \n",
      "2025-01-16 22:24:47.069871: Epoch 734\n",
      "2025-01-16 22:24:47.069996: Current learning rate: 0.00304\n",
      "2025-01-16 22:26:37.662958: train_loss -0.8112\n",
      "2025-01-16 22:26:37.663157: val_loss -0.7533\n",
      "2025-01-16 22:26:37.663193: Pseudo dice [np.float32(0.7471)]\n",
      "2025-01-16 22:26:37.663226: Epoch time: 110.59 s\n",
      "2025-01-16 22:26:38.259700: \n",
      "2025-01-16 22:26:38.259894: Epoch 735\n",
      "2025-01-16 22:26:38.259967: Current learning rate: 0.00303\n",
      "2025-01-16 22:28:28.955550: train_loss -0.824\n",
      "2025-01-16 22:28:28.955781: val_loss -0.6924\n",
      "2025-01-16 22:28:28.955827: Pseudo dice [np.float32(0.7001)]\n",
      "2025-01-16 22:28:28.955862: Epoch time: 110.7 s\n",
      "2025-01-16 22:28:29.558180: \n",
      "2025-01-16 22:28:29.558544: Epoch 736\n",
      "2025-01-16 22:28:29.558619: Current learning rate: 0.00302\n",
      "2025-01-16 22:30:20.276444: train_loss -0.809\n",
      "2025-01-16 22:30:20.276575: val_loss -0.7153\n",
      "2025-01-16 22:30:20.276608: Pseudo dice [np.float32(0.6852)]\n",
      "2025-01-16 22:30:20.276641: Epoch time: 110.72 s\n",
      "2025-01-16 22:30:20.888559: \n",
      "2025-01-16 22:30:20.888994: Epoch 737\n",
      "2025-01-16 22:30:20.889089: Current learning rate: 0.00301\n",
      "2025-01-16 22:32:11.490189: train_loss -0.8583\n",
      "2025-01-16 22:32:11.490319: val_loss -0.6079\n",
      "2025-01-16 22:32:11.490353: Pseudo dice [np.float32(0.5874)]\n",
      "2025-01-16 22:32:11.490386: Epoch time: 110.6 s\n",
      "2025-01-16 22:32:12.091972: \n",
      "2025-01-16 22:32:12.092146: Epoch 738\n",
      "2025-01-16 22:32:12.092222: Current learning rate: 0.003\n",
      "2025-01-16 22:34:02.848693: train_loss -0.8392\n",
      "2025-01-16 22:34:02.848884: val_loss -0.6276\n",
      "2025-01-16 22:34:02.848929: Pseudo dice [np.float32(0.6164)]\n",
      "2025-01-16 22:34:02.848965: Epoch time: 110.76 s\n",
      "2025-01-16 22:34:03.453580: \n",
      "2025-01-16 22:34:03.453682: Epoch 739\n",
      "2025-01-16 22:34:03.453748: Current learning rate: 0.00299\n",
      "2025-01-16 22:35:54.182154: train_loss -0.8072\n",
      "2025-01-16 22:35:54.182291: val_loss -0.7475\n",
      "2025-01-16 22:35:54.182327: Pseudo dice [np.float32(0.7758)]\n",
      "2025-01-16 22:35:54.182361: Epoch time: 110.73 s\n",
      "2025-01-16 22:35:54.779594: \n",
      "2025-01-16 22:35:54.779842: Epoch 740\n",
      "2025-01-16 22:35:54.780020: Current learning rate: 0.00297\n",
      "2025-01-16 22:37:45.464683: train_loss -0.7999\n",
      "2025-01-16 22:37:45.465021: val_loss -0.6297\n",
      "2025-01-16 22:37:45.465063: Pseudo dice [np.float32(0.6371)]\n",
      "2025-01-16 22:37:45.465096: Epoch time: 110.69 s\n",
      "2025-01-16 22:37:46.067147: \n",
      "2025-01-16 22:37:46.067530: Epoch 741\n",
      "2025-01-16 22:37:46.067677: Current learning rate: 0.00296\n",
      "2025-01-16 22:39:36.793591: train_loss -0.8304\n",
      "2025-01-16 22:39:36.793888: val_loss -0.6196\n",
      "2025-01-16 22:39:36.793994: Pseudo dice [np.float32(0.6361)]\n",
      "2025-01-16 22:39:36.794037: Epoch time: 110.73 s\n",
      "2025-01-16 22:39:37.393660: \n",
      "2025-01-16 22:39:37.393755: Epoch 742\n",
      "2025-01-16 22:39:37.393817: Current learning rate: 0.00295\n",
      "2025-01-16 22:41:28.162069: train_loss -0.8175\n",
      "2025-01-16 22:41:28.162211: val_loss -0.6672\n",
      "2025-01-16 22:41:28.162324: Pseudo dice [np.float32(0.6849)]\n",
      "2025-01-16 22:41:28.162452: Epoch time: 110.77 s\n",
      "2025-01-16 22:41:28.764109: \n",
      "2025-01-16 22:41:28.764520: Epoch 743\n",
      "2025-01-16 22:41:28.764600: Current learning rate: 0.00294\n",
      "2025-01-16 22:43:19.488653: train_loss -0.8127\n",
      "2025-01-16 22:43:19.488772: val_loss -0.6823\n",
      "2025-01-16 22:43:19.488804: Pseudo dice [np.float32(0.6779)]\n",
      "2025-01-16 22:43:19.488835: Epoch time: 110.73 s\n",
      "2025-01-16 22:43:20.089880: \n",
      "2025-01-16 22:43:20.090216: Epoch 744\n",
      "2025-01-16 22:43:20.090292: Current learning rate: 0.00293\n",
      "2025-01-16 22:45:10.889209: train_loss -0.811\n",
      "2025-01-16 22:45:10.889412: val_loss -0.5911\n",
      "2025-01-16 22:45:10.889549: Pseudo dice [np.float32(0.6983)]\n",
      "2025-01-16 22:45:10.889634: Epoch time: 110.8 s\n",
      "2025-01-16 22:45:11.735036: \n",
      "2025-01-16 22:45:11.735201: Epoch 745\n",
      "2025-01-16 22:45:11.735310: Current learning rate: 0.00292\n",
      "2025-01-16 22:47:02.310699: train_loss -0.8239\n",
      "2025-01-16 22:47:02.310830: val_loss -0.5929\n",
      "2025-01-16 22:47:02.310863: Pseudo dice [np.float32(0.6208)]\n",
      "2025-01-16 22:47:02.310897: Epoch time: 110.58 s\n",
      "2025-01-16 22:47:02.916605: \n",
      "2025-01-16 22:47:02.916717: Epoch 746\n",
      "2025-01-16 22:47:02.916779: Current learning rate: 0.00291\n",
      "2025-01-16 22:48:53.473486: train_loss -0.8303\n",
      "2025-01-16 22:48:53.473617: val_loss -0.5296\n",
      "2025-01-16 22:48:53.473651: Pseudo dice [np.float32(0.5623)]\n",
      "2025-01-16 22:48:53.473685: Epoch time: 110.56 s\n",
      "2025-01-16 22:48:54.073501: \n",
      "2025-01-16 22:48:54.073722: Epoch 747\n",
      "2025-01-16 22:48:54.073871: Current learning rate: 0.0029\n",
      "2025-01-16 22:50:44.869057: train_loss -0.8061\n",
      "2025-01-16 22:50:44.869250: val_loss -0.7118\n",
      "2025-01-16 22:50:44.869329: Pseudo dice [np.float32(0.7014)]\n",
      "2025-01-16 22:50:44.869374: Epoch time: 110.8 s\n",
      "2025-01-16 22:50:45.481017: \n",
      "2025-01-16 22:50:45.481393: Epoch 748\n",
      "2025-01-16 22:50:45.481528: Current learning rate: 0.00289\n",
      "2025-01-16 22:52:36.181393: train_loss -0.8112\n",
      "2025-01-16 22:52:36.181800: val_loss -0.6754\n",
      "2025-01-16 22:52:36.181845: Pseudo dice [np.float32(0.6712)]\n",
      "2025-01-16 22:52:36.181880: Epoch time: 110.7 s\n",
      "2025-01-16 22:52:36.784934: \n",
      "2025-01-16 22:52:36.785020: Epoch 749\n",
      "2025-01-16 22:52:36.785082: Current learning rate: 0.00288\n",
      "2025-01-16 22:54:27.486269: train_loss -0.841\n",
      "2025-01-16 22:54:27.486500: val_loss -0.6594\n",
      "2025-01-16 22:54:27.486619: Pseudo dice [np.float32(0.6867)]\n",
      "2025-01-16 22:54:27.486666: Epoch time: 110.7 s\n",
      "2025-01-16 22:54:28.332294: \n",
      "2025-01-16 22:54:28.332640: Epoch 750\n",
      "2025-01-16 22:54:28.332715: Current learning rate: 0.00287\n",
      "2025-01-16 22:56:19.121883: train_loss -0.8269\n",
      "2025-01-16 22:56:19.122078: val_loss -0.6544\n",
      "2025-01-16 22:56:19.122112: Pseudo dice [np.float32(0.6879)]\n",
      "2025-01-16 22:56:19.122143: Epoch time: 110.79 s\n",
      "2025-01-16 22:56:19.723283: \n",
      "2025-01-16 22:56:19.723631: Epoch 751\n",
      "2025-01-16 22:56:19.723781: Current learning rate: 0.00286\n",
      "2025-01-16 22:58:10.480377: train_loss -0.8203\n",
      "2025-01-16 22:58:10.480553: val_loss -0.6802\n",
      "2025-01-16 22:58:10.480586: Pseudo dice [np.float32(0.6615)]\n",
      "2025-01-16 22:58:10.480618: Epoch time: 110.76 s\n",
      "2025-01-16 22:58:11.085762: \n",
      "2025-01-16 22:58:11.086171: Epoch 752\n",
      "2025-01-16 22:58:11.086259: Current learning rate: 0.00285\n",
      "2025-01-16 23:00:01.793048: train_loss -0.8399\n",
      "2025-01-16 23:00:01.793272: val_loss -0.6916\n",
      "2025-01-16 23:00:01.793319: Pseudo dice [np.float32(0.694)]\n",
      "2025-01-16 23:00:01.793360: Epoch time: 110.71 s\n",
      "2025-01-16 23:00:02.403836: \n",
      "2025-01-16 23:00:02.404202: Epoch 753\n",
      "2025-01-16 23:00:02.404330: Current learning rate: 0.00284\n",
      "2025-01-16 23:01:52.983891: train_loss -0.8253\n",
      "2025-01-16 23:01:52.984035: val_loss -0.6784\n",
      "2025-01-16 23:01:52.984069: Pseudo dice [np.float32(0.5996)]\n",
      "2025-01-16 23:01:52.984101: Epoch time: 110.58 s\n",
      "2025-01-16 23:01:53.588263: \n",
      "2025-01-16 23:01:53.588678: Epoch 754\n",
      "2025-01-16 23:01:53.588785: Current learning rate: 0.00283\n",
      "2025-01-16 23:03:44.319172: train_loss -0.8415\n",
      "2025-01-16 23:03:44.319366: val_loss -0.7024\n",
      "2025-01-16 23:03:44.319402: Pseudo dice [np.float32(0.6932)]\n",
      "2025-01-16 23:03:44.319434: Epoch time: 110.73 s\n",
      "2025-01-16 23:03:44.923562: \n",
      "2025-01-16 23:03:44.923918: Epoch 755\n",
      "2025-01-16 23:03:44.923995: Current learning rate: 0.00282\n",
      "2025-01-16 23:05:35.629613: train_loss -0.8353\n",
      "2025-01-16 23:05:35.629738: val_loss -0.6823\n",
      "2025-01-16 23:05:35.629773: Pseudo dice [np.float32(0.6527)]\n",
      "2025-01-16 23:05:35.629807: Epoch time: 110.71 s\n",
      "2025-01-16 23:05:36.242996: \n",
      "2025-01-16 23:05:36.243196: Epoch 756\n",
      "2025-01-16 23:05:36.243315: Current learning rate: 0.00281\n",
      "2025-01-16 23:07:26.992261: train_loss -0.8213\n",
      "2025-01-16 23:07:26.992383: val_loss -0.7197\n",
      "2025-01-16 23:07:26.992417: Pseudo dice [np.float32(0.6459)]\n",
      "2025-01-16 23:07:26.992449: Epoch time: 110.75 s\n",
      "2025-01-16 23:07:27.835248: \n",
      "2025-01-16 23:07:27.835438: Epoch 757\n",
      "2025-01-16 23:07:27.835565: Current learning rate: 0.0028\n",
      "2025-01-16 23:09:18.579546: train_loss -0.8419\n",
      "2025-01-16 23:09:18.579711: val_loss -0.5998\n",
      "2025-01-16 23:09:18.579835: Pseudo dice [np.float32(0.5972)]\n",
      "2025-01-16 23:09:18.579938: Epoch time: 110.74 s\n",
      "2025-01-16 23:09:19.185763: \n",
      "2025-01-16 23:09:19.185880: Epoch 758\n",
      "2025-01-16 23:09:19.185945: Current learning rate: 0.00279\n",
      "2025-01-16 23:11:09.937142: train_loss -0.8288\n",
      "2025-01-16 23:11:09.937322: val_loss -0.5978\n",
      "2025-01-16 23:11:09.937356: Pseudo dice [np.float32(0.6628)]\n",
      "2025-01-16 23:11:09.937387: Epoch time: 110.75 s\n",
      "2025-01-16 23:11:10.539697: \n",
      "2025-01-16 23:11:10.540051: Epoch 759\n",
      "2025-01-16 23:11:10.540209: Current learning rate: 0.00278\n",
      "2025-01-16 23:13:01.235124: train_loss -0.8302\n",
      "2025-01-16 23:13:01.235252: val_loss -0.6363\n",
      "2025-01-16 23:13:01.235299: Pseudo dice [np.float32(0.6757)]\n",
      "2025-01-16 23:13:01.235337: Epoch time: 110.7 s\n",
      "2025-01-16 23:13:01.945525: \n",
      "2025-01-16 23:13:01.945880: Epoch 760\n",
      "2025-01-16 23:13:01.946065: Current learning rate: 0.00277\n",
      "2025-01-16 23:14:52.719459: train_loss -0.8282\n",
      "2025-01-16 23:14:52.719577: val_loss -0.6255\n",
      "2025-01-16 23:14:52.719609: Pseudo dice [np.float32(0.5371)]\n",
      "2025-01-16 23:14:52.719764: Epoch time: 110.77 s\n",
      "2025-01-16 23:14:53.330809: \n",
      "2025-01-16 23:14:53.331134: Epoch 761\n",
      "2025-01-16 23:14:53.331225: Current learning rate: 0.00276\n",
      "2025-01-16 23:16:44.113551: train_loss -0.8491\n",
      "2025-01-16 23:16:44.113678: val_loss -0.7591\n",
      "2025-01-16 23:16:44.113710: Pseudo dice [np.float32(0.7806)]\n",
      "2025-01-16 23:16:44.113742: Epoch time: 110.78 s\n",
      "2025-01-16 23:16:44.722941: \n",
      "2025-01-16 23:16:44.723302: Epoch 762\n",
      "2025-01-16 23:16:44.723373: Current learning rate: 0.00275\n",
      "2025-01-16 23:18:35.413788: train_loss -0.8315\n",
      "2025-01-16 23:18:35.413909: val_loss -0.7236\n",
      "2025-01-16 23:18:35.413940: Pseudo dice [np.float32(0.723)]\n",
      "2025-01-16 23:18:35.413975: Epoch time: 110.69 s\n",
      "2025-01-16 23:18:36.016815: \n",
      "2025-01-16 23:18:36.017226: Epoch 763\n",
      "2025-01-16 23:18:36.017324: Current learning rate: 0.00274\n",
      "2025-01-16 23:20:26.720140: train_loss -0.8356\n",
      "2025-01-16 23:20:26.720283: val_loss -0.682\n",
      "2025-01-16 23:20:26.720319: Pseudo dice [np.float32(0.6733)]\n",
      "2025-01-16 23:20:26.720352: Epoch time: 110.7 s\n",
      "2025-01-16 23:20:27.330513: \n",
      "2025-01-16 23:20:27.330871: Epoch 764\n",
      "2025-01-16 23:20:27.330951: Current learning rate: 0.00273\n",
      "2025-01-16 23:22:18.116810: train_loss -0.8303\n",
      "2025-01-16 23:22:18.116940: val_loss -0.6397\n",
      "2025-01-16 23:22:18.116975: Pseudo dice [np.float32(0.6467)]\n",
      "2025-01-16 23:22:18.117007: Epoch time: 110.79 s\n",
      "2025-01-16 23:22:18.729403: \n",
      "2025-01-16 23:22:18.729671: Epoch 765\n",
      "2025-01-16 23:22:18.729765: Current learning rate: 0.00272\n",
      "2025-01-16 23:24:09.465911: train_loss -0.8269\n",
      "2025-01-16 23:24:09.466052: val_loss -0.6346\n",
      "2025-01-16 23:24:09.466085: Pseudo dice [np.float32(0.7018)]\n",
      "2025-01-16 23:24:09.466119: Epoch time: 110.74 s\n",
      "2025-01-16 23:24:10.071339: \n",
      "2025-01-16 23:24:10.071723: Epoch 766\n",
      "2025-01-16 23:24:10.071821: Current learning rate: 0.00271\n",
      "2025-01-16 23:26:00.850441: train_loss -0.8247\n",
      "2025-01-16 23:26:00.850569: val_loss -0.6205\n",
      "2025-01-16 23:26:00.850605: Pseudo dice [np.float32(0.6792)]\n",
      "2025-01-16 23:26:00.850638: Epoch time: 110.78 s\n",
      "2025-01-16 23:26:01.457998: \n",
      "2025-01-16 23:26:01.458158: Epoch 767\n",
      "2025-01-16 23:26:01.458230: Current learning rate: 0.0027\n",
      "2025-01-16 23:27:52.085834: train_loss -0.8054\n",
      "2025-01-16 23:27:52.085976: val_loss -0.5821\n",
      "2025-01-16 23:27:52.086015: Pseudo dice [np.float32(0.6848)]\n",
      "2025-01-16 23:27:52.086047: Epoch time: 110.63 s\n",
      "2025-01-16 23:27:52.698827: \n",
      "2025-01-16 23:27:52.699189: Epoch 768\n",
      "2025-01-16 23:27:52.699340: Current learning rate: 0.00268\n",
      "2025-01-16 23:29:43.469229: train_loss -0.8295\n",
      "2025-01-16 23:29:43.469354: val_loss -0.6298\n",
      "2025-01-16 23:29:43.469387: Pseudo dice [np.float32(0.6065)]\n",
      "2025-01-16 23:29:43.469418: Epoch time: 110.77 s\n",
      "2025-01-16 23:29:44.339605: \n",
      "2025-01-16 23:29:44.339990: Epoch 769\n",
      "2025-01-16 23:29:44.340136: Current learning rate: 0.00267\n",
      "2025-01-16 23:31:34.958778: train_loss -0.8221\n",
      "2025-01-16 23:31:34.958907: val_loss -0.7122\n",
      "2025-01-16 23:31:34.958944: Pseudo dice [np.float32(0.7232)]\n",
      "2025-01-16 23:31:34.958976: Epoch time: 110.62 s\n",
      "2025-01-16 23:31:35.569076: \n",
      "2025-01-16 23:31:35.569449: Epoch 770\n",
      "2025-01-16 23:31:35.569646: Current learning rate: 0.00266\n",
      "2025-01-16 23:33:26.368824: train_loss -0.8353\n",
      "2025-01-16 23:33:26.368962: val_loss -0.7512\n",
      "2025-01-16 23:33:26.368998: Pseudo dice [np.float32(0.7298)]\n",
      "2025-01-16 23:33:26.369114: Epoch time: 110.8 s\n",
      "2025-01-16 23:33:26.986412: \n",
      "2025-01-16 23:33:26.986745: Epoch 771\n",
      "2025-01-16 23:33:26.987005: Current learning rate: 0.00265\n",
      "2025-01-16 23:35:17.645533: train_loss -0.8218\n",
      "2025-01-16 23:35:17.645664: val_loss -0.756\n",
      "2025-01-16 23:35:17.645700: Pseudo dice [np.float32(0.7254)]\n",
      "2025-01-16 23:35:17.645733: Epoch time: 110.66 s\n",
      "2025-01-16 23:35:18.257678: \n",
      "2025-01-16 23:35:18.257782: Epoch 772\n",
      "2025-01-16 23:35:18.257862: Current learning rate: 0.00264\n",
      "2025-01-16 23:37:08.842804: train_loss -0.8245\n",
      "2025-01-16 23:37:08.842999: val_loss -0.7061\n",
      "2025-01-16 23:37:08.843034: Pseudo dice [np.float32(0.676)]\n",
      "2025-01-16 23:37:08.843067: Epoch time: 110.59 s\n",
      "2025-01-16 23:37:09.463502: \n",
      "2025-01-16 23:37:09.463593: Epoch 773\n",
      "2025-01-16 23:37:09.463655: Current learning rate: 0.00263\n",
      "2025-01-16 23:39:00.036671: train_loss -0.8486\n",
      "2025-01-16 23:39:00.036798: val_loss -0.6137\n",
      "2025-01-16 23:39:00.036833: Pseudo dice [np.float32(0.692)]\n",
      "2025-01-16 23:39:00.036868: Epoch time: 110.57 s\n",
      "2025-01-16 23:39:00.645141: \n",
      "2025-01-16 23:39:00.645323: Epoch 774\n",
      "2025-01-16 23:39:00.645491: Current learning rate: 0.00262\n",
      "2025-01-16 23:40:51.388801: train_loss -0.8352\n",
      "2025-01-16 23:40:51.388934: val_loss -0.7324\n",
      "2025-01-16 23:40:51.388968: Pseudo dice [np.float32(0.744)]\n",
      "2025-01-16 23:40:51.389002: Epoch time: 110.74 s\n",
      "2025-01-16 23:40:52.007838: \n",
      "2025-01-16 23:40:52.008041: Epoch 775\n",
      "2025-01-16 23:40:52.008115: Current learning rate: 0.00261\n",
      "2025-01-16 23:42:43.233973: train_loss -0.8251\n",
      "2025-01-16 23:42:43.234110: val_loss -0.6471\n",
      "2025-01-16 23:42:43.234143: Pseudo dice [np.float32(0.6707)]\n",
      "2025-01-16 23:42:43.234202: Epoch time: 111.23 s\n",
      "2025-01-16 23:42:43.856153: \n",
      "2025-01-16 23:42:43.856317: Epoch 776\n",
      "2025-01-16 23:42:43.856393: Current learning rate: 0.0026\n",
      "2025-01-16 23:44:35.993016: train_loss -0.8592\n",
      "2025-01-16 23:44:35.993142: val_loss -0.6551\n",
      "2025-01-16 23:44:35.993174: Pseudo dice [np.float32(0.6177)]\n",
      "2025-01-16 23:44:35.993222: Epoch time: 112.14 s\n",
      "2025-01-16 23:44:36.609945: \n",
      "2025-01-16 23:44:36.610041: Epoch 777\n",
      "2025-01-16 23:44:36.610205: Current learning rate: 0.00259\n",
      "2025-01-16 23:46:27.815032: train_loss -0.8487\n",
      "2025-01-16 23:46:27.815192: val_loss -0.6644\n",
      "2025-01-16 23:46:27.815366: Pseudo dice [np.float32(0.6595)]\n",
      "2025-01-16 23:46:27.815442: Epoch time: 111.21 s\n",
      "2025-01-16 23:46:28.435580: \n",
      "2025-01-16 23:46:28.435672: Epoch 778\n",
      "2025-01-16 23:46:28.435733: Current learning rate: 0.00258\n",
      "2025-01-16 23:48:19.567659: train_loss -0.8273\n",
      "2025-01-16 23:48:19.567773: val_loss -0.7491\n",
      "2025-01-16 23:48:19.567805: Pseudo dice [np.float32(0.7543)]\n",
      "2025-01-16 23:48:19.567836: Epoch time: 111.13 s\n",
      "2025-01-16 23:48:20.187654: \n",
      "2025-01-16 23:48:20.187740: Epoch 779\n",
      "2025-01-16 23:48:20.187804: Current learning rate: 0.00257\n",
      "2025-01-16 23:50:11.431244: train_loss -0.8435\n",
      "2025-01-16 23:50:11.431365: val_loss -0.6751\n",
      "2025-01-16 23:50:11.431398: Pseudo dice [np.float32(0.6981)]\n",
      "2025-01-16 23:50:11.431431: Epoch time: 111.24 s\n",
      "2025-01-16 23:50:12.056754: \n",
      "2025-01-16 23:50:12.056853: Epoch 780\n",
      "2025-01-16 23:50:12.057017: Current learning rate: 0.00256\n",
      "2025-01-16 23:52:03.487203: train_loss -0.8282\n",
      "2025-01-16 23:52:03.487344: val_loss -0.5866\n",
      "2025-01-16 23:52:03.487378: Pseudo dice [np.float32(0.61)]\n",
      "2025-01-16 23:52:03.487410: Epoch time: 111.43 s\n",
      "2025-01-16 23:52:04.377055: \n",
      "2025-01-16 23:52:04.377600: Epoch 781\n",
      "2025-01-16 23:52:04.377682: Current learning rate: 0.00255\n",
      "2025-01-16 23:53:55.673819: train_loss -0.8465\n",
      "2025-01-16 23:53:55.673959: val_loss -0.525\n",
      "2025-01-16 23:53:55.673993: Pseudo dice [np.float32(0.5844)]\n",
      "2025-01-16 23:53:55.674027: Epoch time: 111.3 s\n",
      "2025-01-16 23:53:56.293480: \n",
      "2025-01-16 23:53:56.293582: Epoch 782\n",
      "2025-01-16 23:53:56.293646: Current learning rate: 0.00254\n",
      "2025-01-16 23:55:47.573017: train_loss -0.8314\n",
      "2025-01-16 23:55:47.573161: val_loss -0.7241\n",
      "2025-01-16 23:55:47.573209: Pseudo dice [np.float32(0.7718)]\n",
      "2025-01-16 23:55:47.573380: Epoch time: 111.28 s\n",
      "2025-01-16 23:55:48.194658: \n",
      "2025-01-16 23:55:48.194845: Epoch 783\n",
      "2025-01-16 23:55:48.195002: Current learning rate: 0.00253\n",
      "2025-01-16 23:57:39.467911: train_loss -0.8387\n",
      "2025-01-16 23:57:39.468048: val_loss -0.7188\n",
      "2025-01-16 23:57:39.468171: Pseudo dice [np.float32(0.7059)]\n",
      "2025-01-16 23:57:39.468241: Epoch time: 111.27 s\n",
      "2025-01-16 23:57:40.093946: \n",
      "2025-01-16 23:57:40.094184: Epoch 784\n",
      "2025-01-16 23:57:40.094259: Current learning rate: 0.00252\n",
      "2025-01-16 23:59:31.330067: train_loss -0.8204\n",
      "2025-01-16 23:59:31.330251: val_loss -0.6318\n",
      "2025-01-16 23:59:31.330378: Pseudo dice [np.float32(0.6367)]\n",
      "2025-01-16 23:59:31.330415: Epoch time: 111.24 s\n",
      "2025-01-16 23:59:31.946197: \n",
      "2025-01-16 23:59:31.946284: Epoch 785\n",
      "2025-01-16 23:59:31.946345: Current learning rate: 0.00251\n",
      "2025-01-17 00:01:23.058170: train_loss -0.8321\n",
      "2025-01-17 00:01:23.058296: val_loss -0.6693\n",
      "2025-01-17 00:01:23.058329: Pseudo dice [np.float32(0.751)]\n",
      "2025-01-17 00:01:23.058361: Epoch time: 111.11 s\n",
      "2025-01-17 00:01:23.680918: \n",
      "2025-01-17 00:01:23.681238: Epoch 786\n",
      "2025-01-17 00:01:23.681313: Current learning rate: 0.0025\n",
      "2025-01-17 00:03:14.924496: train_loss -0.8262\n",
      "2025-01-17 00:03:14.924620: val_loss -0.7295\n",
      "2025-01-17 00:03:14.924653: Pseudo dice [np.float32(0.7045)]\n",
      "2025-01-17 00:03:14.924687: Epoch time: 111.24 s\n",
      "2025-01-17 00:03:15.545544: \n",
      "2025-01-17 00:03:15.545638: Epoch 787\n",
      "2025-01-17 00:03:15.545700: Current learning rate: 0.00249\n",
      "2025-01-17 00:05:06.833097: train_loss -0.8389\n",
      "2025-01-17 00:05:06.833231: val_loss -0.4646\n",
      "2025-01-17 00:05:06.833266: Pseudo dice [np.float32(0.6423)]\n",
      "2025-01-17 00:05:06.833301: Epoch time: 111.29 s\n",
      "2025-01-17 00:05:07.454958: \n",
      "2025-01-17 00:05:07.455127: Epoch 788\n",
      "2025-01-17 00:05:07.455208: Current learning rate: 0.00248\n",
      "2025-01-17 00:06:58.719730: train_loss -0.8267\n",
      "2025-01-17 00:06:58.720040: val_loss -0.6083\n",
      "2025-01-17 00:06:58.720084: Pseudo dice [np.float32(0.6405)]\n",
      "2025-01-17 00:06:58.720120: Epoch time: 111.27 s\n",
      "2025-01-17 00:06:59.336802: \n",
      "2025-01-17 00:06:59.336895: Epoch 789\n",
      "2025-01-17 00:06:59.336955: Current learning rate: 0.00247\n",
      "2025-01-17 00:08:50.595811: train_loss -0.8083\n",
      "2025-01-17 00:08:50.595937: val_loss -0.7121\n",
      "2025-01-17 00:08:50.595972: Pseudo dice [np.float32(0.6761)]\n",
      "2025-01-17 00:08:50.596004: Epoch time: 111.26 s\n",
      "2025-01-17 00:08:51.211555: \n",
      "2025-01-17 00:08:51.211919: Epoch 790\n",
      "2025-01-17 00:08:51.211994: Current learning rate: 0.00245\n",
      "2025-01-17 00:10:42.492638: train_loss -0.8044\n",
      "2025-01-17 00:10:42.492828: val_loss -0.5721\n",
      "2025-01-17 00:10:42.492862: Pseudo dice [np.float32(0.6144)]\n",
      "2025-01-17 00:10:42.492896: Epoch time: 111.28 s\n",
      "2025-01-17 00:10:43.115156: \n",
      "2025-01-17 00:10:43.115461: Epoch 791\n",
      "2025-01-17 00:10:43.115600: Current learning rate: 0.00244\n",
      "2025-01-17 00:12:34.322011: train_loss -0.8374\n",
      "2025-01-17 00:12:34.322147: val_loss -0.5392\n",
      "2025-01-17 00:12:34.322179: Pseudo dice [np.float32(0.6157)]\n",
      "2025-01-17 00:12:34.322381: Epoch time: 111.21 s\n",
      "2025-01-17 00:12:35.181771: \n",
      "2025-01-17 00:12:35.181957: Epoch 792\n",
      "2025-01-17 00:12:35.182132: Current learning rate: 0.00243\n",
      "2025-01-17 00:14:26.421256: train_loss -0.8398\n",
      "2025-01-17 00:14:26.421382: val_loss -0.7149\n",
      "2025-01-17 00:14:26.421415: Pseudo dice [np.float32(0.7925)]\n",
      "2025-01-17 00:14:26.421448: Epoch time: 111.24 s\n",
      "2025-01-17 00:14:27.043819: \n",
      "2025-01-17 00:14:27.043996: Epoch 793\n",
      "2025-01-17 00:14:27.044074: Current learning rate: 0.00242\n",
      "2025-01-17 00:16:18.321683: train_loss -0.8352\n",
      "2025-01-17 00:16:18.321817: val_loss -0.6736\n",
      "2025-01-17 00:16:18.321856: Pseudo dice [np.float32(0.8119)]\n",
      "2025-01-17 00:16:18.321890: Epoch time: 111.28 s\n",
      "2025-01-17 00:16:18.944929: \n",
      "2025-01-17 00:16:18.945331: Epoch 794\n",
      "2025-01-17 00:16:18.945562: Current learning rate: 0.00241\n",
      "2025-01-17 00:18:10.232307: train_loss -0.8309\n",
      "2025-01-17 00:18:10.232443: val_loss -0.6032\n",
      "2025-01-17 00:18:10.232475: Pseudo dice [np.float32(0.6584)]\n",
      "2025-01-17 00:18:10.232509: Epoch time: 111.29 s\n",
      "2025-01-17 00:18:10.857062: \n",
      "2025-01-17 00:18:10.857384: Epoch 795\n",
      "2025-01-17 00:18:10.857538: Current learning rate: 0.0024\n",
      "2025-01-17 00:20:02.171641: train_loss -0.8367\n",
      "2025-01-17 00:20:02.171828: val_loss -0.6702\n",
      "2025-01-17 00:20:02.171861: Pseudo dice [np.float32(0.7054)]\n",
      "2025-01-17 00:20:02.171949: Epoch time: 111.32 s\n",
      "2025-01-17 00:20:02.796152: \n",
      "2025-01-17 00:20:02.796255: Epoch 796\n",
      "2025-01-17 00:20:02.796317: Current learning rate: 0.00239\n",
      "2025-01-17 00:21:54.073357: train_loss -0.8593\n",
      "2025-01-17 00:21:54.073719: val_loss -0.6124\n",
      "2025-01-17 00:21:54.073826: Pseudo dice [np.float32(0.6211)]\n",
      "2025-01-17 00:21:54.073892: Epoch time: 111.28 s\n",
      "2025-01-17 00:21:54.688396: \n",
      "2025-01-17 00:21:54.688492: Epoch 797\n",
      "2025-01-17 00:21:54.688557: Current learning rate: 0.00238\n",
      "2025-01-17 00:23:45.998688: train_loss -0.8489\n",
      "2025-01-17 00:23:45.998911: val_loss -0.6054\n",
      "2025-01-17 00:23:45.998955: Pseudo dice [np.float32(0.6452)]\n",
      "2025-01-17 00:23:45.998995: Epoch time: 111.31 s\n",
      "2025-01-17 00:23:46.625432: \n",
      "2025-01-17 00:23:46.625614: Epoch 798\n",
      "2025-01-17 00:23:46.625761: Current learning rate: 0.00237\n",
      "2025-01-17 00:25:37.900379: train_loss -0.8387\n",
      "2025-01-17 00:25:37.900506: val_loss -0.667\n",
      "2025-01-17 00:25:37.900539: Pseudo dice [np.float32(0.7337)]\n",
      "2025-01-17 00:25:37.900572: Epoch time: 111.28 s\n",
      "2025-01-17 00:25:38.526398: \n",
      "2025-01-17 00:25:38.526759: Epoch 799\n",
      "2025-01-17 00:25:38.526973: Current learning rate: 0.00236\n",
      "2025-01-17 00:27:29.848366: train_loss -0.832\n",
      "2025-01-17 00:27:29.848490: val_loss -0.6919\n",
      "2025-01-17 00:27:29.848522: Pseudo dice [np.float32(0.7189)]\n",
      "2025-01-17 00:27:29.848557: Epoch time: 111.32 s\n",
      "2025-01-17 00:27:30.726239: \n",
      "2025-01-17 00:27:30.726329: Epoch 800\n",
      "2025-01-17 00:27:30.726391: Current learning rate: 0.00235\n",
      "2025-01-17 00:29:21.899411: train_loss -0.8274\n",
      "2025-01-17 00:29:21.899606: val_loss -0.6913\n",
      "2025-01-17 00:29:21.899642: Pseudo dice [np.float32(0.7111)]\n",
      "2025-01-17 00:29:21.899675: Epoch time: 111.17 s\n",
      "2025-01-17 00:29:22.515784: \n",
      "2025-01-17 00:29:22.516222: Epoch 801\n",
      "2025-01-17 00:29:22.516325: Current learning rate: 0.00234\n",
      "2025-01-17 00:31:13.983048: train_loss -0.8272\n",
      "2025-01-17 00:31:13.983379: val_loss -0.7582\n",
      "2025-01-17 00:31:13.983621: Pseudo dice [np.float32(0.7575)]\n",
      "2025-01-17 00:31:13.983687: Epoch time: 111.47 s\n",
      "2025-01-17 00:31:14.604434: \n",
      "2025-01-17 00:31:14.604726: Epoch 802\n",
      "2025-01-17 00:31:14.604804: Current learning rate: 0.00233\n",
      "2025-01-17 00:33:05.965909: train_loss -0.826\n",
      "2025-01-17 00:33:05.966056: val_loss -0.7146\n",
      "2025-01-17 00:33:05.966096: Pseudo dice [np.float32(0.7247)]\n",
      "2025-01-17 00:33:05.966128: Epoch time: 111.36 s\n",
      "2025-01-17 00:33:06.576695: \n",
      "2025-01-17 00:33:06.576785: Epoch 803\n",
      "2025-01-17 00:33:06.576844: Current learning rate: 0.00232\n",
      "2025-01-17 00:34:57.829934: train_loss -0.8505\n",
      "2025-01-17 00:34:57.830130: val_loss -0.6734\n",
      "2025-01-17 00:34:57.830175: Pseudo dice [np.float32(0.6959)]\n",
      "2025-01-17 00:34:57.830234: Epoch time: 111.25 s\n",
      "2025-01-17 00:34:58.716909: \n",
      "2025-01-17 00:34:58.717134: Epoch 804\n",
      "2025-01-17 00:34:58.717209: Current learning rate: 0.00231\n",
      "2025-01-17 00:36:49.835497: train_loss -0.8459\n",
      "2025-01-17 00:36:49.835756: val_loss -0.7272\n",
      "2025-01-17 00:36:49.835872: Pseudo dice [np.float32(0.6913)]\n",
      "2025-01-17 00:36:49.835929: Epoch time: 111.12 s\n",
      "2025-01-17 00:36:50.459150: \n",
      "2025-01-17 00:36:50.459517: Epoch 805\n",
      "2025-01-17 00:36:50.459605: Current learning rate: 0.0023\n",
      "2025-01-17 00:38:41.648092: train_loss -0.8425\n",
      "2025-01-17 00:38:41.648244: val_loss -0.7461\n",
      "2025-01-17 00:38:41.648280: Pseudo dice [np.float32(0.7509)]\n",
      "2025-01-17 00:38:41.648312: Epoch time: 111.19 s\n",
      "2025-01-17 00:38:41.648338: Yayy! New best EMA pseudo Dice: 0.7039999961853027\n",
      "2025-01-17 00:38:42.610981: \n",
      "2025-01-17 00:38:42.611310: Epoch 806\n",
      "2025-01-17 00:38:42.611399: Current learning rate: 0.00229\n",
      "2025-01-17 00:40:33.784131: train_loss -0.84\n",
      "2025-01-17 00:40:33.784303: val_loss -0.7051\n",
      "2025-01-17 00:40:33.784347: Pseudo dice [np.float32(0.7077)]\n",
      "2025-01-17 00:40:33.784384: Epoch time: 111.17 s\n",
      "2025-01-17 00:40:33.784405: Yayy! New best EMA pseudo Dice: 0.7044000029563904\n",
      "2025-01-17 00:40:34.701485: \n",
      "Epoch 8077 00:40:34.701668: \n",
      "2025-01-17 00:40:34.701824: Current learning rate: 0.00228\n",
      "2025-01-17 00:42:25.840370: train_loss -0.8337\n",
      "2025-01-17 00:42:25.840560: val_loss -0.6833\n",
      "2025-01-17 00:42:25.840605: Pseudo dice [np.float32(0.7372)]\n",
      "2025-01-17 00:42:25.840640: Epoch time: 111.14 s\n",
      "2025-01-17 00:42:25.840662: Yayy! New best EMA pseudo Dice: 0.7077000141143799\n",
      "2025-01-17 00:42:26.699510: \n",
      "2025-01-17 00:42:26.699867: Epoch 808\n",
      "2025-01-17 00:42:26.699991: Current learning rate: 0.00226\n",
      "2025-01-17 00:44:17.632766: train_loss -0.8326\n",
      "2025-01-17 00:44:17.632888: val_loss -0.6372\n",
      "2025-01-17 00:44:17.632920: Pseudo dice [np.float32(0.6675)]\n",
      "2025-01-17 00:44:17.632966: Epoch time: 110.93 s\n",
      "2025-01-17 00:44:18.244280: \n",
      "2025-01-17 00:44:18.244371: Epoch 809\n",
      "2025-01-17 00:44:18.244433: Current learning rate: 0.00225\n",
      "2025-01-17 00:46:08.963514: train_loss -0.8316\n",
      "2025-01-17 00:46:08.963644: val_loss -0.6656\n",
      "2025-01-17 00:46:08.963677: Pseudo dice [np.float32(0.6733)]\n",
      "2025-01-17 00:46:08.963712: Epoch time: 110.72 s\n",
      "2025-01-17 00:46:09.578522: \n",
      "2025-01-17 00:46:09.578675: Epoch 810\n",
      "2025-01-17 00:46:09.578748: Current learning rate: 0.00224\n",
      "2025-01-17 00:48:00.380048: train_loss -0.8409\n",
      "2025-01-17 00:48:00.380243: val_loss -0.7079\n",
      "2025-01-17 00:48:00.380279: Pseudo dice [np.float32(0.706)]\n",
      "2025-01-17 00:48:00.380314: Epoch time: 110.8 s\n",
      "2025-01-17 00:48:00.992870: \n",
      "2025-01-17 00:48:00.992981: Epoch 811\n",
      "2025-01-17 00:48:00.993125: Current learning rate: 0.00223\n",
      "2025-01-17 00:49:51.723378: train_loss -0.8327\n",
      "2025-01-17 00:49:51.723559: val_loss -0.6713\n",
      "2025-01-17 00:49:51.723665: Pseudo dice [np.float32(0.6995)]\n",
      "2025-01-17 00:49:51.723795: Epoch time: 110.73 s\n",
      "2025-01-17 00:49:52.345572: \n",
      "2025-01-17 00:49:52.345952: Epoch 812\n",
      "2025-01-17 00:49:52.346095: Current learning rate: 0.00222\n",
      "2025-01-17 00:51:43.162014: train_loss -0.8651\n",
      "2025-01-17 00:51:43.162209: val_loss -0.7152\n",
      "2025-01-17 00:51:43.162254: Pseudo dice [np.float32(0.7481)]\n",
      "2025-01-17 00:51:43.162289: Epoch time: 110.82 s\n",
      "2025-01-17 00:51:43.776116: \n",
      "2025-01-17 00:51:43.776263: Epoch 813\n",
      "2025-01-17 00:51:43.776335: Current learning rate: 0.00221\n",
      "2025-01-17 00:53:34.364891: train_loss -0.8504\n",
      "2025-01-17 00:53:34.365021: val_loss -0.7453\n",
      "2025-01-17 00:53:34.365055: Pseudo dice [np.float32(0.7579)]\n",
      "2025-01-17 00:53:34.365089: Epoch time: 110.59 s\n",
      "2025-01-17 00:53:34.365111: Yayy! New best EMA pseudo Dice: 0.7109000086784363\n",
      "2025-01-17 00:53:35.213068: \n",
      "2025-01-17 00:53:35.213456: Epoch 814\n",
      "2025-01-17 00:53:35.213528: Current learning rate: 0.0022\n",
      "2025-01-17 00:55:25.833179: train_loss -0.8382\n",
      "2025-01-17 00:55:25.833315: val_loss -0.6857\n",
      "2025-01-17 00:55:25.833348: Pseudo dice [np.float32(0.7061)]\n",
      "2025-01-17 00:55:25.833382: Epoch time: 110.62 s\n",
      "2025-01-17 00:55:26.710937: \n",
      "2025-01-17 00:55:26.711143: Epoch 815\n",
      "2025-01-17 00:55:26.711218: Current learning rate: 0.00219\n",
      "2025-01-17 00:57:17.418019: train_loss -0.8526\n",
      "2025-01-17 00:57:17.418123: val_loss -0.611\n",
      "2025-01-17 00:57:17.418216: Pseudo dice [np.float32(0.7265)]\n",
      "2025-01-17 00:57:17.418299: Epoch time: 110.71 s\n",
      "2025-01-17 00:57:17.418355: Yayy! New best EMA pseudo Dice: 0.7121000289916992\n",
      "2025-01-17 00:57:18.268144: \n",
      "2025-01-17 00:57:18.268526: Epoch 816\n",
      "2025-01-17 00:57:18.268680: Current learning rate: 0.00218\n",
      "2025-01-17 00:59:09.030414: train_loss -0.8392\n",
      "2025-01-17 00:59:09.030556: val_loss -0.5956\n",
      "2025-01-17 00:59:09.030589: Pseudo dice [np.float32(0.6857)]\n",
      "2025-01-17 00:59:09.030622: Epoch time: 110.76 s\n",
      "2025-01-17 00:59:09.650102: \n",
      "2025-01-17 00:59:09.650213: Epoch 817\n",
      "2025-01-17 00:59:09.650320: Current learning rate: 0.00217\n",
      "2025-01-17 01:01:00.274509: train_loss -0.8531\n",
      "2025-01-17 01:01:00.274647: val_loss -0.5182\n",
      "2025-01-17 01:01:00.274687: Pseudo dice [np.float32(0.5975)]\n",
      "2025-01-17 01:01:00.274721: Epoch time: 110.62 s\n",
      "2025-01-17 01:01:00.889778: \n",
      "2025-01-17 01:01:00.889874: Epoch 818\n",
      "2025-01-17 01:01:00.889940: Current learning rate: 0.00216\n",
      "2025-01-17 01:02:51.632489: train_loss -0.8294\n",
      "2025-01-17 01:02:51.632618: val_loss -0.6985\n",
      "2025-01-17 01:02:51.632653: Pseudo dice [np.float32(0.7069)]\n",
      "2025-01-17 01:02:51.632685: Epoch time: 110.74 s\n",
      "2025-01-17 01:02:52.248519: \n",
      "2025-01-17 01:02:52.248613: Epoch 819\n",
      "2025-01-17 01:02:52.248677: Current learning rate: 0.00215\n",
      "2025-01-17 01:04:43.018957: train_loss -0.8403\n",
      "2025-01-17 01:04:43.019314: val_loss -0.6709\n",
      "2025-01-17 01:04:43.019455: Pseudo dice [np.float32(0.7485)]\n",
      "2025-01-17 01:04:43.019499: Epoch time: 110.77 s\n",
      "2025-01-17 01:04:43.613697: \n",
      "2025-01-17 01:04:43.614023: Epoch 820\n",
      "2025-01-17 01:04:43.614096: Current learning rate: 0.00214\n",
      "2025-01-17 01:06:34.182759: train_loss -0.8432\n",
      "2025-01-17 01:06:34.182885: val_loss -0.6878\n",
      "2025-01-17 01:06:34.182919: Pseudo dice [np.float32(0.7901)]\n",
      "2025-01-17 01:06:34.182952: Epoch time: 110.57 s\n",
      "2025-01-17 01:06:34.182977: Yayy! New best EMA pseudo Dice: 0.7125999927520752\n",
      "2025-01-17 01:06:35.015868: \n",
      "2025-01-17 01:06:35.016052: Epoch 821\n",
      "2025-01-17 01:06:35.016162: Current learning rate: 0.00213\n",
      "2025-01-17 01:08:25.752894: train_loss -0.8566\n",
      "2025-01-17 01:08:25.753025: val_loss -0.7401\n",
      "2025-01-17 01:08:25.753059: Pseudo dice [np.float32(0.7544)]\n",
      "2025-01-17 01:08:25.753094: Epoch time: 110.74 s\n",
      "2025-01-17 01:08:25.753115: Yayy! New best EMA pseudo Dice: 0.7167999744415283\n",
      "2025-01-17 01:08:26.583509: \n",
      "2025-01-17 01:08:26.583598: Epoch 822\n",
      "2025-01-17 01:08:26.583658: Current learning rate: 0.00212\n",
      "2025-01-17 01:10:17.421876: train_loss -0.859\n",
      "2025-01-17 01:10:17.422053: val_loss -0.6939\n",
      "2025-01-17 01:10:17.422104: Pseudo dice [np.float32(0.7001)]\n",
      "2025-01-17 01:10:17.422138: Epoch time: 110.84 s\n",
      "2025-01-17 01:10:18.017280: \n",
      "2025-01-17 01:10:18.017376: Epoch 823\n",
      "2025-01-17 01:10:18.017440: Current learning rate: 0.0021\n",
      "2025-01-17 01:12:08.631677: train_loss -0.8386\n",
      "2025-01-17 01:12:08.631808: val_loss -0.6822\n",
      "2025-01-17 01:12:08.631841: Pseudo dice [np.float32(0.7102)]\n",
      "2025-01-17 01:12:08.631875: Epoch time: 110.61 s\n",
      "2025-01-17 01:12:09.224726: \n",
      "2025-01-17 01:12:09.224860: Epoch 824\n",
      "2025-01-17 01:12:09.224933: Current learning rate: 0.00209\n",
      "2025-01-17 01:13:59.964793: train_loss -0.8437\n",
      "2025-01-17 01:13:59.964914: val_loss -0.7064\n",
      "2025-01-17 01:13:59.964945: Pseudo dice [np.float32(0.7303)]\n",
      "2025-01-17 01:13:59.964979: Epoch time: 110.74 s\n",
      "2025-01-17 01:14:00.556734: \n",
      "2025-01-17 01:14:00.556889: Epoch 825\n",
      "2025-01-17 01:14:00.556962: Current learning rate: 0.00208\n",
      "2025-01-17 01:15:51.353070: train_loss -0.8552\n",
      "2025-01-17 01:15:51.353204: val_loss -0.7268\n",
      "2025-01-17 01:15:51.353239: Pseudo dice [np.float32(0.7248)]\n",
      "2025-01-17 01:15:51.353273: Epoch time: 110.8 s\n",
      "2025-01-17 01:15:51.353293: Yayy! New best EMA pseudo Dice: 0.7171000242233276\n",
      "2025-01-17 01:15:52.419937: \n",
      "2025-01-17 01:15:52.420262: Epoch 826\n",
      "2025-01-17 01:15:52.420415: Current learning rate: 0.00207\n",
      "2025-01-17 01:17:42.973438: train_loss -0.825\n",
      "2025-01-17 01:17:42.973637: val_loss -0.5226\n",
      "2025-01-17 01:17:42.973672: Pseudo dice [np.float32(0.6995)]\n",
      "2025-01-17 01:17:42.973705: Epoch time: 110.55 s\n",
      "2025-01-17 01:17:43.566535: \n",
      "2025-01-17 01:17:43.566717: Epoch 827\n",
      "2025-01-17 01:17:43.566784: Current learning rate: 0.00206\n",
      "2025-01-17 01:19:34.304505: train_loss -0.8494\n",
      "2025-01-17 01:19:34.304636: val_loss -0.5884\n",
      "2025-01-17 01:19:34.304666: Pseudo dice [np.float32(0.578)]\n",
      "2025-01-17 01:19:34.304698: Epoch time: 110.74 s\n",
      "2025-01-17 01:19:34.891604: \n",
      "2025-01-17 01:19:34.892020: Epoch 828\n",
      "2025-01-17 01:19:34.892121: Current learning rate: 0.00205\n",
      "2025-01-17 01:21:25.551535: train_loss -0.8398\n",
      "2025-01-17 01:21:25.551698: val_loss -0.578\n",
      "2025-01-17 01:21:25.551765: Pseudo dice [np.float32(0.6719)]\n",
      "2025-01-17 01:21:25.551806: Epoch time: 110.66 s\n",
      "2025-01-17 01:21:26.145694: \n",
      "2025-01-17 01:21:26.146110: Epoch 829\n",
      "2025-01-17 01:21:26.146251: Current learning rate: 0.00204\n",
      "2025-01-17 01:23:16.770314: train_loss -0.824\n",
      "2025-01-17 01:23:16.770442: val_loss -0.5384\n",
      "2025-01-17 01:23:16.770478: Pseudo dice [np.float32(0.5993)]\n",
      "2025-01-17 01:23:16.770512: Epoch time: 110.63 s\n",
      "2025-01-17 01:23:17.359416: \n",
      "2025-01-17 01:23:17.359510: Epoch 830\n",
      "2025-01-17 01:23:17.359574: Current learning rate: 0.00203\n",
      "2025-01-17 01:25:08.056205: train_loss -0.8514\n",
      "2025-01-17 01:25:08.056372: val_loss -0.6595\n",
      "2025-01-17 01:25:08.056441: Pseudo dice [np.float32(0.686)]\n",
      "2025-01-17 01:25:08.056535: Epoch time: 110.7 s\n",
      "2025-01-17 01:25:08.665437: \n",
      "2025-01-17 01:25:08.665769: Epoch 831\n",
      "2025-01-17 01:25:08.665840: Current learning rate: 0.00202\n",
      "2025-01-17 01:26:59.426420: train_loss -0.8473\n",
      "2025-01-17 01:26:59.426551: val_loss -0.6206\n",
      "2025-01-17 01:26:59.426584: Pseudo dice [np.float32(0.7142)]\n",
      "2025-01-17 01:26:59.426789: Epoch time: 110.76 s\n",
      "2025-01-17 01:27:00.028631: \n",
      "2025-01-17 01:27:00.028815: Epoch 832\n",
      "2025-01-17 01:27:00.028888: Current learning rate: 0.00201\n",
      "2025-01-17 01:28:50.766454: train_loss -0.8512\n",
      "2025-01-17 01:28:50.766584: val_loss -0.5743\n",
      "2025-01-17 01:28:50.766617: Pseudo dice [np.float32(0.4166)]\n",
      "2025-01-17 01:28:50.766651: Epoch time: 110.74 s\n",
      "2025-01-17 01:28:51.358813: \n",
      "2025-01-17 01:28:51.359154: Epoch 833\n",
      "2025-01-17 01:28:51.359320: Current learning rate: 0.002\n",
      "2025-01-17 01:30:41.955951: train_loss -0.8485\n",
      "2025-01-17 01:30:41.956089: val_loss -0.5898\n",
      "2025-01-17 01:30:41.956120: Pseudo dice [np.float32(0.6233)]\n",
      "2025-01-17 01:30:41.956172: Epoch time: 110.6 s\n",
      "2025-01-17 01:30:42.559752: \n",
      "2025-01-17 01:30:42.559840: Epoch 834\n",
      "2025-01-17 01:30:42.559901: Current learning rate: 0.00199\n",
      "2025-01-17 01:32:33.582309: train_loss -0.8472\n",
      "2025-01-17 01:32:33.582608: val_loss -0.6462\n",
      "2025-01-17 01:32:33.582809: Pseudo dice [np.float32(0.6835)]\n",
      "2025-01-17 01:32:33.582874: Epoch time: 111.02 s\n",
      "2025-01-17 01:32:34.174758: \n",
      "2025-01-17 01:32:34.175094: Epoch 835\n",
      "2025-01-17 01:32:34.175263: Current learning rate: 0.00198\n",
      "2025-01-17 01:34:24.910504: train_loss -0.8317\n",
      "2025-01-17 01:34:24.910638: val_loss -0.6817\n",
      "2025-01-17 01:34:24.910671: Pseudo dice [np.float32(0.7167)]\n",
      "2025-01-17 01:34:24.910706: Epoch time: 110.74 s\n",
      "2025-01-17 01:34:25.506208: \n",
      "2025-01-17 01:34:25.506591: Epoch 836\n",
      "2025-01-17 01:34:25.506688: Current learning rate: 0.00196\n",
      "2025-01-17 01:36:16.324836: train_loss -0.8368\n",
      "2025-01-17 01:36:16.324972: val_loss -0.6462\n",
      "2025-01-17 01:36:16.325018: Pseudo dice [np.float32(0.7109)]\n",
      "2025-01-17 01:36:16.325065: Epoch time: 110.82 s\n",
      "2025-01-17 01:36:16.915152: \n",
      "2025-01-17 01:36:16.915287: Epoch 837\n",
      "2025-01-17 01:36:16.915353: Current learning rate: 0.00195\n",
      "2025-01-17 01:38:07.740458: train_loss -0.846\n",
      "2025-01-17 01:38:07.740602: val_loss -0.6547\n",
      "2025-01-17 01:38:07.740635: Pseudo dice [np.float32(0.6862)]\n",
      "2025-01-17 01:38:07.740668: Epoch time: 110.83 s\n",
      "2025-01-17 01:38:08.500372: \n",
      "2025-01-17 01:38:08.500455: Epoch 838\n",
      "2025-01-17 01:38:08.500568: Current learning rate: 0.00194\n",
      "2025-01-17 01:39:59.361006: train_loss -0.8526\n",
      "2025-01-17 01:39:59.361134: val_loss -0.7393\n",
      "2025-01-17 01:39:59.361206: Pseudo dice [np.float32(0.7661)]\n",
      "2025-01-17 01:39:59.361262: Epoch time: 110.86 s\n",
      "2025-01-17 01:40:00.238862: \n",
      "2025-01-17 01:40:00.239339: Epoch 839\n",
      "2025-01-17 01:40:00.239438: Current learning rate: 0.00193\n",
      "2025-01-17 01:41:51.025505: train_loss -0.8293\n",
      "2025-01-17 01:41:51.025645: val_loss -0.6352\n",
      "2025-01-17 01:41:51.025678: Pseudo dice [np.float32(0.6201)]\n",
      "2025-01-17 01:41:51.025714: Epoch time: 110.79 s\n",
      "2025-01-17 01:41:51.612971: \n",
      "2025-01-17 01:41:51.613349: Epoch 840\n",
      "2025-01-17 01:41:51.613440: Current learning rate: 0.00192\n",
      "2025-01-17 01:43:42.353657: train_loss -0.8279\n",
      "2025-01-17 01:43:42.353791: val_loss -0.653\n",
      "2025-01-17 01:43:42.353824: Pseudo dice [np.float32(0.6548)]\n",
      "2025-01-17 01:43:42.353878: Epoch time: 110.74 s\n",
      "2025-01-17 01:43:42.968363: \n",
      "2025-01-17 01:43:42.968708: Epoch 841\n",
      "2025-01-17 01:43:42.968804: Current learning rate: 0.00191\n",
      "2025-01-17 01:45:33.763603: train_loss -0.8588\n",
      "2025-01-17 01:45:33.763729: val_loss -0.6408\n",
      "2025-01-17 01:45:33.763762: Pseudo dice [np.float32(0.7195)]\n",
      "2025-01-17 01:45:33.763795: Epoch time: 110.8 s\n",
      "2025-01-17 01:45:34.353113: \n",
      "2025-01-17 01:45:34.353459: Epoch 842\n",
      "2025-01-17 01:45:34.353528: Current learning rate: 0.0019\n",
      "2025-01-17 01:47:25.129351: train_loss -0.8465\n",
      "2025-01-17 01:47:25.129524: val_loss -0.607\n",
      "2025-01-17 01:47:25.129588: Pseudo dice [np.float32(0.6587)]\n",
      "2025-01-17 01:47:25.129631: Epoch time: 110.78 s\n",
      "2025-01-17 01:47:25.723294: \n",
      "2025-01-17 01:47:25.723636: Epoch 843\n",
      "2025-01-17 01:47:25.723730: Current learning rate: 0.00189\n",
      "2025-01-17 01:49:16.396339: train_loss -0.8511\n",
      "2025-01-17 01:49:16.396534: val_loss -0.6812\n",
      "2025-01-17 01:49:16.396569: Pseudo dice [np.float32(0.7434)]\n",
      "2025-01-17 01:49:16.396602: Epoch time: 110.67 s\n",
      "2025-01-17 01:49:17.001340: \n",
      "2025-01-17 01:49:17.001441: Epoch 844\n",
      "2025-01-17 01:49:17.001503: Current learning rate: 0.00188\n",
      "2025-01-17 01:51:07.790344: train_loss -0.833\n",
      "2025-01-17 01:51:07.790471: val_loss -0.6985\n",
      "2025-01-17 01:51:07.790502: Pseudo dice [np.float32(0.6852)]\n",
      "2025-01-17 01:51:07.790535: Epoch time: 110.79 s\n",
      "2025-01-17 01:51:08.377993: \n",
      "2025-01-17 01:51:08.378357: Epoch 845\n",
      "2025-01-17 01:51:08.378459: Current learning rate: 0.00187\n",
      "2025-01-17 01:52:59.152651: train_loss -0.861\n",
      "2025-01-17 01:52:59.152778: val_loss -0.6529\n",
      "2025-01-17 01:52:59.152864: Pseudo dice [np.float32(0.6574)]\n",
      "2025-01-17 01:52:59.153028: Epoch time: 110.78 s\n",
      "2025-01-17 01:52:59.745908: \n",
      "2025-01-17 01:52:59.745997: Epoch 846\n",
      "2025-01-17 01:52:59.746058: Current learning rate: 0.00186\n",
      "2025-01-17 01:54:50.545230: train_loss -0.8514\n",
      "2025-01-17 01:54:50.545351: val_loss -0.7275\n",
      "2025-01-17 01:54:50.545384: Pseudo dice [np.float32(0.7591)]\n",
      "2025-01-17 01:54:50.545418: Epoch time: 110.8 s\n",
      "2025-01-17 01:54:51.148304: \n",
      "2025-01-17 01:54:51.148612: Epoch 847\n",
      "2025-01-17 01:54:51.148718: Current learning rate: 0.00185\n",
      "2025-01-17 01:56:41.891652: train_loss -0.8486\n",
      "2025-01-17 01:56:41.891820: val_loss -0.6943\n",
      "2025-01-17 01:56:41.891852: Pseudo dice [np.float32(0.6713)]\n",
      "2025-01-17 01:56:41.891885: Epoch time: 110.74 s\n",
      "2025-01-17 01:56:42.477393: \n",
      "2025-01-17 01:56:42.477709: Epoch 848\n",
      "2025-01-17 01:56:42.477782: Current learning rate: 0.00184\n",
      "2025-01-17 01:58:33.200020: train_loss -0.8396\n",
      "2025-01-17 01:58:33.200244: val_loss -0.6358\n",
      "2025-01-17 01:58:33.200290: Pseudo dice [np.float32(0.6856)]\n",
      "2025-01-17 01:58:33.200325: Epoch time: 110.72 s\n",
      "2025-01-17 01:58:33.792738: \n",
      "2025-01-17 01:58:33.792883: Epoch 849\n",
      "2025-01-17 01:58:33.792948: Current learning rate: 0.00182\n",
      "2025-01-17 02:00:24.532861: train_loss -0.8472\n",
      "2025-01-17 02:00:24.532983: val_loss -0.6833\n",
      "2025-01-17 02:00:24.533015: Pseudo dice [np.float32(0.7011)]\n",
      "2025-01-17 02:00:24.533046: Epoch time: 110.74 s\n",
      "2025-01-17 02:00:25.366227: \n",
      "2025-01-17 02:00:25.366557: Epoch 850\n",
      "2025-01-17 02:00:25.366632: Current learning rate: 0.00181\n",
      "2025-01-17 02:02:16.182145: train_loss -0.8553\n",
      "2025-01-17 02:02:16.182272: val_loss -0.687\n",
      "2025-01-17 02:02:16.182305: Pseudo dice [np.float32(0.7029)]\n",
      "2025-01-17 02:02:16.182362: Epoch time: 110.82 s\n",
      "2025-01-17 02:02:16.997894: \n",
      "2025-01-17 02:02:16.998019: Epoch 851\n",
      "2025-01-17 02:02:16.998087: Current learning rate: 0.0018\n",
      "2025-01-17 02:04:07.769473: train_loss -0.8452\n",
      "2025-01-17 02:04:07.769619: val_loss -0.7261\n",
      "2025-01-17 02:04:07.769656: Pseudo dice [np.float32(0.7555)]\n",
      "2025-01-17 02:04:07.769690: Epoch time: 110.77 s\n",
      "2025-01-17 02:04:08.362244: \n",
      "2025-01-17 02:04:08.362500: Epoch 852\n",
      "2025-01-17 02:04:08.362596: Current learning rate: 0.00179\n",
      "2025-01-17 02:05:58.951519: train_loss -0.8512\n",
      "2025-01-17 02:05:58.951679: val_loss -0.7011\n",
      "2025-01-17 02:05:58.951753: Pseudo dice [np.float32(0.704)]\n",
      "2025-01-17 02:05:58.951796: Epoch time: 110.59 s\n",
      "2025-01-17 02:05:59.541884: \n",
      "2025-01-17 02:05:59.541982: Epoch 853\n",
      "2025-01-17 02:05:59.542046: Current learning rate: 0.00178\n",
      "2025-01-17 02:07:50.309736: train_loss -0.8629\n",
      "2025-01-17 02:07:50.309917: val_loss -0.6705\n",
      "2025-01-17 02:07:50.309951: Pseudo dice [np.float32(0.6948)]\n",
      "2025-01-17 02:07:50.309983: Epoch time: 110.77 s\n",
      "2025-01-17 02:07:50.897432: \n",
      "2025-01-17 02:07:50.897846: Epoch 854\n",
      "2025-01-17 02:07:50.897970: Current learning rate: 0.00177\n",
      "2025-01-17 02:09:41.523099: train_loss -0.8545\n",
      "2025-01-17 02:09:41.523249: val_loss -0.711\n",
      "2025-01-17 02:09:41.523287: Pseudo dice [np.float32(0.6891)]\n",
      "2025-01-17 02:09:41.523325: Epoch time: 110.63 s\n",
      "2025-01-17 02:09:42.107920: \n",
      "2025-01-17 02:09:42.108237: Epoch 855\n",
      "2025-01-17 02:09:42.108462: Current learning rate: 0.00176\n",
      "2025-01-17 02:11:32.900398: train_loss -0.8671\n",
      "2025-01-17 02:11:32.900530: val_loss -0.6851\n",
      "2025-01-17 02:11:32.900563: Pseudo dice [np.float32(0.6984)]\n",
      "2025-01-17 02:11:32.900697: Epoch time: 110.79 s\n",
      "2025-01-17 02:11:33.484704: \n",
      "2025-01-17 02:11:33.484853: Epoch 856\n",
      "2025-01-17 02:11:33.484930: Current learning rate: 0.00175\n",
      "2025-01-17 02:13:24.217838: train_loss -0.8596\n",
      "2025-01-17 02:13:24.217972: val_loss -0.5665\n",
      "2025-01-17 02:13:24.218006: Pseudo dice [np.float32(0.5524)]\n",
      "2025-01-17 02:13:24.218043: Epoch time: 110.73 s\n",
      "2025-01-17 02:13:24.812063: \n",
      "2025-01-17 02:13:24.812271: Epoch 857\n",
      "2025-01-17 02:13:24.812343: Current learning rate: 0.00174\n",
      "2025-01-17 02:15:15.584134: train_loss -0.8512\n",
      "2025-01-17 02:15:15.584292: val_loss -0.6267\n",
      "2025-01-17 02:15:15.584325: Pseudo dice [np.float32(0.6251)]\n",
      "2025-01-17 02:15:15.584359: Epoch time: 110.77 s\n",
      "2025-01-17 02:15:16.171058: \n",
      "2025-01-17 02:15:16.171151: Epoch 858\n",
      "2025-01-17 02:15:16.171248: Current learning rate: 0.00173\n",
      "2025-01-17 02:17:06.924726: train_loss -0.8562\n",
      "2025-01-17 02:17:06.924848: val_loss -0.7091\n",
      "2025-01-17 02:17:06.924883: Pseudo dice [np.float32(0.6747)]\n",
      "2025-01-17 02:17:06.924916: Epoch time: 110.75 s\n",
      "2025-01-17 02:17:07.513556: \n",
      "2025-01-17 02:17:07.513650: Epoch 859\n",
      "2025-01-17 02:17:07.513716: Current learning rate: 0.00172\n",
      "2025-01-17 02:18:58.237520: train_loss -0.8555\n",
      "2025-01-17 02:18:58.237646: val_loss -0.7124\n",
      "2025-01-17 02:18:58.237783: Pseudo dice [np.float32(0.6354)]\n",
      "2025-01-17 02:18:58.237977: Epoch time: 110.72 s\n",
      "2025-01-17 02:18:58.829449: \n",
      "2025-01-17 02:18:58.829786: Epoch 860\n",
      "2025-01-17 02:18:58.829859: Current learning rate: 0.0017\n",
      "2025-01-17 02:20:49.569448: train_loss -0.8429\n",
      "2025-01-17 02:20:49.569564: val_loss -0.7171\n",
      "2025-01-17 02:20:49.569597: Pseudo dice [np.float32(0.7048)]\n",
      "2025-01-17 02:20:49.569631: Epoch time: 110.74 s\n",
      "2025-01-17 02:20:50.152294: \n",
      "2025-01-17 02:20:50.152384: Epoch 861\n",
      "2025-01-17 02:20:50.152444: Current learning rate: 0.00169\n",
      "2025-01-17 02:22:40.730464: train_loss -0.8446\n",
      "2025-01-17 02:22:40.730592: val_loss -0.5821\n",
      "2025-01-17 02:22:40.730628: Pseudo dice [np.float32(0.6545)]\n",
      "2025-01-17 02:22:40.730662: Epoch time: 110.58 s\n",
      "2025-01-17 02:22:41.319958: \n",
      "2025-01-17 02:22:41.320049: Epoch 862\n",
      "2025-01-17 02:22:41.320112: Current learning rate: 0.00168\n",
      "2025-01-17 02:24:32.021361: train_loss -0.8243\n",
      "2025-01-17 02:24:32.021547: val_loss -0.7036\n",
      "2025-01-17 02:24:32.021591: Pseudo dice [np.float32(0.7148)]\n",
      "2025-01-17 02:24:32.021622: Epoch time: 110.7 s\n",
      "2025-01-17 02:24:32.608191: \n",
      "2025-01-17 02:24:32.608273: Epoch 863\n",
      "2025-01-17 02:24:32.608336: Current learning rate: 0.00167\n",
      "2025-01-17 02:26:23.296141: train_loss -0.8414\n",
      "2025-01-17 02:26:23.296296: val_loss -0.7205\n",
      "2025-01-17 02:26:23.296331: Pseudo dice [np.float32(0.725)]\n",
      "2025-01-17 02:26:23.296365: Epoch time: 110.69 s\n",
      "2025-01-17 02:26:24.104882: \n",
      "2025-01-17 02:26:24.105238: Epoch 864\n",
      "2025-01-17 02:26:24.105311: Current learning rate: 0.00166\n",
      "2025-01-17 02:28:14.885959: train_loss -0.8364\n",
      "2025-01-17 02:28:14.886148: val_loss -0.7404\n",
      "2025-01-17 02:28:14.886181: Pseudo dice [np.float32(0.7277)]\n",
      "2025-01-17 02:28:14.886217: Epoch time: 110.78 s\n",
      "2025-01-17 02:28:15.476367: \n",
      "2025-01-17 02:28:15.476531: Epoch 865\n",
      "2025-01-17 02:28:15.476658: Current learning rate: 0.00165\n",
      "2025-01-17 02:30:06.178072: train_loss -0.8552\n",
      "2025-01-17 02:30:06.178263: val_loss -0.6025\n",
      "2025-01-17 02:30:06.178306: Pseudo dice [np.float32(0.5908)]\n",
      "2025-01-17 02:30:06.178342: Epoch time: 110.7 s\n",
      "2025-01-17 02:30:06.768874: \n",
      "2025-01-17 02:30:06.769234: Epoch 866\n",
      "2025-01-17 02:30:06.769318: Current learning rate: 0.00164\n",
      "2025-01-17 02:31:57.536687: train_loss -0.846\n",
      "2025-01-17 02:31:57.536826: val_loss -0.6892\n",
      "2025-01-17 02:31:57.536862: Pseudo dice [np.float32(0.7051)]\n",
      "2025-01-17 02:31:57.536896: Epoch time: 110.77 s\n",
      "2025-01-17 02:31:58.124708: \n",
      "2025-01-17 02:31:58.124886: Epoch 867\n",
      "2025-01-17 02:31:58.125019: Current learning rate: 0.00163\n",
      "2025-01-17 02:33:48.766998: train_loss -0.8353\n",
      "2025-01-17 02:33:48.767122: val_loss -0.6973\n",
      "2025-01-17 02:33:48.767155: Pseudo dice [np.float32(0.6907)]\n",
      "2025-01-17 02:33:48.767189: Epoch time: 110.64 s\n",
      "2025-01-17 02:33:49.350161: \n",
      "2025-01-17 02:33:49.350479: Epoch 868\n",
      "2025-01-17 02:33:49.350619: Current learning rate: 0.00162\n",
      "2025-01-17 02:35:40.136886: train_loss -0.8442\n",
      "2025-01-17 02:35:40.137083: val_loss -0.728\n",
      "2025-01-17 02:35:40.137129: Pseudo dice [np.float32(0.7614)]\n",
      "2025-01-17 02:35:40.137162: Epoch time: 110.79 s\n",
      "2025-01-17 02:35:40.727686: \n",
      "2025-01-17 02:35:40.727773: Epoch 869\n",
      "2025-01-17 02:35:40.727834: Current learning rate: 0.00161\n",
      "2025-01-17 02:37:31.461282: train_loss -0.8375\n",
      "2025-01-17 02:37:31.461456: val_loss -0.4581\n",
      "2025-01-17 02:37:31.461489: Pseudo dice [np.float32(0.6227)]\n",
      "2025-01-17 02:37:31.461523: Epoch time: 110.73 s\n",
      "2025-01-17 02:37:32.046871: \n",
      "2025-01-17 02:37:32.047026: Epoch 870\n",
      "2025-01-17 02:37:32.047097: Current learning rate: 0.00159\n",
      "2025-01-17 02:39:22.830893: train_loss -0.837\n",
      "2025-01-17 02:39:22.831054: val_loss -0.7523\n",
      "2025-01-17 02:39:22.831195: Pseudo dice [np.float32(0.7821)]\n",
      "2025-01-17 02:39:22.831247: Epoch time: 110.78 s\n",
      "2025-01-17 02:39:23.414642: \n",
      "2025-01-17 02:39:23.414794: Epoch 871\n",
      "2025-01-17 02:39:23.414858: Current learning rate: 0.00158\n",
      "2025-01-17 02:41:14.096512: train_loss -0.8368\n",
      "2025-01-17 02:41:14.096854: val_loss -0.7021\n",
      "2025-01-17 02:41:14.096892: Pseudo dice [np.float32(0.6739)]\n",
      "2025-01-17 02:41:14.096927: Epoch time: 110.68 s\n",
      "2025-01-17 02:41:14.685617: \n",
      "2025-01-17 02:41:14.685715: Epoch 872\n",
      "2025-01-17 02:41:14.685777: Current learning rate: 0.00157\n",
      "2025-01-17 02:43:05.408063: train_loss -0.848\n",
      "2025-01-17 02:43:05.408284: val_loss -0.6839\n",
      "2025-01-17 02:43:05.408328: Pseudo dice [np.float32(0.7302)]\n",
      "2025-01-17 02:43:05.408364: Epoch time: 110.72 s\n",
      "2025-01-17 02:43:05.999760: \n",
      "2025-01-17 02:43:06.000074: Epoch 873\n",
      "2025-01-17 02:43:06.000200: Current learning rate: 0.00156\n",
      "2025-01-17 02:44:56.709257: train_loss -0.8587\n",
      "2025-01-17 02:44:56.709391: val_loss -0.7217\n",
      "2025-01-17 02:44:56.709423: Pseudo dice [np.float32(0.7163)]\n",
      "2025-01-17 02:44:56.709455: Epoch time: 110.71 s\n",
      "2025-01-17 02:44:57.295907: \n",
      "2025-01-17 02:44:57.296356: Epoch 874\n",
      "2025-01-17 02:44:57.296486: Current learning rate: 0.00155\n",
      "2025-01-17 02:46:48.081270: train_loss -0.8588\n",
      "2025-01-17 02:46:48.081413: val_loss -0.7229\n",
      "2025-01-17 02:46:48.081447: Pseudo dice [np.float32(0.7389)]\n",
      "2025-01-17 02:46:48.081482: Epoch time: 110.79 s\n",
      "2025-01-17 02:46:48.666515: \n",
      "2025-01-17 02:46:48.666657: Epoch 875\n",
      "2025-01-17 02:46:48.666733: Current learning rate: 0.00154\n",
      "2025-01-17 02:48:39.377973: train_loss -0.8671\n",
      "2025-01-17 02:48:39.378109: val_loss -0.7321\n",
      "2025-01-17 02:48:39.378253: Pseudo dice [np.float32(0.7143)]\n",
      "2025-01-17 02:48:39.378352: Epoch time: 110.71 s\n",
      "2025-01-17 02:48:39.954964: \n",
      "2025-01-17 02:48:39.955048: Epoch 876\n",
      "2025-01-17 02:48:39.955109: Current learning rate: 0.00153\n",
      "2025-01-17 02:50:30.782728: train_loss -0.8626\n",
      "2025-01-17 02:50:30.782854: val_loss -0.7328\n",
      "2025-01-17 02:50:30.782888: Pseudo dice [np.float32(0.7133)]\n",
      "2025-01-17 02:50:30.782928: Epoch time: 110.83 s\n",
      "2025-01-17 02:50:31.606478: \n",
      "2025-01-17 02:50:31.606768: Epoch 877\n",
      "2025-01-17 02:50:31.606915: Current learning rate: 0.00152\n",
      "2025-01-17 02:52:22.396594: train_loss -0.8623\n",
      "2025-01-17 02:52:22.396722: val_loss -0.7014\n",
      "2025-01-17 02:52:22.396756: Pseudo dice [np.float32(0.7323)]\n",
      "2025-01-17 02:52:22.396789: Epoch time: 110.79 s\n",
      "2025-01-17 02:52:22.982414: \n",
      "2025-01-17 02:52:22.982601: Epoch 878\n",
      "2025-01-17 02:52:22.982677: Current learning rate: 0.00151\n",
      "2025-01-17 02:54:13.751297: train_loss -0.8347\n",
      "2025-01-17 02:54:13.751526: val_loss -0.5585\n",
      "2025-01-17 02:54:13.751592: Pseudo dice [np.float32(0.7505)]\n",
      "2025-01-17 02:54:13.751631: Epoch time: 110.77 s\n",
      "2025-01-17 02:54:14.340991: \n",
      "2025-01-17 02:54:14.341400: Epoch 879\n",
      "2025-01-17 02:54:14.341488: Current learning rate: 0.00149\n",
      "2025-01-17 02:56:04.915189: train_loss -0.8546\n",
      "2025-01-17 02:56:04.915310: val_loss -0.6193\n",
      "2025-01-17 02:56:04.915342: Pseudo dice [np.float32(0.7182)]\n",
      "2025-01-17 02:56:04.915374: Epoch time: 110.57 s\n",
      "2025-01-17 02:56:05.498587: \n",
      "2025-01-17 02:56:05.498730: Epoch 880\n",
      "2025-01-17 02:56:05.498792: Current learning rate: 0.00148\n",
      "2025-01-17 02:57:56.266561: train_loss -0.8416\n",
      "2025-01-17 02:57:56.266690: val_loss -0.7366\n",
      "2025-01-17 02:57:56.266779: Pseudo dice [np.float32(0.7952)]\n",
      "2025-01-17 02:57:56.266821: Epoch time: 110.77 s\n",
      "2025-01-17 02:57:56.266841: Yayy! New best EMA pseudo Dice: 0.7197999954223633\n",
      "2025-01-17 02:57:57.095480: \n",
      "2025-01-17 02:57:57.095851: Epoch 881\n",
      "2025-01-17 02:57:57.095935: Current learning rate: 0.00147\n",
      "2025-01-17 02:59:47.852740: train_loss -0.8269\n",
      "2025-01-17 02:59:47.852867: val_loss -0.6515\n",
      "2025-01-17 02:59:47.852901: Pseudo dice [np.float32(0.7194)]\n",
      "2025-01-17 02:59:47.852935: Epoch time: 110.76 s\n",
      "2025-01-17 02:59:48.445437: \n",
      "2025-01-17 02:59:48.445579: Epoch 882\n",
      "2025-01-17 02:59:48.445674: Current learning rate: 0.00146\n",
      "2025-01-17 03:01:39.191279: train_loss -0.854\n",
      "2025-01-17 03:01:39.191416: val_loss -0.744\n",
      "2025-01-17 03:01:39.191449: Pseudo dice [np.float32(0.7229)]\n",
      "2025-01-17 03:01:39.191481: Epoch time: 110.75 s\n",
      "2025-01-17 03:01:39.191501: Yayy! New best EMA pseudo Dice: 0.7200999855995178\n",
      "2025-01-17 03:01:40.006920: \n",
      "2025-01-17 03:01:40.007279: Epoch 883\n",
      "2025-01-17 03:01:40.007433: Current learning rate: 0.00145\n",
      "2025-01-17 03:03:30.753897: train_loss -0.8303\n",
      "2025-01-17 03:03:30.754074: val_loss -0.6998\n",
      "2025-01-17 03:03:30.754109: Pseudo dice [np.float32(0.6739)]\n",
      "2025-01-17 03:03:30.754145: Epoch time: 110.75 s\n",
      "2025-01-17 03:03:31.344933: \n",
      "2025-01-17 03:03:31.345132: Epoch 884\n",
      "2025-01-17 03:03:31.345454: Current learning rate: 0.00144\n",
      "2025-01-17 03:05:22.097841: train_loss -0.8516\n",
      "2025-01-17 03:05:22.097977: val_loss -0.6766\n",
      "2025-01-17 03:05:22.098013: Pseudo dice [np.float32(0.6915)]\n",
      "2025-01-17 03:05:22.098048: Epoch time: 110.75 s\n",
      "2025-01-17 03:05:22.687539: \n",
      "2025-01-17 03:05:22.688034: Epoch 885\n",
      "2025-01-17 03:05:22.688153: Current learning rate: 0.00143\n",
      "2025-01-17 03:07:13.387516: train_loss -0.858\n",
      "2025-01-17 03:07:13.387640: val_loss -0.7136\n",
      "2025-01-17 03:07:13.387674: Pseudo dice [np.float32(0.7284)]\n",
      "2025-01-17 03:07:13.387707: Epoch time: 110.7 s\n",
      "2025-01-17 03:07:13.975820: \n",
      "2025-01-17 03:07:13.975914: Epoch 886\n",
      "2025-01-17 03:07:13.975979: Current learning rate: 0.00142\n",
      "2025-01-17 03:09:04.599826: train_loss -0.8441\n",
      "2025-01-17 03:09:04.599965: val_loss -0.7258\n",
      "2025-01-17 03:09:04.600003: Pseudo dice [np.float32(0.7383)]\n",
      "2025-01-17 03:09:04.600041: Epoch time: 110.62 s\n",
      "2025-01-17 03:09:05.188918: \n",
      "2025-01-17 03:09:05.189158: Epoch 887\n",
      "2025-01-17 03:09:05.189346: Current learning rate: 0.00141\n",
      "2025-01-17 03:10:55.777610: train_loss -0.8551\n",
      "2025-01-17 03:10:55.777758: val_loss -0.7464\n",
      "2025-01-17 03:10:55.777799: Pseudo dice [np.float32(0.7391)]\n",
      "2025-01-17 03:10:55.777839: Epoch time: 110.59 s\n",
      "2025-01-17 03:10:56.362782: \n",
      "2025-01-17 03:10:56.362959: Epoch 888\n",
      "2025-01-17 03:10:56.363070: Current learning rate: 0.00139\n",
      "2025-01-17 03:12:47.129773: train_loss -0.8649\n",
      "2025-01-17 03:12:47.129944: val_loss -0.6683\n",
      "2025-01-17 03:12:47.129977: Pseudo dice [np.float32(0.6482)]\n",
      "2025-01-17 03:12:47.130011: Epoch time: 110.77 s\n",
      "2025-01-17 03:12:47.713381: \n",
      "2025-01-17 03:12:47.713640: Epoch 889\n",
      "2025-01-17 03:12:47.713739: Current learning rate: 0.00138\n",
      "2025-01-17 03:14:38.500523: train_loss -0.8471\n",
      "2025-01-17 03:14:38.500910: val_loss -0.645\n",
      "2025-01-17 03:14:38.501013: Pseudo dice [np.float32(0.6248)]\n",
      "2025-01-17 03:14:38.501058: Epoch time: 110.79 s\n",
      "2025-01-17 03:14:39.338749: \n",
      "2025-01-17 03:14:39.338846: Epoch 890\n",
      "2025-01-17 03:14:39.338909: Current learning rate: 0.00137\n",
      "2025-01-17 03:16:30.158966: train_loss -0.8578\n",
      "2025-01-17 03:16:30.159091: val_loss -0.6938\n",
      "2025-01-17 03:16:30.159217: Pseudo dice [np.float32(0.733)]\n",
      "2025-01-17 03:16:30.159277: Epoch time: 110.82 s\n",
      "2025-01-17 03:16:30.753249: \n",
      "2025-01-17 03:16:30.753440: Epoch 891\n",
      "2025-01-17 03:16:30.753525: Current learning rate: 0.00136\n",
      "2025-01-17 03:18:21.335837: train_loss -0.8688\n",
      "2025-01-17 03:18:21.335966: val_loss -0.7117\n",
      "2025-01-17 03:18:21.335999: Pseudo dice [np.float32(0.7431)]\n",
      "2025-01-17 03:18:21.336033: Epoch time: 110.58 s\n",
      "2025-01-17 03:18:21.919822: \n",
      "2025-01-17 03:18:21.919978: Epoch 892\n",
      "2025-01-17 03:18:21.920046: Current learning rate: 0.00135\n",
      "2025-01-17 03:20:12.640936: train_loss -0.8347\n",
      "2025-01-17 03:20:12.641079: val_loss -0.7127\n",
      "2025-01-17 03:20:12.641442: Pseudo dice [np.float32(0.7435)]\n",
      "2025-01-17 03:20:12.641654: Epoch time: 110.72 s\n",
      "2025-01-17 03:20:13.238239: \n",
      "2025-01-17 03:20:13.238562: Epoch 893\n",
      "2025-01-17 03:20:13.238732: Current learning rate: 0.00134\n",
      "2025-01-17 03:22:04.034446: train_loss -0.8571\n",
      "2025-01-17 03:22:04.034636: val_loss -0.6867\n",
      "2025-01-17 03:22:04.034671: Pseudo dice [np.float32(0.6913)]\n",
      "2025-01-17 03:22:04.034704: Epoch time: 110.8 s\n",
      "2025-01-17 03:22:04.617159: \n",
      "2025-01-17 03:22:04.617251: Epoch 894\n",
      "2025-01-17 03:22:04.617315: Current learning rate: 0.00133\n",
      "2025-01-17 03:23:55.294054: train_loss -0.8533\n",
      "2025-01-17 03:23:55.294232: val_loss -0.7247\n",
      "2025-01-17 03:23:55.294305: Pseudo dice [np.float32(0.6901)]\n",
      "2025-01-17 03:23:55.294348: Epoch time: 110.68 s\n",
      "2025-01-17 03:23:55.881786: \n",
      "2025-01-17 03:23:55.881877: Epoch 895\n",
      "2025-01-17 03:23:55.881938: Current learning rate: 0.00132\n",
      "2025-01-17 03:25:46.600908: train_loss -0.8606\n",
      "2025-01-17 03:25:46.601118: val_loss -0.633\n",
      "2025-01-17 03:25:46.601236: Pseudo dice [np.float32(0.6361)]\n",
      "2025-01-17 03:25:46.601300: Epoch time: 110.72 s\n",
      "2025-01-17 03:25:47.190160: \n",
      "2025-01-17 03:25:47.190506: Epoch 896\n",
      "2025-01-17 03:25:47.190587: Current learning rate: 0.0013\n",
      "2025-01-17 03:27:37.977629: train_loss -0.8638\n",
      "2025-01-17 03:27:37.977741: val_loss -0.6774\n",
      "2025-01-17 03:27:37.977773: Pseudo dice [np.float32(0.6698)]\n",
      "2025-01-17 03:27:37.977804: Epoch time: 110.79 s\n",
      "2025-01-17 03:27:38.560522: \n",
      "2025-01-17 03:27:38.560612: Epoch 897\n",
      "2025-01-17 03:27:38.560676: Current learning rate: 0.00129\n",
      "2025-01-17 03:29:29.337373: train_loss -0.8621\n",
      "2025-01-17 03:29:29.337507: val_loss -0.7186\n",
      "2025-01-17 03:29:29.337540: Pseudo dice [np.float32(0.7142)]\n",
      "2025-01-17 03:29:29.337572: Epoch time: 110.78 s\n",
      "2025-01-17 03:29:29.924432: \n",
      "2025-01-17 03:29:29.924768: Epoch 898\n",
      "2025-01-17 03:29:29.924844: Current learning rate: 0.00128\n",
      "2025-01-17 03:31:20.796268: train_loss -0.8427\n",
      "2025-01-17 03:31:20.796388: val_loss -0.5971\n",
      "2025-01-17 03:31:20.796420: Pseudo dice [np.float32(0.5918)]\n",
      "2025-01-17 03:31:20.796452: Epoch time: 110.87 s\n",
      "2025-01-17 03:31:21.383308: \n",
      "2025-01-17 03:31:21.383472: Epoch 899\n",
      "2025-01-17 03:31:21.383546: Current learning rate: 0.00127\n",
      "2025-01-17 03:33:12.142023: train_loss -0.8424\n",
      "2025-01-17 03:33:12.142231: val_loss -0.7058\n",
      "2025-01-17 03:33:12.142275: Pseudo dice [np.float32(0.7277)]\n",
      "2025-01-17 03:33:12.142308: Epoch time: 110.76 s\n",
      "2025-01-17 03:33:12.956927: \n",
      "2025-01-17 03:33:12.957136: Epoch 900\n",
      "2025-01-17 03:33:12.957211: Current learning rate: 0.00126\n",
      "2025-01-17 03:35:03.737056: train_loss -0.8521\n",
      "2025-01-17 03:35:03.737188: val_loss -0.7157\n",
      "2025-01-17 03:35:03.737224: Pseudo dice [np.float32(0.7172)]\n",
      "2025-01-17 03:35:03.737267: Epoch time: 110.78 s\n",
      "2025-01-17 03:35:04.345831: \n",
      "2025-01-17 03:35:04.345926: Epoch 901\n",
      "2025-01-17 03:35:04.345991: Current learning rate: 0.00125\n",
      "2025-01-17 03:36:55.148077: train_loss -0.8601\n",
      "2025-01-17 03:36:55.148226: val_loss -0.7241\n",
      "2025-01-17 03:36:55.148261: Pseudo dice [np.float32(0.6693)]\n",
      "2025-01-17 03:36:55.148293: Epoch time: 110.8 s\n",
      "2025-01-17 03:36:55.725432: \n",
      "2025-01-17 03:36:55.725785: Epoch 902\n",
      "2025-01-17 03:36:55.725911: Current learning rate: 0.00124\n",
      "2025-01-17 03:38:46.317694: train_loss -0.8739\n",
      "2025-01-17 03:38:46.317827: val_loss -0.6932\n",
      "2025-01-17 03:38:46.317998: Pseudo dice [np.float32(0.712)]\n",
      "2025-01-17 03:38:46.318138: Epoch time: 110.59 s\n",
      "2025-01-17 03:38:47.163456: \n",
      "2025-01-17 03:38:47.163809: Epoch 903\n",
      "2025-01-17 03:38:47.163918: Current learning rate: 0.00122\n",
      "2025-01-17 03:40:37.733008: train_loss -0.834\n",
      "2025-01-17 03:40:37.733133: val_loss -0.6343\n",
      "2025-01-17 03:40:37.733165: Pseudo dice [np.float32(0.6653)]\n",
      "2025-01-17 03:40:37.733197: Epoch time: 110.57 s\n",
      "2025-01-17 03:40:38.316772: \n",
      "2025-01-17 03:40:38.317110: Epoch 904\n",
      "2025-01-17 03:40:38.317238: Current learning rate: 0.00121\n",
      "2025-01-17 03:42:28.879658: train_loss -0.8563\n",
      "2025-01-17 03:42:28.879799: val_loss -0.6995\n",
      "2025-01-17 03:42:28.879832: Pseudo dice [np.float32(0.6906)]\n",
      "2025-01-17 03:42:28.879866: Epoch time: 110.56 s\n",
      "2025-01-17 03:42:29.469538: \n",
      "Epoch 9057 03:42:29.469673: \n",
      "2025-01-17 03:42:29.469788: Current learning rate: 0.0012\n",
      "2025-01-17 03:44:20.022660: train_loss -0.8597\n",
      "2025-01-17 03:44:20.022807: val_loss -0.6535\n",
      "2025-01-17 03:44:20.022845: Pseudo dice [np.float32(0.606)]\n",
      "2025-01-17 03:44:20.022877: Epoch time: 110.55 s\n",
      "2025-01-17 03:44:20.605903: \n",
      "2025-01-17 03:44:20.605997: Epoch 906\n",
      "2025-01-17 03:44:20.606168: Current learning rate: 0.00119\n",
      "2025-01-17 03:46:11.396843: train_loss -0.8628\n",
      "2025-01-17 03:46:11.396974: val_loss -0.6253\n",
      "2025-01-17 03:46:11.397010: Pseudo dice [np.float32(0.6612)]\n",
      "2025-01-17 03:46:11.397043: Epoch time: 110.79 s\n",
      "2025-01-17 03:46:11.977847: \n",
      "2025-01-17 03:46:11.977966: Epoch 907\n",
      "2025-01-17 03:46:11.978034: Current learning rate: 0.00118\n",
      "2025-01-17 03:48:02.706479: train_loss -0.8596\n",
      "2025-01-17 03:48:02.706601: val_loss -0.7249\n",
      "2025-01-17 03:48:02.706632: Pseudo dice [np.float32(0.7193)]\n",
      "2025-01-17 03:48:02.706666: Epoch time: 110.73 s\n",
      "2025-01-17 03:48:03.291682: \n",
      "2025-01-17 03:48:03.292102: Epoch 908\n",
      "2025-01-17 03:48:03.292212: Current learning rate: 0.00117\n",
      "2025-01-17 03:49:54.053405: train_loss -0.8579\n",
      "2025-01-17 03:49:54.053775: val_loss -0.6823\n",
      "2025-01-17 03:49:54.053848: Pseudo dice [np.float32(0.7079)]\n",
      "2025-01-17 03:49:54.053890: Epoch time: 110.76 s\n",
      "2025-01-17 03:49:54.638307: \n",
      "2025-01-17 03:49:54.638534: Epoch 909\n",
      "2025-01-17 03:49:54.638629: Current learning rate: 0.00116\n",
      "2025-01-17 03:51:45.396124: train_loss -0.8509\n",
      "2025-01-17 03:51:45.396332: val_loss -0.6278\n",
      "2025-01-17 03:51:45.396368: Pseudo dice [np.float32(0.6167)]\n",
      "2025-01-17 03:51:45.396401: Epoch time: 110.76 s\n",
      "2025-01-17 03:51:45.980505: \n",
      "2025-01-17 03:51:45.980598: Epoch 910\n",
      "2025-01-17 03:51:45.980661: Current learning rate: 0.00115\n",
      "2025-01-17 03:53:36.722463: train_loss -0.8649\n",
      "2025-01-17 03:53:36.722658: val_loss -0.6959\n",
      "2025-01-17 03:53:36.722760: Pseudo dice [np.float32(0.6467)]\n",
      "2025-01-17 03:53:36.722832: Epoch time: 110.74 s\n",
      "2025-01-17 03:53:37.305582: \n",
      "2025-01-17 03:53:37.305737: Epoch 911\n",
      "2025-01-17 03:53:37.305808: Current learning rate: 0.00113\n",
      "2025-01-17 03:55:28.082479: train_loss -0.8527\n",
      "2025-01-17 03:55:28.082635: val_loss -0.7252\n",
      "2025-01-17 03:55:28.082704: Pseudo dice [np.float32(0.6316)]\n",
      "2025-01-17 03:55:28.082745: Epoch time: 110.78 s\n",
      "2025-01-17 03:55:28.671887: \n",
      "2025-01-17 03:55:28.671979: Epoch 912\n",
      "2025-01-17 03:55:28.672099: Current learning rate: 0.00112\n",
      "2025-01-17 03:57:19.233657: train_loss -0.8555\n",
      "2025-01-17 03:57:19.233795: val_loss -0.7018\n",
      "2025-01-17 03:57:19.233833: Pseudo dice [np.float32(0.707)]\n",
      "2025-01-17 03:57:19.233865: Epoch time: 110.56 s\n",
      "2025-01-17 03:57:19.801946: \n",
      "2025-01-17 03:57:19.802118: Epoch 913\n",
      "2025-01-17 03:57:19.802188: Current learning rate: 0.00111\n",
      "2025-01-17 03:59:10.319580: train_loss -0.8362\n",
      "2025-01-17 03:59:10.319702: val_loss -0.6988\n",
      "2025-01-17 03:59:10.319747: Pseudo dice [np.float32(0.708)]\n",
      "2025-01-17 03:59:10.319788: Epoch time: 110.52 s\n",
      "2025-01-17 03:59:10.887671: \n",
      "2025-01-17 03:59:10.887805: Epoch 914\n",
      "2025-01-17 03:59:10.887869: Current learning rate: 0.0011\n",
      "2025-01-17 04:01:01.534550: train_loss -0.8564\n",
      "2025-01-17 04:01:01.534675: val_loss -0.7138\n",
      "2025-01-17 04:01:01.534707: Pseudo dice [np.float32(0.6988)]\n",
      "2025-01-17 04:01:01.534739: Epoch time: 110.65 s\n",
      "2025-01-17 04:01:02.111896: \n",
      "2025-01-17 04:01:02.112038: Epoch 915\n",
      "2025-01-17 04:01:02.112102: Current learning rate: 0.00109\n",
      "2025-01-17 04:02:52.795373: train_loss -0.8454\n",
      "2025-01-17 04:02:52.795553: val_loss -0.6375\n",
      "2025-01-17 04:02:52.795588: Pseudo dice [np.float32(0.7181)]\n",
      "2025-01-17 04:02:52.795621: Epoch time: 110.68 s\n",
      "2025-01-17 04:02:53.565533: \n",
      "2025-01-17 04:02:53.565678: Epoch 916\n",
      "2025-01-17 04:02:53.565750: Current learning rate: 0.00108\n",
      "2025-01-17 04:04:44.217187: train_loss -0.8634\n",
      "2025-01-17 04:04:44.217299: val_loss -0.5489\n",
      "2025-01-17 04:04:44.217331: Pseudo dice [np.float32(0.6808)]\n",
      "2025-01-17 04:04:44.217365: Epoch time: 110.65 s\n",
      "2025-01-17 04:04:44.792009: \n",
      "2025-01-17 04:04:44.792203: Epoch 917\n",
      "2025-01-17 04:04:44.792276: Current learning rate: 0.00106\n",
      "2025-01-17 04:06:35.279497: train_loss -0.8555\n",
      "2025-01-17 04:06:35.279627: val_loss -0.7408\n",
      "2025-01-17 04:06:35.279661: Pseudo dice [np.float32(0.7476)]\n",
      "2025-01-17 04:06:35.279708: Epoch time: 110.49 s\n",
      "2025-01-17 04:06:35.853010: \n",
      "2025-01-17 04:06:35.853195: Epoch 918\n",
      "2025-01-17 04:06:35.853345: Current learning rate: 0.00105\n",
      "2025-01-17 04:08:26.513619: train_loss -0.8606\n",
      "2025-01-17 04:08:26.513743: val_loss -0.6488\n",
      "2025-01-17 04:08:26.513777: Pseudo dice [np.float32(0.6983)]\n",
      "2025-01-17 04:08:26.513811: Epoch time: 110.66 s\n",
      "2025-01-17 04:08:27.081402: \n",
      "2025-01-17 04:08:27.081489: Epoch 919\n",
      "2025-01-17 04:08:27.081551: Current learning rate: 0.00104\n",
      "2025-01-17 04:10:17.570016: train_loss -0.8639\n",
      "2025-01-17 04:10:17.570138: val_loss -0.7137\n",
      "2025-01-17 04:10:17.570171: Pseudo dice [np.float32(0.7575)]\n",
      "2025-01-17 04:10:17.570202: Epoch time: 110.49 s\n",
      "2025-01-17 04:10:18.148994: \n",
      "2025-01-17 04:10:18.149131: Epoch 920\n",
      "2025-01-17 04:10:18.149202: Current learning rate: 0.00103\n",
      "2025-01-17 04:12:08.751208: train_loss -0.854\n",
      "2025-01-17 04:12:08.751334: val_loss -0.6908\n",
      "2025-01-17 04:12:08.751366: Pseudo dice [np.float32(0.7102)]\n",
      "2025-01-17 04:12:08.751399: Epoch time: 110.6 s\n",
      "2025-01-17 04:12:09.324502: \n",
      "2025-01-17 04:12:09.324657: Epoch 921\n",
      "2025-01-17 04:12:09.324728: Current learning rate: 0.00102\n",
      "2025-01-17 04:13:59.989989: train_loss -0.8786\n",
      "2025-01-17 04:13:59.990112: val_loss -0.6771\n",
      "2025-01-17 04:13:59.990146: Pseudo dice [np.float32(0.6735)]\n",
      "2025-01-17 04:13:59.990178: Epoch time: 110.67 s\n",
      "2025-01-17 04:14:00.583811: \n",
      "2025-01-17 04:14:00.583985: Epoch 922\n",
      "2025-01-17 04:14:00.584137: Current learning rate: 0.00101\n",
      "2025-01-17 04:15:51.245460: train_loss -0.8574\n",
      "2025-01-17 04:15:51.245576: val_loss -0.6763\n",
      "2025-01-17 04:15:51.245611: Pseudo dice [np.float32(0.7083)]\n",
      "2025-01-17 04:15:51.245643: Epoch time: 110.66 s\n",
      "2025-01-17 04:15:51.817513: \n",
      "2025-01-17 04:15:51.817769: Epoch 923\n",
      "2025-01-17 04:15:51.817910: Current learning rate: 0.001\n",
      "2025-01-17 04:17:42.512865: train_loss -0.8617\n",
      "2025-01-17 04:17:42.512986: val_loss -0.7161\n",
      "2025-01-17 04:17:42.513018: Pseudo dice [np.float32(0.6758)]\n",
      "2025-01-17 04:17:42.513050: Epoch time: 110.7 s\n",
      "2025-01-17 04:17:43.083608: \n",
      "2025-01-17 04:17:43.083704: Epoch 924\n",
      "2025-01-17 04:17:43.083771: Current learning rate: 0.00098\n",
      "2025-01-17 04:19:33.569498: train_loss -0.8609\n",
      "2025-01-17 04:19:33.569616: val_loss -0.7063\n",
      "2025-01-17 04:19:33.569650: Pseudo dice [np.float32(0.6806)]\n",
      "2025-01-17 04:19:33.569696: Epoch time: 110.49 s\n",
      "2025-01-17 04:19:34.139117: \n",
      "2025-01-17 04:19:34.139440: Epoch 925\n",
      "2025-01-17 04:19:34.139521: Current learning rate: 0.00097\n",
      "2025-01-17 04:21:24.659423: train_loss -0.8602\n",
      "2025-01-17 04:21:24.659626: val_loss -0.6933\n",
      "2025-01-17 04:21:24.659695: Pseudo dice [np.float32(0.6703)]\n",
      "2025-01-17 04:21:24.659749: Epoch time: 110.52 s\n",
      "2025-01-17 04:21:25.234167: \n",
      "2025-01-17 04:21:25.234284: Epoch 926\n",
      "2025-01-17 04:21:25.234414: Current learning rate: 0.00096\n",
      "2025-01-17 04:23:15.752964: train_loss -0.8504\n",
      "2025-01-17 04:23:15.753179: val_loss -0.7081\n",
      "2025-01-17 04:23:15.753223: Pseudo dice [np.float32(0.7376)]\n",
      "2025-01-17 04:23:15.753255: Epoch time: 110.52 s\n",
      "2025-01-17 04:23:16.322635: \n",
      "2025-01-17 04:23:16.322820: Epoch 927\n",
      "2025-01-17 04:23:16.322896: Current learning rate: 0.00095\n",
      "2025-01-17 04:25:06.965350: train_loss -0.8583\n",
      "2025-01-17 04:25:06.965479: val_loss -0.6654\n",
      "2025-01-17 04:25:06.965512: Pseudo dice [np.float32(0.6543)]\n",
      "2025-01-17 04:25:06.965545: Epoch time: 110.64 s\n",
      "2025-01-17 04:25:07.538924: \n",
      "2025-01-17 04:25:07.539093: Epoch 928\n",
      "2025-01-17 04:25:07.539230: Current learning rate: 0.00094\n",
      "2025-01-17 04:26:58.221826: train_loss -0.8499\n",
      "2025-01-17 04:26:58.221975: val_loss -0.7464\n",
      "2025-01-17 04:26:58.222044: Pseudo dice [np.float32(0.7497)]\n",
      "2025-01-17 04:26:58.222083: Epoch time: 110.68 s\n",
      "2025-01-17 04:26:59.002433: \n",
      "2025-01-17 04:26:59.002620: Epoch 929\n",
      "2025-01-17 04:26:59.002738: Current learning rate: 0.00092\n",
      "2025-01-17 04:28:49.660728: train_loss -0.8707\n",
      "2025-01-17 04:28:49.660846: val_loss -0.6513\n",
      "2025-01-17 04:28:49.660880: Pseudo dice [np.float32(0.6765)]\n",
      "2025-01-17 04:28:49.660911: Epoch time: 110.66 s\n",
      "2025-01-17 04:28:50.228914: \n",
      "Epoch 9307 04:28:50.229250: \n",
      "2025-01-17 04:28:50.229369: Current learning rate: 0.00091\n",
      "2025-01-17 04:30:40.721236: train_loss -0.8758\n",
      "2025-01-17 04:30:40.721352: val_loss -0.7224\n",
      "2025-01-17 04:30:40.721385: Pseudo dice [np.float32(0.6936)]\n",
      "2025-01-17 04:30:40.721416: Epoch time: 110.49 s\n",
      "2025-01-17 04:30:41.293552: \n",
      "2025-01-17 04:30:41.293692: Epoch 931\n",
      "2025-01-17 04:30:41.293772: Current learning rate: 0.0009\n",
      "2025-01-17 04:32:32.134148: train_loss -0.8537\n",
      "2025-01-17 04:32:32.134427: val_loss -0.7417\n",
      "2025-01-17 04:32:32.134662: Pseudo dice [np.float32(0.748)]\n",
      "2025-01-17 04:32:32.134717: Epoch time: 110.84 s\n",
      "2025-01-17 04:32:32.707767: \n",
      "2025-01-17 04:32:32.707937: Epoch 932\n",
      "2025-01-17 04:32:32.708009: Current learning rate: 0.00089\n",
      "2025-01-17 04:34:23.357879: train_loss -0.8584\n",
      "2025-01-17 04:34:23.358006: val_loss -0.718\n",
      "2025-01-17 04:34:23.358039: Pseudo dice [np.float32(0.7212)]\n",
      "2025-01-17 04:34:23.358073: Epoch time: 110.65 s\n",
      "2025-01-17 04:34:23.927183: \n",
      "2025-01-17 04:34:23.927333: Epoch 933\n",
      "2025-01-17 04:34:23.927406: Current learning rate: 0.00088\n",
      "2025-01-17 04:36:14.572931: train_loss -0.8769\n",
      "2025-01-17 04:36:14.573052: val_loss -0.7821\n",
      "2025-01-17 04:36:14.573084: Pseudo dice [np.float32(0.8227)]\n",
      "2025-01-17 04:36:14.573116: Epoch time: 110.65 s\n",
      "2025-01-17 04:36:15.141039: \n",
      "2025-01-17 04:36:15.141165: Epoch 934\n",
      "2025-01-17 04:36:15.141306: Current learning rate: 0.00087\n",
      "2025-01-17 04:38:05.789015: train_loss -0.8447\n",
      "2025-01-17 04:38:05.789137: val_loss -0.6614\n",
      "2025-01-17 04:38:05.789171: Pseudo dice [np.float32(0.5565)]\n",
      "2025-01-17 04:38:05.789202: Epoch time: 110.65 s\n",
      "2025-01-17 04:38:06.353104: \n",
      "2025-01-17 04:38:06.353263: Epoch 935\n",
      "2025-01-17 04:38:06.353331: Current learning rate: 0.00085\n",
      "2025-01-17 04:39:57.001496: train_loss -0.8694\n",
      "2025-01-17 04:39:57.001705: val_loss -0.7093\n",
      "2025-01-17 04:39:57.001834: Pseudo dice [np.float32(0.6488)]\n",
      "2025-01-17 04:39:57.001940: Epoch time: 110.65 s\n",
      "2025-01-17 04:39:57.563405: \n",
      "2025-01-17 04:39:57.563582: Epoch 936\n",
      "2025-01-17 04:39:57.563651: Current learning rate: 0.00084\n",
      "2025-01-17 04:41:48.224864: train_loss -0.8563\n",
      "2025-01-17 04:41:48.224993: val_loss -0.6711\n",
      "2025-01-17 04:41:48.225025: Pseudo dice [np.float32(0.6465)]\n",
      "2025-01-17 04:41:48.225055: Epoch time: 110.66 s\n",
      "2025-01-17 04:41:48.805972: \n",
      "2025-01-17 04:41:48.806253: Epoch 937\n",
      "2025-01-17 04:41:48.806432: Current learning rate: 0.00083\n",
      "2025-01-17 04:43:39.434738: train_loss -0.8555\n",
      "2025-01-17 04:43:39.434870: val_loss -0.7461\n",
      "2025-01-17 04:43:39.434909: Pseudo dice [np.float32(0.7721)]\n",
      "2025-01-17 04:43:39.434941: Epoch time: 110.63 s\n",
      "2025-01-17 04:43:39.997316: \n",
      "2025-01-17 04:43:39.997444: Epoch 938\n",
      "2025-01-17 04:43:39.997589: Current learning rate: 0.00082\n",
      "2025-01-17 04:45:30.649601: train_loss -0.876\n",
      "2025-01-17 04:45:30.649723: val_loss -0.7449\n",
      "2025-01-17 04:45:30.649757: Pseudo dice [np.float32(0.7765)]\n",
      "2025-01-17 04:45:30.649795: Epoch time: 110.65 s\n",
      "2025-01-17 04:45:31.221006: \n",
      "2025-01-17 04:45:31.221354: Epoch 939\n",
      "2025-01-17 04:45:31.221464: Current learning rate: 0.00081\n",
      "2025-01-17 04:47:21.879410: train_loss -0.8415\n",
      "2025-01-17 04:47:21.879533: val_loss -0.7034\n",
      "2025-01-17 04:47:21.879567: Pseudo dice [np.float32(0.6515)]\n",
      "2025-01-17 04:47:21.879599: Epoch time: 110.66 s\n",
      "2025-01-17 04:47:22.449522: \n",
      "2025-01-17 04:47:22.449675: Epoch 940\n",
      "2025-01-17 04:47:22.449739: Current learning rate: 0.00079\n",
      "2025-01-17 04:49:13.107905: train_loss -0.8626\n",
      "2025-01-17 04:49:13.108026: val_loss -0.7243\n",
      "2025-01-17 04:49:13.108057: Pseudo dice [np.float32(0.7476)]\n",
      "2025-01-17 04:49:13.108089: Epoch time: 110.66 s\n",
      "2025-01-17 04:49:13.675194: \n",
      "2025-01-17 04:49:13.675279: Epoch 941\n",
      "2025-01-17 04:49:13.675341: Current learning rate: 0.00078\n",
      "2025-01-17 04:51:04.341182: train_loss -0.8759\n",
      "2025-01-17 04:51:04.341302: val_loss -0.7196\n",
      "2025-01-17 04:51:04.341334: Pseudo dice [np.float32(0.6951)]\n",
      "2025-01-17 04:51:04.341367: Epoch time: 110.67 s\n",
      "2025-01-17 04:51:05.100523: \n",
      "2025-01-17 04:51:05.100896: Epoch 942\n",
      "2025-01-17 04:51:05.101021: Current learning rate: 0.00077\n",
      "2025-01-17 04:52:55.782821: train_loss -0.8677\n",
      "2025-01-17 04:52:55.782939: val_loss -0.7547\n",
      "2025-01-17 04:52:55.782971: Pseudo dice [np.float32(0.7437)]\n",
      "2025-01-17 04:52:55.783003: Epoch time: 110.68 s\n",
      "2025-01-17 04:52:56.347721: \n",
      "2025-01-17 04:52:56.347810: Epoch 943\n",
      "2025-01-17 04:52:56.347873: Current learning rate: 0.00076\n",
      "2025-01-17 04:54:46.960663: train_loss -0.8589\n",
      "2025-01-17 04:54:46.960789: val_loss -0.6935\n",
      "2025-01-17 04:54:46.960822: Pseudo dice [np.float32(0.6886)]\n",
      "2025-01-17 04:54:46.960855: Epoch time: 110.61 s\n",
      "2025-01-17 04:54:47.530158: \n",
      "2025-01-17 04:54:47.530334: Epoch 944\n",
      "2025-01-17 04:54:47.530398: Current learning rate: 0.00075\n",
      "2025-01-17 04:56:38.019555: train_loss -0.8704\n",
      "2025-01-17 04:56:38.019730: val_loss -0.7528\n",
      "2025-01-17 04:56:38.019764: Pseudo dice [np.float32(0.703)]\n",
      "2025-01-17 04:56:38.019795: Epoch time: 110.49 s\n",
      "2025-01-17 04:56:38.583057: \n",
      "2025-01-17 04:56:38.583388: Epoch 945\n",
      "2025-01-17 04:56:38.583463: Current learning rate: 0.00074\n",
      "2025-01-17 04:58:29.341843: train_loss -0.8686\n",
      "2025-01-17 04:58:29.341970: val_loss -0.6659\n",
      "2025-01-17 04:58:29.342002: Pseudo dice [np.float32(0.5688)]\n",
      "2025-01-17 04:58:29.342052: Epoch time: 110.76 s\n",
      "2025-01-17 04:58:29.920454: \n",
      "2025-01-17 04:58:29.920629: Epoch 946\n",
      "2025-01-17 04:58:29.920705: Current learning rate: 0.00072\n",
      "2025-01-17 05:00:20.628361: train_loss -0.8391\n",
      "2025-01-17 05:00:20.628487: val_loss -0.7442\n",
      "2025-01-17 05:00:20.628521: Pseudo dice [np.float32(0.7139)]\n",
      "2025-01-17 05:00:20.628553: Epoch time: 110.71 s\n",
      "2025-01-17 05:00:21.211693: \n",
      "2025-01-17 05:00:21.211791: Epoch 947\n",
      "2025-01-17 05:00:21.211852: Current learning rate: 0.00071\n",
      "2025-01-17 05:02:11.776243: train_loss -0.8687\n",
      "2025-01-17 05:02:11.776459: val_loss -0.7357\n",
      "2025-01-17 05:02:11.776504: Pseudo dice [np.float32(0.7094)]\n",
      "2025-01-17 05:02:11.776539: Epoch time: 110.57 s\n",
      "2025-01-17 05:02:12.348085: \n",
      "2025-01-17 05:02:12.348170: Epoch 948\n",
      "2025-01-17 05:02:12.348229: Current learning rate: 0.0007\n",
      "2025-01-17 05:04:02.902168: train_loss -0.8571\n",
      "2025-01-17 05:04:02.902439: val_loss -0.6088\n",
      "2025-01-17 05:04:02.902483: Pseudo dice [np.float32(0.6492)]\n",
      "2025-01-17 05:04:02.902516: Epoch time: 110.55 s\n",
      "2025-01-17 05:04:03.476732: \n",
      "2025-01-17 05:04:03.476907: Epoch 949\n",
      "2025-01-17 05:04:03.476979: Current learning rate: 0.00069\n",
      "2025-01-17 05:05:54.236480: train_loss -0.868\n",
      "2025-01-17 05:05:54.236637: val_loss -0.6624\n",
      "2025-01-17 05:05:54.236716: Pseudo dice [np.float32(0.6752)]\n",
      "2025-01-17 05:05:54.236758: Epoch time: 110.76 s\n",
      "2025-01-17 05:05:55.086514: \n",
      "2025-01-17 05:05:55.086765: Epoch 950\n",
      "2025-01-17 05:05:55.086904: Current learning rate: 0.00067\n",
      "2025-01-17 05:07:45.598240: train_loss -0.8672\n",
      "2025-01-17 05:07:45.598454: val_loss -0.7237\n",
      "2025-01-17 05:07:45.598492: Pseudo dice [np.float32(0.7355)]\n",
      "2025-01-17 05:07:45.598525: Epoch time: 110.51 s\n",
      "2025-01-17 05:07:46.177927: \n",
      "2025-01-17 05:07:46.178019: Epoch 951\n",
      "2025-01-17 05:07:46.178081: Current learning rate: 0.00066\n",
      "2025-01-17 05:09:36.999340: train_loss -0.8431\n",
      "2025-01-17 05:09:36.999474: val_loss -0.6622\n",
      "2025-01-17 05:09:36.999512: Pseudo dice [np.float32(0.6646)]\n",
      "2025-01-17 05:09:36.999544: Epoch time: 110.82 s\n",
      "2025-01-17 05:09:37.577150: \n",
      "2025-01-17 05:09:37.577236: Epoch 952\n",
      "2025-01-17 05:09:37.577298: Current learning rate: 0.00065\n",
      "2025-01-17 05:11:28.259209: train_loss -0.8735\n",
      "2025-01-17 05:11:28.259347: val_loss -0.6841\n",
      "2025-01-17 05:11:28.259382: Pseudo dice [np.float32(0.6661)]\n",
      "2025-01-17 05:11:28.259415: Epoch time: 110.68 s\n",
      "2025-01-17 05:11:28.836008: \n",
      "2025-01-17 05:11:28.836099: Epoch 953\n",
      "2025-01-17 05:11:28.836162: Current learning rate: 0.00064\n",
      "2025-01-17 05:13:19.534757: train_loss -0.8623\n",
      "2025-01-17 05:13:19.534875: val_loss -0.5719\n",
      "2025-01-17 05:13:19.534906: Pseudo dice [np.float32(0.6473)]\n",
      "2025-01-17 05:13:19.534938: Epoch time: 110.7 s\n",
      "2025-01-17 05:13:20.113753: \n",
      "2025-01-17 05:13:20.114166: Epoch 954\n",
      "2025-01-17 05:13:20.114313: Current learning rate: 0.00063\n",
      "2025-01-17 05:15:10.672075: train_loss -0.8653\n",
      "2025-01-17 05:15:10.672199: val_loss -0.7537\n",
      "2025-01-17 05:15:10.672233: Pseudo dice [np.float32(0.7665)]\n",
      "2025-01-17 05:15:10.672265: Epoch time: 110.56 s\n",
      "2025-01-17 05:15:11.501682: \n",
      "Epoch 9557 05:15:11.501890: \n",
      "2025-01-17 05:15:11.502017: Current learning rate: 0.00061\n",
      "2025-01-17 05:17:02.091331: train_loss -0.8749\n",
      "2025-01-17 05:17:02.091466: val_loss -0.7386\n",
      "2025-01-17 05:17:02.091501: Pseudo dice [np.float32(0.7049)]\n",
      "2025-01-17 05:17:02.091532: Epoch time: 110.59 s\n",
      "2025-01-17 05:17:02.668559: \n",
      "2025-01-17 05:17:02.668920: Epoch 956\n",
      "2025-01-17 05:17:02.668988: Current learning rate: 0.0006\n",
      "2025-01-17 05:18:53.403193: train_loss -0.8585\n",
      "2025-01-17 05:18:53.403310: val_loss -0.7273\n",
      "2025-01-17 05:18:53.403342: Pseudo dice [np.float32(0.713)]\n",
      "2025-01-17 05:18:53.403375: Epoch time: 110.74 s\n",
      "2025-01-17 05:18:53.985771: \n",
      "2025-01-17 05:18:53.985862: Epoch 957\n",
      "2025-01-17 05:18:53.985925: Current learning rate: 0.00059\n",
      "2025-01-17 05:20:44.713470: train_loss -0.8717\n",
      "2025-01-17 05:20:44.713805: val_loss -0.7485\n",
      "2025-01-17 05:20:44.713851: Pseudo dice [np.float32(0.7291)]\n",
      "2025-01-17 05:20:44.713887: Epoch time: 110.73 s\n",
      "2025-01-17 05:20:45.335162: \n",
      "2025-01-17 05:20:45.335549: Epoch 958\n",
      "2025-01-17 05:20:45.335635: Current learning rate: 0.00058\n",
      "2025-01-17 05:22:36.089816: train_loss -0.8491\n",
      "2025-01-17 05:22:36.090161: val_loss -0.7262\n",
      "2025-01-17 05:22:36.090269: Pseudo dice [np.float32(0.6739)]\n",
      "2025-01-17 05:22:36.090311: Epoch time: 110.76 s\n",
      "2025-01-17 05:22:36.676126: \n",
      "2025-01-17 05:22:36.676419: Epoch 959\n",
      "2025-01-17 05:22:36.676550: Current learning rate: 0.00056\n",
      "2025-01-17 05:24:27.384373: train_loss -0.8708\n",
      "2025-01-17 05:24:27.384504: val_loss -0.7054\n",
      "2025-01-17 05:24:27.384546: Pseudo dice [np.float32(0.6687)]\n",
      "2025-01-17 05:24:27.384696: Epoch time: 110.71 s\n",
      "2025-01-17 05:24:27.967634: \n",
      "2025-01-17 05:24:27.967828: Epoch 960\n",
      "2025-01-17 05:24:27.967900: Current learning rate: 0.00055\n",
      "2025-01-17 05:26:18.539977: train_loss -0.8737\n",
      "2025-01-17 05:26:18.540095: val_loss -0.7095\n",
      "2025-01-17 05:26:18.540127: Pseudo dice [np.float32(0.6767)]\n",
      "2025-01-17 05:26:18.540159: Epoch time: 110.57 s\n",
      "2025-01-17 05:26:19.125816: \n",
      "2025-01-17 05:26:19.125958: Epoch 961\n",
      "2025-01-17 05:26:19.126029: Current learning rate: 0.00054\n",
      "2025-01-17 05:28:09.833113: train_loss -0.8716\n",
      "2025-01-17 05:28:09.833274: val_loss -0.7198\n",
      "2025-01-17 05:28:09.833318: Pseudo dice [np.float32(0.6314)]\n",
      "2025-01-17 05:28:09.833352: Epoch time: 110.71 s\n",
      "2025-01-17 05:28:10.417941: \n",
      "2025-01-17 05:28:10.418032: Epoch 962\n",
      "2025-01-17 05:28:10.418092: Current learning rate: 0.00053\n",
      "2025-01-17 05:30:01.146826: train_loss -0.8779\n",
      "2025-01-17 05:30:01.147021: val_loss -0.7543\n",
      "2025-01-17 05:30:01.147062: Pseudo dice [np.float32(0.7117)]\n",
      "2025-01-17 05:30:01.147097: Epoch time: 110.73 s\n",
      "2025-01-17 05:30:01.731954: \n",
      "2025-01-17 05:30:01.732148: Epoch 963\n",
      "2025-01-17 05:30:01.732229: Current learning rate: 0.00051\n",
      "2025-01-17 05:31:52.438124: train_loss -0.8581\n",
      "2025-01-17 05:31:52.438250: val_loss -0.7698\n",
      "2025-01-17 05:31:52.438283: Pseudo dice [np.float32(0.757)]\n",
      "2025-01-17 05:31:52.438317: Epoch time: 110.71 s\n",
      "2025-01-17 05:31:53.029932: \n",
      "2025-01-17 05:31:53.030124: Epoch 964\n",
      "2025-01-17 05:31:53.030197: Current learning rate: 0.0005\n",
      "2025-01-17 05:33:43.768973: train_loss -0.8573\n",
      "2025-01-17 05:33:43.769094: val_loss -0.7161\n",
      "2025-01-17 05:33:43.769127: Pseudo dice [np.float32(0.6903)]\n",
      "2025-01-17 05:33:43.769161: Epoch time: 110.74 s\n",
      "2025-01-17 05:33:44.358515: \n",
      "2025-01-17 05:33:44.358711: Epoch 965\n",
      "2025-01-17 05:33:44.358783: Current learning rate: 0.00049\n",
      "2025-01-17 05:35:34.946890: train_loss -0.8621\n",
      "2025-01-17 05:35:34.947061: val_loss -0.764\n",
      "2025-01-17 05:35:34.947093: Pseudo dice [np.float32(0.7183)]\n",
      "2025-01-17 05:35:34.947127: Epoch time: 110.59 s\n",
      "2025-01-17 05:35:35.526308: \n",
      "2025-01-17 05:35:35.526538: Epoch 966\n",
      "2025-01-17 05:35:35.526701: Current learning rate: 0.00048\n",
      "2025-01-17 05:37:26.283890: train_loss -0.8706\n",
      "2025-01-17 05:37:26.284028: val_loss -0.7326\n",
      "2025-01-17 05:37:26.284059: Pseudo dice [np.float32(0.7239)]\n",
      "2025-01-17 05:37:26.284091: Epoch time: 110.76 s\n",
      "2025-01-17 05:37:26.868239: \n",
      "2025-01-17 05:37:26.868324: Epoch 967\n",
      "2025-01-17 05:37:26.868386: Current learning rate: 0.00046\n",
      "2025-01-17 05:39:17.639805: train_loss -0.879\n",
      "2025-01-17 05:39:17.639934: val_loss -0.7311\n",
      "2025-01-17 05:39:17.639968: Pseudo dice [np.float32(0.6872)]\n",
      "2025-01-17 05:39:17.640001: Epoch time: 110.77 s\n",
      "2025-01-17 05:39:18.228130: \n",
      "2025-01-17 05:39:18.228220: Epoch 968\n",
      "2025-01-17 05:39:18.228290: Current learning rate: 0.00045\n",
      "2025-01-17 05:41:08.846890: train_loss -0.8663\n",
      "2025-01-17 05:41:08.847038: val_loss -0.7307\n",
      "2025-01-17 05:41:08.847174: Pseudo dice [np.float32(0.7126)]\n",
      "2025-01-17 05:41:08.847259: Epoch time: 110.62 s\n",
      "2025-01-17 05:41:09.689052: \n",
      "2025-01-17 05:41:09.689157: Epoch 969\n",
      "2025-01-17 05:41:09.689235: Current learning rate: 0.00044\n",
      "2025-01-17 05:43:00.413134: train_loss -0.8804\n",
      "2025-01-17 05:43:00.413424: val_loss -0.7463\n",
      "2025-01-17 05:43:00.413495: Pseudo dice [np.float32(0.7665)]\n",
      "2025-01-17 05:43:00.413536: Epoch time: 110.72 s\n",
      "2025-01-17 05:43:01.008824: \n",
      "2025-01-17 05:43:01.008921: Epoch 970\n",
      "2025-01-17 05:43:01.008983: Current learning rate: 0.00043\n",
      "2025-01-17 05:44:51.762999: train_loss -0.8513\n",
      "2025-01-17 05:44:51.763114: val_loss -0.6767\n",
      "2025-01-17 05:44:51.763145: Pseudo dice [np.float32(0.6928)]\n",
      "2025-01-17 05:44:51.763176: Epoch time: 110.75 s\n",
      "2025-01-17 05:44:52.350898: \n",
      "2025-01-17 05:44:52.350980: Epoch 971\n",
      "2025-01-17 05:44:52.351038: Current learning rate: 0.00041\n",
      "2025-01-17 05:46:43.119409: train_loss -0.8658\n",
      "2025-01-17 05:46:43.119578: val_loss -0.7439\n",
      "2025-01-17 05:46:43.119631: Pseudo dice [np.float32(0.7188)]\n",
      "2025-01-17 05:46:43.119667: Epoch time: 110.77 s\n",
      "2025-01-17 05:46:43.712557: \n",
      "2025-01-17 05:46:43.712645: Epoch 972\n",
      "2025-01-17 05:46:43.712708: Current learning rate: 0.0004\n",
      "2025-01-17 05:48:34.285732: train_loss -0.8649\n",
      "2025-01-17 05:48:34.286022: val_loss -0.7074\n",
      "2025-01-17 05:48:34.286153: Pseudo dice [np.float32(0.6404)]\n",
      "2025-01-17 05:48:34.286216: Epoch time: 110.57 s\n",
      "2025-01-17 05:48:34.872974: \n",
      "2025-01-17 05:48:34.873381: Epoch 973\n",
      "2025-01-17 05:48:34.873470: Current learning rate: 0.00039\n",
      "2025-01-17 05:50:25.611765: train_loss -0.8509\n",
      "2025-01-17 05:50:25.611945: val_loss -0.7943\n",
      "2025-01-17 05:50:25.611979: Pseudo dice [np.float32(0.8001)]\n",
      "2025-01-17 05:50:25.612010: Epoch time: 110.74 s\n",
      "2025-01-17 05:50:26.199299: \n",
      "2025-01-17 05:50:26.199669: Epoch 974\n",
      "2025-01-17 05:50:26.199837: Current learning rate: 0.00037\n",
      "2025-01-17 05:52:16.966042: train_loss -0.8707\n",
      "2025-01-17 05:52:16.966230: val_loss -0.7637\n",
      "2025-01-17 05:52:16.966265: Pseudo dice [np.float32(0.7339)]\n",
      "2025-01-17 05:52:16.966298: Epoch time: 110.77 s\n",
      "2025-01-17 05:52:17.564219: \n",
      "2025-01-17 05:52:17.564314: Epoch 975\n",
      "2025-01-17 05:52:17.564375: Current learning rate: 0.00036\n",
      "2025-01-17 05:54:08.291564: train_loss -0.8705\n",
      "2025-01-17 05:54:08.291775: val_loss -0.7888\n",
      "2025-01-17 05:54:08.291820: Pseudo dice [np.float32(0.8154)]\n",
      "2025-01-17 05:54:08.291855: Epoch time: 110.73 s\n",
      "2025-01-17 05:54:08.291876: Yayy! New best EMA pseudo Dice: 0.7226999998092651\n",
      "2025-01-17 05:54:09.139352: \n",
      "2025-01-17 05:54:09.139531: Epoch 976\n",
      "2025-01-17 05:54:09.139604: Current learning rate: 0.00035\n",
      "2025-01-17 05:55:59.792387: train_loss -0.8645\n",
      "2025-01-17 05:55:59.792517: val_loss -0.7609\n",
      "2025-01-17 05:55:59.792551: Pseudo dice [np.float32(0.7775)]\n",
      "2025-01-17 05:55:59.792583: Epoch time: 110.65 s\n",
      "2025-01-17 05:55:59.792602: Yayy! New best EMA pseudo Dice: 0.7282000184059143\n",
      "2025-01-17 05:56:00.634519: \n",
      "2025-01-17 05:56:00.634838: Epoch 977\n",
      "2025-01-17 05:56:00.634940: Current learning rate: 0.00034\n",
      "2025-01-17 05:57:51.181930: train_loss -0.8496\n",
      "2025-01-17 05:57:51.182060: val_loss -0.719\n",
      "2025-01-17 05:57:51.182091: Pseudo dice [np.float32(0.6549)]\n",
      "2025-01-17 05:57:51.182124: Epoch time: 110.55 s\n",
      "2025-01-17 05:57:51.779842: \n",
      "2025-01-17 05:57:51.780158: Epoch 978\n",
      "2025-01-17 05:57:51.780233: Current learning rate: 0.00032\n",
      "2025-01-17 05:59:42.350099: train_loss -0.8561\n",
      "2025-01-17 05:59:42.350519: val_loss -0.7719\n",
      "2025-01-17 05:59:42.350594: Pseudo dice [np.float32(0.7944)]\n",
      "2025-01-17 05:59:42.350637: Epoch time: 110.57 s\n",
      "2025-01-17 05:59:42.350660: Yayy! New best EMA pseudo Dice: 0.7282000184059143\n",
      "2025-01-17 05:59:43.189657: \n",
      "2025-01-17 05:59:43.189836: Epoch 979\n",
      "2025-01-17 05:59:43.189909: Current learning rate: 0.00031\n",
      "2025-01-17 06:01:33.947614: train_loss -0.8787\n",
      "2025-01-17 06:01:33.947805: val_loss -0.7315\n",
      "2025-01-17 06:01:33.947905: Pseudo dice [np.float32(0.6828)]\n",
      "2025-01-17 06:01:33.947971: Epoch time: 110.76 s\n",
      "2025-01-17 06:01:34.532129: \n",
      "2025-01-17 06:01:34.532554: Epoch 980\n",
      "2025-01-17 06:01:34.532715: Current learning rate: 0.0003\n",
      "2025-01-17 06:03:25.139688: train_loss -0.8706\n",
      "2025-01-17 06:03:25.139806: val_loss -0.7283\n",
      "2025-01-17 06:03:25.139840: Pseudo dice [np.float32(0.6062)]\n",
      "2025-01-17 06:03:25.139873: Epoch time: 110.61 s\n",
      "2025-01-17 06:03:25.953432: \n",
      "2025-01-17 06:03:25.953615: Epoch 981\n",
      "2025-01-17 06:03:25.953696: Current learning rate: 0.00028\n",
      "2025-01-17 06:05:16.751238: train_loss -0.8674\n",
      "2025-01-17 06:05:16.751415: val_loss -0.7486\n",
      "2025-01-17 06:05:16.751532: Pseudo dice [np.float32(0.7289)]\n",
      "2025-01-17 06:05:16.751599: Epoch time: 110.8 s\n",
      "2025-01-17 06:05:17.342249: \n",
      "2025-01-17 06:05:17.342346: Epoch 982\n",
      "2025-01-17 06:05:17.342412: Current learning rate: 0.00027\n",
      "2025-01-17 06:07:08.104663: train_loss -0.8704\n",
      "2025-01-17 06:07:08.104780: val_loss -0.7813\n",
      "2025-01-17 06:07:08.104812: Pseudo dice [np.float32(0.7233)]\n",
      "2025-01-17 06:07:08.104846: Epoch time: 110.76 s\n",
      "2025-01-17 06:07:08.693176: \n",
      "2025-01-17 06:07:08.693519: Epoch 983\n",
      "2025-01-17 06:07:08.693718: Current learning rate: 0.00026\n",
      "2025-01-17 06:08:59.488628: train_loss -0.8728\n",
      "2025-01-17 06:08:59.488750: val_loss -0.7536\n",
      "2025-01-17 06:08:59.488781: Pseudo dice [np.float32(0.731)]\n",
      "2025-01-17 06:08:59.488814: Epoch time: 110.8 s\n",
      "2025-01-17 06:09:00.079026: \n",
      "2025-01-17 06:09:00.079114: Epoch 984\n",
      "2025-01-17 06:09:00.079175: Current learning rate: 0.00024\n",
      "2025-01-17 06:10:50.860931: train_loss -0.8612\n",
      "2025-01-17 06:10:50.861079: val_loss -0.7594\n",
      "2025-01-17 06:10:50.861116: Pseudo dice [np.float32(0.7266)]\n",
      "2025-01-17 06:10:50.861149: Epoch time: 110.78 s\n",
      "2025-01-17 06:10:51.450550: \n",
      "2025-01-17 06:10:51.450643: Epoch 985\n",
      "2025-01-17 06:10:51.450704: Current learning rate: 0.00023\n",
      "2025-01-17 06:12:42.178729: train_loss -0.8596\n",
      "2025-01-17 06:12:42.178857: val_loss -0.6867\n",
      "2025-01-17 06:12:42.178890: Pseudo dice [np.float32(0.6552)]\n",
      "2025-01-17 06:12:42.178924: Epoch time: 110.73 s\n",
      "2025-01-17 06:12:42.773451: \n",
      "2025-01-17 06:12:42.773817: Epoch 986\n",
      "2025-01-17 06:12:42.773887: Current learning rate: 0.00021\n",
      "2025-01-17 06:14:33.499097: train_loss -0.8752\n",
      "2025-01-17 06:14:33.499256: val_loss -0.7414\n",
      "2025-01-17 06:14:33.499435: Pseudo dice [np.float32(0.7294)]\n",
      "2025-01-17 06:14:33.499509: Epoch time: 110.73 s\n",
      "2025-01-17 06:14:34.095071: \n",
      "2025-01-17 06:14:34.095258: Epoch 987\n",
      "2025-01-17 06:14:34.095427: Current learning rate: 0.0002\n",
      "2025-01-17 06:16:24.818617: train_loss -0.8742\n",
      "2025-01-17 06:16:24.818734: val_loss -0.7152\n",
      "2025-01-17 06:16:24.818765: Pseudo dice [np.float32(0.736)]\n",
      "2025-01-17 06:16:24.818799: Epoch time: 110.72 s\n",
      "2025-01-17 06:16:25.413734: \n",
      "2025-01-17 06:16:25.414104: Epoch 988\n",
      "2025-01-17 06:16:25.414250: Current learning rate: 0.00019\n",
      "2025-01-17 06:18:16.163808: train_loss -0.867\n",
      "2025-01-17 06:18:16.163979: val_loss -0.7308\n",
      "2025-01-17 06:18:16.164018: Pseudo dice [np.float32(0.7323)]\n",
      "2025-01-17 06:18:16.164058: Epoch time: 110.75 s\n",
      "2025-01-17 06:18:16.760132: \n",
      "2025-01-17 06:18:16.760554: Epoch 989\n",
      "2025-01-17 06:18:16.760651: Current learning rate: 0.00017\n",
      "2025-01-17 06:20:07.541361: train_loss -0.8623\n",
      "2025-01-17 06:20:07.541481: val_loss -0.723\n",
      "2025-01-17 06:20:07.541513: Pseudo dice [np.float32(0.6571)]\n",
      "2025-01-17 06:20:07.541545: Epoch time: 110.78 s\n",
      "2025-01-17 06:20:08.136356: \n",
      "2025-01-17 06:20:08.136444: Epoch 990\n",
      "2025-01-17 06:20:08.136504: Current learning rate: 0.00016\n",
      "2025-01-17 06:21:58.876735: train_loss -0.8638\n",
      "2025-01-17 06:21:58.876947: val_loss -0.7469\n",
      "2025-01-17 06:21:58.876981: Pseudo dice [np.float32(0.7388)]\n",
      "2025-01-17 06:21:58.877014: Epoch time: 110.74 s\n",
      "2025-01-17 06:21:59.463784: \n",
      "2025-01-17 06:21:59.463874: Epoch 991\n",
      "2025-01-17 06:21:59.463938: Current learning rate: 0.00014\n",
      "2025-01-17 06:23:50.199069: train_loss -0.8516\n",
      "2025-01-17 06:23:50.199200: val_loss -0.7026\n",
      "2025-01-17 06:23:50.199230: Pseudo dice [np.float32(0.6735)]\n",
      "2025-01-17 06:23:50.199263: Epoch time: 110.74 s\n",
      "2025-01-17 06:23:50.789472: \n",
      "2025-01-17 06:23:50.789556: Epoch 992\n",
      "2025-01-17 06:23:50.789616: Current learning rate: 0.00013\n",
      "2025-01-17 06:25:41.479867: train_loss -0.8797\n",
      "2025-01-17 06:25:41.480006: val_loss -0.7381\n",
      "2025-01-17 06:25:41.480039: Pseudo dice [np.float32(0.7483)]\n",
      "2025-01-17 06:25:41.480075: Epoch time: 110.69 s\n",
      "2025-01-17 06:25:42.081715: \n",
      "2025-01-17 06:25:42.081800: Epoch 993\n",
      "2025-01-17 06:25:42.081863: Current learning rate: 0.00011\n",
      "2025-01-17 06:27:32.840816: train_loss -0.8597\n",
      "2025-01-17 06:27:32.840935: val_loss -0.7673\n",
      "2025-01-17 06:27:32.840969: Pseudo dice [np.float32(0.7608)]\n",
      "2025-01-17 06:27:32.841001: Epoch time: 110.76 s\n",
      "2025-01-17 06:27:33.649890: \n",
      "2025-01-17 06:27:33.650235: Epoch 994\n",
      "2025-01-17 06:27:33.650314: Current learning rate: 0.0001\n",
      "2025-01-17 06:29:24.397345: train_loss -0.875\n",
      "2025-01-17 06:29:24.397473: val_loss -0.7297\n",
      "2025-01-17 06:29:24.397509: Pseudo dice [np.float32(0.7274)]\n",
      "2025-01-17 06:29:24.397541: Epoch time: 110.75 s\n",
      "2025-01-17 06:29:24.993031: \n",
      "2025-01-17 06:29:24.993329: Epoch 995\n",
      "2025-01-17 06:29:24.993457: Current learning rate: 8e-05\n",
      "2025-01-17 06:31:15.586758: train_loss -0.8682\n",
      "2025-01-17 06:31:15.586890: val_loss -0.7167\n",
      "2025-01-17 06:31:15.586923: Pseudo dice [np.float32(0.6868)]\n",
      "2025-01-17 06:31:15.586957: Epoch time: 110.59 s\n",
      "2025-01-17 06:31:16.179976: \n",
      "2025-01-17 06:31:16.180271: Epoch 996\n",
      "2025-01-17 06:31:16.180455: Current learning rate: 7e-05\n",
      "2025-01-17 06:33:06.952050: train_loss -0.8601\n",
      "2025-01-17 06:33:06.952225: val_loss -0.7877\n",
      "2025-01-17 06:33:06.952294: Pseudo dice [np.float32(0.805)]\n",
      "2025-01-17 06:33:06.952336: Epoch time: 110.77 s\n",
      "2025-01-17 06:33:07.542278: \n",
      "2025-01-17 06:33:07.542374: Epoch 997\n",
      "2025-01-17 06:33:07.542439: Current learning rate: 5e-05\n",
      "2025-01-17 06:34:58.312308: train_loss -0.8727\n",
      "2025-01-17 06:34:58.312436: val_loss -0.7125\n",
      "2025-01-17 06:34:58.312468: Pseudo dice [np.float32(0.6721)]\n",
      "2025-01-17 06:34:58.312499: Epoch time: 110.77 s\n",
      "2025-01-17 06:34:58.901757: \n",
      "2025-01-17 06:34:58.902052: Epoch 998\n",
      "2025-01-17 06:34:58.902196: Current learning rate: 4e-05\n",
      "2025-01-17 06:36:49.639401: train_loss -0.8667\n",
      "2025-01-17 06:36:49.639524: val_loss -0.7282\n",
      "2025-01-17 06:36:49.639556: Pseudo dice [np.float32(0.7554)]\n",
      "2025-01-17 06:36:49.639595: Epoch time: 110.74 s\n",
      "2025-01-17 06:36:50.245056: \n",
      "2025-01-17 06:36:50.245417: Epoch 999\n",
      "2025-01-17 06:36:50.245548: Current learning rate: 2e-05\n",
      "2025-01-17 06:38:41.037198: train_loss -0.8796\n",
      "2025-01-17 06:38:41.037314: val_loss -0.7315\n",
      "2025-01-17 06:38:41.037346: Pseudo dice [np.float32(0.7074)]\n",
      "2025-01-17 06:38:41.037408: Epoch time: 110.79 s\n",
      "2025-01-17 06:38:41.903403: Training done.\n",
      "2025-01-17 06:38:41.909919: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-17 06:38:41.910139: The split file contains 5 splits.\n",
      "2025-01-17 06:38:41.910189: Desired fold for training: 3\n",
      "2025-01-17 06:38:41.910224: This split has 40 training and 10 validation cases.\n",
      "2025-01-17 06:38:41.910343: predicting lung_015\n",
      "2025-01-17 06:38:41.911126: lung_015, shape torch.Size([1, 278, 530, 530]), rank 0\n",
      "2025-01-17 06:40:19.657226: predicting lung_022\n",
      "2025-01-17 06:40:19.661011: lung_022, shape torch.Size([1, 242, 544, 544]), rank 0\n",
      "2025-01-17 06:41:51.311463: predicting lung_023\n",
      "2025-01-17 06:41:51.314819: lung_023, shape torch.Size([1, 267, 530, 530]), rank 0\n",
      "2025-01-17 06:43:22.955685: predicting lung_025\n",
      "2025-01-17 06:43:22.958965: lung_025, shape torch.Size([1, 301, 518, 518]), rank 0\n",
      "2025-01-17 06:45:09.951800: predicting lung_033\n",
      "2025-01-17 06:45:09.955361: lung_033, shape torch.Size([1, 260, 530, 530]), rank 0\n",
      "2025-01-17 06:46:41.575257: predicting lung_034\n",
      "2025-01-17 06:46:41.578213: lung_034, shape torch.Size([1, 296, 581, 581]), rank 0\n",
      "2025-01-17 06:49:11.347093: predicting lung_037\n",
      "2025-01-17 06:49:11.350864: lung_037, shape torch.Size([1, 253, 505, 505]), rank 0\n",
      "2025-01-17 06:50:42.961608: predicting lung_038\n",
      "2025-01-17 06:50:42.964506: lung_038, shape torch.Size([1, 251, 505, 505]), rank 0\n",
      "2025-01-17 06:52:14.663269: predicting lung_041\n",
      "2025-01-17 06:52:14.667958: lung_041, shape torch.Size([1, 240, 530, 530]), rank 0\n",
      "2025-01-17 06:53:31.063912: predicting lung_051\n",
      "2025-01-17 06:53:31.066793: lung_051, shape torch.Size([1, 311, 505, 505]), rank 0\n",
      "2025-01-17 06:55:51.156989: Validation complete\n",
      "2025-01-17 06:55:51.157329: Mean Validation Dice:  0.6225856300954022\n",
      "Standard U-Net training completed for 3d_fullres fold 3 -epoch 1000\n",
      "Starting standard U-Net training: nnUNetv2_train 2 3d_fullres 4 --npz --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "WARNING: Cannot continue training because there seems to be no checkpoint available to continue from. Starting a new training...\n",
      "2025-01-17 06:55:57.782032: do_dummy_2d_data_aug: False\n",
      "2025-01-17 06:55:57.782304: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-17 06:55:57.782395: The split file contains 5 splits.\n",
      "2025-01-17 06:55:57.782413: Desired fold for training: 4\n",
      "2025-01-17 06:55:57.782424: This split has 40 training and 10 validation cases.\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pin_memory on device 0\n",
      "2025-01-17 06:56:07.445958: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [252.0, 512.0, 512.0], 'spacing': [1.2450199127197266, 0.7919921875, 0.7919921875], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset002_Lung_split', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.2450199127197266, 0.7919921875, 0.7919921875], 'original_median_shape_after_transp': [251, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -272.8529357910156, 'median': -155.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 303.0, 'std': 348.3833923339844}}} \n",
      "\n",
      "2025-01-17 06:56:08.672868: unpacking dataset...\n",
      "2025-01-17 06:56:12.609846: unpacking done...\n",
      "2025-01-17 06:56:12.611803: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-01-17 06:56:12.618405: \n",
      "2025-01-17 06:56:12.618638: Epoch 0\n",
      "2025-01-17 06:56:12.618723: Current learning rate: 0.01\n",
      "2025-01-17 06:58:22.688879: train_loss -0.0144\n",
      "2025-01-17 06:58:22.689076: val_loss -0.2057\n",
      "2025-01-17 06:58:22.689124: Pseudo dice [np.float32(0.0922)]\n",
      "2025-01-17 06:58:22.689157: Epoch time: 130.07 s\n",
      "2025-01-17 06:58:22.689177: Yayy! New best EMA pseudo Dice: 0.09220000356435776\n",
      "2025-01-17 06:58:23.334285: \n",
      "2025-01-17 06:58:23.334408: Epoch 1\n",
      "2025-01-17 06:58:23.334469: Current learning rate: 0.00999\n",
      "2025-01-17 07:00:13.885374: train_loss -0.3424\n",
      "2025-01-17 07:00:13.885545: val_loss -0.3531\n",
      "2025-01-17 07:00:13.885588: Pseudo dice [np.float32(0.2462)]\n",
      "2025-01-17 07:00:13.885644: Epoch time: 110.55 s\n",
      "2025-01-17 07:00:13.885672: Yayy! New best EMA pseudo Dice: 0.10760000348091125\n",
      "2025-01-17 07:00:14.780632: \n",
      "2025-01-17 07:00:14.780990: Epoch 2\n",
      "2025-01-17 07:00:14.781088: Current learning rate: 0.00998\n",
      "2025-01-17 07:02:05.490115: train_loss -0.3862\n",
      "2025-01-17 07:02:05.490252: val_loss -0.4424\n",
      "2025-01-17 07:02:05.490293: Pseudo dice [np.float32(0.3188)]\n",
      "2025-01-17 07:02:05.490330: Epoch time: 110.71 s\n",
      "2025-01-17 07:02:05.490350: Yayy! New best EMA pseudo Dice: 0.12870000302791595\n",
      "2025-01-17 07:02:06.231977: \n",
      "2025-01-17 07:02:06.232125: Epoch 3\n",
      "2025-01-17 07:02:06.232326: Current learning rate: 0.00997\n",
      "2025-01-17 07:03:56.902667: train_loss -0.4666\n",
      "2025-01-17 07:03:56.902789: val_loss -0.2882\n",
      "2025-01-17 07:03:56.902822: Pseudo dice [np.float32(0.1984)]\n",
      "2025-01-17 07:03:56.902870: Epoch time: 110.67 s\n",
      "2025-01-17 07:03:56.903013: Yayy! New best EMA pseudo Dice: 0.13570000231266022\n",
      "2025-01-17 07:03:57.632002: \n",
      "2025-01-17 07:03:57.632094: Epoch 4\n",
      "2025-01-17 07:03:57.632298: Current learning rate: 0.00996\n",
      "2025-01-17 07:05:48.176211: train_loss -0.5024\n",
      "2025-01-17 07:05:48.176617: val_loss -0.3429\n",
      "2025-01-17 07:05:48.176721: Pseudo dice [np.float32(0.2726)]\n",
      "2025-01-17 07:05:48.176765: Epoch time: 110.54 s\n",
      "2025-01-17 07:05:48.176788: Yayy! New best EMA pseudo Dice: 0.149399995803833\n",
      "2025-01-17 07:05:48.923424: \n",
      "2025-01-17 07:05:48.923741: Epoch 5\n",
      "2025-01-17 07:05:48.923814: Current learning rate: 0.00995\n",
      "2025-01-17 07:07:39.468812: train_loss -0.4726\n",
      "2025-01-17 07:07:39.468943: val_loss -0.4512\n",
      "2025-01-17 07:07:39.468978: Pseudo dice [np.float32(0.3141)]\n",
      "2025-01-17 07:07:39.469085: Epoch time: 110.55 s\n",
      "2025-01-17 07:07:39.469252: Yayy! New best EMA pseudo Dice: 0.16580000519752502\n",
      "2025-01-17 07:07:40.196342: \n",
      "2025-01-17 07:07:40.196426: Epoch 6\n",
      "2025-01-17 07:07:40.196486: Current learning rate: 0.00995\n",
      "2025-01-17 07:09:30.863777: train_loss -0.5373\n",
      "2025-01-17 07:09:30.864071: val_loss -0.5268\n",
      "2025-01-17 07:09:30.864237: Pseudo dice [np.float32(0.4857)]\n",
      "2025-01-17 07:09:30.864310: Epoch time: 110.67 s\n",
      "2025-01-17 07:09:30.864344: Yayy! New best EMA pseudo Dice: 0.19779999554157257\n",
      "2025-01-17 07:09:31.642061: \n",
      "2025-01-17 07:09:31.642148: Epoch 7\n",
      "2025-01-17 07:09:31.642210: Current learning rate: 0.00994\n",
      "2025-01-17 07:11:22.176508: train_loss -0.511\n",
      "2025-01-17 07:11:22.176630: val_loss -0.4476\n",
      "2025-01-17 07:11:22.176663: Pseudo dice [np.float32(0.2693)]\n",
      "2025-01-17 07:11:22.176695: Epoch time: 110.54 s\n",
      "2025-01-17 07:11:22.176716: Yayy! New best EMA pseudo Dice: 0.20499999821186066\n",
      "2025-01-17 07:11:22.928725: \n",
      "2025-01-17 07:11:22.928825: Epoch 8\n",
      "2025-01-17 07:11:22.928893: Current learning rate: 0.00993\n",
      "2025-01-17 07:13:13.610133: train_loss -0.5188\n",
      "2025-01-17 07:13:13.610265: val_loss -0.5375\n",
      "2025-01-17 07:13:13.610397: Pseudo dice [np.float32(0.5151)]\n",
      "2025-01-17 07:13:13.610469: Epoch time: 110.68 s\n",
      "2025-01-17 07:13:13.610498: Yayy! New best EMA pseudo Dice: 0.23600000143051147\n",
      "2025-01-17 07:13:14.354701: \n",
      "2025-01-17 07:13:14.354789: Epoch 9\n",
      "2025-01-17 07:13:14.354854: Current learning rate: 0.00992\n",
      "2025-01-17 07:15:05.088942: train_loss -0.518\n",
      "2025-01-17 07:15:05.089272: val_loss -0.5135\n",
      "2025-01-17 07:15:05.089419: Pseudo dice [np.float32(0.578)]\n",
      "2025-01-17 07:15:05.089466: Epoch time: 110.73 s\n",
      "2025-01-17 07:15:05.089490: Yayy! New best EMA pseudo Dice: 0.2702000141143799\n",
      "2025-01-17 07:15:05.817305: \n",
      "2025-01-17 07:15:05.817391: Epoch 10\n",
      "2025-01-17 07:15:05.817450: Current learning rate: 0.00991\n",
      "2025-01-17 07:16:56.520871: train_loss -0.4556\n",
      "2025-01-17 07:16:56.521018: val_loss -0.6109\n",
      "2025-01-17 07:16:56.521061: Pseudo dice [np.float32(0.5176)]\n",
      "2025-01-17 07:16:56.521096: Epoch time: 110.7 s\n",
      "2025-01-17 07:16:56.521117: Yayy! New best EMA pseudo Dice: 0.29490000009536743\n",
      "2025-01-17 07:16:57.261718: \n",
      "2025-01-17 07:16:57.261804: Epoch 11\n",
      "2025-01-17 07:16:57.261865: Current learning rate: 0.0099\n",
      "2025-01-17 07:18:47.972474: train_loss -0.5375\n",
      "2025-01-17 07:18:47.972604: val_loss -0.4648\n",
      "2025-01-17 07:18:47.972643: Pseudo dice [np.float32(0.6287)]\n",
      "2025-01-17 07:18:47.972674: Epoch time: 110.71 s\n",
      "2025-01-17 07:18:47.972695: Yayy! New best EMA pseudo Dice: 0.32829999923706055\n",
      "2025-01-17 07:18:48.709544: \n",
      "2025-01-17 07:18:48.709870: Epoch 12\n",
      "2025-01-17 07:18:48.709947: Current learning rate: 0.00989\n",
      "2025-01-17 07:20:39.282932: train_loss -0.5623\n",
      "2025-01-17 07:20:39.283064: val_loss -0.6199\n",
      "2025-01-17 07:20:39.283097: Pseudo dice [np.float32(0.7356)]\n",
      "2025-01-17 07:20:39.283129: Epoch time: 110.57 s\n",
      "2025-01-17 07:20:39.283150: Yayy! New best EMA pseudo Dice: 0.36899998784065247\n",
      "2025-01-17 07:20:40.041708: \n",
      "2025-01-17 07:20:40.041804: Epoch 13\n",
      "2025-01-17 07:20:40.041867: Current learning rate: 0.00988\n",
      "2025-01-17 07:22:30.758968: train_loss -0.5678\n",
      "2025-01-17 07:22:30.759449: val_loss -0.5862\n",
      "2025-01-17 07:22:30.759563: Pseudo dice [np.float32(0.7058)]\n",
      "2025-01-17 07:22:30.759612: Epoch time: 110.72 s\n",
      "2025-01-17 07:22:30.759634: Yayy! New best EMA pseudo Dice: 0.4027000069618225\n",
      "2025-01-17 07:22:31.722258: \n",
      "2025-01-17 07:22:31.722360: Epoch 14\n",
      "2025-01-17 07:22:31.722430: Current learning rate: 0.00987\n",
      "2025-01-17 07:24:22.424476: train_loss -0.5764\n",
      "2025-01-17 07:24:22.424607: val_loss -0.5946\n",
      "2025-01-17 07:24:22.424643: Pseudo dice [np.float32(0.6254)]\n",
      "2025-01-17 07:24:22.424675: Epoch time: 110.7 s\n",
      "2025-01-17 07:24:22.424695: Yayy! New best EMA pseudo Dice: 0.42500001192092896\n",
      "2025-01-17 07:24:23.175778: \n",
      "2025-01-17 07:24:23.176090: Epoch 15\n",
      "2025-01-17 07:24:23.176164: Current learning rate: 0.00986\n",
      "2025-01-17 07:26:13.747726: train_loss -0.5601\n",
      "2025-01-17 07:26:13.748158: val_loss -0.5567\n",
      "2025-01-17 07:26:13.748206: Pseudo dice [np.float32(0.5462)]\n",
      "2025-01-17 07:26:13.748241: Epoch time: 110.57 s\n",
      "2025-01-17 07:26:13.748262: Yayy! New best EMA pseudo Dice: 0.43709999322891235\n",
      "2025-01-17 07:26:14.509586: \n",
      "2025-01-17 07:26:14.509686: Epoch 16\n",
      "2025-01-17 07:26:14.509748: Current learning rate: 0.00986\n",
      "2025-01-17 07:28:05.094275: train_loss -0.554\n",
      "2025-01-17 07:28:05.094410: val_loss -0.52\n",
      "2025-01-17 07:28:05.094446: Pseudo dice [np.float32(0.5863)]\n",
      "2025-01-17 07:28:05.094477: Epoch time: 110.59 s\n",
      "2025-01-17 07:28:05.094497: Yayy! New best EMA pseudo Dice: 0.4519999921321869\n",
      "2025-01-17 07:28:05.855624: \n",
      "2025-01-17 07:28:05.855719: Epoch 17\n",
      "2025-01-17 07:28:05.855779: Current learning rate: 0.00985\n",
      "2025-01-17 07:29:56.415165: train_loss -0.5795\n",
      "2025-01-17 07:29:56.415322: val_loss -0.5909\n",
      "2025-01-17 07:29:56.415390: Pseudo dice [np.float32(0.6998)]\n",
      "2025-01-17 07:29:56.415431: Epoch time: 110.56 s\n",
      "2025-01-17 07:29:56.415453: Yayy! New best EMA pseudo Dice: 0.47679999470710754\n",
      "2025-01-17 07:29:57.181876: \n",
      "2025-01-17 07:29:57.182018: Epoch 18\n",
      "2025-01-17 07:29:57.182109: Current learning rate: 0.00984\n",
      "2025-01-17 07:31:47.915044: train_loss -0.5709\n",
      "2025-01-17 07:31:47.915175: val_loss -0.4815\n",
      "2025-01-17 07:31:47.915424: Pseudo dice [np.float32(0.3915)]\n",
      "2025-01-17 07:31:47.915482: Epoch time: 110.73 s\n",
      "2025-01-17 07:31:48.463492: \n",
      "2025-01-17 07:31:48.463592: Epoch 19\n",
      "2025-01-17 07:31:48.463660: Current learning rate: 0.00983\n",
      "2025-01-17 07:33:39.356585: train_loss -0.5652\n",
      "2025-01-17 07:33:39.356905: val_loss -0.4013\n",
      "2025-01-17 07:33:39.357385: Pseudo dice [np.float32(0.6123)]\n",
      "2025-01-17 07:33:39.357753: Epoch time: 110.89 s\n",
      "2025-01-17 07:33:39.357921: Yayy! New best EMA pseudo Dice: 0.482699990272522\n",
      "2025-01-17 07:33:40.129781: \n",
      "2025-01-17 07:33:40.129939: Epoch 20\n",
      "2025-01-17 07:33:40.130144: Current learning rate: 0.00982\n",
      "2025-01-17 07:35:30.853859: train_loss -0.6162\n",
      "2025-01-17 07:35:30.854240: val_loss -0.5931\n",
      "2025-01-17 07:35:30.854315: Pseudo dice [np.float32(0.741)]\n",
      "2025-01-17 07:35:30.854382: Epoch time: 110.72 s\n",
      "2025-01-17 07:35:30.854425: Yayy! New best EMA pseudo Dice: 0.5084999799728394\n",
      "2025-01-17 07:35:31.630992: \n",
      "2025-01-17 07:35:31.631084: Epoch 21\n",
      "2025-01-17 07:35:31.631144: Current learning rate: 0.00981\n",
      "2025-01-17 07:37:22.168988: train_loss -0.6109\n",
      "2025-01-17 07:37:22.169122: val_loss -0.5427\n",
      "2025-01-17 07:37:22.169154: Pseudo dice [np.float32(0.7831)]\n",
      "2025-01-17 07:37:22.169192: Epoch time: 110.54 s\n",
      "2025-01-17 07:37:22.169213: Yayy! New best EMA pseudo Dice: 0.5360000133514404\n",
      "2025-01-17 07:37:22.912186: \n",
      "2025-01-17 07:37:22.912357: Epoch 22\n",
      "2025-01-17 07:37:22.912433: Current learning rate: 0.0098\n",
      "2025-01-17 07:39:13.491192: train_loss -0.5697\n",
      "2025-01-17 07:39:13.491322: val_loss -0.5649\n",
      "2025-01-17 07:39:13.491356: Pseudo dice [np.float32(0.4401)]\n",
      "2025-01-17 07:39:13.491389: Epoch time: 110.58 s\n",
      "2025-01-17 07:39:14.026526: \n",
      "2025-01-17 07:39:14.026619: Epoch 23\n",
      "2025-01-17 07:39:14.026684: Current learning rate: 0.00979\n",
      "2025-01-17 07:41:04.738046: train_loss -0.5406\n",
      "2025-01-17 07:41:04.738176: val_loss -0.366\n",
      "2025-01-17 07:41:04.738214: Pseudo dice [np.float32(0.4252)]\n",
      "2025-01-17 07:41:04.738247: Epoch time: 110.71 s\n",
      "2025-01-17 07:41:05.262908: \n",
      "2025-01-17 07:41:05.263058: Epoch 24\n",
      "2025-01-17 07:41:05.263132: Current learning rate: 0.00978\n",
      "2025-01-17 07:42:55.834791: train_loss -0.5273\n",
      "2025-01-17 07:42:55.834929: val_loss -0.541\n",
      "2025-01-17 07:42:55.834962: Pseudo dice [np.float32(0.6288)]\n",
      "2025-01-17 07:42:55.834993: Epoch time: 110.57 s\n",
      "2025-01-17 07:42:56.363404: \n",
      "2025-01-17 07:42:56.363497: Epoch 25\n",
      "2025-01-17 07:42:56.363564: Current learning rate: 0.00977\n",
      "2025-01-17 07:44:47.089885: train_loss -0.5657\n",
      "2025-01-17 07:44:47.090011: val_loss -0.4745\n",
      "2025-01-17 07:44:47.090045: Pseudo dice [np.float32(0.6392)]\n",
      "2025-01-17 07:44:47.090078: Epoch time: 110.73 s\n",
      "2025-01-17 07:44:47.090102: Yayy! New best EMA pseudo Dice: 0.5386999845504761\n",
      "2025-01-17 07:44:48.063254: \n",
      "2025-01-17 07:44:48.063428: Epoch 26\n",
      "2025-01-17 07:44:48.063506: Current learning rate: 0.00977\n",
      "2025-01-17 07:46:38.593987: train_loss -0.5979\n",
      "2025-01-17 07:46:38.594209: val_loss -0.6319\n",
      "2025-01-17 07:46:38.594245: Pseudo dice [np.float32(0.7471)]\n",
      "2025-01-17 07:46:38.594278: Epoch time: 110.53 s\n",
      "2025-01-17 07:46:38.594298: Yayy! New best EMA pseudo Dice: 0.559499979019165\n",
      "2025-01-17 07:46:39.354939: \n",
      "2025-01-17 07:46:39.355207: Epoch 27\n",
      "2025-01-17 07:46:39.355362: Current learning rate: 0.00976\n",
      "2025-01-17 07:48:30.067245: train_loss -0.6232\n",
      "2025-01-17 07:48:30.067376: val_loss -0.5621\n",
      "2025-01-17 07:48:30.067410: Pseudo dice [np.float32(0.7088)]\n",
      "2025-01-17 07:48:30.067442: Epoch time: 110.71 s\n",
      "2025-01-17 07:48:30.067462: Yayy! New best EMA pseudo Dice: 0.5745000243186951\n",
      "2025-01-17 07:48:30.814634: \n",
      "2025-01-17 07:48:30.814727: Epoch 28\n",
      "2025-01-17 07:48:30.814788: Current learning rate: 0.00975\n",
      "2025-01-17 07:50:21.505566: train_loss -0.6355\n",
      "2025-01-17 07:50:21.505805: val_loss -0.6535\n",
      "2025-01-17 07:50:21.505851: Pseudo dice [np.float32(0.6963)]\n",
      "2025-01-17 07:50:21.505887: Epoch time: 110.69 s\n",
      "2025-01-17 07:50:21.505908: Yayy! New best EMA pseudo Dice: 0.5866000056266785\n",
      "2025-01-17 07:50:22.254347: \n",
      "2025-01-17 07:50:22.254793: Epoch 29\n",
      "2025-01-17 07:50:22.254877: Current learning rate: 0.00974\n",
      "2025-01-17 07:52:12.846740: train_loss -0.6102\n",
      "2025-01-17 07:52:12.847066: val_loss -0.6511\n",
      "2025-01-17 07:52:12.847110: Pseudo dice [np.float32(0.82)]\n",
      "2025-01-17 07:52:12.847142: Epoch time: 110.59 s\n",
      "2025-01-17 07:52:12.847164: Yayy! New best EMA pseudo Dice: 0.6100000143051147\n",
      "2025-01-17 07:52:13.610990: \n",
      "2025-01-17 07:52:13.611087: Epoch 30\n",
      "2025-01-17 07:52:13.611149: Current learning rate: 0.00973\n",
      "2025-01-17 07:54:04.322898: train_loss -0.6675\n",
      "2025-01-17 07:54:04.323167: val_loss -0.5874\n",
      "2025-01-17 07:54:04.323259: Pseudo dice [np.float32(0.7505)]\n",
      "2025-01-17 07:54:04.323316: Epoch time: 110.71 s\n",
      "2025-01-17 07:54:04.323346: Yayy! New best EMA pseudo Dice: 0.6240000128746033\n",
      "2025-01-17 07:54:05.074673: \n",
      "2025-01-17 07:54:05.075141: Epoch 31\n",
      "2025-01-17 07:54:05.075323: Current learning rate: 0.00972\n",
      "2025-01-17 07:55:55.616024: train_loss -0.635\n",
      "2025-01-17 07:55:55.616173: val_loss -0.5624\n",
      "2025-01-17 07:55:55.616207: Pseudo dice [np.float32(0.7509)]\n",
      "2025-01-17 07:55:55.616240: Epoch time: 110.54 s\n",
      "2025-01-17 07:55:55.616260: Yayy! New best EMA pseudo Dice: 0.6366999745368958\n",
      "2025-01-17 07:55:56.371895: \n",
      "2025-01-17 07:55:56.371991: Epoch 32\n",
      "2025-01-17 07:55:56.372054: Current learning rate: 0.00971\n",
      "2025-01-17 07:57:47.117238: train_loss -0.6233\n",
      "2025-01-17 07:57:47.117380: val_loss -0.6558\n",
      "2025-01-17 07:57:47.117413: Pseudo dice [np.float32(0.8049)]\n",
      "2025-01-17 07:57:47.117447: Epoch time: 110.75 s\n",
      "2025-01-17 07:57:47.117467: Yayy! New best EMA pseudo Dice: 0.6535000205039978\n",
      "2025-01-17 07:57:47.894978: \n",
      "2025-01-17 07:57:47.895320: Epoch 33\n",
      "2025-01-17 07:57:47.895391: Current learning rate: 0.0097\n",
      "2025-01-17 07:59:38.466183: train_loss -0.6123\n",
      "2025-01-17 07:59:38.466365: val_loss -0.6484\n",
      "2025-01-17 07:59:38.466398: Pseudo dice [np.float32(0.8074)]\n",
      "2025-01-17 07:59:38.466430: Epoch time: 110.57 s\n",
      "2025-01-17 07:59:38.466450: Yayy! New best EMA pseudo Dice: 0.6689000129699707\n",
      "2025-01-17 07:59:39.227437: \n",
      "2025-01-17 07:59:39.227625: Epoch 34\n",
      "2025-01-17 07:59:39.227694: Current learning rate: 0.00969\n",
      "2025-01-17 08:01:29.790168: train_loss -0.6004\n",
      "2025-01-17 08:01:29.790300: val_loss -0.604\n",
      "2025-01-17 08:01:29.790333: Pseudo dice [np.float32(0.7519)]\n",
      "2025-01-17 08:01:29.790366: Epoch time: 110.56 s\n",
      "2025-01-17 08:01:29.790528: Yayy! New best EMA pseudo Dice: 0.6772000193595886\n",
      "2025-01-17 08:01:30.567275: \n",
      "2025-01-17 08:01:30.567379: Epoch 35\n",
      "2025-01-17 08:01:30.567448: Current learning rate: 0.00968\n",
      "2025-01-17 08:03:21.121533: train_loss -0.6412\n",
      "2025-01-17 08:03:21.121665: val_loss -0.5419\n",
      "2025-01-17 08:03:21.121702: Pseudo dice [np.float32(0.685)]\n",
      "2025-01-17 08:03:21.121733: Epoch time: 110.55 s\n",
      "2025-01-17 08:03:21.121754: Yayy! New best EMA pseudo Dice: 0.6779999732971191\n",
      "2025-01-17 08:03:21.909156: \n",
      "2025-01-17 08:03:21.909239: Epoch 36\n",
      "2025-01-17 08:03:21.909298: Current learning rate: 0.00968\n",
      "2025-01-17 08:05:12.636151: train_loss -0.5993\n",
      "2025-01-17 08:05:12.636276: val_loss -0.494\n",
      "2025-01-17 08:05:12.636308: Pseudo dice [np.float32(0.6962)]\n",
      "2025-01-17 08:05:12.636340: Epoch time: 110.73 s\n",
      "2025-01-17 08:05:12.636360: Yayy! New best EMA pseudo Dice: 0.6797999739646912\n",
      "2025-01-17 08:05:13.401042: \n",
      "2025-01-17 08:05:13.401384: Epoch 37\n",
      "2025-01-17 08:05:13.401530: Current learning rate: 0.00967\n",
      "2025-01-17 08:07:03.941201: train_loss -0.5894\n",
      "2025-01-17 08:07:03.941408: val_loss -0.5196\n",
      "2025-01-17 08:07:03.941518: Pseudo dice [np.float32(0.7843)]\n",
      "2025-01-17 08:07:03.941614: Epoch time: 110.54 s\n",
      "2025-01-17 08:07:03.941653: Yayy! New best EMA pseudo Dice: 0.6902999877929688\n",
      "2025-01-17 08:07:04.952984: \n",
      "2025-01-17 08:07:04.953404: Epoch 38\n",
      "2025-01-17 08:07:04.953498: Current learning rate: 0.00966\n",
      "2025-01-17 08:08:55.695829: train_loss -0.6024\n",
      "2025-01-17 08:08:55.696029: val_loss -0.564\n",
      "2025-01-17 08:08:55.696074: Pseudo dice [np.float32(0.7153)]\n",
      "2025-01-17 08:08:55.696107: Epoch time: 110.74 s\n",
      "2025-01-17 08:08:55.696129: Yayy! New best EMA pseudo Dice: 0.692799985408783\n",
      "2025-01-17 08:08:56.475203: \n",
      "2025-01-17 08:08:56.475528: Epoch 39\n",
      "2025-01-17 08:08:56.475600: Current learning rate: 0.00965\n",
      "2025-01-17 08:10:47.191610: train_loss -0.6182\n",
      "2025-01-17 08:10:47.191742: val_loss -0.6511\n",
      "2025-01-17 08:10:47.191778: Pseudo dice [np.float32(0.7731)]\n",
      "2025-01-17 08:10:47.191811: Epoch time: 110.72 s\n",
      "2025-01-17 08:10:47.191831: Yayy! New best EMA pseudo Dice: 0.7008000016212463\n",
      "2025-01-17 08:10:47.966671: \n",
      "2025-01-17 08:10:47.966775: Epoch 40\n",
      "2025-01-17 08:10:47.966836: Current learning rate: 0.00964\n",
      "2025-01-17 08:12:38.639290: train_loss -0.6552\n",
      "2025-01-17 08:12:38.639692: val_loss -0.6522\n",
      "2025-01-17 08:12:38.639876: Pseudo dice [np.float32(0.7496)]\n",
      "Epoch time: 110.67 s639952: \n",
      "2025-01-17 08:12:38.640040: Yayy! New best EMA pseudo Dice: 0.7056999802589417\n",
      "2025-01-17 08:12:39.411107: \n",
      "2025-01-17 08:12:39.411216: Epoch 41\n",
      "2025-01-17 08:12:39.411277: Current learning rate: 0.00963\n",
      "2025-01-17 08:14:29.989479: train_loss -0.6278\n",
      "2025-01-17 08:14:29.989726: val_loss -0.5897\n",
      "2025-01-17 08:14:29.989767: Pseudo dice [np.float32(0.7624)]\n",
      "2025-01-17 08:14:29.989800: Epoch time: 110.58 s\n",
      "2025-01-17 08:14:29.989819: Yayy! New best EMA pseudo Dice: 0.7113000154495239\n",
      "2025-01-17 08:14:30.748344: \n",
      "2025-01-17 08:14:30.748520: Epoch 42\n",
      "2025-01-17 08:14:30.748592: Current learning rate: 0.00962\n",
      "2025-01-17 08:16:21.303186: train_loss -0.6693\n",
      "2025-01-17 08:16:21.303313: val_loss -0.7201\n",
      "2025-01-17 08:16:21.303345: Pseudo dice [np.float32(0.8378)]\n",
      "2025-01-17 08:16:21.303378: Epoch time: 110.56 s\n",
      "2025-01-17 08:16:21.303401: Yayy! New best EMA pseudo Dice: 0.7239999771118164\n",
      "2025-01-17 08:16:22.065574: \n",
      "2025-01-17 08:16:22.065888: Epoch 43\n",
      "2025-01-17 08:16:22.066042: Current learning rate: 0.00961\n",
      "2025-01-17 08:18:12.775469: train_loss -0.6327\n",
      "2025-01-17 08:18:12.775590: val_loss -0.5297\n",
      "2025-01-17 08:18:12.775680: Pseudo dice [np.float32(0.8116)]\n",
      "2025-01-17 08:18:12.775755: Epoch time: 110.71 s\n",
      "2025-01-17 08:18:12.775786: Yayy! New best EMA pseudo Dice: 0.7328000068664551\n",
      "2025-01-17 08:18:13.533619: \n",
      "2025-01-17 08:18:13.533710: Epoch 44\n",
      "2025-01-17 08:18:13.533771: Current learning rate: 0.0096\n",
      "2025-01-17 08:20:04.278330: train_loss -0.6201\n",
      "2025-01-17 08:20:04.278473: val_loss -0.5408\n",
      "2025-01-17 08:20:04.278510: Pseudo dice [np.float32(0.7265)]\n",
      "2025-01-17 08:20:04.278542: Epoch time: 110.75 s\n",
      "2025-01-17 08:20:04.820765: \n",
      "2025-01-17 08:20:04.820950: Epoch 45\n",
      "2025-01-17 08:20:04.821049: Current learning rate: 0.00959\n",
      "2025-01-17 08:21:55.537980: train_loss -0.6435\n",
      "2025-01-17 08:21:55.538204: val_loss -0.6124\n",
      "2025-01-17 08:21:55.538248: Pseudo dice [np.float32(0.7374)]\n",
      "2025-01-17 08:21:55.538335: Epoch time: 110.72 s\n",
      "2025-01-17 08:21:56.076652: \n",
      "2025-01-17 08:21:56.076741: Epoch 46\n",
      "2025-01-17 08:21:56.076843: Current learning rate: 0.00959\n",
      "2025-01-17 08:23:46.753186: train_loss -0.6373\n",
      "2025-01-17 08:23:46.753314: val_loss -0.6828\n",
      "2025-01-17 08:23:46.753349: Pseudo dice [np.float32(0.6145)]\n",
      "2025-01-17 08:23:46.753382: Epoch time: 110.68 s\n",
      "2025-01-17 08:23:47.289934: \n",
      "2025-01-17 08:23:47.290021: Epoch 47\n",
      "2025-01-17 08:23:47.290083: Current learning rate: 0.00958\n",
      "2025-01-17 08:25:38.003044: train_loss -0.5946\n",
      "2025-01-17 08:25:38.003420: val_loss -0.623\n",
      "2025-01-17 08:25:38.003472: Pseudo dice [np.float32(0.8083)]\n",
      "2025-01-17 08:25:38.003512: Epoch time: 110.71 s\n",
      "2025-01-17 08:25:38.531757: \n",
      "2025-01-17 08:25:38.531927: Epoch 48\n",
      "2025-01-17 08:25:38.532061: Current learning rate: 0.00957\n",
      "2025-01-17 08:27:29.121811: train_loss -0.6043\n",
      "2025-01-17 08:27:29.121942: val_loss -0.6754\n",
      "2025-01-17 08:27:29.121978: Pseudo dice [np.float32(0.8198)]\n",
      "2025-01-17 08:27:29.122011: Epoch time: 110.59 s\n",
      "2025-01-17 08:27:29.122031: Yayy! New best EMA pseudo Dice: 0.7386000156402588\n",
      "2025-01-17 08:27:29.900505: \n",
      "2025-01-17 08:27:29.900722: Epoch 49\n",
      "2025-01-17 08:27:29.900889: Current learning rate: 0.00956\n",
      "2025-01-17 08:29:20.496880: train_loss -0.6472\n",
      "2025-01-17 08:29:20.497096: val_loss -0.5703\n",
      "2025-01-17 08:29:20.497157: Pseudo dice [np.float32(0.7424)]\n",
      "2025-01-17 08:29:20.497195: Epoch time: 110.6 s\n",
      "2025-01-17 08:29:20.679243: Yayy! New best EMA pseudo Dice: 0.7390000224113464\n",
      "2025-01-17 08:29:21.668315: \n",
      "2025-01-17 08:29:21.668813: Epoch 50\n",
      "2025-01-17 08:29:21.668908: Current learning rate: 0.00955\n",
      "2025-01-17 08:31:12.360211: train_loss -0.6539\n",
      "2025-01-17 08:31:12.360662: val_loss -0.5736\n",
      "2025-01-17 08:31:12.360737: Pseudo dice [np.float32(0.7029)]\n",
      "2025-01-17 08:31:12.360778: Epoch time: 110.69 s\n",
      "2025-01-17 08:31:12.896791: \n",
      "2025-01-17 08:31:12.897223: Epoch 51\n",
      "2025-01-17 08:31:12.897315: Current learning rate: 0.00954\n",
      "2025-01-17 08:33:03.659330: train_loss -0.608\n",
      "2025-01-17 08:33:03.659469: val_loss -0.679\n",
      "2025-01-17 08:33:03.659509: Pseudo dice [np.float32(0.8049)]\n",
      "2025-01-17 08:33:03.659549: Epoch time: 110.76 s\n",
      "2025-01-17 08:33:03.659577: Yayy! New best EMA pseudo Dice: 0.7422999739646912\n",
      "2025-01-17 08:33:04.414093: \n",
      "2025-01-17 08:33:04.414210: Epoch 52\n",
      "2025-01-17 08:33:04.414275: Current learning rate: 0.00953\n",
      "2025-01-17 08:34:55.115494: train_loss -0.684\n",
      "2025-01-17 08:34:55.115864: val_loss -0.6427\n",
      "2025-01-17 08:34:55.115913: Pseudo dice [np.float32(0.5819)]\n",
      "2025-01-17 08:34:55.115952: Epoch time: 110.7 s\n",
      "2025-01-17 08:34:55.658457: \n",
      "2025-01-17 08:34:55.658564: Epoch 53\n",
      "2025-01-17 08:34:55.658629: Current learning rate: 0.00952\n",
      "2025-01-17 08:36:46.374443: train_loss -0.6766\n",
      "2025-01-17 08:36:46.374638: val_loss -0.6472\n",
      "2025-01-17 08:36:46.374684: Pseudo dice [np.float32(0.7471)]\n",
      "2025-01-17 08:36:46.374797: Epoch time: 110.72 s\n",
      "2025-01-17 08:36:46.925656: \n",
      "2025-01-17 08:36:46.926039: Epoch 54\n",
      "2025-01-17 08:36:46.926165: Current learning rate: 0.00951\n",
      "2025-01-17 08:38:37.500565: train_loss -0.6737\n",
      "2025-01-17 08:38:37.500696: val_loss -0.7041\n",
      "2025-01-17 08:38:37.500731: Pseudo dice [np.float32(0.7998)]\n",
      "2025-01-17 08:38:37.500763: Epoch time: 110.58 s\n",
      "2025-01-17 08:38:38.045210: \n",
      "2025-01-17 08:38:38.045300: Epoch 55\n",
      "2025-01-17 08:38:38.045362: Current learning rate: 0.0095\n",
      "2025-01-17 08:40:28.594809: train_loss -0.6837\n",
      "2025-01-17 08:40:28.594934: val_loss -0.6281\n",
      "2025-01-17 08:40:28.594966: Pseudo dice [np.float32(0.7718)]\n",
      "2025-01-17 08:40:28.594998: Epoch time: 110.55 s\n",
      "2025-01-17 08:40:29.136318: \n",
      "2025-01-17 08:40:29.136598: Epoch 56\n",
      "2025-01-17 08:40:29.136773: Current learning rate: 0.00949\n",
      "2025-01-17 08:42:19.805939: train_loss -0.6896\n",
      "2025-01-17 08:42:19.806104: val_loss -0.659\n",
      "2025-01-17 08:42:19.806176: Pseudo dice [np.float32(0.753)]\n",
      "2025-01-17 08:42:19.806217: Epoch time: 110.67 s\n",
      "2025-01-17 08:42:20.347621: \n",
      "2025-01-17 08:42:20.348012: Epoch 57\n",
      "2025-01-17 08:42:20.348115: Current learning rate: 0.00949\n",
      "2025-01-17 08:44:11.048965: train_loss -0.6984\n",
      "2025-01-17 08:44:11.049101: val_loss -0.6546\n",
      "2025-01-17 08:44:11.049136: Pseudo dice [np.float32(0.8022)]\n",
      "2025-01-17 08:44:11.049169: Epoch time: 110.7 s\n",
      "2025-01-17 08:44:11.049190: Yayy! New best EMA pseudo Dice: 0.7466999888420105\n",
      "2025-01-17 08:44:11.808407: \n",
      "2025-01-17 08:44:11.808504: Epoch 58\n",
      "2025-01-17 08:44:11.808567: Current learning rate: 0.00948\n",
      "2025-01-17 08:46:02.577131: train_loss -0.6369\n",
      "2025-01-17 08:46:02.577267: val_loss -0.685\n",
      "2025-01-17 08:46:02.577299: Pseudo dice [np.float32(0.7554)]\n",
      "2025-01-17 08:46:02.577332: Epoch time: 110.77 s\n",
      "2025-01-17 08:46:02.577352: Yayy! New best EMA pseudo Dice: 0.7476000189781189\n",
      "2025-01-17 08:46:03.354550: \n",
      "2025-01-17 08:46:03.354915: Epoch 59\n",
      "2025-01-17 08:46:03.355024: Current learning rate: 0.00947\n",
      "2025-01-17 08:47:53.916357: train_loss -0.6492\n",
      "2025-01-17 08:47:53.916491: val_loss -0.6154\n",
      "2025-01-17 08:47:53.916530: Pseudo dice [np.float32(0.7306)]\n",
      "2025-01-17 08:47:53.916561: Epoch time: 110.56 s\n",
      "2025-01-17 08:47:54.459110: \n",
      "2025-01-17 08:47:54.459198: Epoch 60\n",
      "2025-01-17 08:47:54.459259: Current learning rate: 0.00946\n",
      "2025-01-17 08:49:45.022305: train_loss -0.639\n",
      "2025-01-17 08:49:45.022434: val_loss -0.6591\n",
      "2025-01-17 08:49:45.022468: Pseudo dice [np.float32(0.8097)]\n",
      "2025-01-17 08:49:45.022501: Epoch time: 110.56 s\n",
      "2025-01-17 08:49:45.022521: Yayy! New best EMA pseudo Dice: 0.7523000240325928\n",
      "2025-01-17 08:49:45.783154: \n",
      "2025-01-17 08:49:45.783237: Epoch 61\n",
      "2025-01-17 08:49:45.783299: Current learning rate: 0.00945\n",
      "2025-01-17 08:51:36.507854: train_loss -0.664\n",
      "2025-01-17 08:51:36.508126: val_loss -0.6541\n",
      "2025-01-17 08:51:36.508177: Pseudo dice [np.float32(0.4762)]\n",
      "2025-01-17 08:51:36.508214: Epoch time: 110.73 s\n",
      "2025-01-17 08:51:37.293526: \n",
      "2025-01-17 08:51:37.293918: Epoch 62\n",
      "2025-01-17 08:51:37.294018: Current learning rate: 0.00944\n",
      "2025-01-17 08:53:27.989318: train_loss -0.6749\n",
      "2025-01-17 08:53:27.989542: val_loss -0.66\n",
      "2025-01-17 08:53:27.989586: Pseudo dice [np.float32(0.7933)]\n",
      "2025-01-17 08:53:27.989619: Epoch time: 110.7 s\n",
      "2025-01-17 08:53:28.542597: \n",
      "2025-01-17 08:53:28.542822: Epoch 63\n",
      "2025-01-17 08:53:28.542933: Current learning rate: 0.00943\n",
      "2025-01-17 08:55:19.263056: train_loss -0.6858\n",
      "val_loss -0.6077:19.263225: \n",
      "2025-01-17 08:55:19.263434: Pseudo dice [np.float32(0.8187)]\n",
      "2025-01-17 08:55:19.263509: Epoch time: 110.72 s\n",
      "2025-01-17 08:55:19.815759: \n",
      "2025-01-17 08:55:19.815859: Epoch 64\n",
      "2025-01-17 08:55:19.815921: Current learning rate: 0.00942\n",
      "2025-01-17 08:57:10.491204: train_loss -0.6578\n",
      "2025-01-17 08:57:10.491334: val_loss -0.6713\n",
      "2025-01-17 08:57:10.491374: Pseudo dice [np.float32(0.7869)]\n",
      "2025-01-17 08:57:10.491408: Epoch time: 110.68 s\n",
      "2025-01-17 08:57:11.044629: \n",
      "2025-01-17 08:57:11.045000: Epoch 65\n",
      "2025-01-17 08:57:11.045088: Current learning rate: 0.00941\n",
      "2025-01-17 08:59:01.762732: train_loss -0.687\n",
      "2025-01-17 08:59:01.762854: val_loss -0.6127\n",
      "2025-01-17 08:59:01.762886: Pseudo dice [np.float32(0.7286)]\n",
      "2025-01-17 08:59:01.762920: Epoch time: 110.72 s\n",
      "2025-01-17 08:59:02.316264: \n",
      "2025-01-17 08:59:02.316642: Epoch 66\n",
      "2025-01-17 08:59:02.316718: Current learning rate: 0.0094\n",
      "2025-01-17 09:00:53.028767: train_loss -0.7122\n",
      "2025-01-17 09:00:53.028903: val_loss -0.6653\n",
      "2025-01-17 09:00:53.028935: Pseudo dice [np.float32(0.7394)]\n",
      "2025-01-17 09:00:53.028968: Epoch time: 110.71 s\n",
      "2025-01-17 09:00:53.577976: \n",
      "2025-01-17 09:00:53.578403: Epoch 67\n",
      "2025-01-17 09:00:53.578500: Current learning rate: 0.00939\n",
      "2025-01-17 09:02:44.168230: train_loss -0.7044\n",
      "2025-01-17 09:02:44.168371: val_loss -0.5817\n",
      "2025-01-17 09:02:44.168405: Pseudo dice [np.float32(0.6631)]\n",
      "2025-01-17 09:02:44.168437: Epoch time: 110.59 s\n",
      "2025-01-17 09:02:44.729128: \n",
      "2025-01-17 09:02:44.729434: Epoch 68\n",
      "2025-01-17 09:02:44.729629: Current learning rate: 0.00939\n",
      "2025-01-17 09:04:35.422500: train_loss -0.6908\n",
      "2025-01-17 09:04:35.422634: val_loss -0.5869\n",
      "2025-01-17 09:04:35.422668: Pseudo dice [np.float32(0.7752)]\n",
      "2025-01-17 09:04:35.422705: Epoch time: 110.69 s\n",
      "2025-01-17 09:04:35.975586: \n",
      "2025-01-17 09:04:35.975729: Epoch 69\n",
      "2025-01-17 09:04:35.975846: Current learning rate: 0.00938\n",
      "2025-01-17 09:06:26.668256: train_loss -0.6562\n",
      "2025-01-17 09:06:26.668399: val_loss -0.6453\n",
      "2025-01-17 09:06:26.668433: Pseudo dice [np.float32(0.6068)]\n",
      "2025-01-17 09:06:26.668466: Epoch time: 110.69 s\n",
      "2025-01-17 09:06:27.225499: \n",
      "2025-01-17 09:06:27.225591: Epoch 70\n",
      "2025-01-17 09:06:27.225653: Current learning rate: 0.00937\n",
      "2025-01-17 09:08:17.763872: train_loss -0.6644\n",
      "2025-01-17 09:08:17.764003: val_loss -0.6678\n",
      "2025-01-17 09:08:17.764038: Pseudo dice [np.float32(0.7791)]\n",
      "2025-01-17 09:08:17.764069: Epoch time: 110.54 s\n",
      "2025-01-17 09:08:18.321179: \n",
      "2025-01-17 09:08:18.321543: Epoch 71\n",
      "2025-01-17 09:08:18.321762: Current learning rate: 0.00936\n",
      "2025-01-17 09:10:08.829421: train_loss -0.7169\n",
      "2025-01-17 09:10:08.829569: val_loss -0.6183\n",
      "2025-01-17 09:10:08.829607: Pseudo dice [np.float32(0.6221)]\n",
      "2025-01-17 09:10:08.829640: Epoch time: 110.51 s\n",
      "2025-01-17 09:10:09.386931: \n",
      "2025-01-17 09:10:09.387171: Epoch 72\n",
      "2025-01-17 09:10:09.387281: Current learning rate: 0.00935\n",
      "2025-01-17 09:11:59.881430: train_loss -0.7129\n",
      "2025-01-17 09:11:59.881587: val_loss -0.5646\n",
      "2025-01-17 09:11:59.881620: Pseudo dice [np.float32(0.6315)]\n",
      "2025-01-17 09:11:59.881655: Epoch time: 110.5 s\n",
      "2025-01-17 09:12:00.437619: \n",
      "2025-01-17 09:12:00.437910: Epoch 73\n",
      "2025-01-17 09:12:00.438210: Current learning rate: 0.00934\n",
      "2025-01-17 09:13:50.958979: train_loss -0.7024\n",
      "2025-01-17 09:13:50.959123: val_loss -0.6177\n",
      "2025-01-17 09:13:50.959165: Pseudo dice [np.float32(0.7185)]\n",
      "2025-01-17 09:13:50.959199: Epoch time: 110.52 s\n",
      "2025-01-17 09:13:51.749774: \n",
      "2025-01-17 09:13:51.749980: Epoch 74\n",
      "2025-01-17 09:13:51.750404: Current learning rate: 0.00933\n",
      "2025-01-17 09:15:42.242540: train_loss -0.6917\n",
      "2025-01-17 09:15:42.242663: val_loss -0.6995\n",
      "2025-01-17 09:15:42.242699: Pseudo dice [np.float32(0.8021)]\n",
      "2025-01-17 09:15:42.242731: Epoch time: 110.49 s\n",
      "2025-01-17 09:15:42.807426: \n",
      "2025-01-17 09:15:42.807611: Epoch 75\n",
      "2025-01-17 09:15:42.807692: Current learning rate: 0.00932\n",
      "2025-01-17 09:17:33.311426: train_loss -0.6649\n",
      "2025-01-17 09:17:33.311568: val_loss -0.671\n",
      "2025-01-17 09:17:33.311602: Pseudo dice [np.float32(0.7748)]\n",
      "2025-01-17 09:17:33.311634: Epoch time: 110.5 s\n",
      "2025-01-17 09:17:33.869419: \n",
      "2025-01-17 09:17:33.869517: Epoch 76\n",
      "2025-01-17 09:17:33.869577: Current learning rate: 0.00931\n",
      "2025-01-17 09:19:24.339741: train_loss -0.6566\n",
      "2025-01-17 09:19:24.339999: val_loss -0.5862\n",
      "2025-01-17 09:19:24.340043: Pseudo dice [np.float32(0.7141)]\n",
      "2025-01-17 09:19:24.340106: Epoch time: 110.47 s\n",
      "2025-01-17 09:19:24.900977: \n",
      "2025-01-17 09:19:24.901080: Epoch 77\n",
      "2025-01-17 09:19:24.901141: Current learning rate: 0.0093\n",
      "2025-01-17 09:21:15.385183: train_loss -0.6723\n",
      "2025-01-17 09:21:15.385312: val_loss -0.6549\n",
      "2025-01-17 09:21:15.385347: Pseudo dice [np.float32(0.7643)]\n",
      "2025-01-17 09:21:15.385379: Epoch time: 110.48 s\n",
      "2025-01-17 09:21:15.953537: \n",
      "2025-01-17 09:21:15.953698: Epoch 78\n",
      "2025-01-17 09:21:15.953771: Current learning rate: 0.0093\n",
      "2025-01-17 09:23:06.425823: train_loss -0.6905\n",
      "2025-01-17 09:23:06.425954: val_loss -0.6308\n",
      "2025-01-17 09:23:06.425987: Pseudo dice [np.float32(0.7326)]\n",
      "2025-01-17 09:23:06.426020: Epoch time: 110.47 s\n",
      "2025-01-17 09:23:06.994662: \n",
      "2025-01-17 09:23:06.994847: Epoch 79\n",
      "2025-01-17 09:23:06.994992: Current learning rate: 0.00929\n",
      "2025-01-17 09:24:57.435783: train_loss -0.6619\n",
      "2025-01-17 09:24:57.435979: val_loss -0.6881\n",
      "2025-01-17 09:24:57.436013: Pseudo dice [np.float32(0.8151)]\n",
      "2025-01-17 09:24:57.436044: Epoch time: 110.44 s\n",
      "2025-01-17 09:24:57.991498: \n",
      "2025-01-17 09:24:57.991586: Epoch 80\n",
      "2025-01-17 09:24:57.991647: Current learning rate: 0.00928\n",
      "2025-01-17 09:26:48.489759: train_loss -0.7063\n",
      "2025-01-17 09:26:48.490277: val_loss -0.696\n",
      "2025-01-17 09:26:48.490349: Pseudo dice [np.float32(0.7726)]\n",
      "2025-01-17 09:26:48.490389: Epoch time: 110.5 s\n",
      "2025-01-17 09:26:49.054914: \n",
      "2025-01-17 09:26:49.055256: Epoch 81\n",
      "2025-01-17 09:26:49.055351: Current learning rate: 0.00927\n",
      "2025-01-17 09:28:39.630626: train_loss -0.6838\n",
      "2025-01-17 09:28:39.631016: val_loss -0.6291\n",
      "2025-01-17 09:28:39.631062: Pseudo dice [np.float32(0.7787)]\n",
      "2025-01-17 09:28:39.631097: Epoch time: 110.58 s\n",
      "2025-01-17 09:28:40.195442: \n",
      "2025-01-17 09:28:40.195657: Epoch 82\n",
      "2025-01-17 09:28:40.195731: Current learning rate: 0.00926\n",
      "2025-01-17 09:30:30.634043: train_loss -0.695\n",
      "2025-01-17 09:30:30.634184: val_loss -0.6338\n",
      "2025-01-17 09:30:30.634223: Pseudo dice [np.float32(0.7042)]\n",
      "2025-01-17 09:30:30.634257: Epoch time: 110.44 s\n",
      "2025-01-17 09:30:31.182661: \n",
      "2025-01-17 09:30:31.182770: Epoch 83\n",
      "2025-01-17 09:30:31.182847: Current learning rate: 0.00925\n",
      "2025-01-17 09:32:21.799690: train_loss -0.6823\n",
      "2025-01-17 09:32:21.800167: val_loss -0.5452\n",
      "2025-01-17 09:32:21.800314: Pseudo dice [np.float32(0.6177)]\n",
      "2025-01-17 09:32:21.800419: Epoch time: 110.62 s\n",
      "2025-01-17 09:32:22.358386: \n",
      "2025-01-17 09:32:22.358474: Epoch 84\n",
      "2025-01-17 09:32:22.358536: Current learning rate: 0.00924\n",
      "2025-01-17 09:34:12.853301: train_loss -0.7364\n",
      "2025-01-17 09:34:12.853446: val_loss -0.6626\n",
      "2025-01-17 09:34:12.853479: Pseudo dice [np.float32(0.7144)]\n",
      "2025-01-17 09:34:12.853512: Epoch time: 110.5 s\n",
      "2025-01-17 09:34:13.392760: \n",
      "2025-01-17 09:34:13.392956: Epoch 85\n",
      "2025-01-17 09:34:13.393136: Current learning rate: 0.00923\n",
      "2025-01-17 09:36:03.838665: train_loss -0.7345\n",
      "2025-01-17 09:36:03.838801: val_loss -0.665\n",
      "2025-01-17 09:36:03.838836: Pseudo dice [np.float32(0.6441)]\n",
      "2025-01-17 09:36:03.838869: Epoch time: 110.45 s\n",
      "2025-01-17 09:36:04.638633: \n",
      "2025-01-17 09:36:04.638724: Epoch 86\n",
      "2025-01-17 09:36:04.638790: Current learning rate: 0.00922\n",
      "2025-01-17 09:37:55.108925: train_loss -0.6977\n",
      "2025-01-17 09:37:55.109059: val_loss -0.6381\n",
      "2025-01-17 09:37:55.109091: Pseudo dice [np.float32(0.6686)]\n",
      "2025-01-17 09:37:55.109124: Epoch time: 110.47 s\n",
      "2025-01-17 09:37:55.656782: \n",
      "2025-01-17 09:37:55.656886: Epoch 87\n",
      "2025-01-17 09:37:55.657037: Current learning rate: 0.00921\n",
      "2025-01-17 09:39:46.163193: train_loss -0.7054\n",
      "2025-01-17 09:39:46.163316: val_loss -0.6359\n",
      "2025-01-17 09:39:46.163348: Pseudo dice [np.float32(0.7313)]\n",
      "2025-01-17 09:39:46.163381: Epoch time: 110.51 s\n",
      "2025-01-17 09:39:46.704830: \n",
      "2025-01-17 09:39:46.704928: Epoch 88\n",
      "2025-01-17 09:39:46.704992: Current learning rate: 0.0092\n",
      "2025-01-17 09:41:37.222096: train_loss -0.727\n",
      "2025-01-17 09:41:37.222259: val_loss -0.6922\n",
      "2025-01-17 09:41:37.222334: Pseudo dice [np.float32(0.7062)]\n",
      "2025-01-17 09:41:37.222509: Epoch time: 110.52 s\n",
      "2025-01-17 09:41:37.767511: \n",
      "2025-01-17 09:41:37.767620: Epoch 89\n",
      "2025-01-17 09:41:37.767684: Current learning rate: 0.0092\n",
      "2025-01-17 09:43:28.254076: train_loss -0.7414\n",
      "2025-01-17 09:43:28.254324: val_loss -0.6902\n",
      "2025-01-17 09:43:28.254372: Pseudo dice [np.float32(0.7623)]\n",
      "2025-01-17 09:43:28.254408: Epoch time: 110.49 s\n",
      "2025-01-17 09:43:28.791994: \n",
      "2025-01-17 09:43:28.792166: Epoch 90\n",
      "2025-01-17 09:43:28.792227: Current learning rate: 0.00919\n",
      "2025-01-17 09:45:19.321515: train_loss -0.6965\n",
      "2025-01-17 09:45:19.321723: val_loss -0.6508\n",
      "2025-01-17 09:45:19.321767: Pseudo dice [np.float32(0.8121)]\n",
      "2025-01-17 09:45:19.321809: Epoch time: 110.53 s\n",
      "2025-01-17 09:45:19.870815: \n",
      "2025-01-17 09:45:19.871010: Epoch 91\n",
      "2025-01-17 09:45:19.871078: Current learning rate: 0.00918\n",
      "2025-01-17 09:47:10.411852: train_loss -0.7281\n",
      "2025-01-17 09:47:10.411988: val_loss -0.5593\n",
      "2025-01-17 09:47:10.412156: Pseudo dice [np.float32(0.7291)]\n",
      "2025-01-17 09:47:10.412223: Epoch time: 110.54 s\n",
      "2025-01-17 09:47:10.960967: \n",
      "2025-01-17 09:47:10.961305: Epoch 92\n",
      "2025-01-17 09:47:10.961399: Current learning rate: 0.00917\n",
      "2025-01-17 09:49:01.439150: train_loss -0.6891\n",
      "2025-01-17 09:49:01.439312: val_loss -0.527\n",
      "2025-01-17 09:49:01.439393: Pseudo dice [np.float32(0.79)]\n",
      "2025-01-17 09:49:01.439434: Epoch time: 110.48 s\n",
      "2025-01-17 09:49:01.966518: \n",
      "2025-01-17 09:49:01.966613: Epoch 93\n",
      "2025-01-17 09:49:01.966676: Current learning rate: 0.00916\n",
      "2025-01-17 09:50:52.444145: train_loss -0.6738\n",
      "2025-01-17 09:50:52.444295: val_loss -0.6013\n",
      "2025-01-17 09:50:52.444328: Pseudo dice [np.float32(0.7136)]\n",
      "2025-01-17 09:50:52.444361: Epoch time: 110.48 s\n",
      "2025-01-17 09:50:52.983241: \n",
      "2025-01-17 09:50:52.983335: Epoch 94\n",
      "2025-01-17 09:50:52.983473: Current learning rate: 0.00915\n",
      "2025-01-17 09:52:43.461787: train_loss -0.6751\n",
      "2025-01-17 09:52:43.461996: val_loss -0.5982\n",
      "2025-01-17 09:52:43.462031: Pseudo dice [np.float32(0.6855)]\n",
      "2025-01-17 09:52:43.462065: Epoch time: 110.48 s\n",
      "2025-01-17 09:52:43.994777: \n",
      "2025-01-17 09:52:43.995135: Epoch 95\n",
      "2025-01-17 09:52:43.995212: Current learning rate: 0.00914\n",
      "2025-01-17 09:54:34.455462: train_loss -0.7164\n",
      "2025-01-17 09:54:34.455585: val_loss -0.6585\n",
      "2025-01-17 09:54:34.455619: Pseudo dice [np.float32(0.7951)]\n",
      "2025-01-17 09:54:34.455652: Epoch time: 110.46 s\n",
      "2025-01-17 09:54:35.005360: \n",
      "2025-01-17 09:54:35.005441: Epoch 96\n",
      "2025-01-17 09:54:35.005501: Current learning rate: 0.00913\n",
      "2025-01-17 09:56:25.496104: train_loss -0.6986\n",
      "2025-01-17 09:56:25.496485: val_loss -0.6049\n",
      "2025-01-17 09:56:25.496609: Pseudo dice [np.float32(0.6882)]\n",
      "2025-01-17 09:56:25.496650: Epoch time: 110.49 s\n",
      "2025-01-17 09:56:26.043236: \n",
      "2025-01-17 09:56:26.043401: Epoch 97\n",
      "2025-01-17 09:56:26.043593: Current learning rate: 0.00912\n",
      "2025-01-17 09:58:16.575382: train_loss -0.6918\n",
      "2025-01-17 09:58:16.575506: val_loss -0.6183\n",
      "2025-01-17 09:58:16.575536: Pseudo dice [np.float32(0.7124)]\n",
      "2025-01-17 09:58:16.575585: Epoch time: 110.53 s\n",
      "2025-01-17 09:58:17.122880: \n",
      "2025-01-17 09:58:17.122964: Epoch 98\n",
      "2025-01-17 09:58:17.123027: Current learning rate: 0.00911\n",
      "2025-01-17 10:00:07.617397: train_loss -0.7003\n",
      "2025-01-17 10:00:07.617621: val_loss -0.6416\n",
      "2025-01-17 10:00:07.617665: Pseudo dice [np.float32(0.7155)]\n",
      "2025-01-17 10:00:07.617699: Epoch time: 110.5 s\n",
      "2025-01-17 10:00:08.394069: \n",
      "2025-01-17 10:00:08.394281: Epoch 99\n",
      "2025-01-17 10:00:08.394355: Current learning rate: 0.0091\n",
      "2025-01-17 10:01:58.868240: train_loss -0.7205\n",
      "2025-01-17 10:01:58.868389: val_loss -0.6528\n",
      "2025-01-17 10:01:58.868427: Pseudo dice [np.float32(0.7958)]\n",
      "2025-01-17 10:01:58.868464: Epoch time: 110.47 s\n",
      "2025-01-17 10:01:59.622185: \n",
      "2025-01-17 10:01:59.622346: Epoch 100\n",
      "2025-01-17 10:01:59.622410: Current learning rate: 0.0091\n",
      "2025-01-17 10:03:52.324642: train_loss -0.7237\n",
      "2025-01-17 10:03:52.324770: val_loss -0.5949\n",
      "2025-01-17 10:03:52.324804: Pseudo dice [np.float32(0.7468)]\n",
      "2025-01-17 10:03:52.324836: Epoch time: 112.7 s\n",
      "2025-01-17 10:03:52.887101: \n",
      "2025-01-17 10:03:52.887215: Epoch 101\n",
      "2025-01-17 10:03:52.887280: Current learning rate: 0.00909\n",
      "2025-01-17 10:05:43.970492: train_loss -0.6939\n",
      "2025-01-17 10:05:43.970633: val_loss -0.6963\n",
      "2025-01-17 10:05:43.970668: Pseudo dice [np.float32(0.7257)]\n",
      "2025-01-17 10:05:43.970709: Epoch time: 111.08 s\n",
      "2025-01-17 10:05:44.517141: \n",
      "2025-01-17 10:05:44.517269: Epoch 102\n",
      "2025-01-17 10:05:44.517333: Current learning rate: 0.00908\n",
      "2025-01-17 10:07:35.502434: train_loss -0.7346\n",
      "2025-01-17 10:07:35.502610: val_loss -0.6307\n",
      "2025-01-17 10:07:35.502653: Pseudo dice [np.float32(0.6918)]\n",
      "2025-01-17 10:07:35.502692: Epoch time: 110.99 s\n",
      "2025-01-17 10:07:36.052985: \n",
      "2025-01-17 10:07:36.053339: Epoch 103\n",
      "2025-01-17 10:07:36.053492: Current learning rate: 0.00907\n",
      "2025-01-17 10:09:27.074023: train_loss -0.628\n",
      "2025-01-17 10:09:27.074156: val_loss -0.6554\n",
      "2025-01-17 10:09:27.074222: Pseudo dice [np.float32(0.7103)]\n",
      "2025-01-17 10:09:27.074260: Epoch time: 111.02 s\n",
      "2025-01-17 10:09:27.627331: \n",
      "2025-01-17 10:09:27.627508: Epoch 104\n",
      "2025-01-17 10:09:27.627591: Current learning rate: 0.00906\n",
      "2025-01-17 10:11:18.611633: train_loss -0.6995\n",
      "2025-01-17 10:11:18.611775: val_loss -0.5969\n",
      "2025-01-17 10:11:18.611812: Pseudo dice [np.float32(0.7177)]\n",
      "2025-01-17 10:11:18.611845: Epoch time: 110.98 s\n",
      "2025-01-17 10:11:19.162204: \n",
      "2025-01-17 10:11:19.162538: Epoch 105\n",
      "2025-01-17 10:11:19.162633: Current learning rate: 0.00905\n",
      "2025-01-17 10:13:10.236317: train_loss -0.7059\n",
      "2025-01-17 10:13:10.236452: val_loss -0.6205\n",
      "2025-01-17 10:13:10.236485: Pseudo dice [np.float32(0.7597)]\n",
      "2025-01-17 10:13:10.236531: Epoch time: 111.07 s\n",
      "2025-01-17 10:13:10.784795: \n",
      "2025-01-17 10:13:10.785201: Epoch 106\n",
      "2025-01-17 10:13:10.785296: Current learning rate: 0.00904\n",
      "2025-01-17 10:15:01.816249: train_loss -0.7042\n",
      "2025-01-17 10:15:01.816378: val_loss -0.6338\n",
      "2025-01-17 10:15:01.816413: Pseudo dice [np.float32(0.7597)]\n",
      "2025-01-17 10:15:01.816445: Epoch time: 111.03 s\n",
      "2025-01-17 10:15:02.373815: \n",
      "2025-01-17 10:15:02.373940: Epoch 107\n",
      "2025-01-17 10:15:02.374099: Current learning rate: 0.00903\n",
      "2025-01-17 10:16:53.369070: train_loss -0.7152\n",
      "2025-01-17 10:16:53.369206: val_loss -0.5826\n",
      "2025-01-17 10:16:53.369240: Pseudo dice [np.float32(0.7217)]\n",
      "2025-01-17 10:16:53.369532: Epoch time: 111.0 s\n",
      "2025-01-17 10:16:53.933972: \n",
      "2025-01-17 10:16:53.934158: Epoch 108\n",
      "2025-01-17 10:16:53.934261: Current learning rate: 0.00902\n",
      "2025-01-17 10:18:44.869535: train_loss -0.6726\n",
      "2025-01-17 10:18:44.869662: val_loss -0.5317\n",
      "2025-01-17 10:18:44.869696: Pseudo dice [np.float32(0.6526)]\n",
      "2025-01-17 10:18:44.869729: Epoch time: 110.94 s\n",
      "2025-01-17 10:18:45.420243: \n",
      "2025-01-17 10:18:45.420329: Epoch 109\n",
      "2025-01-17 10:18:45.420390: Current learning rate: 0.00901\n",
      "2025-01-17 10:20:36.410915: train_loss -0.7062\n",
      "2025-01-17 10:20:36.411181: val_loss -0.6829\n",
      "2025-01-17 10:20:36.411268: Pseudo dice [np.float32(0.7807)]\n",
      "2025-01-17 10:20:36.411466: Epoch time: 110.99 s\n",
      "2025-01-17 10:20:36.969399: \n",
      "2025-01-17 10:20:36.969496: Epoch 110\n",
      "2025-01-17 10:20:36.969562: Current learning rate: 0.009\n",
      "2025-01-17 10:22:27.978458: train_loss -0.7386\n",
      "2025-01-17 10:22:27.978593: val_loss -0.6476\n",
      "2025-01-17 10:22:27.978703: Pseudo dice [np.float32(0.728)]\n",
      "2025-01-17 10:22:27.978825: Epoch time: 111.01 s\n",
      "2025-01-17 10:22:28.765852: \n",
      "2025-01-17 10:22:28.766118: Epoch 111\n",
      "2025-01-17 10:22:28.766196: Current learning rate: 0.009\n",
      "2025-01-17 10:24:19.748654: train_loss -0.7242\n",
      "2025-01-17 10:24:19.748776: val_loss -0.7083\n",
      "2025-01-17 10:24:19.748807: Pseudo dice [np.float32(0.7724)]\n",
      "2025-01-17 10:24:19.748838: Epoch time: 110.98 s\n",
      "2025-01-17 10:24:20.303305: \n",
      "2025-01-17 10:24:20.303410: Epoch 112\n",
      "2025-01-17 10:24:20.303474: Current learning rate: 0.00899\n",
      "2025-01-17 10:26:11.283237: train_loss -0.6962\n",
      "2025-01-17 10:26:11.283459: val_loss -0.6433\n",
      "2025-01-17 10:26:11.283494: Pseudo dice [np.float32(0.7724)]\n",
      "2025-01-17 10:26:11.283526: Epoch time: 110.98 s\n",
      "2025-01-17 10:26:11.839664: \n",
      "2025-01-17 10:26:11.840011: Epoch 113\n",
      "2025-01-17 10:26:11.840101: Current learning rate: 0.00898\n",
      "2025-01-17 10:28:02.892905: train_loss -0.6558\n",
      "2025-01-17 10:28:02.893054: val_loss -0.5764\n",
      "2025-01-17 10:28:02.893092: Pseudo dice [np.float32(0.5305)]\n",
      "2025-01-17 10:28:02.893201: Epoch time: 111.05 s\n",
      "2025-01-17 10:28:03.447819: \n",
      "2025-01-17 10:28:03.448233: Epoch 114\n",
      "2025-01-17 10:28:03.448401: Current learning rate: 0.00897\n",
      "2025-01-17 10:29:54.862631: train_loss -0.6786\n",
      "2025-01-17 10:29:54.862756: val_loss -0.5722\n",
      "2025-01-17 10:29:54.862788: Pseudo dice [np.float32(0.6768)]\n",
      "2025-01-17 10:29:54.862836: Epoch time: 111.42 s\n",
      "2025-01-17 10:29:55.417032: \n",
      "2025-01-17 10:29:55.417175: Epoch 115\n",
      "2025-01-17 10:29:55.417315: Current learning rate: 0.00896\n",
      "2025-01-17 10:31:46.550823: train_loss -0.7123\n",
      "2025-01-17 10:31:46.551030: val_loss -0.6732\n",
      "2025-01-17 10:31:46.551164: Pseudo dice [np.float32(0.8136)]\n",
      "2025-01-17 10:31:46.551234: Epoch time: 111.13 s\n",
      "2025-01-17 10:31:47.106764: \n",
      "2025-01-17 10:31:47.107110: Epoch 116\n",
      "2025-01-17 10:31:47.107184: Current learning rate: 0.00895\n",
      "2025-01-17 10:33:38.101615: train_loss -0.723\n",
      "2025-01-17 10:33:38.101828: val_loss -0.6268\n",
      "2025-01-17 10:33:38.101863: Pseudo dice [np.float32(0.62)]\n",
      "2025-01-17 10:33:38.101896: Epoch time: 111.0 s\n",
      "2025-01-17 10:33:38.665464: \n",
      "2025-01-17 10:33:38.665861: Epoch 117\n",
      "2025-01-17 10:33:38.665983: Current learning rate: 0.00894\n",
      "2025-01-17 10:35:29.759981: train_loss -0.6945\n",
      "2025-01-17 10:35:29.760192: val_loss -0.5828\n",
      "2025-01-17 10:35:29.760239: Pseudo dice [np.float32(0.6267)]\n",
      "2025-01-17 10:35:29.760412: Epoch time: 111.1 s\n",
      "2025-01-17 10:35:30.327385: \n",
      "2025-01-17 10:35:30.327483: Epoch 118\n",
      "2025-01-17 10:35:30.327547: Current learning rate: 0.00893\n",
      "2025-01-17 10:37:21.311908: train_loss -0.7253\n",
      "2025-01-17 10:37:21.312045: val_loss -0.6299\n",
      "2025-01-17 10:37:21.312078: Pseudo dice [np.float32(0.781)]\n",
      "2025-01-17 10:37:21.312112: Epoch time: 110.99 s\n",
      "2025-01-17 10:37:21.869989: \n",
      "2025-01-17 10:37:21.870148: Epoch 119\n",
      "2025-01-17 10:37:21.870221: Current learning rate: 0.00892\n",
      "2025-01-17 10:39:12.854024: train_loss -0.7263\n",
      "2025-01-17 10:39:12.854264: val_loss -0.7065\n",
      "2025-01-17 10:39:12.854326: Pseudo dice [np.float32(0.8204)]\n",
      "2025-01-17 10:39:12.854364: Epoch time: 110.98 s\n",
      "2025-01-17 10:39:13.418438: \n",
      "2025-01-17 10:39:13.418535: Epoch 120\n",
      "2025-01-17 10:39:13.418597: Current learning rate: 0.00891\n",
      "2025-01-17 10:41:04.441899: train_loss -0.7424\n",
      "2025-01-17 10:41:04.442029: val_loss -0.6795\n",
      "2025-01-17 10:41:04.442064: Pseudo dice [np.float32(0.7585)]\n",
      "2025-01-17 10:41:04.442097: Epoch time: 111.02 s\n",
      "2025-01-17 10:41:05.024173: \n",
      "2025-01-17 10:41:05.024581: Epoch 121\n",
      "2025-01-17 10:41:05.024679: Current learning rate: 0.0089\n",
      "2025-01-17 10:42:56.047284: train_loss -0.7211\n",
      "2025-01-17 10:42:56.047408: val_loss -0.6207\n",
      "2025-01-17 10:42:56.047442: Pseudo dice [np.float32(0.7611)]\n",
      "2025-01-17 10:42:56.047477: Epoch time: 111.02 s\n",
      "2025-01-17 10:42:56.605629: \n",
      "2025-01-17 10:42:56.605920: Epoch 122\n",
      "2025-01-17 10:42:56.606085: Current learning rate: 0.00889\n",
      "2025-01-17 10:44:48.521859: train_loss -0.6844\n",
      "2025-01-17 10:44:48.521985: val_loss -0.7385\n",
      "2025-01-17 10:44:48.522018: Pseudo dice [np.float32(0.7607)]\n",
      "2025-01-17 10:44:48.522053: Epoch time: 111.92 s\n",
      "2025-01-17 10:44:49.328252: \n",
      "2025-01-17 10:44:49.328477: Epoch 123\n",
      "2025-01-17 10:44:49.328562: Current learning rate: 0.00889\n",
      "2025-01-17 10:46:43.414776: train_loss -0.7227\n",
      "2025-01-17 10:46:43.414903: val_loss -0.6014\n",
      "2025-01-17 10:46:43.414936: Pseudo dice [np.float32(0.7448)]\n",
      "2025-01-17 10:46:43.414970: Epoch time: 114.09 s\n",
      "2025-01-17 10:46:43.987780: \n",
      "2025-01-17 10:46:43.987923: Epoch 124\n",
      "2025-01-17 10:46:43.987996: Current learning rate: 0.00888\n",
      "2025-01-17 10:48:38.345118: train_loss -0.7406\n",
      "2025-01-17 10:48:38.345243: val_loss -0.6505\n",
      "2025-01-17 10:48:38.345276: Pseudo dice [np.float32(0.6944)]\n",
      "2025-01-17 10:48:38.345308: Epoch time: 114.36 s\n",
      "2025-01-17 10:48:38.913100: \n",
      "2025-01-17 10:48:38.913194: Epoch 125\n",
      "2025-01-17 10:48:38.913255: Current learning rate: 0.00887\n",
      "2025-01-17 10:50:30.997278: train_loss -0.757\n",
      "2025-01-17 10:50:30.997411: val_loss -0.6978\n",
      "2025-01-17 10:50:30.997443: Pseudo dice [np.float32(0.6838)]\n",
      "2025-01-17 10:50:30.997476: Epoch time: 112.08 s\n",
      "2025-01-17 10:50:31.560881: \n",
      "2025-01-17 10:50:31.560977: Epoch 126\n",
      "2025-01-17 10:50:31.561037: Current learning rate: 0.00886\n",
      "2025-01-17 10:52:22.960111: train_loss -0.7583\n",
      "2025-01-17 10:52:22.960244: val_loss -0.6377\n",
      "2025-01-17 10:52:22.960278: Pseudo dice [np.float32(0.6761)]\n",
      "2025-01-17 10:52:22.960314: Epoch time: 111.4 s\n",
      "2025-01-17 10:52:23.526328: \n",
      "2025-01-17 10:52:23.526436: Epoch 127\n",
      "2025-01-17 10:52:23.526501: Current learning rate: 0.00885\n",
      "2025-01-17 10:54:14.625739: train_loss -0.7425\n",
      "2025-01-17 10:54:14.625889: val_loss -0.6785\n",
      "2025-01-17 10:54:14.625925: Pseudo dice [np.float32(0.6829)]\n",
      "2025-01-17 10:54:14.625959: Epoch time: 111.1 s\n",
      "2025-01-17 10:54:15.200996: \n",
      "2025-01-17 10:54:15.201102: Epoch 128\n",
      "2025-01-17 10:54:15.201166: Current learning rate: 0.00884\n",
      "2025-01-17 10:56:06.231283: train_loss -0.763\n",
      "2025-01-17 10:56:06.231469: val_loss -0.6406\n",
      "2025-01-17 10:56:06.231521: Pseudo dice [np.float32(0.7348)]\n",
      "2025-01-17 10:56:06.231566: Epoch time: 111.03 s\n",
      "2025-01-17 10:56:06.798427: \n",
      "2025-01-17 10:56:06.798532: Epoch 129\n",
      "2025-01-17 10:56:06.798596: Current learning rate: 0.00883\n",
      "2025-01-17 10:57:58.021835: train_loss -0.7574\n",
      "2025-01-17 10:57:58.021980: val_loss -0.664\n",
      "2025-01-17 10:57:58.022013: Pseudo dice [np.float32(0.7069)]\n",
      "2025-01-17 10:57:58.022047: Epoch time: 111.22 s\n",
      "2025-01-17 10:57:58.585702: \n",
      "2025-01-17 10:57:58.585795: Epoch 130\n",
      "2025-01-17 10:57:58.585860: Current learning rate: 0.00882\n",
      "2025-01-17 10:59:49.777293: train_loss -0.7176\n",
      "2025-01-17 10:59:49.777503: val_loss -0.6363\n",
      "2025-01-17 10:59:49.777538: Pseudo dice [np.float32(0.6891)]\n",
      "2025-01-17 10:59:49.777570: Epoch time: 111.19 s\n",
      "2025-01-17 10:59:50.352401: \n",
      "2025-01-17 10:59:50.352716: Epoch 131\n",
      "2025-01-17 10:59:50.353024: Current learning rate: 0.00881\n",
      "2025-01-17 11:01:41.759730: train_loss -0.7617\n",
      "2025-01-17 11:01:41.759856: val_loss -0.7125\n",
      "2025-01-17 11:01:41.759890: Pseudo dice [np.float32(0.7434)]\n",
      "2025-01-17 11:01:41.759923: Epoch time: 111.41 s\n",
      "2025-01-17 11:01:42.321290: \n",
      "2025-01-17 11:01:42.321684: Epoch 132\n",
      "2025-01-17 11:01:42.321929: Current learning rate: 0.0088\n",
      "2025-01-17 11:03:33.359706: train_loss -0.7434\n",
      "2025-01-17 11:03:33.359832: val_loss -0.6983\n",
      "2025-01-17 11:03:33.359866: Pseudo dice [np.float32(0.7195)]\n",
      "2025-01-17 11:03:33.359900: Epoch time: 111.04 s\n",
      "2025-01-17 11:03:33.930647: \n",
      "2025-01-17 11:03:33.930742: Epoch 133\n",
      "2025-01-17 11:03:33.930805: Current learning rate: 0.00879\n",
      "2025-01-17 11:05:24.909256: train_loss -0.6687\n",
      "2025-01-17 11:05:24.909385: val_loss -0.635\n",
      "2025-01-17 11:05:24.909417: Pseudo dice [np.float32(0.7626)]\n",
      "2025-01-17 11:05:24.909450: Epoch time: 110.98 s\n",
      "2025-01-17 11:05:25.483455: \n",
      "2025-01-17 11:05:25.483762: Epoch 134\n",
      "2025-01-17 11:05:25.484000: Current learning rate: 0.00879\n",
      "2025-01-17 11:07:16.437508: train_loss -0.6563\n",
      "2025-01-17 11:07:16.438015: val_loss -0.6336\n",
      "2025-01-17 11:07:16.438087: Pseudo dice [np.float32(0.7869)]\n",
      "2025-01-17 11:07:16.438128: Epoch time: 110.95 s\n",
      "2025-01-17 11:07:17.268569: \n",
      "2025-01-17 11:07:17.268927: Epoch 135\n",
      "2025-01-17 11:07:17.269008: Current learning rate: 0.00878\n",
      "2025-01-17 11:09:09.585346: train_loss -0.7388\n",
      "2025-01-17 11:09:09.585483: val_loss -0.6899\n",
      "2025-01-17 11:09:09.585516: Pseudo dice [np.float32(0.7395)]\n",
      "2025-01-17 11:09:09.585549: Epoch time: 112.32 s\n",
      "2025-01-17 11:09:10.157185: \n",
      "2025-01-17 11:09:10.157359: Epoch 136\n",
      "2025-01-17 11:09:10.157439: Current learning rate: 0.00877\n",
      "2025-01-17 11:11:01.166241: train_loss -0.7271\n",
      "2025-01-17 11:11:01.166368: val_loss -0.5663\n",
      "2025-01-17 11:11:01.166400: Pseudo dice [np.float32(0.6053)]\n",
      "2025-01-17 11:11:01.166432: Epoch time: 111.01 s\n",
      "2025-01-17 11:11:01.740182: \n",
      "2025-01-17 11:11:01.740583: Epoch 137\n",
      "2025-01-17 11:11:01.740774: Current learning rate: 0.00876\n",
      "2025-01-17 11:12:53.354516: train_loss -0.728\n",
      "2025-01-17 11:12:53.354727: val_loss -0.6992\n",
      "2025-01-17 11:12:53.354909: Pseudo dice [np.float32(0.6533)]\n",
      "2025-01-17 11:12:53.355031: Epoch time: 111.61 s\n",
      "2025-01-17 11:12:53.940814: \n",
      "2025-01-17 11:12:53.941297: Epoch 138\n",
      "2025-01-17 11:12:53.941382: Current learning rate: 0.00875\n",
      "2025-01-17 11:14:45.463491: train_loss -0.7021\n",
      "2025-01-17 11:14:45.463678: val_loss -0.6623\n",
      "2025-01-17 11:14:45.463711: Pseudo dice [np.float32(0.7521)]\n",
      "2025-01-17 11:14:45.463815: Epoch time: 111.52 s\n",
      "2025-01-17 11:14:46.053086: \n",
      "2025-01-17 11:14:46.053187: Epoch 139\n",
      "2025-01-17 11:14:46.053249: Current learning rate: 0.00874\n",
      "2025-01-17 11:16:37.041182: train_loss -0.667\n",
      "2025-01-17 11:16:37.041311: val_loss -0.6351\n",
      "2025-01-17 11:16:37.041342: Pseudo dice [np.float32(0.7706)]\n",
      "2025-01-17 11:16:37.041374: Epoch time: 110.99 s\n",
      "2025-01-17 11:16:37.616072: \n",
      "2025-01-17 11:16:37.616271: Epoch 140\n",
      "2025-01-17 11:16:37.616344: Current learning rate: 0.00873\n",
      "2025-01-17 11:18:28.602784: train_loss -0.7076\n",
      "2025-01-17 11:18:28.602933: val_loss -0.6136\n",
      "2025-01-17 11:18:28.603105: Pseudo dice [np.float32(0.6762)]\n",
      "2025-01-17 11:18:28.603167: Epoch time: 110.99 s\n",
      "2025-01-17 11:18:29.169155: \n",
      "2025-01-17 11:18:29.169251: Epoch 141\n",
      "2025-01-17 11:18:29.169313: Current learning rate: 0.00872\n",
      "2025-01-17 11:20:20.162567: train_loss -0.7331\n",
      "2025-01-17 11:20:20.162717: val_loss -0.5901\n",
      "2025-01-17 11:20:20.162763: Pseudo dice [np.float32(0.6277)]\n",
      "2025-01-17 11:20:20.162817: Epoch time: 110.99 s\n",
      "2025-01-17 11:20:20.797946: \n",
      "2025-01-17 11:20:20.798062: Epoch 142\n",
      "2025-01-17 11:20:20.798138: Current learning rate: 0.00871\n",
      "2025-01-17 11:22:11.880370: train_loss -0.7507\n",
      "2025-01-17 11:22:11.880499: val_loss -0.6089\n",
      "2025-01-17 11:22:11.880533: Pseudo dice [np.float32(0.6806)]\n",
      "2025-01-17 11:22:11.880567: Epoch time: 111.08 s\n",
      "2025-01-17 11:22:12.452846: \n",
      "2025-01-17 11:22:12.453041: Epoch 143\n",
      "2025-01-17 11:22:12.453115: Current learning rate: 0.0087\n",
      "2025-01-17 11:24:03.437228: train_loss -0.7152\n",
      "2025-01-17 11:24:03.437367: val_loss -0.6862\n",
      "2025-01-17 11:24:03.437400: Pseudo dice [np.float32(0.6893)]\n",
      "2025-01-17 11:24:03.437433: Epoch time: 110.98 s\n",
      "2025-01-17 11:24:04.012278: \n",
      "2025-01-17 11:24:04.012577: Epoch 144\n",
      "2025-01-17 11:24:04.012654: Current learning rate: 0.00869\n",
      "2025-01-17 11:25:55.392287: train_loss -0.6515\n",
      "2025-01-17 11:25:55.392412: val_loss -0.6735\n",
      "2025-01-17 11:25:55.392446: Pseudo dice [np.float32(0.8066)]\n",
      "2025-01-17 11:25:55.392479: Epoch time: 111.38 s\n",
      "2025-01-17 11:25:55.960507: \n",
      "2025-01-17 11:25:55.960598: Epoch 145\n",
      "2025-01-17 11:25:55.960664: Current learning rate: 0.00868\n",
      "2025-01-17 11:27:47.180296: train_loss -0.7093\n",
      "2025-01-17 11:27:47.180437: val_loss -0.6331\n",
      "2025-01-17 11:27:47.180473: Pseudo dice [np.float32(0.7528)]\n",
      "2025-01-17 11:27:47.180507: Epoch time: 111.22 s\n",
      "2025-01-17 11:27:47.755414: \n",
      "2025-01-17 11:27:47.755763: Epoch 146\n",
      "2025-01-17 11:27:47.755896: Current learning rate: 0.00868\n",
      "2025-01-17 11:29:38.816949: train_loss -0.7342\n",
      "2025-01-17 11:29:38.817103: val_loss -0.6158\n",
      "2025-01-17 11:29:38.817374: Pseudo dice [np.float32(0.7346)]\n",
      "2025-01-17 11:29:38.817584: Epoch time: 111.06 s\n",
      "2025-01-17 11:29:39.645588: \n",
      "2025-01-17 11:29:39.645696: Epoch 147\n",
      "2025-01-17 11:29:39.645762: Current learning rate: 0.00867\n",
      "2025-01-17 11:31:30.624118: train_loss -0.7125\n",
      "2025-01-17 11:31:30.624267: val_loss -0.6011\n",
      "2025-01-17 11:31:30.624307: Pseudo dice [np.float32(0.7173)]\n",
      "2025-01-17 11:31:30.624341: Epoch time: 110.98 s\n",
      "2025-01-17 11:31:31.203359: \n",
      "2025-01-17 11:31:31.203464: Epoch 148\n",
      "2025-01-17 11:31:31.203528: Current learning rate: 0.00866\n",
      "2025-01-17 11:33:22.199695: train_loss -0.7462\n",
      "2025-01-17 11:33:22.199959: val_loss -0.6389\n",
      "2025-01-17 11:33:22.200008: Pseudo dice [np.float32(0.7293)]\n",
      "2025-01-17 11:33:22.200043: Epoch time: 111.0 s\n",
      "2025-01-17 11:33:22.782324: \n",
      "2025-01-17 11:33:22.782520: Epoch 149\n",
      "2025-01-17 11:33:22.782594: Current learning rate: 0.00865\n",
      "2025-01-17 11:35:14.045752: train_loss -0.6539\n",
      "2025-01-17 11:35:14.045954: val_loss -0.6448\n",
      "2025-01-17 11:35:14.045998: Pseudo dice [np.float32(0.7746)]\n",
      "2025-01-17 11:35:14.046036: Epoch time: 111.26 s\n",
      "2025-01-17 11:35:14.892547: \n",
      "2025-01-17 11:35:14.892650: Epoch 150\n",
      "2025-01-17 11:35:14.892715: Current learning rate: 0.00864\n",
      "2025-01-17 11:37:05.908517: train_loss -0.692\n",
      "2025-01-17 11:37:05.908664: val_loss -0.6061\n",
      "2025-01-17 11:37:05.908699: Pseudo dice [np.float32(0.7671)]\n",
      "2025-01-17 11:37:05.908732: Epoch time: 111.02 s\n",
      "2025-01-17 11:37:06.500987: \n",
      "2025-01-17 11:37:06.501331: Epoch 151\n",
      "2025-01-17 11:37:06.501512: Current learning rate: 0.00863\n",
      "2025-01-17 11:38:57.507853: train_loss -0.705\n",
      "2025-01-17 11:38:57.508081: val_loss -0.6107\n",
      "2025-01-17 11:38:57.508126: Pseudo dice [np.float32(0.7716)]\n",
      "2025-01-17 11:38:57.508236: Epoch time: 111.01 s\n",
      "2025-01-17 11:38:58.085225: \n",
      "2025-01-17 11:38:58.085400: Epoch 152\n",
      "2025-01-17 11:38:58.085480: Current learning rate: 0.00862\n",
      "2025-01-17 11:40:49.073056: train_loss -0.7249\n",
      "2025-01-17 11:40:49.073308: val_loss -0.6719\n",
      "2025-01-17 11:40:49.073379: Pseudo dice [np.float32(0.7802)]\n",
      "2025-01-17 11:40:49.073421: Epoch time: 110.99 s\n",
      "2025-01-17 11:40:49.644570: \n",
      "2025-01-17 11:40:49.644658: Epoch 153\n",
      "2025-01-17 11:40:49.644718: Current learning rate: 0.00861\n",
      "2025-01-17 11:42:40.648241: train_loss -0.7195\n",
      "2025-01-17 11:42:40.648449: val_loss -0.6765\n",
      "2025-01-17 11:42:40.648485: Pseudo dice [np.float32(0.8507)]\n",
      "2025-01-17 11:42:40.648517: Epoch time: 111.0 s\n",
      "2025-01-17 11:42:41.225654: \n",
      "2025-01-17 11:42:41.225868: Epoch 154\n",
      "2025-01-17 11:42:41.225976: Current learning rate: 0.0086\n",
      "2025-01-17 11:44:32.221757: train_loss -0.7184\n",
      "2025-01-17 11:44:32.221978: val_loss -0.5948\n",
      "2025-01-17 11:44:32.222022: Pseudo dice [np.float32(0.7016)]\n",
      "2025-01-17 11:44:32.222056: Epoch time: 111.0 s\n",
      "2025-01-17 11:44:32.801213: \n",
      "2025-01-17 11:44:32.801302: Epoch 155\n",
      "2025-01-17 11:44:32.801363: Current learning rate: 0.00859\n",
      "2025-01-17 11:46:24.064722: train_loss -0.7007\n",
      "2025-01-17 11:46:24.065025: val_loss -0.6589\n",
      "2025-01-17 11:46:24.065069: Pseudo dice [np.float32(0.7246)]\n",
      "2025-01-17 11:46:24.065104: Epoch time: 111.26 s\n",
      "2025-01-17 11:46:24.650368: \n",
      "2025-01-17 11:46:24.650530: Epoch 156\n",
      "2025-01-17 11:46:24.650604: Current learning rate: 0.00858\n",
      "2025-01-17 11:48:15.865746: train_loss -0.7384\n",
      "2025-01-17 11:48:15.865884: val_loss -0.5877\n",
      "2025-01-17 11:48:15.866022: Pseudo dice [np.float32(0.6229)]\n",
      "2025-01-17 11:48:15.866104: Epoch time: 111.22 s\n",
      "2025-01-17 11:48:16.450404: \n",
      "2025-01-17 11:48:16.450669: Epoch 157\n",
      "2025-01-17 11:48:16.450854: Current learning rate: 0.00858\n",
      "2025-01-17 11:50:08.225721: train_loss -0.7469\n",
      "2025-01-17 11:50:08.225964: val_loss -0.7028\n",
      "2025-01-17 11:50:08.226105: Pseudo dice [np.float32(0.7855)]\n",
      "2025-01-17 11:50:08.226173: Epoch time: 111.78 s\n",
      "2025-01-17 11:50:09.045825: \n",
      "2025-01-17 11:50:09.046185: Epoch 158\n",
      "2025-01-17 11:50:09.046393: Current learning rate: 0.00857\n",
      "2025-01-17 11:51:59.927135: train_loss -0.7244\n",
      "2025-01-17 11:51:59.927356: val_loss -0.729\n",
      "2025-01-17 11:51:59.927421: Pseudo dice [np.float32(0.771)]\n",
      "2025-01-17 11:51:59.927462: Epoch time: 110.88 s\n",
      "2025-01-17 11:52:00.506195: \n",
      "2025-01-17 11:52:00.506459: Epoch 159\n",
      "2025-01-17 11:52:00.506661: Current learning rate: 0.00856\n",
      "2025-01-17 11:53:51.393924: train_loss -0.7375\n",
      "2025-01-17 11:53:51.394057: val_loss -0.7077\n",
      "2025-01-17 11:53:51.394094: Pseudo dice [np.float32(0.7652)]\n",
      "2025-01-17 11:53:51.394128: Epoch time: 110.89 s\n",
      "2025-01-17 11:53:51.978873: \n",
      "2025-01-17 11:53:51.979130: Epoch 160\n",
      "2025-01-17 11:53:51.979228: Current learning rate: 0.00855\n",
      "2025-01-17 11:55:42.890713: train_loss -0.7444\n",
      "2025-01-17 11:55:42.890844: val_loss -0.6338\n",
      "2025-01-17 11:55:42.890877: Pseudo dice [np.float32(0.7373)]\n",
      "2025-01-17 11:55:42.890910: Epoch time: 110.91 s\n",
      "2025-01-17 11:55:43.473137: \n",
      "2025-01-17 11:55:43.473591: Epoch 161\n",
      "2025-01-17 11:55:43.473688: Current learning rate: 0.00854\n",
      "2025-01-17 11:57:34.320824: train_loss -0.7638\n",
      "2025-01-17 11:57:34.321024: val_loss -0.6996\n",
      "2025-01-17 11:57:34.321059: Pseudo dice [np.float32(0.8191)]\n",
      "2025-01-17 11:57:34.321091: Epoch time: 110.85 s\n",
      "2025-01-17 11:57:34.900563: \n",
      "2025-01-17 11:57:34.900666: Epoch 162\n",
      "2025-01-17 11:57:34.900728: Current learning rate: 0.00853\n",
      "2025-01-17 11:59:25.899437: train_loss -0.7406\n",
      "2025-01-17 11:59:25.899564: val_loss -0.6586\n",
      "2025-01-17 11:59:25.899603: Pseudo dice [np.float32(0.7101)]\n",
      "2025-01-17 11:59:25.899636: Epoch time: 111.0 s\n",
      "2025-01-17 11:59:26.477312: \n",
      "2025-01-17 11:59:26.477417: Epoch 163\n",
      "2025-01-17 11:59:26.477634: Current learning rate: 0.00852\n",
      "2025-01-17 12:01:16.972883: train_loss -0.7394\n",
      "2025-01-17 12:01:16.973019: val_loss -0.6944\n",
      "2025-01-17 12:01:16.973053: Pseudo dice [np.float32(0.744)]\n",
      "2025-01-17 12:01:16.973153: Epoch time: 110.5 s\n",
      "2025-01-17 12:01:17.556897: \n",
      "2025-01-17 12:01:17.556998: Epoch 164\n",
      "2025-01-17 12:01:17.557059: Current learning rate: 0.00851\n",
      "2025-01-17 12:03:08.136127: train_loss -0.7456\n",
      "2025-01-17 12:03:08.136286: val_loss -0.6747\n",
      "2025-01-17 12:03:08.136350: Pseudo dice [np.float32(0.7117)]\n",
      "2025-01-17 12:03:08.136390: Epoch time: 110.58 s\n",
      "2025-01-17 12:03:08.699467: \n",
      "2025-01-17 12:03:08.699563: Epoch 165\n",
      "2025-01-17 12:03:08.699626: Current learning rate: 0.0085\n",
      "2025-01-17 12:04:59.190377: train_loss -0.7762\n",
      "2025-01-17 12:04:59.190519: val_loss -0.7191\n",
      "2025-01-17 12:04:59.190554: Pseudo dice [np.float32(0.7798)]\n",
      "2025-01-17 12:04:59.190587: Epoch time: 110.49 s\n",
      "2025-01-17 12:04:59.758949: \n",
      "2025-01-17 12:04:59.759044: Epoch 166\n",
      "2025-01-17 12:04:59.759111: Current learning rate: 0.00849\n",
      "2025-01-17 12:06:50.221132: train_loss -0.7501\n",
      "2025-01-17 12:06:50.221265: val_loss -0.6695\n",
      "2025-01-17 12:06:50.221298: Pseudo dice [np.float32(0.7169)]\n",
      "2025-01-17 12:06:50.221331: Epoch time: 110.46 s\n",
      "2025-01-17 12:06:50.785150: \n",
      "2025-01-17 12:06:50.785306: Epoch 167\n",
      "2025-01-17 12:06:50.785379: Current learning rate: 0.00848\n",
      "2025-01-17 12:08:41.342685: train_loss -0.7696\n",
      "2025-01-17 12:08:41.342850: val_loss -0.4883\n",
      "2025-01-17 12:08:41.343037: Pseudo dice [np.float32(0.2235)]\n",
      "2025-01-17 12:08:41.343109: Epoch time: 110.56 s\n",
      "2025-01-17 12:08:41.914579: \n",
      "2025-01-17 12:08:41.914768: Epoch 168\n",
      "2025-01-17 12:08:41.914903: Current learning rate: 0.00847\n",
      "2025-01-17 12:10:32.392319: train_loss -0.7177\n",
      "2025-01-17 12:10:32.392462: val_loss -0.724\n",
      "2025-01-17 12:10:32.392505: Pseudo dice [np.float32(0.7866)]\n",
      "2025-01-17 12:10:32.392603: Epoch time: 110.48 s\n",
      "2025-01-17 12:10:32.969805: \n",
      "2025-01-17 12:10:32.969888: Epoch 169\n",
      "2025-01-17 12:10:32.969949: Current learning rate: 0.00847\n",
      "2025-01-17 12:12:23.615586: train_loss -0.7468\n",
      "2025-01-17 12:12:23.615711: val_loss -0.6751\n",
      "2025-01-17 12:12:23.615742: Pseudo dice [np.float32(0.7626)]\n",
      "2025-01-17 12:12:23.615776: Epoch time: 110.65 s\n",
      "2025-01-17 12:12:24.185933: \n",
      "2025-01-17 12:12:24.186042: Epoch 170\n",
      "2025-01-17 12:12:24.186106: Current learning rate: 0.00846\n",
      "2025-01-17 12:14:14.661695: train_loss -0.7679\n",
      "2025-01-17 12:14:14.661846: val_loss -0.6633\n",
      "2025-01-17 12:14:14.661891: Pseudo dice [np.float32(0.7427)]\n",
      "2025-01-17 12:14:14.661934: Epoch time: 110.48 s\n",
      "2025-01-17 12:14:15.232171: \n",
      "2025-01-17 12:14:15.232273: Epoch 171\n",
      "2025-01-17 12:14:15.232333: Current learning rate: 0.00845\n",
      "2025-01-17 12:16:05.698235: train_loss -0.7646\n",
      "2025-01-17 12:16:05.698570: val_loss -0.6756\n",
      "2025-01-17 12:16:05.698615: Pseudo dice [np.float32(0.7723)]\n",
      "2025-01-17 12:16:05.698650: Epoch time: 110.47 s\n",
      "2025-01-17 12:16:06.275206: \n",
      "2025-01-17 12:16:06.275292: Epoch 172\n",
      "2025-01-17 12:16:06.275352: Current learning rate: 0.00844\n",
      "2025-01-17 12:17:56.729614: train_loss -0.7788\n",
      "2025-01-17 12:17:56.729748: val_loss -0.7295\n",
      "2025-01-17 12:17:56.729783: Pseudo dice [np.float32(0.7534)]\n",
      "2025-01-17 12:17:56.729817: Epoch time: 110.45 s\n",
      "2025-01-17 12:17:57.300464: \n",
      "2025-01-17 12:17:57.300883: Epoch 173\n",
      "2025-01-17 12:17:57.301066: Current learning rate: 0.00843\n",
      "2025-01-17 12:19:47.801246: train_loss -0.7743\n",
      "2025-01-17 12:19:47.801566: val_loss -0.6641\n",
      "2025-01-17 12:19:47.801611: Pseudo dice [np.float32(0.7413)]\n",
      "2025-01-17 12:19:47.801647: Epoch time: 110.5 s\n",
      "2025-01-17 12:19:48.418977: \n",
      "2025-01-17 12:19:48.419313: Epoch 174\n",
      "2025-01-17 12:19:48.419474: Current learning rate: 0.00842\n",
      "2025-01-17 12:21:38.919339: train_loss -0.7334\n",
      "2025-01-17 12:21:38.919477: val_loss -0.6461\n",
      "2025-01-17 12:21:38.919512: Pseudo dice [np.float32(0.6498)]\n",
      "2025-01-17 12:21:38.919544: Epoch time: 110.5 s\n",
      "2025-01-17 12:21:39.493136: \n",
      "2025-01-17 12:21:39.493241: Epoch 175\n",
      "2025-01-17 12:21:39.493365: Current learning rate: 0.00841\n",
      "2025-01-17 12:23:30.064989: train_loss -0.7215\n",
      "2025-01-17 12:23:30.065112: val_loss -0.6185\n",
      "2025-01-17 12:23:30.065145: Pseudo dice [np.float32(0.731)]\n",
      "2025-01-17 12:23:30.065231: Epoch time: 110.57 s\n",
      "2025-01-17 12:23:30.645656: \n",
      "2025-01-17 12:23:30.645763: Epoch 176\n",
      "2025-01-17 12:23:30.645830: Current learning rate: 0.0084\n",
      "2025-01-17 12:25:21.144015: train_loss -0.7652\n",
      "2025-01-17 12:25:21.144154: val_loss -0.569\n",
      "2025-01-17 12:25:21.144202: Pseudo dice [np.float32(0.6483)]\n",
      "2025-01-17 12:25:21.144237: Epoch time: 110.5 s\n",
      "2025-01-17 12:25:21.713797: \n",
      "2025-01-17 12:25:21.713897: Epoch 177\n",
      "2025-01-17 12:25:21.713959: Current learning rate: 0.00839\n",
      "2025-01-17 12:27:12.247486: train_loss -0.6772\n",
      "2025-01-17 12:27:12.247615: val_loss -0.6084\n",
      "2025-01-17 12:27:12.247648: Pseudo dice [np.float32(0.4889)]\n",
      "2025-01-17 12:27:12.247680: Epoch time: 110.53 s\n",
      "2025-01-17 12:27:12.822659: \n",
      "2025-01-17 12:27:12.823034: Epoch 178\n",
      "2025-01-17 12:27:12.823246: Current learning rate: 0.00838\n",
      "2025-01-17 12:29:03.356978: train_loss -0.6799\n",
      "2025-01-17 12:29:03.357100: val_loss -0.668\n",
      "2025-01-17 12:29:03.357131: Pseudo dice [np.float32(0.7651)]\n",
      "2025-01-17 12:29:03.357162: Epoch time: 110.53 s\n",
      "2025-01-17 12:29:03.929403: \n",
      "2025-01-17 12:29:03.929769: Epoch 179\n",
      "2025-01-17 12:29:03.929887: Current learning rate: 0.00837\n",
      "2025-01-17 12:30:54.490592: train_loss -0.7452\n",
      "2025-01-17 12:30:54.490856: val_loss -0.6839\n",
      "2025-01-17 12:30:54.490917: Pseudo dice [np.float32(0.8548)]\n",
      "2025-01-17 12:30:54.490957: Epoch time: 110.56 s\n",
      "2025-01-17 12:30:55.060107: \n",
      "2025-01-17 12:30:55.060426: Epoch 180\n",
      "2025-01-17 12:30:55.060574: Current learning rate: 0.00836\n",
      "2025-01-17 12:32:45.564046: train_loss -0.7634\n",
      "2025-01-17 12:32:45.564413: val_loss -0.6036\n",
      "2025-01-17 12:32:45.564567: Pseudo dice [np.float32(0.56)]\n",
      "2025-01-17 12:32:45.564616: Epoch time: 110.5 s\n",
      "2025-01-17 12:32:46.375252: \n",
      "2025-01-17 12:32:46.375682: Epoch 181\n",
      "2025-01-17 12:32:46.375789: Current learning rate: 0.00836\n",
      "2025-01-17 12:34:36.884524: train_loss -0.7188\n",
      "2025-01-17 12:34:36.884803: val_loss -0.6053\n",
      "2025-01-17 12:34:36.884894: Pseudo dice [np.float32(0.7066)]\n",
      "2025-01-17 12:34:36.885037: Epoch time: 110.51 s\n",
      "2025-01-17 12:34:37.452804: \n",
      "2025-01-17 12:34:37.452901: Epoch 182\n",
      "2025-01-17 12:34:37.452962: Current learning rate: 0.00835\n",
      "2025-01-17 12:36:27.914435: train_loss -0.7292\n",
      "2025-01-17 12:36:27.914631: val_loss -0.6062\n",
      "2025-01-17 12:36:27.914663: Pseudo dice [np.float32(0.7388)]\n",
      "2025-01-17 12:36:27.914695: Epoch time: 110.46 s\n",
      "2025-01-17 12:36:28.483027: \n",
      "2025-01-17 12:36:28.483229: Epoch 183\n",
      "2025-01-17 12:36:28.483294: Current learning rate: 0.00834\n",
      "2025-01-17 12:38:19.085988: train_loss -0.7012\n",
      "2025-01-17 12:38:19.086115: val_loss -0.6156\n",
      "2025-01-17 12:38:19.086147: Pseudo dice [np.float32(0.6626)]\n",
      "2025-01-17 12:38:19.086180: Epoch time: 110.6 s\n",
      "2025-01-17 12:38:19.662188: \n",
      "2025-01-17 12:38:19.662285: Epoch 184\n",
      "2025-01-17 12:38:19.662344: Current learning rate: 0.00833\n",
      "2025-01-17 12:40:10.269790: train_loss -0.7132\n",
      "2025-01-17 12:40:10.269932: val_loss -0.7332\n",
      "2025-01-17 12:40:10.269966: Pseudo dice [np.float32(0.7648)]\n",
      "2025-01-17 12:40:10.269999: Epoch time: 110.61 s\n",
      "2025-01-17 12:40:10.850756: \n",
      "2025-01-17 12:40:10.851108: Epoch 185\n",
      "2025-01-17 12:40:10.851242: Current learning rate: 0.00832\n",
      "2025-01-17 12:42:01.332600: train_loss -0.7547\n",
      "2025-01-17 12:42:01.333261: val_loss -0.6257\n",
      "2025-01-17 12:42:01.333341: Pseudo dice [np.float32(0.7654)]\n",
      "2025-01-17 12:42:01.333383: Epoch time: 110.48 s\n",
      "2025-01-17 12:42:01.900538: \n",
      "2025-01-17 12:42:01.900951: Epoch 186\n",
      "2025-01-17 12:42:01.901076: Current learning rate: 0.00831\n",
      "2025-01-17 12:43:52.469309: train_loss -0.7696\n",
      "2025-01-17 12:43:52.469431: val_loss -0.604\n",
      "2025-01-17 12:43:52.469462: Pseudo dice [np.float32(0.6864)]\n",
      "2025-01-17 12:43:52.469493: Epoch time: 110.57 s\n",
      "2025-01-17 12:43:53.042321: \n",
      "2025-01-17 12:43:53.042478: Epoch 187\n",
      "2025-01-17 12:43:53.042640: Current learning rate: 0.0083\n",
      "2025-01-17 12:45:43.638608: train_loss -0.7475\n",
      "2025-01-17 12:45:43.638755: val_loss -0.6353\n",
      "2025-01-17 12:45:43.638789: Pseudo dice [np.float32(0.7615)]\n",
      "2025-01-17 12:45:43.638822: Epoch time: 110.6 s\n",
      "2025-01-17 12:45:44.211162: \n",
      "2025-01-17 12:45:44.211337: Epoch 188\n",
      "2025-01-17 12:45:44.211477: Current learning rate: 0.00829\n",
      "2025-01-17 12:47:34.914547: train_loss -0.678\n",
      "2025-01-17 12:47:34.914672: val_loss -0.6338\n",
      "2025-01-17 12:47:34.914704: Pseudo dice [np.float32(0.7127)]\n",
      "2025-01-17 12:47:34.914793: Epoch time: 110.7 s\n",
      "2025-01-17 12:47:35.488509: \n",
      "2025-01-17 12:47:35.488664: Epoch 189\n",
      "2025-01-17 12:47:35.488746: Current learning rate: 0.00828\n",
      "2025-01-17 12:49:25.999219: train_loss -0.7105\n",
      "2025-01-17 12:49:25.999420: val_loss -0.5717\n",
      "2025-01-17 12:49:25.999453: Pseudo dice [np.float32(0.6351)]\n",
      "2025-01-17 12:49:25.999485: Epoch time: 110.51 s\n",
      "2025-01-17 12:49:26.583071: \n",
      "2025-01-17 12:49:26.583164: Epoch 190\n",
      "2025-01-17 12:49:26.583225: Current learning rate: 0.00827\n",
      "2025-01-17 12:51:17.085009: train_loss -0.7354\n",
      "2025-01-17 12:51:17.085150: val_loss -0.6477\n",
      "2025-01-17 12:51:17.085185: Pseudo dice [np.float32(0.6199)]\n",
      "2025-01-17 12:51:17.085218: Epoch time: 110.5 s\n",
      "2025-01-17 12:51:17.739992: \n",
      "2025-01-17 12:51:17.740451: Epoch 191\n",
      "2025-01-17 12:51:17.740710: Current learning rate: 0.00826\n",
      "2025-01-17 12:53:08.202791: train_loss -0.7733\n",
      "2025-01-17 12:53:08.202995: val_loss -0.6801\n",
      "2025-01-17 12:53:08.203030: Pseudo dice [np.float32(0.6476)]\n",
      "2025-01-17 12:53:08.203062: Epoch time: 110.46 s\n",
      "2025-01-17 12:53:08.786330: \n",
      "2025-01-17 12:53:08.786431: Epoch 192\n",
      "2025-01-17 12:53:08.786497: Current learning rate: 0.00825\n",
      "2025-01-17 12:54:59.346828: train_loss -0.7566\n",
      "2025-01-17 12:54:59.347019: val_loss -0.6621\n",
      "2025-01-17 12:54:59.347054: Pseudo dice [np.float32(0.7435)]\n",
      "2025-01-17 12:54:59.347087: Epoch time: 110.56 s\n",
      "2025-01-17 12:55:00.186112: \n",
      "2025-01-17 12:55:00.186500: Epoch 193\n",
      "2025-01-17 12:55:00.186598: Current learning rate: 0.00824\n",
      "2025-01-17 12:56:50.701495: train_loss -0.7554\n",
      "2025-01-17 12:56:50.701622: val_loss -0.6157\n",
      "2025-01-17 12:56:50.701654: Pseudo dice [np.float32(0.7506)]\n",
      "2025-01-17 12:56:50.701687: Epoch time: 110.52 s\n",
      "2025-01-17 12:56:51.278816: \n",
      "2025-01-17 12:56:51.278920: Epoch 194\n",
      "2025-01-17 12:56:51.278984: Current learning rate: 0.00824\n",
      "2025-01-17 12:58:41.800244: train_loss -0.7511\n",
      "2025-01-17 12:58:41.800653: val_loss -0.6748\n",
      "2025-01-17 12:58:41.800767: Pseudo dice [np.float32(0.6639)]\n",
      "2025-01-17 12:58:41.800809: Epoch time: 110.52 s\n",
      "2025-01-17 12:58:42.381423: \n",
      "2025-01-17 12:58:42.381530: Epoch 195\n",
      "2025-01-17 12:58:42.381592: Current learning rate: 0.00823\n",
      "2025-01-17 13:00:32.899811: train_loss -0.7315\n",
      "2025-01-17 13:00:32.899940: val_loss -0.6578\n",
      "2025-01-17 13:00:32.899975: Pseudo dice [np.float32(0.6633)]\n",
      "2025-01-17 13:00:32.900008: Epoch time: 110.52 s\n",
      "2025-01-17 13:00:33.475582: \n",
      "2025-01-17 13:00:33.475743: Epoch 196\n",
      "2025-01-17 13:00:33.475807: Current learning rate: 0.00822\n",
      "2025-01-17 13:02:23.952905: train_loss -0.7355\n",
      "2025-01-17 13:02:23.953032: val_loss -0.6732\n",
      "2025-01-17 13:02:23.953066: Pseudo dice [np.float32(0.742)]\n",
      "2025-01-17 13:02:23.953098: Epoch time: 110.48 s\n",
      "2025-01-17 13:02:24.531323: \n",
      "2025-01-17 13:02:24.531737: Epoch 197\n",
      "2025-01-17 13:02:24.531811: Current learning rate: 0.00821\n",
      "2025-01-17 13:04:15.086928: train_loss -0.7547\n",
      "2025-01-17 13:04:15.087054: val_loss -0.6237\n",
      "2025-01-17 13:04:15.087088: Pseudo dice [np.float32(0.7414)]\n",
      "2025-01-17 13:04:15.087123: Epoch time: 110.56 s\n",
      "2025-01-17 13:04:15.667198: \n",
      "2025-01-17 13:04:15.667429: Epoch 198\n",
      "2025-01-17 13:04:15.667615: Current learning rate: 0.0082\n",
      "2025-01-17 13:06:06.168442: train_loss -0.7542\n",
      "2025-01-17 13:06:06.168585: val_loss -0.6085\n",
      "2025-01-17 13:06:06.168622: Pseudo dice [np.float32(0.6279)]\n",
      "2025-01-17 13:06:06.168655: Epoch time: 110.5 s\n",
      "2025-01-17 13:06:06.761222: \n",
      "2025-01-17 13:06:06.761315: Epoch 199\n",
      "2025-01-17 13:06:06.761375: Current learning rate: 0.00819\n",
      "2025-01-17 13:07:57.304029: train_loss -0.722\n",
      "2025-01-17 13:07:57.304281: val_loss -0.7095\n",
      "2025-01-17 13:07:57.304326: Pseudo dice [np.float32(0.7112)]\n",
      "2025-01-17 13:07:57.304359: Epoch time: 110.54 s\n",
      "2025-01-17 13:07:58.096749: \n",
      "2025-01-17 13:07:58.096911: Epoch 200\n",
      "2025-01-17 13:07:58.097080: Current learning rate: 0.00818\n",
      "2025-01-17 13:09:48.578979: train_loss -0.7759\n",
      "2025-01-17 13:09:48.579354: val_loss -0.6728\n",
      "2025-01-17 13:09:48.579399: Pseudo dice [np.float32(0.711)]\n",
      "2025-01-17 13:09:48.579432: Epoch time: 110.48 s\n",
      "2025-01-17 13:09:49.166439: \n",
      "2025-01-17 13:09:49.166678: Epoch 201\n",
      "2025-01-17 13:09:49.166847: Current learning rate: 0.00817\n",
      "2025-01-17 13:11:39.788567: train_loss -0.7697\n",
      "2025-01-17 13:11:39.788789: val_loss -0.652\n",
      "2025-01-17 13:11:39.788831: Pseudo dice [np.float32(0.7858)]\n",
      "2025-01-17 13:11:39.788865: Epoch time: 110.62 s\n",
      "2025-01-17 13:11:40.370188: \n",
      "2025-01-17 13:11:40.370610: Epoch 202\n",
      "2025-01-17 13:11:40.370747: Current learning rate: 0.00816\n",
      "2025-01-17 13:13:30.846921: train_loss -0.752\n",
      "2025-01-17 13:13:30.847058: val_loss -0.6365\n",
      "2025-01-17 13:13:30.847092: Pseudo dice [np.float32(0.7289)]\n",
      "2025-01-17 13:13:30.847125: Epoch time: 110.48 s\n",
      "2025-01-17 13:13:31.422693: \n",
      "2025-01-17 13:13:31.422893: Epoch 203\n",
      "2025-01-17 13:13:31.422988: Current learning rate: 0.00815\n",
      "2025-01-17 13:15:21.887214: train_loss -0.725\n",
      "2025-01-17 13:15:21.887343: val_loss -0.6209\n",
      "2025-01-17 13:15:21.887376: Pseudo dice [np.float32(0.6737)]\n",
      "2025-01-17 13:15:21.887414: Epoch time: 110.47 s\n",
      "2025-01-17 13:15:22.706590: \n",
      "2025-01-17 13:15:22.706705: Epoch 204\n",
      "2025-01-17 13:15:22.706769: Current learning rate: 0.00814\n",
      "2025-01-17 13:17:13.204937: train_loss -0.6984\n",
      "2025-01-17 13:17:13.205089: val_loss -0.5947\n",
      "2025-01-17 13:17:13.205377: Pseudo dice [np.float32(0.7185)]\n",
      "2025-01-17 13:17:13.205430: Epoch time: 110.5 s\n",
      "2025-01-17 13:17:13.783918: \n",
      "2025-01-17 13:17:13.784266: Epoch 205\n",
      "2025-01-17 13:17:13.784366: Current learning rate: 0.00813\n",
      "2025-01-17 13:19:04.299270: train_loss -0.7393\n",
      "2025-01-17 13:19:04.299394: val_loss -0.5266\n",
      "2025-01-17 13:19:04.299425: Pseudo dice [np.float32(0.5389)]\n",
      "2025-01-17 13:19:04.299457: Epoch time: 110.52 s\n",
      "2025-01-17 13:19:04.855841: \n",
      "2025-01-17 13:19:04.856199: Epoch 206\n",
      "2025-01-17 13:19:04.856305: Current learning rate: 0.00813\n",
      "2025-01-17 13:20:55.391820: train_loss -0.6923\n",
      "2025-01-17 13:20:55.391959: val_loss -0.6543\n",
      "2025-01-17 13:20:55.391995: Pseudo dice [np.float32(0.7874)]\n",
      "2025-01-17 13:20:55.392027: Epoch time: 110.54 s\n",
      "2025-01-17 13:20:55.946226: \n",
      "2025-01-17 13:20:55.946605: Epoch 207\n",
      "2025-01-17 13:20:55.946678: Current learning rate: 0.00812\n",
      "2025-01-17 13:22:46.493317: train_loss -0.7306\n",
      "2025-01-17 13:22:46.493443: val_loss -0.6678\n",
      "2025-01-17 13:22:46.493475: Pseudo dice [np.float32(0.6695)]\n",
      "2025-01-17 13:22:46.493511: Epoch time: 110.55 s\n",
      "2025-01-17 13:22:47.049216: \n",
      "2025-01-17 13:22:47.049323: Epoch 208\n",
      "2025-01-17 13:22:47.049383: Current learning rate: 0.00811\n",
      "2025-01-17 13:24:37.547580: train_loss -0.7638\n",
      "2025-01-17 13:24:37.547718: val_loss -0.6575\n",
      "2025-01-17 13:24:37.547751: Pseudo dice [np.float32(0.7574)]\n",
      "2025-01-17 13:24:37.547783: Epoch time: 110.5 s\n",
      "2025-01-17 13:24:38.098621: \n",
      "2025-01-17 13:24:38.098917: Epoch 209\n",
      "2025-01-17 13:24:38.099099: Current learning rate: 0.0081\n",
      "2025-01-17 13:26:28.702600: train_loss -0.7698\n",
      "2025-01-17 13:26:28.702742: val_loss -0.6285\n",
      "2025-01-17 13:26:28.702780: Pseudo dice [np.float32(0.7289)]\n",
      "2025-01-17 13:26:28.702820: Epoch time: 110.6 s\n",
      "2025-01-17 13:26:29.264011: \n",
      "2025-01-17 13:26:29.264225: Epoch 210\n",
      "2025-01-17 13:26:29.264299: Current learning rate: 0.00809\n",
      "2025-01-17 13:28:19.760152: train_loss -0.772\n",
      "2025-01-17 13:28:19.760295: val_loss -0.7006\n",
      "2025-01-17 13:28:19.760332: Pseudo dice [np.float32(0.7035)]\n",
      "2025-01-17 13:28:19.760364: Epoch time: 110.5 s\n",
      "2025-01-17 13:28:20.322654: \n",
      "2025-01-17 13:28:20.323001: Epoch 211\n",
      "2025-01-17 13:28:20.323194: Current learning rate: 0.00808\n",
      "2025-01-17 13:30:10.827191: train_loss -0.7509\n",
      "2025-01-17 13:30:10.827387: val_loss -0.649\n",
      "2025-01-17 13:30:10.827422: Pseudo dice [np.float32(0.6789)]\n",
      "2025-01-17 13:30:10.827464: Epoch time: 110.51 s\n",
      "2025-01-17 13:30:11.394270: \n",
      "2025-01-17 13:30:11.394575: Epoch 212\n",
      "2025-01-17 13:30:11.394753: Current learning rate: 0.00807\n",
      "2025-01-17 13:32:01.904682: train_loss -0.7407\n",
      "2025-01-17 13:32:01.904800: val_loss -0.6356\n",
      "2025-01-17 13:32:01.904833: Pseudo dice [np.float32(0.6845)]\n",
      "2025-01-17 13:32:01.904865: Epoch time: 110.51 s\n",
      "2025-01-17 13:32:02.470657: \n",
      "2025-01-17 13:32:02.471047: Epoch 213\n",
      "2025-01-17 13:32:02.471122: Current learning rate: 0.00806\n",
      "2025-01-17 13:33:52.985973: train_loss -0.7629\n",
      "2025-01-17 13:33:52.986121: val_loss -0.6519\n",
      "2025-01-17 13:33:52.986209: Pseudo dice [np.float32(0.6808)]\n",
      "2025-01-17 13:33:52.986373: Epoch time: 110.52 s\n",
      "2025-01-17 13:33:53.546070: \n",
      "2025-01-17 13:33:53.546531: Epoch 214\n",
      "2025-01-17 13:33:53.546786: Current learning rate: 0.00805\n",
      "2025-01-17 13:35:44.083171: train_loss -0.738\n",
      "2025-01-17 13:35:44.083384: val_loss -0.6731\n",
      "2025-01-17 13:35:44.083422: Pseudo dice [np.float32(0.7639)]\n",
      "2025-01-17 13:35:44.083454: Epoch time: 110.54 s\n",
      "2025-01-17 13:35:44.643520: \n",
      "2025-01-17 13:35:44.643890: Epoch 215\n",
      "2025-01-17 13:35:44.644029: Current learning rate: 0.00804\n",
      "2025-01-17 13:37:35.130449: train_loss -0.677\n",
      "2025-01-17 13:37:35.130569: val_loss -0.6206\n",
      "2025-01-17 13:37:35.130601: Pseudo dice [np.float32(0.6837)]\n",
      "2025-01-17 13:37:35.130632: Epoch time: 110.49 s\n",
      "2025-01-17 13:37:35.928484: \n",
      "2025-01-17 13:37:35.928852: Epoch 216\n",
      "2025-01-17 13:37:35.929026: Current learning rate: 0.00803\n",
      "2025-01-17 13:39:26.592814: train_loss -0.6792\n",
      "2025-01-17 13:39:26.593038: val_loss -0.6006\n",
      "2025-01-17 13:39:26.593073: Pseudo dice [np.float32(0.7887)]\n",
      "2025-01-17 13:39:26.593107: Epoch time: 110.66 s\n",
      "2025-01-17 13:39:27.160435: \n",
      "2025-01-17 13:39:27.160545: Epoch 217\n",
      "2025-01-17 13:39:27.160623: Current learning rate: 0.00802\n",
      "2025-01-17 13:41:17.646891: train_loss -0.727\n",
      "2025-01-17 13:41:17.647022: val_loss -0.6989\n",
      "2025-01-17 13:41:17.647058: Pseudo dice [np.float32(0.7425)]\n",
      "2025-01-17 13:41:17.647091: Epoch time: 110.49 s\n",
      "2025-01-17 13:41:18.211299: \n",
      "2025-01-17 13:41:18.211402: Epoch 218\n",
      "2025-01-17 13:41:18.211466: Current learning rate: 0.00801\n",
      "2025-01-17 13:43:08.681000: train_loss -0.7639\n",
      "2025-01-17 13:43:08.681168: val_loss -0.6828\n",
      "2025-01-17 13:43:08.681220: Pseudo dice [np.float32(0.7355)]\n",
      "2025-01-17 13:43:08.681271: Epoch time: 110.47 s\n",
      "2025-01-17 13:43:09.241767: \n",
      "2025-01-17 13:43:09.241880: Epoch 219\n",
      "2025-01-17 13:43:09.241985: Current learning rate: 0.00801\n",
      "2025-01-17 13:44:59.905182: train_loss -0.7679\n",
      "2025-01-17 13:44:59.905302: val_loss -0.6506\n",
      "2025-01-17 13:44:59.905336: Pseudo dice [np.float32(0.6722)]\n",
      "2025-01-17 13:44:59.905489: Epoch time: 110.66 s\n",
      "2025-01-17 13:45:00.465556: \n",
      "2025-01-17 13:45:00.465874: Epoch 220\n",
      "2025-01-17 13:45:00.466059: Current learning rate: 0.008\n",
      "2025-01-17 13:46:51.044309: train_loss -0.7707\n",
      "2025-01-17 13:46:51.044497: val_loss -0.5973\n",
      "2025-01-17 13:46:51.044545: Pseudo dice [np.float32(0.5619)]\n",
      "2025-01-17 13:46:51.044644: Epoch time: 110.58 s\n",
      "2025-01-17 13:46:51.611972: \n",
      "2025-01-17 13:46:51.612070: Epoch 221\n",
      "2025-01-17 13:46:51.612133: Current learning rate: 0.00799\n",
      "2025-01-17 13:48:42.255194: train_loss -0.7475\n",
      "2025-01-17 13:48:42.255329: val_loss -0.5268\n",
      "2025-01-17 13:48:42.255362: Pseudo dice [np.float32(0.2999)]\n",
      "2025-01-17 13:48:42.255396: Epoch time: 110.64 s\n",
      "2025-01-17 13:48:42.815159: \n",
      "2025-01-17 13:48:42.815257: Epoch 222\n",
      "2025-01-17 13:48:42.815317: Current learning rate: 0.00798\n",
      "2025-01-17 13:50:33.342152: train_loss -0.7415\n",
      "2025-01-17 13:50:33.342287: val_loss -0.6055\n",
      "2025-01-17 13:50:33.342320: Pseudo dice [np.float32(0.7155)]\n",
      "2025-01-17 13:50:33.342355: Epoch time: 110.53 s\n",
      "2025-01-17 13:50:33.905445: \n",
      "2025-01-17 13:50:33.905786: Epoch 223\n",
      "2025-01-17 13:50:33.905859: Current learning rate: 0.00797\n",
      "2025-01-17 13:52:24.428728: train_loss -0.7671\n",
      "2025-01-17 13:52:24.428855: val_loss -0.5818\n",
      "2025-01-17 13:52:24.428887: Pseudo dice [np.float32(0.7255)]\n",
      "2025-01-17 13:52:24.428923: Epoch time: 110.52 s\n",
      "2025-01-17 13:52:24.991185: \n",
      "2025-01-17 13:52:24.991592: Epoch 224\n",
      "2025-01-17 13:52:24.991718: Current learning rate: 0.00796\n",
      "2025-01-17 13:54:15.502280: train_loss -0.7307\n",
      "2025-01-17 13:54:15.502404: val_loss -0.6559\n",
      "2025-01-17 13:54:15.502452: Pseudo dice [np.float32(0.7622)]\n",
      "2025-01-17 13:54:15.502492: Epoch time: 110.51 s\n",
      "2025-01-17 13:54:16.069223: \n",
      "2025-01-17 13:54:16.069315: Epoch 225\n",
      "2025-01-17 13:54:16.069378: Current learning rate: 0.00795\n",
      "2025-01-17 13:56:06.605223: train_loss -0.7659\n",
      "2025-01-17 13:56:06.605488: val_loss -0.6457\n",
      "2025-01-17 13:56:06.605557: Pseudo dice [np.float32(0.7635)]\n",
      "2025-01-17 13:56:06.605598: Epoch time: 110.54 s\n",
      "2025-01-17 13:56:07.176523: \n",
      "2025-01-17 13:56:07.176611: Epoch 226\n",
      "2025-01-17 13:56:07.176759: Current learning rate: 0.00794\n",
      "2025-01-17 13:57:57.741478: train_loss -0.7828\n",
      "2025-01-17 13:57:57.741613: val_loss -0.6847\n",
      "2025-01-17 13:57:57.741648: Pseudo dice [np.float32(0.7409)]\n",
      "2025-01-17 13:57:57.741681: Epoch time: 110.57 s\n",
      "2025-01-17 13:57:58.294809: \n",
      "2025-01-17 13:57:58.294899: Epoch 227\n",
      "2025-01-17 13:57:58.294962: Current learning rate: 0.00793\n",
      "2025-01-17 13:59:48.766275: train_loss -0.7958\n",
      "2025-01-17 13:59:48.766423: val_loss -0.6094\n",
      "2025-01-17 13:59:48.766455: Pseudo dice [np.float32(0.7502)]\n",
      "2025-01-17 13:59:48.766488: Epoch time: 110.47 s\n",
      "2025-01-17 13:59:49.557824: \n",
      "2025-01-17 13:59:49.558120: Epoch 228\n",
      "2025-01-17 13:59:49.558252: Current learning rate: 0.00792\n",
      "2025-01-17 14:01:40.024961: train_loss -0.7417\n",
      "2025-01-17 14:01:40.025148: val_loss -0.6876\n",
      "2025-01-17 14:01:40.025313: Pseudo dice [np.float32(0.8006)]\n",
      "2025-01-17 14:01:40.025383: Epoch time: 110.47 s\n",
      "2025-01-17 14:01:40.580149: \n",
      "2025-01-17 14:01:40.580256: Epoch 229\n",
      "2025-01-17 14:01:40.580320: Current learning rate: 0.00791\n",
      "2025-01-17 14:03:31.066089: train_loss -0.7495\n",
      "2025-01-17 14:03:31.066232: val_loss -0.6597\n",
      "2025-01-17 14:03:31.066272: Pseudo dice [np.float32(0.7377)]\n",
      "2025-01-17 14:03:31.066305: Epoch time: 110.49 s\n",
      "2025-01-17 14:03:31.616996: \n",
      "2025-01-17 14:03:31.617350: Epoch 230\n",
      "2025-01-17 14:03:31.617425: Current learning rate: 0.0079\n",
      "2025-01-17 14:05:22.140966: train_loss -0.7562\n",
      "2025-01-17 14:05:22.141161: val_loss -0.6203\n",
      "2025-01-17 14:05:22.141318: Pseudo dice [np.float32(0.6917)]\n",
      "2025-01-17 14:05:22.141371: Epoch time: 110.52 s\n",
      "2025-01-17 14:05:22.701986: \n",
      "2025-01-17 14:05:22.702088: Epoch 231\n",
      "2025-01-17 14:05:22.702150: Current learning rate: 0.00789\n",
      "2025-01-17 14:07:13.161101: train_loss -0.7487\n",
      "2025-01-17 14:07:13.161222: val_loss -0.6364\n",
      "2025-01-17 14:07:13.161254: Pseudo dice [np.float32(0.7156)]\n",
      "2025-01-17 14:07:13.161286: Epoch time: 110.46 s\n",
      "2025-01-17 14:07:13.712666: \n",
      "2025-01-17 14:07:13.712801: Epoch 232\n",
      "2025-01-17 14:07:13.712863: Current learning rate: 0.00789\n",
      "2025-01-17 14:09:04.217927: train_loss -0.6977\n",
      "2025-01-17 14:09:04.218050: val_loss -0.6515\n",
      "2025-01-17 14:09:04.218081: Pseudo dice [np.float32(0.7185)]\n",
      "2025-01-17 14:09:04.218114: Epoch time: 110.51 s\n",
      "2025-01-17 14:09:04.772656: \n",
      "2025-01-17 14:09:04.772760: Epoch 233\n",
      "2025-01-17 14:09:04.772824: Current learning rate: 0.00788\n",
      "2025-01-17 14:10:55.273560: train_loss -0.7185\n",
      "2025-01-17 14:10:55.273995: val_loss -0.6068\n",
      "2025-01-17 14:10:55.274067: Pseudo dice [np.float32(0.7292)]\n",
      "2025-01-17 14:10:55.274111: Epoch time: 110.5 s\n",
      "2025-01-17 14:10:55.823654: \n",
      "2025-01-17 14:10:55.824075: Epoch 234\n",
      "2025-01-17 14:10:55.824209: Current learning rate: 0.00787\n",
      "2025-01-17 14:12:46.366398: train_loss -0.719\n",
      "2025-01-17 14:12:46.366532: val_loss -0.716\n",
      "2025-01-17 14:12:46.366564: Pseudo dice [np.float32(0.7267)]\n",
      "2025-01-17 14:12:46.366598: Epoch time: 110.54 s\n",
      "2025-01-17 14:12:46.922771: \n",
      "2025-01-17 14:12:46.922868: Epoch 235\n",
      "2025-01-17 14:12:46.922934: Current learning rate: 0.00786\n",
      "2025-01-17 14:14:37.464330: train_loss -0.7138\n",
      "2025-01-17 14:14:37.464563: val_loss -0.6843\n",
      "2025-01-17 14:14:37.464608: Pseudo dice [np.float32(0.7374)]\n",
      "2025-01-17 14:14:37.464641: Epoch time: 110.54 s\n",
      "2025-01-17 14:14:38.016231: \n",
      "2025-01-17 14:14:38.016326: Epoch 236\n",
      "2025-01-17 14:14:38.016392: Current learning rate: 0.00785\n",
      "2025-01-17 14:16:28.584057: train_loss -0.6996\n",
      "2025-01-17 14:16:28.584205: val_loss -0.7142\n",
      "2025-01-17 14:16:28.584240: Pseudo dice [np.float32(0.8212)]\n",
      "2025-01-17 14:16:28.584275: Epoch time: 110.57 s\n",
      "2025-01-17 14:16:29.146980: \n",
      "2025-01-17 14:16:29.147076: Epoch 237\n",
      "2025-01-17 14:16:29.147187: Current learning rate: 0.00784\n",
      "2025-01-17 14:18:19.651538: train_loss -0.7743\n",
      "2025-01-17 14:18:19.651923: val_loss -0.6502\n",
      "2025-01-17 14:18:19.652226: Pseudo dice [np.float32(0.7642)]\n",
      "2025-01-17 14:18:19.652304: Epoch time: 110.51 s\n",
      "2025-01-17 14:18:20.203699: \n",
      "2025-01-17 14:18:20.204083: Epoch 238\n",
      "2025-01-17 14:18:20.204316: Current learning rate: 0.00783\n",
      "2025-01-17 14:20:10.695477: train_loss -0.763\n",
      "2025-01-17 14:20:10.695610: val_loss -0.6584\n",
      "2025-01-17 14:20:10.695643: Pseudo dice [np.float32(0.769)]\n",
      "2025-01-17 14:20:10.695738: Epoch time: 110.49 s\n",
      "2025-01-17 14:20:11.247905: \n",
      "2025-01-17 14:20:11.248047: Epoch 239\n",
      "2025-01-17 14:20:11.248119: Current learning rate: 0.00782\n",
      "2025-01-17 14:22:01.745963: train_loss -0.7658\n",
      "2025-01-17 14:22:01.746156: val_loss -0.6723\n",
      "2025-01-17 14:22:01.746194: Pseudo dice [np.float32(0.6868)]\n",
      "2025-01-17 14:22:01.746228: Epoch time: 110.5 s\n",
      "2025-01-17 14:22:02.309039: \n",
      "2025-01-17 14:22:02.309242: Epoch 240\n",
      "2025-01-17 14:22:02.309314: Current learning rate: 0.00781\n",
      "2025-01-17 14:23:52.812721: train_loss -0.7998\n",
      "2025-01-17 14:23:52.812856: val_loss -0.6913\n",
      "2025-01-17 14:23:52.812888: Pseudo dice [np.float32(0.7863)]\n",
      "2025-01-17 14:23:52.812920: Epoch time: 110.5 s\n",
      "2025-01-17 14:23:53.619359: \n",
      "2025-01-17 14:23:53.619461: Epoch 241\n",
      "2025-01-17 14:23:53.619529: Current learning rate: 0.0078\n",
      "2025-01-17 14:25:44.201441: train_loss -0.7821\n",
      "2025-01-17 14:25:44.201572: val_loss -0.7056\n",
      "2025-01-17 14:25:44.201604: Pseudo dice [np.float32(0.7365)]\n",
      "2025-01-17 14:25:44.201637: Epoch time: 110.58 s\n",
      "2025-01-17 14:25:44.761679: \n",
      "2025-01-17 14:25:44.761788: Epoch 242\n",
      "2025-01-17 14:25:44.761849: Current learning rate: 0.00779\n",
      "2025-01-17 14:27:35.306838: train_loss -0.7779\n",
      "2025-01-17 14:27:35.306971: val_loss -0.6687\n",
      "2025-01-17 14:27:35.307005: Pseudo dice [np.float32(0.7832)]\n",
      "2025-01-17 14:27:35.307037: Epoch time: 110.55 s\n",
      "2025-01-17 14:27:35.863914: \n",
      "2025-01-17 14:27:35.864018: Epoch 243\n",
      "2025-01-17 14:27:35.864080: Current learning rate: 0.00778\n",
      "2025-01-17 14:29:26.356597: train_loss -0.7737\n",
      "2025-01-17 14:29:26.356975: val_loss -0.596\n",
      "2025-01-17 14:29:26.357026: Pseudo dice [np.float32(0.7298)]\n",
      "2025-01-17 14:29:26.357064: Epoch time: 110.49 s\n",
      "2025-01-17 14:29:26.919003: \n",
      "2025-01-17 14:29:26.919112: Epoch 244\n",
      "2025-01-17 14:29:26.919176: Current learning rate: 0.00777\n",
      "2025-01-17 14:31:17.420806: train_loss -0.7892\n",
      "2025-01-17 14:31:17.421050: val_loss -0.7234\n",
      "2025-01-17 14:31:17.421098: Pseudo dice [np.float32(0.7387)]\n",
      "2025-01-17 14:31:17.421134: Epoch time: 110.5 s\n",
      "2025-01-17 14:31:17.980515: \n",
      "2025-01-17 14:31:17.980609: Epoch 245\n",
      "2025-01-17 14:31:17.980673: Current learning rate: 0.00777\n",
      "2025-01-17 14:33:08.485464: train_loss -0.7606\n",
      "2025-01-17 14:33:08.485662: val_loss -0.6986\n",
      "2025-01-17 14:33:08.485698: Pseudo dice [np.float32(0.7883)]\n",
      "2025-01-17 14:33:08.485733: Epoch time: 110.51 s\n",
      "2025-01-17 14:33:09.041928: \n",
      "2025-01-17 14:33:09.042205: Epoch 246\n",
      "2025-01-17 14:33:09.042445: Current learning rate: 0.00776\n",
      "2025-01-17 14:34:59.557997: train_loss -0.7509\n",
      "2025-01-17 14:34:59.558231: val_loss -0.6418\n",
      "2025-01-17 14:34:59.558294: Pseudo dice [np.float32(0.6787)]\n",
      "2025-01-17 14:34:59.558334: Epoch time: 110.52 s\n",
      "2025-01-17 14:35:00.119914: \n",
      "2025-01-17 14:35:00.120132: Epoch 247\n",
      "2025-01-17 14:35:00.120382: Current learning rate: 0.00775\n",
      "2025-01-17 14:36:50.631540: train_loss -0.722\n",
      "2025-01-17 14:36:50.631668: val_loss -0.6476\n",
      "2025-01-17 14:36:50.631701: Pseudo dice [np.float32(0.6645)]\n",
      "2025-01-17 14:36:50.631734: Epoch time: 110.51 s\n",
      "2025-01-17 14:36:51.189401: \n",
      "2025-01-17 14:36:51.189502: Epoch 248\n",
      "2025-01-17 14:36:51.189564: Current learning rate: 0.00774\n",
      "2025-01-17 14:38:41.694825: train_loss -0.7584\n",
      "2025-01-17 14:38:41.694983: val_loss -0.5644\n",
      "2025-01-17 14:38:41.695019: Pseudo dice [np.float32(0.5999)]\n",
      "2025-01-17 14:38:41.695051: Epoch time: 110.51 s\n",
      "2025-01-17 14:38:42.259120: \n",
      "2025-01-17 14:38:42.259497: Epoch 249\n",
      "2025-01-17 14:38:42.259570: Current learning rate: 0.00773\n",
      "2025-01-17 14:40:32.864694: train_loss -0.7392\n",
      "2025-01-17 14:40:32.864829: val_loss -0.5764\n",
      "2025-01-17 14:40:32.864864: Pseudo dice [np.float32(0.7228)]\n",
      "2025-01-17 14:40:32.864899: Epoch time: 110.61 s\n",
      "2025-01-17 14:40:33.637249: \n",
      "2025-01-17 14:40:33.637367: Epoch 250\n",
      "2025-01-17 14:40:33.637435: Current learning rate: 0.00772\n",
      "2025-01-17 14:42:24.109036: train_loss -0.7791\n",
      "2025-01-17 14:42:24.109167: val_loss -0.6458\n",
      "2025-01-17 14:42:24.109203: Pseudo dice [np.float32(0.7389)]\n",
      "2025-01-17 14:42:24.109257: Epoch time: 110.47 s\n",
      "2025-01-17 14:42:24.667149: \n",
      "2025-01-17 14:42:24.667244: Epoch 251\n",
      "2025-01-17 14:42:24.667305: Current learning rate: 0.00771\n",
      "2025-01-17 14:44:15.143894: train_loss -0.7892\n",
      "2025-01-17 14:44:15.144024: val_loss -0.664\n",
      "2025-01-17 14:44:15.144057: Pseudo dice [np.float32(0.7344)]\n",
      "2025-01-17 14:44:15.144280: Epoch time: 110.48 s\n",
      "2025-01-17 14:44:15.701335: \n",
      "2025-01-17 14:44:15.701424: Epoch 252\n",
      "2025-01-17 14:44:15.701486: Current learning rate: 0.0077\n",
      "2025-01-17 14:46:06.282062: train_loss -0.7503\n",
      "2025-01-17 14:46:06.282199: val_loss -0.5959\n",
      "2025-01-17 14:46:06.282232: Pseudo dice [np.float32(0.6742)]\n",
      "2025-01-17 14:46:06.282266: Epoch time: 110.58 s\n",
      "2025-01-17 14:46:07.076562: \n",
      "2025-01-17 14:46:07.076952: Epoch 253\n",
      "2025-01-17 14:46:07.077106: Current learning rate: 0.00769\n",
      "2025-01-17 14:47:57.580975: train_loss -0.7081\n",
      "2025-01-17 14:47:57.581107: val_loss -0.5946\n",
      "2025-01-17 14:47:57.581143: Pseudo dice [np.float32(0.7264)]\n",
      "2025-01-17 14:47:57.581180: Epoch time: 110.5 s\n",
      "2025-01-17 14:47:58.137123: \n",
      "2025-01-17 14:47:58.137454: Epoch 254\n",
      "2025-01-17 14:47:58.137537: Current learning rate: 0.00768\n",
      "2025-01-17 14:49:48.713203: train_loss -0.7416\n",
      "2025-01-17 14:49:48.713353: val_loss -0.6275\n",
      "2025-01-17 14:49:48.713388: Pseudo dice [np.float32(0.7413)]\n",
      "2025-01-17 14:49:48.713422: Epoch time: 110.58 s\n",
      "2025-01-17 14:49:49.269224: \n",
      "2025-01-17 14:49:49.269334: Epoch 255\n",
      "2025-01-17 14:49:49.269394: Current learning rate: 0.00767\n",
      "2025-01-17 14:51:39.879147: train_loss -0.7459\n",
      "2025-01-17 14:51:39.879283: val_loss -0.6373\n",
      "2025-01-17 14:51:39.879318: Pseudo dice [np.float32(0.7084)]\n",
      "2025-01-17 14:51:39.879350: Epoch time: 110.61 s\n",
      "2025-01-17 14:51:40.437157: \n",
      "2025-01-17 14:51:40.437266: Epoch 256\n",
      "2025-01-17 14:51:40.437329: Current learning rate: 0.00766\n",
      "2025-01-17 14:53:30.980640: train_loss -0.7433\n",
      "2025-01-17 14:53:30.980770: val_loss -0.6154\n",
      "2025-01-17 14:53:30.980802: Pseudo dice [np.float32(0.797)]\n",
      "2025-01-17 14:53:30.980836: Epoch time: 110.54 s\n",
      "2025-01-17 14:53:31.541867: \n",
      "2025-01-17 14:53:31.541964: Epoch 257\n",
      "2025-01-17 14:53:31.542026: Current learning rate: 0.00765\n",
      "2025-01-17 14:55:22.064880: train_loss -0.7482\n",
      "2025-01-17 14:55:22.065009: val_loss -0.629\n",
      "2025-01-17 14:55:22.065044: Pseudo dice [np.float32(0.7596)]\n",
      "2025-01-17 14:55:22.065077: Epoch time: 110.52 s\n",
      "2025-01-17 14:55:22.627422: \n",
      "2025-01-17 14:55:22.627529: Epoch 258\n",
      "2025-01-17 14:55:22.627589: Current learning rate: 0.00764\n",
      "2025-01-17 14:57:13.284978: train_loss -0.7557\n",
      "2025-01-17 14:57:13.285133: val_loss -0.6585\n",
      "2025-01-17 14:57:13.285251: Pseudo dice [np.float32(0.7613)]\n",
      "2025-01-17 14:57:13.285291: Epoch time: 110.66 s\n",
      "2025-01-17 14:57:13.850721: \n",
      "2025-01-17 14:57:13.851117: Epoch 259\n",
      "2025-01-17 14:57:13.851367: Current learning rate: 0.00764\n",
      "2025-01-17 14:59:04.408343: train_loss -0.7345\n",
      "2025-01-17 14:59:04.408476: val_loss -0.5856\n",
      "2025-01-17 14:59:04.408511: Pseudo dice [np.float32(0.7177)]\n",
      "2025-01-17 14:59:04.408544: Epoch time: 110.56 s\n",
      "2025-01-17 14:59:04.963745: \n",
      "2025-01-17 14:59:04.963841: Epoch 260\n",
      "2025-01-17 14:59:04.963902: Current learning rate: 0.00763\n",
      "2025-01-17 15:00:55.620655: train_loss -0.7248\n",
      "2025-01-17 15:00:55.620783: val_loss -0.5841\n",
      "2025-01-17 15:00:55.620821: Pseudo dice [np.float32(0.699)]\n",
      "2025-01-17 15:00:55.620945: Epoch time: 110.66 s\n",
      "2025-01-17 15:00:56.178365: \n",
      "2025-01-17 15:00:56.178570: Epoch 261\n",
      "2025-01-17 15:00:56.178646: Current learning rate: 0.00762\n",
      "2025-01-17 15:02:46.700786: train_loss -0.7209\n",
      "2025-01-17 15:02:46.700924: val_loss -0.6622\n",
      "2025-01-17 15:02:46.700958: Pseudo dice [np.float32(0.7307)]\n",
      "2025-01-17 15:02:46.700990: Epoch time: 110.52 s\n",
      "2025-01-17 15:02:47.255393: \n",
      "2025-01-17 15:02:47.255485: Epoch 262\n",
      "2025-01-17 15:02:47.255548: Current learning rate: 0.00761\n",
      "2025-01-17 15:04:37.785696: train_loss -0.7606\n",
      "2025-01-17 15:04:37.785815: val_loss -0.6579\n",
      "2025-01-17 15:04:37.785848: Pseudo dice [np.float32(0.7511)]\n",
      "2025-01-17 15:04:37.785884: Epoch time: 110.53 s\n",
      "2025-01-17 15:04:38.348068: \n",
      "2025-01-17 15:04:38.348421: Epoch 263\n",
      "2025-01-17 15:04:38.348487: Current learning rate: 0.0076\n",
      "2025-01-17 15:06:28.842143: train_loss -0.78\n",
      "2025-01-17 15:06:28.842278: val_loss -0.6493\n",
      "2025-01-17 15:06:28.842312: Pseudo dice [np.float32(0.7318)]\n",
      "2025-01-17 15:06:28.842403: Epoch time: 110.49 s\n",
      "2025-01-17 15:06:29.398759: \n",
      "2025-01-17 15:06:29.398849: Epoch 264\n",
      "2025-01-17 15:06:29.398913: Current learning rate: 0.00759\n",
      "2025-01-17 15:08:19.889329: train_loss -0.7839\n",
      "2025-01-17 15:08:19.889503: val_loss -0.5724\n",
      "2025-01-17 15:08:19.889570: Pseudo dice [np.float32(0.6626)]\n",
      "2025-01-17 15:08:19.889612: Epoch time: 110.49 s\n",
      "2025-01-17 15:08:20.451714: \n",
      "2025-01-17 15:08:20.451802: Epoch 265\n",
      "2025-01-17 15:08:20.451866: Current learning rate: 0.00758\n",
      "2025-01-17 15:10:10.979671: train_loss -0.8017\n",
      "2025-01-17 15:10:10.979803: val_loss -0.597\n",
      "2025-01-17 15:10:10.979837: Pseudo dice [np.float32(0.6359)]\n",
      "2025-01-17 15:10:10.979871: Epoch time: 110.53 s\n",
      "2025-01-17 15:10:11.778598: \n",
      "2025-01-17 15:10:11.778948: Epoch 266\n",
      "2025-01-17 15:10:11.779076: Current learning rate: 0.00757\n",
      "2025-01-17 15:12:02.291831: train_loss -0.7803\n",
      "2025-01-17 15:12:02.292043: val_loss -0.6058\n",
      "2025-01-17 15:12:02.292095: Pseudo dice [np.float32(0.6684)]\n",
      "2025-01-17 15:12:02.292131: Epoch time: 110.51 s\n",
      "2025-01-17 15:12:02.856681: \n",
      "2025-01-17 15:12:02.856789: Epoch 267\n",
      "2025-01-17 15:12:02.856863: Current learning rate: 0.00756\n",
      "2025-01-17 15:13:53.350238: train_loss -0.7745\n",
      "2025-01-17 15:13:53.350450: val_loss -0.6527\n",
      "2025-01-17 15:13:53.350503: Pseudo dice [np.float32(0.7166)]\n",
      "2025-01-17 15:13:53.350540: Epoch time: 110.49 s\n",
      "2025-01-17 15:13:53.913439: \n",
      "2025-01-17 15:13:53.913851: Epoch 268\n",
      "2025-01-17 15:13:53.913946: Current learning rate: 0.00755\n",
      "2025-01-17 15:15:44.475590: train_loss -0.7805\n",
      "2025-01-17 15:15:44.475769: val_loss -0.6472\n",
      "2025-01-17 15:15:44.475806: Pseudo dice [np.float32(0.7592)]\n",
      "2025-01-17 15:15:44.475840: Epoch time: 110.56 s\n",
      "2025-01-17 15:15:45.040298: \n",
      "2025-01-17 15:15:45.040411: Epoch 269\n",
      "2025-01-17 15:15:45.040530: Current learning rate: 0.00754\n",
      "2025-01-17 15:17:35.557912: train_loss -0.7799\n",
      "2025-01-17 15:17:35.558077: val_loss -0.6327\n",
      "2025-01-17 15:17:35.558167: Pseudo dice [np.float32(0.7305)]\n",
      "2025-01-17 15:17:35.558212: Epoch time: 110.52 s\n",
      "2025-01-17 15:17:36.123851: \n",
      "2025-01-17 15:17:36.123959: Epoch 270\n",
      "2025-01-17 15:17:36.124021: Current learning rate: 0.00753\n",
      "2025-01-17 15:19:26.766599: train_loss -0.7848\n",
      "2025-01-17 15:19:26.766725: val_loss -0.6821\n",
      "2025-01-17 15:19:26.766759: Pseudo dice [np.float32(0.7769)]\n",
      "2025-01-17 15:19:26.766793: Epoch time: 110.64 s\n",
      "2025-01-17 15:19:27.335690: \n",
      "2025-01-17 15:19:27.335950: Epoch 271\n",
      "2025-01-17 15:19:27.336097: Current learning rate: 0.00752\n",
      "2025-01-17 15:21:17.866735: train_loss -0.8001\n",
      "2025-01-17 15:21:17.867090: val_loss -0.6442\n",
      "2025-01-17 15:21:17.867332: Pseudo dice [np.float32(0.7144)]\n",
      "2025-01-17 15:21:17.867401: Epoch time: 110.53 s\n",
      "2025-01-17 15:21:18.428187: \n",
      "2025-01-17 15:21:18.428560: Epoch 272\n",
      "2025-01-17 15:21:18.428638: Current learning rate: 0.00751\n",
      "2025-01-17 15:23:08.973268: train_loss -0.7928\n",
      "2025-01-17 15:23:08.973480: val_loss -0.6897\n",
      "2025-01-17 15:23:08.973516: Pseudo dice [np.float32(0.7528)]\n",
      "2025-01-17 15:23:08.973551: Epoch time: 110.55 s\n",
      "2025-01-17 15:23:09.535010: \n",
      "2025-01-17 15:23:09.535384: Epoch 273\n",
      "2025-01-17 15:23:09.535453: Current learning rate: 0.00751\n",
      "2025-01-17 15:25:00.065408: train_loss -0.7989\n",
      "2025-01-17 15:25:00.065548: val_loss -0.6314\n",
      "2025-01-17 15:25:00.065599: Pseudo dice [np.float32(0.7264)]\n",
      "2025-01-17 15:25:00.065732: Epoch time: 110.53 s\n",
      "2025-01-17 15:25:00.618810: \n",
      "2025-01-17 15:25:00.619167: Epoch 274\n",
      "2025-01-17 15:25:00.619431: Current learning rate: 0.0075\n",
      "2025-01-17 15:26:51.152020: train_loss -0.7844\n",
      "2025-01-17 15:26:51.152154: val_loss -0.713\n",
      "2025-01-17 15:26:51.152187: Pseudo dice [np.float32(0.7168)]\n",
      "2025-01-17 15:26:51.152221: Epoch time: 110.53 s\n",
      "2025-01-17 15:26:51.714614: \n",
      "2025-01-17 15:26:51.714810: Epoch 275\n",
      "2025-01-17 15:26:51.714888: Current learning rate: 0.00749\n",
      "2025-01-17 15:28:42.209554: train_loss -0.7914\n",
      "2025-01-17 15:28:42.209743: val_loss -0.6763\n",
      "2025-01-17 15:28:42.209780: Pseudo dice [np.float32(0.7262)]\n",
      "2025-01-17 15:28:42.209814: Epoch time: 110.5 s\n",
      "2025-01-17 15:28:42.770069: \n",
      "2025-01-17 15:28:42.770307: Epoch 276\n",
      "2025-01-17 15:28:42.770431: Current learning rate: 0.00748\n",
      "2025-01-17 15:30:33.285720: train_loss -0.7992\n",
      "2025-01-17 15:30:33.285846: val_loss -0.6778\n",
      "2025-01-17 15:30:33.285880: Pseudo dice [np.float32(0.7274)]\n",
      "2025-01-17 15:30:33.285912: Epoch time: 110.52 s\n",
      "2025-01-17 15:30:33.839888: \n",
      "2025-01-17 15:30:33.840054: Epoch 277\n",
      "2025-01-17 15:30:33.840127: Current learning rate: 0.00747\n",
      "2025-01-17 15:32:24.419572: train_loss -0.8059\n",
      "2025-01-17 15:32:24.419963: val_loss -0.6815\n",
      "2025-01-17 15:32:24.420008: Pseudo dice [np.float32(0.7787)]\n",
      "2025-01-17 15:32:24.420042: Epoch time: 110.58 s\n",
      "2025-01-17 15:32:25.251489: \n",
      "2025-01-17 15:32:25.251597: Epoch 278\n",
      "2025-01-17 15:32:25.251680: Current learning rate: 0.00746\n",
      "2025-01-17 15:34:15.806627: train_loss -0.7971\n",
      "2025-01-17 15:34:15.806768: val_loss -0.703\n",
      "2025-01-17 15:34:15.806822: Pseudo dice [np.float32(0.7689)]\n",
      "2025-01-17 15:34:15.806858: Epoch time: 110.56 s\n",
      "2025-01-17 15:34:16.369781: \n",
      "2025-01-17 15:34:16.369890: Epoch 279\n",
      "2025-01-17 15:34:16.369955: Current learning rate: 0.00745\n",
      "2025-01-17 15:36:06.970792: train_loss -0.7732\n",
      "2025-01-17 15:36:06.971089: val_loss -0.6274\n",
      "2025-01-17 15:36:06.971404: Pseudo dice [np.float32(0.742)]\n",
      "2025-01-17 15:36:06.971483: Epoch time: 110.6 s\n",
      "2025-01-17 15:36:07.531656: \n",
      "2025-01-17 15:36:07.531984: Epoch 280\n",
      "2025-01-17 15:36:07.532070: Current learning rate: 0.00744\n",
      "2025-01-17 15:37:58.035114: train_loss -0.776\n",
      "2025-01-17 15:37:58.035242: val_loss -0.6604\n",
      "2025-01-17 15:37:58.035277: Pseudo dice [np.float32(0.7111)]\n",
      "2025-01-17 15:37:58.035310: Epoch time: 110.5 s\n",
      "2025-01-17 15:37:58.601843: \n",
      "2025-01-17 15:37:58.601951: Epoch 281\n",
      "2025-01-17 15:37:58.602020: Current learning rate: 0.00743\n",
      "2025-01-17 15:39:49.218899: train_loss -0.8001\n",
      "2025-01-17 15:39:49.219036: val_loss -0.6735\n",
      "2025-01-17 15:39:49.219069: Pseudo dice [np.float32(0.71)]\n",
      "2025-01-17 15:39:49.219208: Epoch time: 110.62 s\n",
      "2025-01-17 15:39:49.784682: \n",
      "2025-01-17 15:39:49.784832: Epoch 282\n",
      "2025-01-17 15:39:49.784953: Current learning rate: 0.00742\n",
      "2025-01-17 15:41:40.300812: train_loss -0.7899\n",
      "2025-01-17 15:41:40.300976: val_loss -0.6053\n",
      "2025-01-17 15:41:40.301049: Pseudo dice [np.float32(0.7035)]\n",
      "2025-01-17 15:41:40.301091: Epoch time: 110.52 s\n",
      "2025-01-17 15:41:40.859987: \n",
      "2025-01-17 15:41:40.860379: Epoch 283\n",
      "2025-01-17 15:41:40.860476: Current learning rate: 0.00741\n",
      "2025-01-17 15:43:31.361817: train_loss -0.7646\n",
      "2025-01-17 15:43:31.361949: val_loss -0.7451\n",
      "2025-01-17 15:43:31.361984: Pseudo dice [np.float32(0.7657)]\n",
      "2025-01-17 15:43:31.362016: Epoch time: 110.5 s\n",
      "2025-01-17 15:43:31.924635: \n",
      "2025-01-17 15:43:31.925036: Epoch 284\n",
      "2025-01-17 15:43:31.925129: Current learning rate: 0.0074\n",
      "2025-01-17 15:45:22.413855: train_loss -0.8\n",
      "2025-01-17 15:45:22.413995: val_loss -0.7375\n",
      "2025-01-17 15:45:22.414141: Pseudo dice [np.float32(0.8224)]\n",
      "2025-01-17 15:45:22.414246: Epoch time: 110.49 s\n",
      "2025-01-17 15:45:22.968600: \n",
      "2025-01-17 15:45:22.968699: Epoch 285\n",
      "2025-01-17 15:45:22.968770: Current learning rate: 0.00739\n",
      "2025-01-17 15:47:13.646802: train_loss -0.7639\n",
      "2025-01-17 15:47:13.646931: val_loss -0.7165\n",
      "2025-01-17 15:47:13.646972: Pseudo dice [np.float32(0.8509)]\n",
      "2025-01-17 15:47:13.647007: Epoch time: 110.68 s\n",
      "2025-01-17 15:47:14.211489: \n",
      "2025-01-17 15:47:14.211582: Epoch 286\n",
      "2025-01-17 15:47:14.211643: Current learning rate: 0.00738\n",
      "2025-01-17 15:49:04.702243: train_loss -0.7648\n",
      "2025-01-17 15:49:04.702367: val_loss -0.6487\n",
      "2025-01-17 15:49:04.702399: Pseudo dice [np.float32(0.7754)]\n",
      "2025-01-17 15:49:04.702431: Epoch time: 110.49 s\n",
      "2025-01-17 15:49:04.702451: Yayy! New best EMA pseudo Dice: 0.7540000081062317\n",
      "2025-01-17 15:49:05.556332: \n",
      "2025-01-17 15:49:05.556418: Epoch 287\n",
      "2025-01-17 15:49:05.556478: Current learning rate: 0.00738\n",
      "2025-01-17 15:50:56.070779: train_loss -0.7881\n",
      "2025-01-17 15:50:56.070915: val_loss -0.6691\n",
      "2025-01-17 15:50:56.070953: Pseudo dice [np.float32(0.7305)]\n",
      "2025-01-17 15:50:56.070987: Epoch time: 110.51 s\n",
      "2025-01-17 15:50:56.633097: \n",
      "2025-01-17 15:50:56.633245: Epoch 288\n",
      "2025-01-17 15:50:56.633318: Current learning rate: 0.00737\n",
      "2025-01-17 15:52:47.192133: train_loss -0.7627\n",
      "2025-01-17 15:52:47.192309: val_loss -0.595\n",
      "2025-01-17 15:52:47.192521: Pseudo dice [np.float32(0.7081)]\n",
      "2025-01-17 15:52:47.192691: Epoch time: 110.56 s\n",
      "2025-01-17 15:52:47.767308: \n",
      "2025-01-17 15:52:47.767397: Epoch 289\n",
      "2025-01-17 15:52:47.767458: Current learning rate: 0.00736\n",
      "2025-01-17 15:54:38.271421: train_loss -0.7341\n",
      "2025-01-17 15:54:38.271644: val_loss -0.6725\n",
      "2025-01-17 15:54:38.271689: Pseudo dice [np.float32(0.6628)]\n",
      "2025-01-17 15:54:38.271724: Epoch time: 110.5 s\n",
      "2025-01-17 15:54:39.078839: \n",
      "2025-01-17 15:54:39.079112: Epoch 290\n",
      "2025-01-17 15:54:39.079257: Current learning rate: 0.00735\n",
      "2025-01-17 15:56:29.557770: train_loss -0.7317\n",
      "2025-01-17 15:56:29.557968: val_loss -0.5363\n",
      "2025-01-17 15:56:29.558002: Pseudo dice [np.float32(0.5052)]\n",
      "2025-01-17 15:56:29.558034: Epoch time: 110.48 s\n",
      "2025-01-17 15:56:30.123880: \n",
      "2025-01-17 15:56:30.124134: Epoch 291\n",
      "2025-01-17 15:56:30.124303: Current learning rate: 0.00734\n",
      "2025-01-17 15:58:20.731312: train_loss -0.7364\n",
      "2025-01-17 15:58:20.731439: val_loss -0.6923\n",
      "2025-01-17 15:58:20.731472: Pseudo dice [np.float32(0.745)]\n",
      "2025-01-17 15:58:20.731599: Epoch time: 110.61 s\n",
      "2025-01-17 15:58:21.299740: \n",
      "2025-01-17 15:58:21.300008: Epoch 292\n",
      "2025-01-17 15:58:21.300149: Current learning rate: 0.00733\n",
      "2025-01-17 16:00:11.786295: train_loss -0.7755\n",
      "2025-01-17 16:00:11.786675: val_loss -0.6576\n",
      "2025-01-17 16:00:11.786714: Pseudo dice [np.float32(0.707)]\n",
      "2025-01-17 16:00:11.786749: Epoch time: 110.49 s\n",
      "2025-01-17 16:00:12.358814: \n",
      "2025-01-17 16:00:12.358911: Epoch 293\n",
      "2025-01-17 16:00:12.358973: Current learning rate: 0.00732\n",
      "2025-01-17 16:02:02.899457: train_loss -0.7986\n",
      "2025-01-17 16:02:02.899630: val_loss -0.6185\n",
      "2025-01-17 16:02:02.899665: Pseudo dice [np.float32(0.6763)]\n",
      "2025-01-17 16:02:02.899699: Epoch time: 110.54 s\n",
      "2025-01-17 16:02:03.460580: \n",
      "2025-01-17 16:02:03.460972: Epoch 294\n",
      "2025-01-17 16:02:03.461056: Current learning rate: 0.00731\n",
      "2025-01-17 16:03:54.006194: train_loss -0.801\n",
      "2025-01-17 16:03:54.006332: val_loss -0.5653\n",
      "2025-01-17 16:03:54.006366: Pseudo dice [np.float32(0.7368)]\n",
      "2025-01-17 16:03:54.006397: Epoch time: 110.55 s\n",
      "2025-01-17 16:03:54.600151: \n",
      "2025-01-17 16:03:54.600314: Epoch 295\n",
      "2025-01-17 16:03:54.600387: Current learning rate: 0.0073\n",
      "2025-01-17 16:05:45.157648: train_loss -0.7682\n",
      "2025-01-17 16:05:45.157790: val_loss -0.5592\n",
      "2025-01-17 16:05:45.157825: Pseudo dice [np.float32(0.7425)]\n",
      "2025-01-17 16:05:45.157859: Epoch time: 110.56 s\n",
      "2025-01-17 16:05:45.736476: \n",
      "2025-01-17 16:05:45.736568: Epoch 296\n",
      "2025-01-17 16:05:45.736629: Current learning rate: 0.00729\n",
      "2025-01-17 16:07:36.207552: train_loss -0.7424\n",
      "2025-01-17 16:07:36.207976: val_loss -0.6755\n",
      "2025-01-17 16:07:36.208039: Pseudo dice [np.float32(0.6758)]\n",
      "2025-01-17 16:07:36.208079: Epoch time: 110.47 s\n",
      "2025-01-17 16:07:36.779705: \n",
      "2025-01-17 16:07:36.779801: Epoch 297\n",
      "2025-01-17 16:07:36.779867: Current learning rate: 0.00728\n",
      "2025-01-17 16:09:27.340922: train_loss -0.7691\n",
      "2025-01-17 16:09:27.341053: val_loss -0.5544\n",
      "2025-01-17 16:09:27.341087: Pseudo dice [np.float32(0.6782)]\n",
      "2025-01-17 16:09:27.341119: Epoch time: 110.56 s\n",
      "2025-01-17 16:09:27.907725: \n",
      "2025-01-17 16:09:27.907893: Epoch 298\n",
      "2025-01-17 16:09:27.907957: Current learning rate: 0.00727\n",
      "2025-01-17 16:11:18.461426: train_loss -0.7859\n",
      "2025-01-17 16:11:18.461561: val_loss -0.6973\n",
      "2025-01-17 16:11:18.461599: Pseudo dice [np.float32(0.708)]\n",
      "2025-01-17 16:11:18.461635: Epoch time: 110.55 s\n",
      "2025-01-17 16:11:19.033750: \n",
      "2025-01-17 16:11:19.033847: Epoch 299\n",
      "2025-01-17 16:11:19.033913: Current learning rate: 0.00726\n",
      "2025-01-17 16:13:09.507697: train_loss -0.8001\n",
      "2025-01-17 16:13:09.507857: val_loss -0.6434\n",
      "2025-01-17 16:13:09.508055: Pseudo dice [np.float32(0.6717)]\n",
      "2025-01-17 16:13:09.508126: Epoch time: 110.47 s\n",
      "2025-01-17 16:13:10.288951: \n",
      "2025-01-17 16:13:10.289160: Epoch 300\n",
      "2025-01-17 16:13:10.289258: Current learning rate: 0.00725\n",
      "2025-01-17 16:15:00.783983: train_loss -0.7988\n",
      "2025-01-17 16:15:00.784328: val_loss -0.6473\n",
      "2025-01-17 16:15:00.784372: Pseudo dice [np.float32(0.7259)]\n",
      "2025-01-17 16:15:00.784407: Epoch time: 110.5 s\n",
      "2025-01-17 16:15:01.347395: \n",
      "2025-01-17 16:15:01.347765: Epoch 301\n",
      "2025-01-17 16:15:01.347841: Current learning rate: 0.00724\n",
      "2025-01-17 16:16:51.810442: train_loss -0.7781\n",
      "2025-01-17 16:16:51.810603: val_loss -0.6951\n",
      "2025-01-17 16:16:51.810644: Pseudo dice [np.float32(0.6274)]\n",
      "2025-01-17 16:16:51.810681: Epoch time: 110.46 s\n",
      "2025-01-17 16:16:52.633758: \n",
      "2025-01-17 16:16:52.633959: Epoch 302\n",
      "2025-01-17 16:16:52.634034: Current learning rate: 0.00724\n",
      "2025-01-17 16:18:43.105867: train_loss -0.7862\n",
      "2025-01-17 16:18:43.106025: val_loss -0.6998\n",
      "2025-01-17 16:18:43.106227: Pseudo dice [np.float32(0.7807)]\n",
      "2025-01-17 16:18:43.106505: Epoch time: 110.47 s\n",
      "2025-01-17 16:18:43.673415: \n",
      "2025-01-17 16:18:43.673539: Epoch 303\n",
      "2025-01-17 16:18:43.673886: Current learning rate: 0.00723\n",
      "2025-01-17 16:20:34.197959: train_loss -0.7699\n",
      "2025-01-17 16:20:34.198097: val_loss -0.6399\n",
      "2025-01-17 16:20:34.198129: Pseudo dice [np.float32(0.7424)]\n",
      "2025-01-17 16:20:34.198161: Epoch time: 110.53 s\n",
      "2025-01-17 16:20:34.776925: \n",
      "2025-01-17 16:20:34.777307: Epoch 304\n",
      "2025-01-17 16:20:34.777406: Current learning rate: 0.00722\n",
      "2025-01-17 16:22:25.251751: train_loss -0.7831\n",
      "2025-01-17 16:22:25.251934: val_loss -0.6677\n",
      "2025-01-17 16:22:25.251970: Pseudo dice [np.float32(0.7646)]\n",
      "2025-01-17 16:22:25.252001: Epoch time: 110.48 s\n",
      "2025-01-17 16:22:25.822557: \n",
      "2025-01-17 16:22:25.823133: Epoch 305\n",
      "2025-01-17 16:22:25.823211: Current learning rate: 0.00721\n",
      "2025-01-17 16:24:16.359463: train_loss -0.793\n",
      "2025-01-17 16:24:16.359655: val_loss -0.6647\n",
      "2025-01-17 16:24:16.359689: Pseudo dice [np.float32(0.6975)]\n",
      "2025-01-17 16:24:16.359722: Epoch time: 110.54 s\n",
      "2025-01-17 16:24:16.927320: \n",
      "2025-01-17 16:24:16.927424: Epoch 306\n",
      "2025-01-17 16:24:16.927489: Current learning rate: 0.0072\n",
      "2025-01-17 16:26:07.432214: train_loss -0.7429\n",
      "2025-01-17 16:26:07.432363: val_loss -0.6025\n",
      "2025-01-17 16:26:07.432397: Pseudo dice [np.float32(0.7302)]\n",
      "2025-01-17 16:26:07.432429: Epoch time: 110.51 s\n",
      "2025-01-17 16:26:08.004550: \n",
      "2025-01-17 16:26:08.004639: Epoch 307\n",
      "2025-01-17 16:26:08.004700: Current learning rate: 0.00719\n",
      "2025-01-17 16:27:58.492320: train_loss -0.7705\n",
      "2025-01-17 16:27:58.492679: val_loss -0.6828\n",
      "2025-01-17 16:27:58.492786: Pseudo dice [np.float32(0.7611)]\n",
      "2025-01-17 16:27:58.492833: Epoch time: 110.49 s\n",
      "2025-01-17 16:27:59.058287: \n",
      "2025-01-17 16:27:59.058371: Epoch 308\n",
      "2025-01-17 16:27:59.058433: Current learning rate: 0.00718\n",
      "2025-01-17 16:29:49.575844: train_loss -0.7705\n",
      "2025-01-17 16:29:49.575962: val_loss -0.6501\n",
      "2025-01-17 16:29:49.575994: Pseudo dice [np.float32(0.6677)]\n",
      "2025-01-17 16:29:49.576028: Epoch time: 110.52 s\n",
      "2025-01-17 16:29:50.145089: \n",
      "2025-01-17 16:29:50.145344: Epoch 309\n",
      "2025-01-17 16:29:50.145420: Current learning rate: 0.00717\n",
      "2025-01-17 16:31:40.637045: train_loss -0.7693\n",
      "2025-01-17 16:31:40.637177: val_loss -0.669\n",
      "2025-01-17 16:31:40.637209: Pseudo dice [np.float32(0.6392)]\n",
      "2025-01-17 16:31:40.637246: Epoch time: 110.49 s\n",
      "2025-01-17 16:31:41.196136: \n",
      "2025-01-17 16:31:41.196467: Epoch 310\n",
      "2025-01-17 16:31:41.196623: Current learning rate: 0.00716\n",
      "2025-01-17 16:33:31.711604: train_loss -0.7868\n",
      "2025-01-17 16:33:31.711729: val_loss -0.5698\n",
      "2025-01-17 16:33:31.711762: Pseudo dice [np.float32(0.6986)]\n",
      "2025-01-17 16:33:31.711795: Epoch time: 110.52 s\n",
      "2025-01-17 16:33:32.282627: \n",
      "2025-01-17 16:33:32.282787: Epoch 311\n",
      "2025-01-17 16:33:32.282859: Current learning rate: 0.00715\n",
      "2025-01-17 16:35:22.818394: train_loss -0.7904\n",
      "2025-01-17 16:35:22.818521: val_loss -0.6359\n",
      "2025-01-17 16:35:22.818554: Pseudo dice [np.float32(0.6475)]\n",
      "2025-01-17 16:35:22.818587: Epoch time: 110.54 s\n",
      "2025-01-17 16:35:23.390244: \n",
      "2025-01-17 16:35:23.390615: Epoch 312\n",
      "2025-01-17 16:35:23.390720: Current learning rate: 0.00714\n",
      "2025-01-17 16:37:13.874887: train_loss -0.7993\n",
      "2025-01-17 16:37:13.875009: val_loss -0.6208\n",
      "2025-01-17 16:37:13.875042: Pseudo dice [np.float32(0.7571)]\n",
      "2025-01-17 16:37:13.875074: Epoch time: 110.49 s\n",
      "2025-01-17 16:37:14.441444: \n",
      "2025-01-17 16:37:14.441530: Epoch 313\n",
      "2025-01-17 16:37:14.441591: Current learning rate: 0.00713\n",
      "2025-01-17 16:39:04.911354: train_loss -0.7929\n",
      "2025-01-17 16:39:04.911537: val_loss -0.6083\n",
      "2025-01-17 16:39:04.911589: Pseudo dice [np.float32(0.6469)]\n",
      "2025-01-17 16:39:04.911628: Epoch time: 110.47 s\n",
      "2025-01-17 16:39:05.723052: \n",
      "2025-01-17 16:39:05.723294: Epoch 314\n",
      "2025-01-17 16:39:05.723481: Current learning rate: 0.00712\n",
      "2025-01-17 16:40:56.342627: train_loss -0.7882\n",
      "2025-01-17 16:40:56.342762: val_loss -0.6265\n",
      "2025-01-17 16:40:56.342798: Pseudo dice [np.float32(0.7657)]\n",
      "2025-01-17 16:40:56.342832: Epoch time: 110.62 s\n",
      "2025-01-17 16:40:56.916241: \n",
      "2025-01-17 16:40:56.916348: Epoch 315\n",
      "2025-01-17 16:40:56.916411: Current learning rate: 0.00711\n",
      "2025-01-17 16:42:47.443404: train_loss -0.7878\n",
      "2025-01-17 16:42:47.443658: val_loss -0.636\n",
      "2025-01-17 16:42:47.443710: Pseudo dice [np.float32(0.7184)]\n",
      "2025-01-17 16:42:47.443765: Epoch time: 110.53 s\n",
      "2025-01-17 16:42:48.019164: \n",
      "2025-01-17 16:42:48.019269: Epoch 316\n",
      "2025-01-17 16:42:48.019331: Current learning rate: 0.0071\n",
      "2025-01-17 16:44:38.637759: train_loss -0.7882\n",
      "2025-01-17 16:44:38.638081: val_loss -0.6102\n",
      "2025-01-17 16:44:38.638128: Pseudo dice [np.float32(0.6923)]\n",
      "2025-01-17 16:44:38.638177: Epoch time: 110.62 s\n",
      "2025-01-17 16:44:39.233441: \n",
      "2025-01-17 16:44:39.233809: Epoch 317\n",
      "2025-01-17 16:44:39.233996: Current learning rate: 0.0071\n",
      "2025-01-17 16:46:29.937475: train_loss -0.8082\n",
      "2025-01-17 16:46:29.937919: val_loss -0.6696\n",
      "2025-01-17 16:46:29.937959: Pseudo dice [np.float32(0.6771)]\n",
      "2025-01-17 16:46:29.937997: Epoch time: 110.7 s\n",
      "2025-01-17 16:46:30.530028: \n",
      "2025-01-17 16:46:30.530144: Epoch 318\n",
      "2025-01-17 16:46:30.530214: Current learning rate: 0.00709\n",
      "2025-01-17 16:48:21.202224: train_loss -0.7902\n",
      "2025-01-17 16:48:21.202376: val_loss -0.6848\n",
      "2025-01-17 16:48:21.202410: Pseudo dice [np.float32(0.7187)]\n",
      "2025-01-17 16:48:21.202442: Epoch time: 110.67 s\n",
      "2025-01-17 16:48:21.799522: \n",
      "2025-01-17 16:48:21.799906: Epoch 319\n",
      "2025-01-17 16:48:21.800199: Current learning rate: 0.00708\n",
      "2025-01-17 16:50:12.514297: train_loss -0.8002\n",
      "2025-01-17 16:50:12.514486: val_loss -0.6349\n",
      "2025-01-17 16:50:12.514522: Pseudo dice [np.float32(0.6966)]\n",
      "2025-01-17 16:50:12.514591: Epoch time: 110.72 s\n",
      "2025-01-17 16:50:13.108300: \n",
      "2025-01-17 16:50:13.108397: Epoch 320\n",
      "2025-01-17 16:50:13.108466: Current learning rate: 0.00707\n",
      "2025-01-17 16:52:03.777866: train_loss -0.7373\n",
      "2025-01-17 16:52:03.778011: val_loss -0.6356\n",
      "2025-01-17 16:52:03.778050: Pseudo dice [np.float32(0.6979)]\n",
      "2025-01-17 16:52:03.778084: Epoch time: 110.67 s\n",
      "2025-01-17 16:52:04.354431: \n",
      "2025-01-17 16:52:04.354736: Epoch 321\n",
      "2025-01-17 16:52:04.354812: Current learning rate: 0.00706\n",
      "2025-01-17 16:53:54.902494: train_loss -0.7677\n",
      "2025-01-17 16:53:54.902796: val_loss -0.6523\n",
      "2025-01-17 16:53:54.902969: Pseudo dice [np.float32(0.659)]\n",
      "2025-01-17 16:53:54.903018: Epoch time: 110.55 s\n",
      "2025-01-17 16:53:55.477423: \n",
      "2025-01-17 16:53:55.477704: Epoch 322\n",
      "2025-01-17 16:53:55.477774: Current learning rate: 0.00705\n",
      "2025-01-17 16:55:46.026198: train_loss -0.8107\n",
      "2025-01-17 16:55:46.026342: val_loss -0.6534\n",
      "2025-01-17 16:55:46.026381: Pseudo dice [np.float32(0.6688)]\n",
      "2025-01-17 16:55:46.026419: Epoch time: 110.55 s\n",
      "2025-01-17 16:55:46.595698: \n",
      "2025-01-17 16:55:46.595855: Epoch 323\n",
      "2025-01-17 16:55:46.595920: Current learning rate: 0.00704\n",
      "2025-01-17 16:57:37.127248: train_loss -0.8026\n",
      "2025-01-17 16:57:37.127477: val_loss -0.7172\n",
      "2025-01-17 16:57:37.127520: Pseudo dice [np.float32(0.7541)]\n",
      "2025-01-17 16:57:37.127554: Epoch time: 110.53 s\n",
      "2025-01-17 16:57:37.696588: \n",
      "2025-01-17 16:57:37.696873: Epoch 324\n",
      "2025-01-17 16:57:37.697017: Current learning rate: 0.00703\n",
      "2025-01-17 16:59:28.199501: train_loss -0.767\n",
      "2025-01-17 16:59:28.199896: val_loss -0.7016\n",
      "2025-01-17 16:59:28.200042: Pseudo dice [np.float32(0.7471)]\n",
      "2025-01-17 16:59:28.200095: Epoch time: 110.5 s\n",
      "2025-01-17 16:59:28.773721: \n",
      "2025-01-17 16:59:28.773966: Epoch 325\n",
      "2025-01-17 16:59:28.774040: Current learning rate: 0.00702\n",
      "2025-01-17 17:01:19.355515: train_loss -0.7741\n",
      "2025-01-17 17:01:19.355643: val_loss -0.4884\n",
      "2025-01-17 17:01:19.355675: Pseudo dice [np.float32(0.1928)]\n",
      "2025-01-17 17:01:19.355708: Epoch time: 110.58 s\n",
      "2025-01-17 17:01:20.168059: \n",
      "2025-01-17 17:01:20.168264: Epoch 326\n",
      "2025-01-17 17:01:20.168385: Current learning rate: 0.00701\n",
      "2025-01-17 17:03:10.787374: train_loss -0.7715\n",
      "2025-01-17 17:03:10.787567: val_loss -0.6605\n",
      "2025-01-17 17:03:10.787605: Pseudo dice [np.float32(0.6664)]\n",
      "2025-01-17 17:03:10.787638: Epoch time: 110.62 s\n",
      "2025-01-17 17:03:11.381641: \n",
      "2025-01-17 17:03:11.381833: Epoch 327\n",
      "2025-01-17 17:03:11.381957: Current learning rate: 0.007\n",
      "2025-01-17 17:05:01.916384: train_loss -0.7782\n",
      "2025-01-17 17:05:01.916664: val_loss -0.6971\n",
      "2025-01-17 17:05:01.916719: Pseudo dice [np.float32(0.7321)]\n",
      "2025-01-17 17:05:01.916754: Epoch time: 110.54 s\n",
      "2025-01-17 17:05:02.491472: \n",
      "2025-01-17 17:05:02.491872: Epoch 328\n",
      "2025-01-17 17:05:02.492029: Current learning rate: 0.00699\n",
      "2025-01-17 17:06:53.056898: train_loss -0.8074\n",
      "2025-01-17 17:06:53.057039: val_loss -0.676\n",
      "2025-01-17 17:06:53.057073: Pseudo dice [np.float32(0.6962)]\n",
      "2025-01-17 17:06:53.057107: Epoch time: 110.57 s\n",
      "2025-01-17 17:06:53.621585: \n",
      "2025-01-17 17:06:53.621765: Epoch 329\n",
      "2025-01-17 17:06:53.621867: Current learning rate: 0.00698\n",
      "2025-01-17 17:08:44.157535: train_loss -0.7771\n",
      "2025-01-17 17:08:44.157739: val_loss -0.6483\n",
      "2025-01-17 17:08:44.157773: Pseudo dice [np.float32(0.7413)]\n",
      "2025-01-17 17:08:44.157806: Epoch time: 110.54 s\n",
      "2025-01-17 17:08:44.732137: \n",
      "2025-01-17 17:08:44.732544: Epoch 330\n",
      "2025-01-17 17:08:44.732617: Current learning rate: 0.00697\n",
      "2025-01-17 17:10:35.300253: train_loss -0.7604\n",
      "2025-01-17 17:10:35.300390: val_loss -0.6477\n",
      "2025-01-17 17:10:35.300424: Pseudo dice [np.float32(0.6937)]\n",
      "2025-01-17 17:10:35.300460: Epoch time: 110.57 s\n",
      "2025-01-17 17:10:35.872683: \n",
      "2025-01-17 17:10:35.872914: Epoch 331\n",
      "2025-01-17 17:10:35.873157: Current learning rate: 0.00696\n",
      "2025-01-17 17:12:26.381510: train_loss -0.7867\n",
      "2025-01-17 17:12:26.381673: val_loss -0.6483\n",
      "2025-01-17 17:12:26.381761: Pseudo dice [np.float32(0.7176)]\n",
      "2025-01-17 17:12:26.381804: Epoch time: 110.51 s\n",
      "2025-01-17 17:12:26.946711: \n",
      "2025-01-17 17:12:26.946816: Epoch 332\n",
      "2025-01-17 17:12:26.946879: Current learning rate: 0.00696\n",
      "2025-01-17 17:14:17.437542: train_loss -0.7669\n",
      "2025-01-17 17:14:17.437745: val_loss -0.6216\n",
      "2025-01-17 17:14:17.437781: Pseudo dice [np.float32(0.7086)]\n",
      "2025-01-17 17:14:17.437813: Epoch time: 110.49 s\n",
      "2025-01-17 17:14:18.007857: \n",
      "2025-01-17 17:14:18.007946: Epoch 333\n",
      "2025-01-17 17:14:18.008009: Current learning rate: 0.00695\n",
      "2025-01-17 17:16:08.703306: train_loss -0.7835\n",
      "2025-01-17 17:16:08.703435: val_loss -0.661\n",
      "2025-01-17 17:16:08.703498: Pseudo dice [np.float32(0.767)]\n",
      "2025-01-17 17:16:08.703574: Epoch time: 110.7 s\n",
      "2025-01-17 17:16:09.278951: \n",
      "2025-01-17 17:16:09.279043: Epoch 334\n",
      "2025-01-17 17:16:09.279135: Current learning rate: 0.00694\n",
      "2025-01-17 17:17:59.817171: train_loss -0.7937\n",
      "2025-01-17 17:17:59.817360: val_loss -0.6428\n",
      "2025-01-17 17:17:59.817395: Pseudo dice [np.float32(0.7025)]\n",
      "2025-01-17 17:17:59.817433: Epoch time: 110.54 s\n",
      "2025-01-17 17:18:00.391806: \n",
      "2025-01-17 17:18:00.391894: Epoch 335\n",
      "2025-01-17 17:18:00.391954: Current learning rate: 0.00693\n",
      "2025-01-17 17:19:51.043710: train_loss -0.7945\n",
      "2025-01-17 17:19:51.043837: val_loss -0.6527\n",
      "2025-01-17 17:19:51.043870: Pseudo dice [np.float32(0.6867)]\n",
      "2025-01-17 17:19:51.043905: Epoch time: 110.65 s\n",
      "2025-01-17 17:19:51.621276: \n",
      "2025-01-17 17:19:51.621367: Epoch 336\n",
      "2025-01-17 17:19:51.621428: Current learning rate: 0.00692\n",
      "2025-01-17 17:21:42.168188: train_loss -0.8124\n",
      "2025-01-17 17:21:42.168327: val_loss -0.6597\n",
      "2025-01-17 17:21:42.168363: Pseudo dice [np.float32(0.6244)]\n",
      "2025-01-17 17:21:42.168394: Epoch time: 110.55 s\n",
      "2025-01-17 17:21:42.749266: \n",
      "2025-01-17 17:21:42.749454: Epoch 337\n",
      "2025-01-17 17:21:42.749559: Current learning rate: 0.00691\n",
      "2025-01-17 17:23:33.323039: train_loss -0.7548\n",
      "2025-01-17 17:23:33.323240: val_loss -0.68\n",
      "2025-01-17 17:23:33.323284: Pseudo dice [np.float32(0.7867)]\n",
      "2025-01-17 17:23:33.323318: Epoch time: 110.57 s\n",
      "2025-01-17 17:23:34.149422: \n",
      "2025-01-17 17:23:34.149530: Epoch 338\n",
      "2025-01-17 17:23:34.149609: Current learning rate: 0.0069\n",
      "2025-01-17 17:25:24.734799: train_loss -0.7854\n",
      "2025-01-17 17:25:24.734972: val_loss -0.6441\n",
      "2025-01-17 17:25:24.735121: Pseudo dice [np.float32(0.687)]\n",
      "2025-01-17 17:25:24.735254: Epoch time: 110.59 s\n",
      "2025-01-17 17:25:25.310944: \n",
      "2025-01-17 17:25:25.311124: Epoch 339\n",
      "2025-01-17 17:25:25.311188: Current learning rate: 0.00689\n",
      "2025-01-17 17:27:15.831704: train_loss -0.755\n",
      "2025-01-17 17:27:15.831864: val_loss -0.6815\n",
      "2025-01-17 17:27:15.831936: Pseudo dice [np.float32(0.7315)]\n",
      "2025-01-17 17:27:15.831976: Epoch time: 110.52 s\n",
      "2025-01-17 17:27:16.413738: \n",
      "2025-01-17 17:27:16.413926: Epoch 340\n",
      "2025-01-17 17:27:16.414000: Current learning rate: 0.00688\n",
      "2025-01-17 17:29:07.011452: train_loss -0.757\n",
      "2025-01-17 17:29:07.011723: val_loss -0.6064\n",
      "2025-01-17 17:29:07.011790: Pseudo dice [np.float32(0.6291)]\n",
      "2025-01-17 17:29:07.011831: Epoch time: 110.6 s\n",
      "2025-01-17 17:29:07.595070: \n",
      "2025-01-17 17:29:07.595175: Epoch 341\n",
      "2025-01-17 17:29:07.595242: Current learning rate: 0.00687\n",
      "2025-01-17 17:30:58.067061: train_loss -0.7989\n",
      "2025-01-17 17:30:58.067286: val_loss -0.6614\n",
      "2025-01-17 17:30:58.067331: Pseudo dice [np.float32(0.6979)]\n",
      "2025-01-17 17:30:58.067367: Epoch time: 110.47 s\n",
      "2025-01-17 17:30:58.641932: \n",
      "2025-01-17 17:30:58.642106: Epoch 342\n",
      "2025-01-17 17:30:58.642182: Current learning rate: 0.00686\n",
      "2025-01-17 17:32:49.190290: train_loss -0.7827\n",
      "2025-01-17 17:32:49.190449: val_loss -0.6757\n",
      "2025-01-17 17:32:49.190528: Pseudo dice [np.float32(0.7566)]\n",
      "2025-01-17 17:32:49.190585: Epoch time: 110.55 s\n",
      "2025-01-17 17:32:49.770615: \n",
      "2025-01-17 17:32:49.770947: Epoch 343\n",
      "2025-01-17 17:32:49.771086: Current learning rate: 0.00685\n",
      "2025-01-17 17:34:40.275253: train_loss -0.8103\n",
      "2025-01-17 17:34:40.275406: val_loss -0.6365\n",
      "2025-01-17 17:34:40.275488: Pseudo dice [np.float32(0.772)]\n",
      "2025-01-17 17:34:40.275625: Epoch time: 110.51 s\n",
      "2025-01-17 17:34:40.863452: \n",
      "2025-01-17 17:34:40.863561: Epoch 344\n",
      "2025-01-17 17:34:40.863629: Current learning rate: 0.00684\n",
      "2025-01-17 17:36:31.362824: train_loss -0.8058\n",
      "2025-01-17 17:36:31.363060: val_loss -0.6832\n",
      "2025-01-17 17:36:31.363106: Pseudo dice [np.float32(0.7632)]\n",
      "2025-01-17 17:36:31.363141: Epoch time: 110.5 s\n",
      "2025-01-17 17:36:31.939914: \n",
      "2025-01-17 17:36:31.940087: Epoch 345\n",
      "2025-01-17 17:36:31.940159: Current learning rate: 0.00683\n",
      "2025-01-17 17:38:22.443116: train_loss -0.8009\n",
      "2025-01-17 17:38:22.443243: val_loss -0.6786\n",
      "2025-01-17 17:38:22.443322: Pseudo dice [np.float32(0.797)]\n",
      "2025-01-17 17:38:22.443521: Epoch time: 110.5 s\n",
      "2025-01-17 17:38:23.010053: \n",
      "2025-01-17 17:38:23.010215: Epoch 346\n",
      "2025-01-17 17:38:23.010293: Current learning rate: 0.00682\n",
      "2025-01-17 17:40:13.540061: train_loss -0.8028\n",
      "2025-01-17 17:40:13.540184: val_loss -0.6507\n",
      "2025-01-17 17:40:13.540215: Pseudo dice [np.float32(0.6198)]\n",
      "2025-01-17 17:40:13.540247: Epoch time: 110.53 s\n",
      "2025-01-17 17:40:14.103370: \n",
      "2025-01-17 17:40:14.103456: Epoch 347\n",
      "2025-01-17 17:40:14.103516: Current learning rate: 0.00681\n",
      "2025-01-17 17:42:04.605588: train_loss -0.8095\n",
      "2025-01-17 17:42:04.605721: val_loss -0.5721\n",
      "2025-01-17 17:42:04.605754: Pseudo dice [np.float32(0.6747)]\n",
      "2025-01-17 17:42:04.605791: Epoch time: 110.5 s\n",
      "2025-01-17 17:42:05.191147: \n",
      "2025-01-17 17:42:05.191335: Epoch 348\n",
      "2025-01-17 17:42:05.191406: Current learning rate: 0.0068\n",
      "2025-01-17 17:43:55.715700: train_loss -0.7706\n",
      "2025-01-17 17:43:55.715889: val_loss -0.6885\n",
      "2025-01-17 17:43:55.715952: Pseudo dice [np.float32(0.7689)]\n",
      "2025-01-17 17:43:55.715990: Epoch time: 110.53 s\n",
      "2025-01-17 17:43:56.522290: \n",
      "2025-01-17 17:43:56.522403: Epoch 349\n",
      "2025-01-17 17:43:56.522465: Current learning rate: 0.0068\n",
      "2025-01-17 17:45:47.030958: train_loss -0.7965\n",
      "2025-01-17 17:45:47.031167: val_loss -0.6747\n",
      "2025-01-17 17:45:47.031204: Pseudo dice [np.float32(0.715)]\n",
      "2025-01-17 17:45:47.031250: Epoch time: 110.51 s\n",
      "2025-01-17 17:45:47.823257: \n",
      "2025-01-17 17:45:47.823440: Epoch 350\n",
      "2025-01-17 17:45:47.823514: Current learning rate: 0.00679\n",
      "2025-01-17 17:47:38.536940: train_loss -0.8043\n",
      "2025-01-17 17:47:38.537071: val_loss -0.6288\n",
      "2025-01-17 17:47:38.537104: Pseudo dice [np.float32(0.6984)]\n",
      "2025-01-17 17:47:38.537138: Epoch time: 110.71 s\n",
      "2025-01-17 17:47:39.129041: \n",
      "2025-01-17 17:47:39.129245: Epoch 351\n",
      "2025-01-17 17:47:39.129320: Current learning rate: 0.00678\n",
      "2025-01-17 17:49:29.643723: train_loss -0.7941\n",
      "2025-01-17 17:49:29.643834: val_loss -0.6242\n",
      "2025-01-17 17:49:29.643863: Pseudo dice [np.float32(0.6697)]\n",
      "2025-01-17 17:49:29.643901: Epoch time: 110.52 s\n",
      "2025-01-17 17:49:30.222065: \n",
      "2025-01-17 17:49:30.222153: Epoch 352\n",
      "2025-01-17 17:49:30.222215: Current learning rate: 0.00677\n",
      "2025-01-17 17:51:20.770670: train_loss -0.7073\n",
      "2025-01-17 17:51:20.770865: val_loss -0.6254\n",
      "2025-01-17 17:51:20.770910: Pseudo dice [np.float32(0.681)]\n",
      "2025-01-17 17:51:20.770943: Epoch time: 110.55 s\n",
      "2025-01-17 17:51:21.350610: \n",
      "2025-01-17 17:51:21.350962: Epoch 353\n",
      "2025-01-17 17:51:21.351052: Current learning rate: 0.00676\n",
      "2025-01-17 17:53:11.878814: train_loss -0.7195\n",
      "2025-01-17 17:53:11.878955: val_loss -0.6668\n",
      "2025-01-17 17:53:11.878991: Pseudo dice [np.float32(0.706)]\n",
      "2025-01-17 17:53:11.879025: Epoch time: 110.53 s\n",
      "2025-01-17 17:53:12.465732: \n",
      "2025-01-17 17:53:12.466013: Epoch 354\n",
      "2025-01-17 17:53:12.466187: Current learning rate: 0.00675\n",
      "2025-01-17 17:55:02.929262: train_loss -0.7649\n",
      "2025-01-17 17:55:02.929433: val_loss -0.6134\n",
      "2025-01-17 17:55:02.929469: Pseudo dice [np.float32(0.7031)]\n",
      "2025-01-17 17:55:02.929505: Epoch time: 110.46 s\n",
      "2025-01-17 17:55:03.504203: \n",
      "2025-01-17 17:55:03.504387: Epoch 355\n",
      "2025-01-17 17:55:03.504461: Current learning rate: 0.00674\n",
      "2025-01-17 17:56:54.050931: train_loss -0.7709\n",
      "2025-01-17 17:56:54.051088: val_loss -0.5839\n",
      "2025-01-17 17:56:54.051211: Pseudo dice [np.float32(0.6315)]\n",
      "2025-01-17 17:56:54.051267: Epoch time: 110.55 s\n",
      "2025-01-17 17:56:54.635031: \n",
      "2025-01-17 17:56:54.635123: Epoch 356\n",
      "2025-01-17 17:56:54.635182: Current learning rate: 0.00673\n",
      "2025-01-17 17:58:45.157180: train_loss -0.7701\n",
      "2025-01-17 17:58:45.157327: val_loss -0.6637\n",
      "2025-01-17 17:58:45.157373: Pseudo dice [np.float32(0.7259)]\n",
      "2025-01-17 17:58:45.157413: Epoch time: 110.52 s\n",
      "2025-01-17 17:58:45.735020: \n",
      "2025-01-17 17:58:45.735197: Epoch 357\n",
      "2025-01-17 17:58:45.735290: Current learning rate: 0.00672\n",
      "2025-01-17 18:00:36.247255: train_loss -0.7401\n",
      "2025-01-17 18:00:36.247374: val_loss -0.714\n",
      "2025-01-17 18:00:36.247406: Pseudo dice [np.float32(0.7157)]\n",
      "2025-01-17 18:00:36.247444: Epoch time: 110.51 s\n",
      "2025-01-17 18:00:36.825995: \n",
      "2025-01-17 18:00:36.826302: Epoch 358\n",
      "2025-01-17 18:00:36.826378: Current learning rate: 0.00671\n",
      "2025-01-17 18:02:27.365771: train_loss -0.7972\n",
      "2025-01-17 18:02:27.365916: val_loss -0.6088\n",
      "2025-01-17 18:02:27.365950: Pseudo dice [np.float32(0.6858)]\n",
      "2025-01-17 18:02:27.365983: Epoch time: 110.54 s\n",
      "2025-01-17 18:02:27.944164: \n",
      "2025-01-17 18:02:27.944266: Epoch 359\n",
      "2025-01-17 18:02:27.944328: Current learning rate: 0.0067\n",
      "2025-01-17 18:04:18.453873: train_loss -0.7929\n",
      "2025-01-17 18:04:18.454164: val_loss -0.6696\n",
      "2025-01-17 18:04:18.454241: Pseudo dice [np.float32(0.67)]\n",
      "2025-01-17 18:04:18.454281: Epoch time: 110.51 s\n",
      "2025-01-17 18:04:19.038365: \n",
      "2025-01-17 18:04:19.038453: Epoch 360\n",
      "2025-01-17 18:04:19.038515: Current learning rate: 0.00669\n",
      "2025-01-17 18:06:09.585775: train_loss -0.7447\n",
      "2025-01-17 18:06:09.585904: val_loss -0.6515\n",
      "2025-01-17 18:06:09.585936: Pseudo dice [np.float32(0.726)]\n",
      "2025-01-17 18:06:09.585968: Epoch time: 110.55 s\n",
      "2025-01-17 18:06:10.410105: \n",
      "2025-01-17 18:06:10.410205: Epoch 361\n",
      "2025-01-17 18:06:10.410514: Current learning rate: 0.00668\n",
      "2025-01-17 18:08:00.977748: train_loss -0.7795\n",
      "2025-01-17 18:08:00.977880: val_loss -0.6571\n",
      "2025-01-17 18:08:00.977913: Pseudo dice [np.float32(0.6777)]\n",
      "2025-01-17 18:08:00.977945: Epoch time: 110.57 s\n",
      "2025-01-17 18:08:01.566249: \n",
      "2025-01-17 18:08:01.566464: Epoch 362\n",
      "2025-01-17 18:08:01.566608: Current learning rate: 0.00667\n",
      "2025-01-17 18:09:52.136671: train_loss -0.8041\n",
      "2025-01-17 18:09:52.136798: val_loss -0.6579\n",
      "2025-01-17 18:09:52.136832: Pseudo dice [np.float32(0.7127)]\n",
      "2025-01-17 18:09:52.136865: Epoch time: 110.57 s\n",
      "2025-01-17 18:09:52.716212: \n",
      "2025-01-17 18:09:52.716320: Epoch 363\n",
      "2025-01-17 18:09:52.716384: Current learning rate: 0.00666\n",
      "2025-01-17 18:11:43.263767: train_loss -0.8136\n",
      "2025-01-17 18:11:43.263946: val_loss -0.6879\n",
      "2025-01-17 18:11:43.264000: Pseudo dice [np.float32(0.7769)]\n",
      "2025-01-17 18:11:43.264042: Epoch time: 110.55 s\n",
      "2025-01-17 18:11:43.848429: \n",
      "2025-01-17 18:11:43.848533: Epoch 364\n",
      "2025-01-17 18:11:43.848600: Current learning rate: 0.00665\n",
      "2025-01-17 18:13:34.352854: train_loss -0.813\n",
      "2025-01-17 18:13:34.352982: val_loss -0.7036\n",
      "2025-01-17 18:13:34.353013: Pseudo dice [np.float32(0.7108)]\n",
      "2025-01-17 18:13:34.353046: Epoch time: 110.5 s\n",
      "2025-01-17 18:13:34.930584: \n",
      "2025-01-17 18:13:34.930791: Epoch 365\n",
      "2025-01-17 18:13:34.930880: Current learning rate: 0.00665\n",
      "2025-01-17 18:15:25.567891: train_loss -0.7764\n",
      "2025-01-17 18:15:25.568080: val_loss -0.5656\n",
      "2025-01-17 18:15:25.568124: Pseudo dice [np.float32(0.5946)]\n",
      "2025-01-17 18:15:25.568201: Epoch time: 110.64 s\n",
      "2025-01-17 18:15:26.161809: \n",
      "2025-01-17 18:15:26.161966: Epoch 366\n",
      "2025-01-17 18:15:26.162039: Current learning rate: 0.00664\n",
      "2025-01-17 18:17:16.785828: train_loss -0.7868\n",
      "2025-01-17 18:17:16.785967: val_loss -0.6482\n",
      "2025-01-17 18:17:16.786008: Pseudo dice [np.float32(0.7093)]\n",
      "2025-01-17 18:17:16.786048: Epoch time: 110.62 s\n",
      "2025-01-17 18:17:17.375673: \n",
      "2025-01-17 18:17:17.375818: Epoch 367\n",
      "2025-01-17 18:17:17.375980: Current learning rate: 0.00663\n",
      "2025-01-17 18:19:08.011764: train_loss -0.7607\n",
      "2025-01-17 18:19:08.011997: val_loss -0.7017\n",
      "2025-01-17 18:19:08.012059: Pseudo dice [np.float32(0.6841)]\n",
      "2025-01-17 18:19:08.012100: Epoch time: 110.64 s\n",
      "2025-01-17 18:19:08.588376: \n",
      "2025-01-17 18:19:08.588552: Epoch 368\n",
      "2025-01-17 18:19:08.588624: Current learning rate: 0.00662\n",
      "2025-01-17 18:20:59.097780: train_loss -0.805\n",
      "2025-01-17 18:20:59.097979: val_loss -0.5374\n",
      "2025-01-17 18:20:59.098061: Pseudo dice [np.float32(0.3948)]\n",
      "2025-01-17 18:20:59.098102: Epoch time: 110.51 s\n",
      "2025-01-17 18:20:59.688224: \n",
      "2025-01-17 18:20:59.688598: Epoch 369\n",
      "2025-01-17 18:20:59.688698: Current learning rate: 0.00661\n",
      "2025-01-17 18:22:50.251862: train_loss -0.7624\n",
      "2025-01-17 18:22:50.252210: val_loss -0.6164\n",
      "2025-01-17 18:22:50.252255: Pseudo dice [np.float32(0.6697)]\n",
      "2025-01-17 18:22:50.252296: Epoch time: 110.56 s\n",
      "2025-01-17 18:22:50.837168: \n",
      "2025-01-17 18:22:50.837362: Epoch 370\n",
      "2025-01-17 18:22:50.837507: Current learning rate: 0.0066\n",
      "2025-01-17 18:24:41.344494: train_loss -0.7962\n",
      "2025-01-17 18:24:41.344682: val_loss -0.6854\n",
      "2025-01-17 18:24:41.344713: Pseudo dice [np.float32(0.71)]\n",
      "2025-01-17 18:24:41.344752: Epoch time: 110.51 s\n",
      "2025-01-17 18:24:41.937523: \n",
      "2025-01-17 18:24:41.937625: Epoch 371\n",
      "2025-01-17 18:24:41.937686: Current learning rate: 0.00659\n",
      "2025-01-17 18:26:32.413979: train_loss -0.7634\n",
      "2025-01-17 18:26:32.414111: val_loss -0.6909\n",
      "2025-01-17 18:26:32.414272: Pseudo dice [np.float32(0.6895)]\n",
      "2025-01-17 18:26:32.414339: Epoch time: 110.48 s\n",
      "2025-01-17 18:26:32.994937: \n",
      "2025-01-17 18:26:32.995115: Epoch 372\n",
      "2025-01-17 18:26:32.995200: Current learning rate: 0.00658\n",
      "2025-01-17 18:28:23.520246: train_loss -0.8032\n",
      "2025-01-17 18:28:23.520473: val_loss -0.6805\n",
      "2025-01-17 18:28:23.520518: Pseudo dice [np.float32(0.7874)]\n",
      "2025-01-17 18:28:23.520559: Epoch time: 110.53 s\n",
      "2025-01-17 18:28:24.382465: \n",
      "2025-01-17 18:28:24.382582: Epoch 373\n",
      "2025-01-17 18:28:24.382647: Current learning rate: 0.00657\n",
      "2025-01-17 18:30:14.877559: train_loss -0.7937\n",
      "2025-01-17 18:30:14.877782: val_loss -0.704\n",
      "2025-01-17 18:30:14.877822: Pseudo dice [np.float32(0.7973)]\n",
      "2025-01-17 18:30:14.877857: Epoch time: 110.5 s\n",
      "2025-01-17 18:30:15.454454: \n",
      "2025-01-17 18:30:15.454624: Epoch 374\n",
      "2025-01-17 18:30:15.454688: Current learning rate: 0.00656\n",
      "2025-01-17 18:32:05.984488: train_loss -0.8095\n",
      "2025-01-17 18:32:05.984625: val_loss -0.6474\n",
      "2025-01-17 18:32:05.984658: Pseudo dice [np.float32(0.6695)]\n",
      "2025-01-17 18:32:05.984689: Epoch time: 110.53 s\n",
      "2025-01-17 18:32:06.570934: \n",
      "2025-01-17 18:32:06.571029: Epoch 375\n",
      "2025-01-17 18:32:06.571092: Current learning rate: 0.00655\n",
      "2025-01-17 18:33:57.134873: train_loss -0.7924\n",
      "2025-01-17 18:33:57.135000: val_loss -0.6505\n",
      "2025-01-17 18:33:57.135033: Pseudo dice [np.float32(0.6596)]\n",
      "2025-01-17 18:33:57.135066: Epoch time: 110.56 s\n",
      "2025-01-17 18:33:57.721302: \n",
      "2025-01-17 18:33:57.721659: Epoch 376\n",
      "2025-01-17 18:33:57.721783: Current learning rate: 0.00654\n",
      "2025-01-17 18:35:48.238425: train_loss -0.7656\n",
      "2025-01-17 18:35:48.238547: val_loss -0.6496\n",
      "2025-01-17 18:35:48.238578: Pseudo dice [np.float32(0.6504)]\n",
      "2025-01-17 18:35:48.238611: Epoch time: 110.52 s\n",
      "2025-01-17 18:35:48.824238: \n",
      "2025-01-17 18:35:48.824324: Epoch 377\n",
      "2025-01-17 18:35:48.824385: Current learning rate: 0.00653\n",
      "2025-01-17 18:37:39.422885: train_loss -0.7715\n",
      "2025-01-17 18:37:39.423045: val_loss -0.6342\n",
      "2025-01-17 18:37:39.423109: Pseudo dice [np.float32(0.7511)]\n",
      "2025-01-17 18:37:39.423152: Epoch time: 110.6 s\n",
      "2025-01-17 18:37:40.010261: \n",
      "2025-01-17 18:37:40.010562: Epoch 378\n",
      "2025-01-17 18:37:40.010660: Current learning rate: 0.00652\n",
      "2025-01-17 18:39:30.600830: train_loss -0.7992\n",
      "2025-01-17 18:39:30.601091: val_loss -0.6869\n",
      "2025-01-17 18:39:30.601136: Pseudo dice [np.float32(0.7019)]\n",
      "2025-01-17 18:39:30.601171: Epoch time: 110.59 s\n",
      "2025-01-17 18:39:31.180331: \n",
      "2025-01-17 18:39:31.180430: Epoch 379\n",
      "2025-01-17 18:39:31.180493: Current learning rate: 0.00651\n",
      "2025-01-17 18:41:21.696755: train_loss -0.8123\n",
      "2025-01-17 18:41:21.696996: val_loss -0.6464\n",
      "2025-01-17 18:41:21.697036: Pseudo dice [np.float32(0.6367)]\n",
      "2025-01-17 18:41:21.697069: Epoch time: 110.52 s\n",
      "2025-01-17 18:41:22.281492: \n",
      "2025-01-17 18:41:22.281583: Epoch 380\n",
      "2025-01-17 18:41:22.281649: Current learning rate: 0.0065\n",
      "2025-01-17 18:43:12.757076: train_loss -0.7858\n",
      "2025-01-17 18:43:12.757216: val_loss -0.6724\n",
      "2025-01-17 18:43:12.757248: Pseudo dice [np.float32(0.6611)]\n",
      "2025-01-17 18:43:12.757282: Epoch time: 110.48 s\n",
      "2025-01-17 18:43:13.333977: \n",
      "2025-01-17 18:43:13.334287: Epoch 381\n",
      "2025-01-17 18:43:13.334600: Current learning rate: 0.00649\n",
      "2025-01-17 18:45:03.839388: train_loss -0.8021\n",
      "2025-01-17 18:45:03.839609: val_loss -0.6543\n",
      "2025-01-17 18:45:03.839653: Pseudo dice [np.float32(0.7807)]\n",
      "2025-01-17 18:45:03.839687: Epoch time: 110.51 s\n",
      "2025-01-17 18:45:04.426842: \n",
      "2025-01-17 18:45:04.427308: Epoch 382\n",
      "2025-01-17 18:45:04.427397: Current learning rate: 0.00648\n",
      "2025-01-17 18:46:55.177467: train_loss -0.8068\n",
      "2025-01-17 18:46:55.177669: val_loss -0.6745\n",
      "2025-01-17 18:46:55.177716: Pseudo dice [np.float32(0.7268)]\n",
      "2025-01-17 18:46:55.177750: Epoch time: 110.75 s\n",
      "2025-01-17 18:46:55.762148: \n",
      "2025-01-17 18:46:55.762244: Epoch 383\n",
      "2025-01-17 18:46:55.762309: Current learning rate: 0.00648\n",
      "2025-01-17 18:48:46.304564: train_loss -0.8231\n",
      "2025-01-17 18:48:46.304740: val_loss -0.7257\n",
      "2025-01-17 18:48:46.304773: Pseudo dice [np.float32(0.7752)]\n",
      "2025-01-17 18:48:46.304807: Epoch time: 110.54 s\n",
      "2025-01-17 18:48:47.130887: \n",
      "2025-01-17 18:48:47.131118: Epoch 384\n",
      "2025-01-17 18:48:47.131265: Current learning rate: 0.00647\n",
      "2025-01-17 18:50:37.647393: train_loss -0.7975\n",
      "2025-01-17 18:50:37.647539: val_loss -0.6659\n",
      "2025-01-17 18:50:37.647622: Pseudo dice [np.float32(0.7286)]\n",
      "2025-01-17 18:50:37.647664: Epoch time: 110.52 s\n",
      "2025-01-17 18:50:38.233060: \n",
      "2025-01-17 18:50:38.233413: Epoch 385\n",
      "2025-01-17 18:50:38.233509: Current learning rate: 0.00646\n",
      "2025-01-17 18:52:28.793304: train_loss -0.7496\n",
      "2025-01-17 18:52:28.793454: val_loss -0.6697\n",
      "2025-01-17 18:52:28.793488: Pseudo dice [np.float32(0.7694)]\n",
      "2025-01-17 18:52:28.793525: Epoch time: 110.56 s\n",
      "2025-01-17 18:52:29.385689: \n",
      "2025-01-17 18:52:29.386092: Epoch 386\n",
      "2025-01-17 18:52:29.386209: Current learning rate: 0.00645\n",
      "2025-01-17 18:54:20.007451: train_loss -0.7951\n",
      "2025-01-17 18:54:20.007590: val_loss -0.6739\n",
      "2025-01-17 18:54:20.007629: Pseudo dice [np.float32(0.7601)]\n",
      "2025-01-17 18:54:20.007669: Epoch time: 110.62 s\n",
      "2025-01-17 18:54:20.600833: \n",
      "2025-01-17 18:54:20.601099: Epoch 387\n",
      "2025-01-17 18:54:20.601253: Current learning rate: 0.00644\n",
      "2025-01-17 18:56:11.212895: train_loss -0.7805\n",
      "2025-01-17 18:56:11.213333: val_loss -0.6874\n",
      "2025-01-17 18:56:11.213394: Pseudo dice [np.float32(0.6517)]\n",
      "2025-01-17 18:56:11.213435: Epoch time: 110.61 s\n",
      "2025-01-17 18:56:11.819708: \n",
      "2025-01-17 18:56:11.819833: Epoch 388\n",
      "2025-01-17 18:56:11.819904: Current learning rate: 0.00643\n",
      "2025-01-17 18:58:02.363941: train_loss -0.7808\n",
      "2025-01-17 18:58:02.364121: val_loss -0.6429\n",
      "2025-01-17 18:58:02.364173: Pseudo dice [np.float32(0.685)]\n",
      "2025-01-17 18:58:02.364377: Epoch time: 110.54 s\n",
      "2025-01-17 18:58:02.959773: \n",
      "2025-01-17 18:58:02.960153: Epoch 389\n",
      "2025-01-17 18:58:02.960248: Current learning rate: 0.00642\n",
      "2025-01-17 18:59:53.490612: train_loss -0.7891\n",
      "2025-01-17 18:59:53.490873: val_loss -0.6935\n",
      "2025-01-17 18:59:53.490915: Pseudo dice [np.float32(0.7486)]\n",
      "2025-01-17 18:59:53.490948: Epoch time: 110.53 s\n",
      "2025-01-17 18:59:54.076825: \n",
      "2025-01-17 18:59:54.076929: Epoch 390\n",
      "2025-01-17 18:59:54.076992: Current learning rate: 0.00641\n",
      "2025-01-17 19:01:44.603602: train_loss -0.8221\n",
      "2025-01-17 19:01:44.603738: val_loss -0.6301\n",
      "2025-01-17 19:01:44.603771: Pseudo dice [np.float32(0.7153)]\n",
      "2025-01-17 19:01:44.603804: Epoch time: 110.53 s\n",
      "2025-01-17 19:01:45.196597: \n",
      "2025-01-17 19:01:45.196970: Epoch 391\n",
      "2025-01-17 19:01:45.197054: Current learning rate: 0.0064\n",
      "2025-01-17 19:03:35.701260: train_loss -0.8232\n",
      "2025-01-17 19:03:35.701390: val_loss -0.6428\n",
      "2025-01-17 19:03:35.701424: Pseudo dice [np.float32(0.746)]\n",
      "2025-01-17 19:03:35.701456: Epoch time: 110.51 s\n",
      "2025-01-17 19:03:36.289418: \n",
      "2025-01-17 19:03:36.289601: Epoch 392\n",
      "2025-01-17 19:03:36.289671: Current learning rate: 0.00639\n",
      "2025-01-17 19:05:26.826654: train_loss -0.8159\n",
      "2025-01-17 19:05:26.826787: val_loss -0.7237\n",
      "2025-01-17 19:05:26.826945: Pseudo dice [np.float32(0.7295)]\n",
      "2025-01-17 19:05:26.826990: Epoch time: 110.54 s\n",
      "2025-01-17 19:05:27.414493: \n",
      "2025-01-17 19:05:27.414586: Epoch 393\n",
      "2025-01-17 19:05:27.414647: Current learning rate: 0.00638\n",
      "2025-01-17 19:07:17.885035: train_loss -0.8323\n",
      "2025-01-17 19:07:17.885165: val_loss -0.614\n",
      "2025-01-17 19:07:17.885196: Pseudo dice [np.float32(0.6419)]\n",
      "2025-01-17 19:07:17.885227: Epoch time: 110.47 s\n",
      "2025-01-17 19:07:18.469566: \n",
      "2025-01-17 19:07:18.469654: Epoch 394\n",
      "2025-01-17 19:07:18.469719: Current learning rate: 0.00637\n",
      "2025-01-17 19:09:08.966503: train_loss -0.7907\n",
      "2025-01-17 19:09:08.966628: val_loss -0.7013\n",
      "2025-01-17 19:09:08.966661: Pseudo dice [np.float32(0.7239)]\n",
      "2025-01-17 19:09:08.966695: Epoch time: 110.5 s\n",
      "2025-01-17 19:09:09.558269: \n",
      "2025-01-17 19:09:09.558686: Epoch 395\n",
      "2025-01-17 19:09:09.558766: Current learning rate: 0.00636\n",
      "2025-01-17 19:11:00.079273: train_loss -0.7945\n",
      "2025-01-17 19:11:00.079393: val_loss -0.6705\n",
      "2025-01-17 19:11:00.079712: Pseudo dice [np.float32(0.77)]\n",
      "2025-01-17 19:11:00.079789: Epoch time: 110.52 s\n",
      "2025-01-17 19:11:00.938586: \n",
      "2025-01-17 19:11:00.938694: Epoch 396\n",
      "2025-01-17 19:11:00.938757: Current learning rate: 0.00635\n",
      "2025-01-17 19:12:51.435655: train_loss -0.7855\n",
      "2025-01-17 19:12:51.435792: val_loss -0.5421\n",
      "2025-01-17 19:12:51.435825: Pseudo dice [np.float32(0.581)]\n",
      "2025-01-17 19:12:51.435858: Epoch time: 110.5 s\n",
      "2025-01-17 19:12:52.030647: \n",
      "2025-01-17 19:12:52.030808: Epoch 397\n",
      "2025-01-17 19:12:52.030879: Current learning rate: 0.00634\n",
      "2025-01-17 19:14:42.663574: train_loss -0.7921\n",
      "2025-01-17 19:14:42.664059: val_loss -0.699\n",
      "2025-01-17 19:14:42.664134: Pseudo dice [np.float32(0.7654)]\n",
      "2025-01-17 19:14:42.664186: Epoch time: 110.63 s\n",
      "2025-01-17 19:14:43.261411: \n",
      "2025-01-17 19:14:43.261624: Epoch 398\n",
      "2025-01-17 19:14:43.261708: Current learning rate: 0.00633\n",
      "2025-01-17 19:16:33.789423: train_loss -0.8093\n",
      "2025-01-17 19:16:33.789557: val_loss -0.7226\n",
      "2025-01-17 19:16:33.789590: Pseudo dice [np.float32(0.773)]\n",
      "2025-01-17 19:16:33.789624: Epoch time: 110.53 s\n",
      "2025-01-17 19:16:34.389699: \n",
      "2025-01-17 19:16:34.389991: Epoch 399\n",
      "2025-01-17 19:16:34.390065: Current learning rate: 0.00632\n",
      "2025-01-17 19:18:24.911538: train_loss -0.7979\n",
      "2025-01-17 19:18:24.911684: val_loss -0.684\n",
      "2025-01-17 19:18:24.911714: Pseudo dice [np.float32(0.725)]\n",
      "2025-01-17 19:18:24.911745: Epoch time: 110.52 s\n",
      "2025-01-17 19:18:25.716013: \n",
      "2025-01-17 19:18:25.716105: Epoch 400\n",
      "2025-01-17 19:18:25.716169: Current learning rate: 0.00631\n",
      "2025-01-17 19:20:16.207005: train_loss -0.826\n",
      "2025-01-17 19:20:16.207120: val_loss -0.5928\n",
      "2025-01-17 19:20:16.207151: Pseudo dice [np.float32(0.7111)]\n",
      "2025-01-17 19:20:16.207183: Epoch time: 110.49 s\n",
      "2025-01-17 19:20:16.803803: \n",
      "2025-01-17 19:20:16.803979: Epoch 401\n",
      "2025-01-17 19:20:16.804046: Current learning rate: 0.0063\n",
      "2025-01-17 19:22:07.351134: train_loss -0.7861\n",
      "2025-01-17 19:22:07.351341: val_loss -0.675\n",
      "2025-01-17 19:22:07.351380: Pseudo dice [np.float32(0.753)]\n",
      "2025-01-17 19:22:07.351416: Epoch time: 110.55 s\n",
      "2025-01-17 19:22:07.954733: \n",
      "2025-01-17 19:22:07.955078: Epoch 402\n",
      "2025-01-17 19:22:07.955169: Current learning rate: 0.0063\n",
      "2025-01-17 19:23:58.467973: train_loss -0.7589\n",
      "2025-01-17 19:23:58.468109: val_loss -0.6954\n",
      "2025-01-17 19:23:58.468144: Pseudo dice [np.float32(0.7341)]\n",
      "2025-01-17 19:23:58.468179: Epoch time: 110.51 s\n",
      "2025-01-17 19:23:59.060914: \n",
      "2025-01-17 19:23:59.061097: Epoch 403\n",
      "2025-01-17 19:23:59.061173: Current learning rate: 0.00629\n",
      "2025-01-17 19:25:49.674619: train_loss -0.8131\n",
      "2025-01-17 19:25:49.674944: val_loss -0.6652\n",
      "2025-01-17 19:25:49.675009: Pseudo dice [np.float32(0.7808)]\n",
      "2025-01-17 19:25:49.675050: Epoch time: 110.61 s\n",
      "2025-01-17 19:25:50.262024: \n",
      "2025-01-17 19:25:50.262142: Epoch 404\n",
      "2025-01-17 19:25:50.262233: Current learning rate: 0.00628\n",
      "2025-01-17 19:27:40.815097: train_loss -0.7811\n",
      "2025-01-17 19:27:40.815275: val_loss -0.5873\n",
      "2025-01-17 19:27:40.815310: Pseudo dice [np.float32(0.6774)]\n",
      "2025-01-17 19:27:40.815343: Epoch time: 110.55 s\n",
      "2025-01-17 19:27:41.412069: \n",
      "2025-01-17 19:27:41.412394: Epoch 405\n",
      "2025-01-17 19:27:41.412525: Current learning rate: 0.00627\n",
      "2025-01-17 19:29:32.031968: train_loss -0.772\n",
      "2025-01-17 19:29:32.032090: val_loss -0.6478\n",
      "2025-01-17 19:29:32.032122: Pseudo dice [np.float32(0.7444)]\n",
      "2025-01-17 19:29:32.032156: Epoch time: 110.62 s\n",
      "2025-01-17 19:29:32.622140: \n",
      "2025-01-17 19:29:32.622380: Epoch 406\n",
      "2025-01-17 19:29:32.622460: Current learning rate: 0.00626\n",
      "2025-01-17 19:31:23.152854: train_loss -0.7495\n",
      "2025-01-17 19:31:23.153047: val_loss -0.7402\n",
      "2025-01-17 19:31:23.153081: Pseudo dice [np.float32(0.8166)]\n",
      "2025-01-17 19:31:23.153113: Epoch time: 110.53 s\n",
      "2025-01-17 19:31:23.986663: \n",
      "2025-01-17 19:31:23.986846: Epoch 407\n",
      "2025-01-17 19:31:23.986922: Current learning rate: 0.00625\n",
      "2025-01-17 19:33:14.526991: train_loss -0.7996\n",
      "2025-01-17 19:33:14.527117: val_loss -0.6529\n",
      "2025-01-17 19:33:14.527328: Pseudo dice [np.float32(0.7434)]\n",
      "2025-01-17 19:33:14.527626: Epoch time: 110.54 s\n",
      "2025-01-17 19:33:15.124349: \n",
      "2025-01-17 19:33:15.124540: Epoch 408\n",
      "2025-01-17 19:33:15.124614: Current learning rate: 0.00624\n",
      "2025-01-17 19:35:05.636180: train_loss -0.7931\n",
      "2025-01-17 19:35:05.636303: val_loss -0.6948\n",
      "2025-01-17 19:35:05.636335: Pseudo dice [np.float32(0.7368)]\n",
      "2025-01-17 19:35:05.636367: Epoch time: 110.51 s\n",
      "2025-01-17 19:35:06.243510: \n",
      "2025-01-17 19:35:06.243799: Epoch 409\n",
      "2025-01-17 19:35:06.244033: Current learning rate: 0.00623\n",
      "2025-01-17 19:36:56.752559: train_loss -0.7814\n",
      "2025-01-17 19:36:56.752867: val_loss -0.6123\n",
      "2025-01-17 19:36:56.752978: Pseudo dice [np.float32(0.6343)]\n",
      "2025-01-17 19:36:56.753018: Epoch time: 110.51 s\n",
      "2025-01-17 19:36:57.351422: \n",
      "2025-01-17 19:36:57.351813: Epoch 410\n",
      "2025-01-17 19:36:57.351931: Current learning rate: 0.00622\n",
      "2025-01-17 19:38:47.925198: train_loss -0.7874\n",
      "2025-01-17 19:38:47.925324: val_loss -0.6513\n",
      "2025-01-17 19:38:47.925359: Pseudo dice [np.float32(0.7007)]\n",
      "2025-01-17 19:38:47.925395: Epoch time: 110.57 s\n",
      "2025-01-17 19:38:48.495080: \n",
      "2025-01-17 19:38:48.495184: Epoch 411\n",
      "2025-01-17 19:38:48.495247: Current learning rate: 0.00621\n",
      "2025-01-17 19:40:39.030584: train_loss -0.7531\n",
      "2025-01-17 19:40:39.030839: val_loss -0.657\n",
      "2025-01-17 19:40:39.030886: Pseudo dice [np.float32(0.7765)]\n",
      "2025-01-17 19:40:39.030920: Epoch time: 110.54 s\n",
      "2025-01-17 19:40:39.612612: \n",
      "2025-01-17 19:40:39.612706: Epoch 412\n",
      "2025-01-17 19:40:39.612768: Current learning rate: 0.0062\n",
      "2025-01-17 19:42:30.175699: train_loss -0.7633\n",
      "2025-01-17 19:42:30.175825: val_loss -0.5845\n",
      "2025-01-17 19:42:30.175862: Pseudo dice [np.float32(0.7301)]\n",
      "2025-01-17 19:42:30.175894: Epoch time: 110.56 s\n",
      "2025-01-17 19:42:30.752101: \n",
      "2025-01-17 19:42:30.752204: Epoch 413\n",
      "2025-01-17 19:42:30.752266: Current learning rate: 0.00619\n",
      "2025-01-17 19:44:21.219279: train_loss -0.7923\n",
      "2025-01-17 19:44:21.219735: val_loss -0.6288\n",
      "2025-01-17 19:44:21.219890: Pseudo dice [np.float32(0.7614)]\n",
      "2025-01-17 19:44:21.220024: Epoch time: 110.47 s\n",
      "2025-01-17 19:44:21.802355: \n",
      "2025-01-17 19:44:21.802512: Epoch 414\n",
      "2025-01-17 19:44:21.802585: Current learning rate: 0.00618\n",
      "2025-01-17 19:46:12.311841: train_loss -0.8157\n",
      "2025-01-17 19:46:12.311991: val_loss -0.5623\n",
      "2025-01-17 19:46:12.312025: Pseudo dice [np.float32(0.7102)]\n",
      "2025-01-17 19:46:12.312057: Epoch time: 110.51 s\n",
      "2025-01-17 19:46:12.885411: \n",
      "2025-01-17 19:46:12.885489: Epoch 415\n",
      "2025-01-17 19:46:12.885548: Current learning rate: 0.00617\n",
      "2025-01-17 19:48:03.487033: train_loss -0.6627\n",
      "2025-01-17 19:48:03.487200: val_loss -0.6575\n",
      "2025-01-17 19:48:03.487268: Pseudo dice [np.float32(0.6961)]\n",
      "2025-01-17 19:48:03.487311: Epoch time: 110.6 s\n",
      "2025-01-17 19:48:04.059216: \n",
      "2025-01-17 19:48:04.059349: Epoch 416\n",
      "2025-01-17 19:48:04.059424: Current learning rate: 0.00616\n",
      "2025-01-17 19:49:54.588539: train_loss -0.7508\n",
      "2025-01-17 19:49:54.588668: val_loss -0.7252\n",
      "2025-01-17 19:49:54.588702: Pseudo dice [np.float32(0.6615)]\n",
      "2025-01-17 19:49:54.588735: Epoch time: 110.53 s\n",
      "2025-01-17 19:49:55.163325: \n",
      "2025-01-17 19:49:55.163653: Epoch 417\n",
      "2025-01-17 19:49:55.163769: Current learning rate: 0.00615\n",
      "2025-01-17 19:51:45.707013: train_loss -0.7719\n",
      "2025-01-17 19:51:45.707130: val_loss -0.678\n",
      "2025-01-17 19:51:45.707162: Pseudo dice [np.float32(0.6997)]\n",
      "2025-01-17 19:51:45.707195: Epoch time: 110.54 s\n",
      "2025-01-17 19:51:46.286012: \n",
      "2025-01-17 19:51:46.286099: Epoch 418\n",
      "2025-01-17 19:51:46.286161: Current learning rate: 0.00614\n",
      "2025-01-17 19:53:36.798308: train_loss -0.7778\n",
      "2025-01-17 19:53:36.798437: val_loss -0.6279\n",
      "2025-01-17 19:53:36.798470: Pseudo dice [np.float32(0.7674)]\n",
      "2025-01-17 19:53:36.798504: Epoch time: 110.51 s\n",
      "2025-01-17 19:53:37.642244: \n",
      "2025-01-17 19:53:37.642353: Epoch 419\n",
      "2025-01-17 19:53:37.642414: Current learning rate: 0.00613\n",
      "2025-01-17 19:55:28.158225: train_loss -0.7986\n",
      "2025-01-17 19:55:28.158341: val_loss -0.6535\n",
      "2025-01-17 19:55:28.158380: Pseudo dice [np.float32(0.7442)]\n",
      "2025-01-17 19:55:28.158417: Epoch time: 110.52 s\n",
      "2025-01-17 19:55:28.730116: \n",
      "2025-01-17 19:55:28.730275: Epoch 420\n",
      "2025-01-17 19:55:28.730338: Current learning rate: 0.00612\n",
      "2025-01-17 19:57:19.246701: train_loss -0.8017\n",
      "2025-01-17 19:57:19.246821: val_loss -0.6292\n",
      "2025-01-17 19:57:19.246852: Pseudo dice [np.float32(0.745)]\n",
      "2025-01-17 19:57:19.246885: Epoch time: 110.52 s\n",
      "2025-01-17 19:57:19.823963: \n",
      "2025-01-17 19:57:19.824062: Epoch 421\n",
      "2025-01-17 19:57:19.824123: Current learning rate: 0.00612\n",
      "2025-01-17 19:59:10.423990: train_loss -0.7643\n",
      "2025-01-17 19:59:10.424117: val_loss -0.6321\n",
      "2025-01-17 19:59:10.424149: Pseudo dice [np.float32(0.7197)]\n",
      "2025-01-17 19:59:10.424213: Epoch time: 110.6 s\n",
      "2025-01-17 19:59:11.005409: \n",
      "2025-01-17 19:59:11.005509: Epoch 422\n",
      "2025-01-17 19:59:11.005571: Current learning rate: 0.00611\n",
      "2025-01-17 20:01:01.591671: train_loss -0.777\n",
      "2025-01-17 20:01:01.591807: val_loss -0.615\n",
      "2025-01-17 20:01:01.591842: Pseudo dice [np.float32(0.4994)]\n",
      "2025-01-17 20:01:01.591875: Epoch time: 110.59 s\n",
      "2025-01-17 20:01:02.174932: \n",
      "2025-01-17 20:01:02.175075: Epoch 423\n",
      "2025-01-17 20:01:02.175140: Current learning rate: 0.0061\n",
      "2025-01-17 20:02:52.690225: train_loss -0.7494\n",
      "2025-01-17 20:02:52.690414: val_loss -0.6355\n",
      "2025-01-17 20:02:52.690450: Pseudo dice [np.float32(0.7032)]\n",
      "2025-01-17 20:02:52.690489: Epoch time: 110.52 s\n",
      "2025-01-17 20:02:53.263946: \n",
      "2025-01-17 20:02:53.264045: Epoch 424\n",
      "2025-01-17 20:02:53.264253: Current learning rate: 0.00609\n",
      "2025-01-17 20:04:43.792492: train_loss -0.7433\n",
      "2025-01-17 20:04:43.792856: val_loss -0.6724\n",
      "2025-01-17 20:04:43.792967: Pseudo dice [np.float32(0.6108)]\n",
      "2025-01-17 20:04:43.793018: Epoch time: 110.53 s\n",
      "2025-01-17 20:04:44.363974: \n",
      "2025-01-17 20:04:44.364162: Epoch 425\n",
      "2025-01-17 20:04:44.364273: Current learning rate: 0.00608\n",
      "2025-01-17 20:06:34.860807: train_loss -0.7837\n",
      "2025-01-17 20:06:34.861233: val_loss -0.6635\n",
      "2025-01-17 20:06:34.861379: Pseudo dice [np.float32(0.6962)]\n",
      "2025-01-17 20:06:34.861428: Epoch time: 110.5 s\n",
      "2025-01-17 20:06:35.436327: \n",
      "2025-01-17 20:06:35.436623: Epoch 426\n",
      "2025-01-17 20:06:35.436795: Current learning rate: 0.00607\n",
      "2025-01-17 20:08:25.948575: train_loss -0.8196\n",
      "2025-01-17 20:08:25.948804: val_loss -0.6527\n",
      "2025-01-17 20:08:25.949139: Pseudo dice [np.float32(0.7282)]\n",
      "2025-01-17 20:08:25.949220: Epoch time: 110.51 s\n",
      "2025-01-17 20:08:26.527009: \n",
      "2025-01-17 20:08:26.527397: Epoch 427\n",
      "2025-01-17 20:08:26.527470: Current learning rate: 0.00606\n",
      "2025-01-17 20:10:17.062321: train_loss -0.8162\n",
      "2025-01-17 20:10:17.062457: val_loss -0.7053\n",
      "2025-01-17 20:10:17.062492: Pseudo dice [np.float32(0.7505)]\n",
      "2025-01-17 20:10:17.062526: Epoch time: 110.54 s\n",
      "2025-01-17 20:10:17.642596: \n",
      "2025-01-17 20:10:17.642908: Epoch 428\n",
      "2025-01-17 20:10:17.643034: Current learning rate: 0.00605\n",
      "2025-01-17 20:12:08.138384: train_loss -0.8048\n",
      "2025-01-17 20:12:08.138516: val_loss -0.677\n",
      "2025-01-17 20:12:08.138552: Pseudo dice [np.float32(0.7303)]\n",
      "2025-01-17 20:12:08.138586: Epoch time: 110.5 s\n",
      "2025-01-17 20:12:08.710569: \n",
      "2025-01-17 20:12:08.710735: Epoch 429\n",
      "2025-01-17 20:12:08.710810: Current learning rate: 0.00604\n",
      "2025-01-17 20:13:59.146851: train_loss -0.8302\n",
      "2025-01-17 20:13:59.146989: val_loss -0.6249\n",
      "2025-01-17 20:13:59.147021: Pseudo dice [np.float32(0.6797)]\n",
      "2025-01-17 20:13:59.147053: Epoch time: 110.44 s\n",
      "2025-01-17 20:13:59.730612: \n",
      "2025-01-17 20:13:59.730923: Epoch 430\n",
      "2025-01-17 20:13:59.731097: Current learning rate: 0.00603\n",
      "2025-01-17 20:15:50.228416: train_loss -0.8146\n",
      "2025-01-17 20:15:50.228777: val_loss -0.6665\n",
      "2025-01-17 20:15:50.228816: Pseudo dice [np.float32(0.6918)]\n",
      "2025-01-17 20:15:50.228850: Epoch time: 110.5 s\n",
      "2025-01-17 20:15:50.807832: \n",
      "2025-01-17 20:15:50.807982: Epoch 431\n",
      "2025-01-17 20:15:50.808054: Current learning rate: 0.00602\n",
      "2025-01-17 20:17:41.354788: train_loss -0.8333\n",
      "2025-01-17 20:17:41.354985: val_loss -0.6532\n",
      "2025-01-17 20:17:41.355031: Pseudo dice [np.float32(0.7747)]\n",
      "2025-01-17 20:17:41.355066: Epoch time: 110.55 s\n",
      "2025-01-17 20:17:42.175462: \n",
      "2025-01-17 20:17:42.175574: Epoch 432\n",
      "2025-01-17 20:17:42.175637: Current learning rate: 0.00601\n",
      "2025-01-17 20:19:32.673526: train_loss -0.8339\n",
      "2025-01-17 20:19:32.673967: val_loss -0.6451\n",
      "2025-01-17 20:19:32.674088: Pseudo dice [np.float32(0.7189)]\n",
      "2025-01-17 20:19:32.674139: Epoch time: 110.5 s\n",
      "2025-01-17 20:19:33.237111: \n",
      "2025-01-17 20:19:33.237216: Epoch 433\n",
      "2025-01-17 20:19:33.237279: Current learning rate: 0.006\n",
      "2025-01-17 20:21:23.789804: train_loss -0.8138\n",
      "2025-01-17 20:21:23.789959: val_loss -0.6419\n",
      "2025-01-17 20:21:23.789992: Pseudo dice [np.float32(0.7308)]\n",
      "2025-01-17 20:21:23.790026: Epoch time: 110.55 s\n",
      "2025-01-17 20:21:24.367284: \n",
      "2025-01-17 20:21:24.367561: Epoch 434\n",
      "2025-01-17 20:21:24.367633: Current learning rate: 0.00599\n",
      "2025-01-17 20:23:14.872389: train_loss -0.7971\n",
      "2025-01-17 20:23:14.872518: val_loss -0.6702\n",
      "2025-01-17 20:23:14.872551: Pseudo dice [np.float32(0.685)]\n",
      "2025-01-17 20:23:14.872583: Epoch time: 110.51 s\n",
      "2025-01-17 20:23:15.433151: \n",
      "2025-01-17 20:23:15.433249: Epoch 435\n",
      "2025-01-17 20:23:15.433311: Current learning rate: 0.00598\n",
      "2025-01-17 20:25:05.908859: train_loss -0.8202\n",
      "2025-01-17 20:25:05.909254: val_loss -0.7049\n",
      "2025-01-17 20:25:05.909301: Pseudo dice [np.float32(0.7013)]\n",
      "2025-01-17 20:25:05.909340: Epoch time: 110.48 s\n",
      "2025-01-17 20:25:06.473526: \n",
      "2025-01-17 20:25:06.473722: Epoch 436\n",
      "2025-01-17 20:25:06.473818: Current learning rate: 0.00597\n",
      "2025-01-17 20:26:56.971746: train_loss -0.8171\n",
      "2025-01-17 20:26:56.971975: val_loss -0.6836\n",
      "2025-01-17 20:26:56.972045: Pseudo dice [np.float32(0.7281)]\n",
      "2025-01-17 20:26:56.972086: Epoch time: 110.5 s\n",
      "2025-01-17 20:26:57.534757: \n",
      "2025-01-17 20:26:57.535038: Epoch 437\n",
      "2025-01-17 20:26:57.535103: Current learning rate: 0.00596\n",
      "2025-01-17 20:28:48.138551: train_loss -0.8114\n",
      "2025-01-17 20:28:48.138666: val_loss -0.6148\n",
      "2025-01-17 20:28:48.138698: Pseudo dice [np.float32(0.6632)]\n",
      "2025-01-17 20:28:48.138732: Epoch time: 110.6 s\n",
      "2025-01-17 20:28:48.705673: \n",
      "2025-01-17 20:28:48.706050: Epoch 438\n",
      "2025-01-17 20:28:48.706172: Current learning rate: 0.00595\n",
      "2025-01-17 20:30:39.230553: train_loss -0.8197\n",
      "2025-01-17 20:30:39.230732: val_loss -0.6308\n",
      "2025-01-17 20:30:39.230785: Pseudo dice [np.float32(0.695)]\n",
      "2025-01-17 20:30:39.230902: Epoch time: 110.53 s\n",
      "2025-01-17 20:30:39.802697: \n",
      "2025-01-17 20:30:39.802871: Epoch 439\n",
      "2025-01-17 20:30:39.802955: Current learning rate: 0.00594\n",
      "2025-01-17 20:32:30.274937: train_loss -0.8356\n",
      "2025-01-17 20:32:30.275059: val_loss -0.6488\n",
      "2025-01-17 20:32:30.275223: Pseudo dice [np.float32(0.6661)]\n",
      "2025-01-17 20:32:30.275291: Epoch time: 110.47 s\n",
      "2025-01-17 20:32:30.836616: \n",
      "2025-01-17 20:32:30.836808: Epoch 440\n",
      "2025-01-17 20:32:30.836935: Current learning rate: 0.00593\n",
      "2025-01-17 20:34:21.480615: train_loss -0.8279\n",
      "2025-01-17 20:34:21.480839: val_loss -0.6405\n",
      "2025-01-17 20:34:21.480962: Pseudo dice [np.float32(0.7108)]\n",
      "2025-01-17 20:34:21.481035: Epoch time: 110.64 s\n",
      "2025-01-17 20:34:22.056358: \n",
      "2025-01-17 20:34:22.056746: Epoch 441\n",
      "2025-01-17 20:34:22.056834: Current learning rate: 0.00592\n",
      "2025-01-17 20:36:12.585928: train_loss -0.829\n",
      "2025-01-17 20:36:12.586073: val_loss -0.6395\n",
      "2025-01-17 20:36:12.586114: Pseudo dice [np.float32(0.7096)]\n",
      "2025-01-17 20:36:12.586149: Epoch time: 110.53 s\n",
      "2025-01-17 20:36:13.146485: \n",
      "2025-01-17 20:36:13.146840: Epoch 442\n",
      "2025-01-17 20:36:13.146916: Current learning rate: 0.00592\n",
      "2025-01-17 20:38:03.795653: train_loss -0.8151\n",
      "2025-01-17 20:38:03.795878: val_loss -0.7273\n",
      "2025-01-17 20:38:03.795921: Pseudo dice [np.float32(0.7502)]\n",
      "2025-01-17 20:38:03.795955: Epoch time: 110.65 s\n",
      "2025-01-17 20:38:04.371118: \n",
      "2025-01-17 20:38:04.371285: Epoch 443\n",
      "2025-01-17 20:38:04.371357: Current learning rate: 0.00591\n",
      "2025-01-17 20:39:54.937413: train_loss -0.8197\n",
      "2025-01-17 20:39:54.937562: val_loss -0.593\n",
      "2025-01-17 20:39:54.937603: Pseudo dice [np.float32(0.6349)]\n",
      "2025-01-17 20:39:54.937651: Epoch time: 110.57 s\n",
      "2025-01-17 20:39:55.747282: \n",
      "2025-01-17 20:39:55.747514: Epoch 444\n",
      "2025-01-17 20:39:55.747724: Current learning rate: 0.0059\n",
      "2025-01-17 20:41:46.309055: train_loss -0.8194\n",
      "2025-01-17 20:41:46.309183: val_loss -0.6813\n",
      "2025-01-17 20:41:46.309217: Pseudo dice [np.float32(0.7331)]\n",
      "2025-01-17 20:41:46.309249: Epoch time: 110.56 s\n",
      "2025-01-17 20:41:46.868248: \n",
      "2025-01-17 20:41:46.868450: Epoch 445\n",
      "2025-01-17 20:41:46.868534: Current learning rate: 0.00589\n",
      "2025-01-17 20:43:37.414006: train_loss -0.7928\n",
      "2025-01-17 20:43:37.414249: val_loss -0.6955\n",
      "2025-01-17 20:43:37.414332: Pseudo dice [np.float32(0.7928)]\n",
      "2025-01-17 20:43:37.414377: Epoch time: 110.55 s\n",
      "2025-01-17 20:43:37.975174: \n",
      "2025-01-17 20:43:37.975274: Epoch 446\n",
      "2025-01-17 20:43:37.975332: Current learning rate: 0.00588\n",
      "2025-01-17 20:45:28.511159: train_loss -0.7576\n",
      "2025-01-17 20:45:28.511285: val_loss -0.6409\n",
      "2025-01-17 20:45:28.511318: Pseudo dice [np.float32(0.7911)]\n",
      "2025-01-17 20:45:28.511353: Epoch time: 110.54 s\n",
      "2025-01-17 20:45:29.073717: \n",
      "2025-01-17 20:45:29.074095: Epoch 447\n",
      "2025-01-17 20:45:29.074210: Current learning rate: 0.00587\n",
      "2025-01-17 20:47:19.728033: train_loss -0.7719\n",
      "2025-01-17 20:47:19.728259: val_loss -0.6831\n",
      "2025-01-17 20:47:19.728318: Pseudo dice [np.float32(0.7409)]\n",
      "2025-01-17 20:47:19.728353: Epoch time: 110.65 s\n",
      "2025-01-17 20:47:20.294404: \n",
      "2025-01-17 20:47:20.294559: Epoch 448\n",
      "2025-01-17 20:47:20.294683: Current learning rate: 0.00586\n",
      "2025-01-17 20:49:10.845917: train_loss -0.8102\n",
      "2025-01-17 20:49:10.846080: val_loss -0.6108\n",
      "2025-01-17 20:49:10.846115: Pseudo dice [np.float32(0.7011)]\n",
      "2025-01-17 20:49:10.846158: Epoch time: 110.55 s\n",
      "2025-01-17 20:49:11.454894: \n",
      "2025-01-17 20:49:11.455287: Epoch 449\n",
      "2025-01-17 20:49:11.455492: Current learning rate: 0.00585\n",
      "2025-01-17 20:51:01.995710: train_loss -0.7875\n",
      "2025-01-17 20:51:01.995852: val_loss -0.667\n",
      "2025-01-17 20:51:01.995891: Pseudo dice [np.float32(0.6833)]\n",
      "2025-01-17 20:51:01.995926: Epoch time: 110.54 s\n",
      "2025-01-17 20:51:02.767370: \n",
      "2025-01-17 20:51:02.767526: Epoch 450\n",
      "2025-01-17 20:51:02.767601: Current learning rate: 0.00584\n",
      "2025-01-17 20:52:53.343197: train_loss -0.8062\n",
      "2025-01-17 20:52:53.343323: val_loss -0.6034\n",
      "2025-01-17 20:52:53.343369: Pseudo dice [np.float32(0.6512)]\n",
      "2025-01-17 20:52:53.343408: Epoch time: 110.58 s\n",
      "2025-01-17 20:52:53.936819: \n",
      "2025-01-17 20:52:53.937253: Epoch 451\n",
      "2025-01-17 20:52:53.937346: Current learning rate: 0.00583\n",
      "2025-01-17 20:54:44.461429: train_loss -0.8183\n",
      "2025-01-17 20:54:44.461618: val_loss -0.6998\n",
      "2025-01-17 20:54:44.461651: Pseudo dice [np.float32(0.7352)]\n",
      "2025-01-17 20:54:44.461684: Epoch time: 110.53 s\n",
      "2025-01-17 20:54:45.023357: \n",
      "2025-01-17 20:54:45.023700: Epoch 452\n",
      "2025-01-17 20:54:45.023797: Current learning rate: 0.00582\n",
      "2025-01-17 20:56:35.582982: train_loss -0.8183\n",
      "2025-01-17 20:56:35.583196: val_loss -0.6808\n",
      "2025-01-17 20:56:35.583240: Pseudo dice [np.float32(0.7684)]\n",
      "2025-01-17 20:56:35.583275: Epoch time: 110.56 s\n",
      "2025-01-17 20:56:36.143910: \n",
      "2025-01-17 20:56:36.144092: Epoch 453\n",
      "2025-01-17 20:56:36.144231: Current learning rate: 0.00581\n",
      "2025-01-17 20:58:26.719973: train_loss -0.826\n",
      "2025-01-17 20:58:26.720162: val_loss -0.7372\n",
      "2025-01-17 20:58:26.720199: Pseudo dice [np.float32(0.7548)]\n",
      "2025-01-17 20:58:26.720232: Epoch time: 110.58 s\n",
      "2025-01-17 20:58:27.282304: \n",
      "2025-01-17 20:58:27.282490: Epoch 454\n",
      "2025-01-17 20:58:27.282574: Current learning rate: 0.0058\n",
      "2025-01-17 21:00:17.845017: train_loss -0.7978\n",
      "2025-01-17 21:00:17.845150: val_loss -0.6125\n",
      "2025-01-17 21:00:17.845184: Pseudo dice [np.float32(0.7477)]\n",
      "2025-01-17 21:00:17.845218: Epoch time: 110.56 s\n",
      "2025-01-17 21:00:18.409007: \n",
      "2025-01-17 21:00:18.409255: Epoch 455\n",
      "2025-01-17 21:00:18.409355: Current learning rate: 0.00579\n",
      "2025-01-17 21:02:08.947817: train_loss -0.8152\n",
      "2025-01-17 21:02:08.947967: val_loss -0.6708\n",
      "2025-01-17 21:02:08.948002: Pseudo dice [np.float32(0.7576)]\n",
      "2025-01-17 21:02:08.948035: Epoch time: 110.54 s\n",
      "2025-01-17 21:02:09.502392: \n",
      "2025-01-17 21:02:09.502477: Epoch 456\n",
      "2025-01-17 21:02:09.502541: Current learning rate: 0.00578\n",
      "2025-01-17 21:04:00.249183: train_loss -0.8262\n",
      "2025-01-17 21:04:00.249308: val_loss -0.633\n",
      "2025-01-17 21:04:00.249343: Pseudo dice [np.float32(0.7287)]\n",
      "2025-01-17 21:04:00.249377: Epoch time: 110.75 s\n",
      "2025-01-17 21:04:01.040026: \n",
      "2025-01-17 21:04:01.040414: Epoch 457\n",
      "2025-01-17 21:04:01.040493: Current learning rate: 0.00577\n",
      "2025-01-17 21:05:51.579202: train_loss -0.8277\n",
      "2025-01-17 21:05:51.579517: val_loss -0.6285\n",
      "2025-01-17 21:05:51.579587: Pseudo dice [np.float32(0.7769)]\n",
      "2025-01-17 21:05:51.579628: Epoch time: 110.54 s\n",
      "2025-01-17 21:05:52.144053: \n",
      "2025-01-17 21:05:52.144249: Epoch 458\n",
      "2025-01-17 21:05:52.144324: Current learning rate: 0.00576\n",
      "2025-01-17 21:07:42.647086: train_loss -0.8177\n",
      "2025-01-17 21:07:42.647212: val_loss -0.6272\n",
      "2025-01-17 21:07:42.647243: Pseudo dice [np.float32(0.7603)]\n",
      "2025-01-17 21:07:42.647278: Epoch time: 110.5 s\n",
      "2025-01-17 21:07:43.203399: \n",
      "2025-01-17 21:07:43.203707: Epoch 459\n",
      "2025-01-17 21:07:43.203850: Current learning rate: 0.00575\n",
      "2025-01-17 21:09:33.819667: train_loss -0.7997\n",
      "2025-01-17 21:09:33.819799: val_loss -0.5666\n",
      "2025-01-17 21:09:33.819832: Pseudo dice [np.float32(0.6846)]\n",
      "2025-01-17 21:09:33.819865: Epoch time: 110.62 s\n",
      "2025-01-17 21:09:34.382318: \n",
      "2025-01-17 21:09:34.382420: Epoch 460\n",
      "2025-01-17 21:09:34.382481: Current learning rate: 0.00574\n",
      "2025-01-17 21:11:24.939170: train_loss -0.8181\n",
      "2025-01-17 21:11:24.939300: val_loss -0.6834\n",
      "2025-01-17 21:11:24.939332: Pseudo dice [np.float32(0.7518)]\n",
      "2025-01-17 21:11:24.939367: Epoch time: 110.56 s\n",
      "2025-01-17 21:11:25.502127: \n",
      "2025-01-17 21:11:25.502462: Epoch 461\n",
      "2025-01-17 21:11:25.502649: Current learning rate: 0.00573\n",
      "2025-01-17 21:13:16.024714: train_loss -0.785\n",
      "2025-01-17 21:13:16.024854: val_loss -0.6299\n",
      "2025-01-17 21:13:16.024892: Pseudo dice [np.float32(0.6954)]\n",
      "2025-01-17 21:13:16.024930: Epoch time: 110.52 s\n",
      "2025-01-17 21:13:16.594865: \n",
      "2025-01-17 21:13:16.595006: Epoch 462\n",
      "2025-01-17 21:13:16.595070: Current learning rate: 0.00572\n",
      "2025-01-17 21:15:07.175204: train_loss -0.758\n",
      "2025-01-17 21:15:07.175329: val_loss -0.6382\n",
      "2025-01-17 21:15:07.175360: Pseudo dice [np.float32(0.7655)]\n",
      "2025-01-17 21:15:07.175392: Epoch time: 110.58 s\n",
      "2025-01-17 21:15:07.732814: \n",
      "2025-01-17 21:15:07.733239: Epoch 463\n",
      "2025-01-17 21:15:07.733341: Current learning rate: 0.00571\n",
      "2025-01-17 21:16:58.411937: train_loss -0.7963\n",
      "2025-01-17 21:16:58.412187: val_loss -0.6951\n",
      "2025-01-17 21:16:58.412240: Pseudo dice [np.float32(0.7133)]\n",
      "2025-01-17 21:16:58.412279: Epoch time: 110.68 s\n",
      "2025-01-17 21:16:58.972178: \n",
      "2025-01-17 21:16:58.972280: Epoch 464\n",
      "2025-01-17 21:16:58.972343: Current learning rate: 0.0057\n",
      "2025-01-17 21:18:49.533941: train_loss -0.7813\n",
      "2025-01-17 21:18:49.534065: val_loss -0.6468\n",
      "2025-01-17 21:18:49.534098: Pseudo dice [np.float32(0.7332)]\n",
      "2025-01-17 21:18:49.534131: Epoch time: 110.56 s\n",
      "2025-01-17 21:18:50.098181: \n",
      "2025-01-17 21:18:50.098392: Epoch 465\n",
      "2025-01-17 21:18:50.098468: Current learning rate: 0.0057\n",
      "2025-01-17 21:20:40.768568: train_loss -0.8106\n",
      "2025-01-17 21:20:40.768704: val_loss -0.7306\n",
      "2025-01-17 21:20:40.768739: Pseudo dice [np.float32(0.6966)]\n",
      "2025-01-17 21:20:40.768771: Epoch time: 110.67 s\n",
      "2025-01-17 21:20:41.328349: \n",
      "2025-01-17 21:20:41.328631: Epoch 466\n",
      "2025-01-17 21:20:41.328779: Current learning rate: 0.00569\n",
      "2025-01-17 21:22:31.884095: train_loss -0.822\n",
      "2025-01-17 21:22:31.884553: val_loss -0.7389\n",
      "2025-01-17 21:22:31.884680: Pseudo dice [np.float32(0.6876)]\n",
      "2025-01-17 21:22:31.884724: Epoch time: 110.56 s\n",
      "2025-01-17 21:22:32.442518: \n",
      "2025-01-17 21:22:32.442611: Epoch 467\n",
      "2025-01-17 21:22:32.442672: Current learning rate: 0.00568\n",
      "2025-01-17 21:24:22.999290: train_loss -0.8087\n",
      "2025-01-17 21:24:22.999416: val_loss -0.6696\n",
      "2025-01-17 21:24:22.999448: Pseudo dice [np.float32(0.7139)]\n",
      "2025-01-17 21:24:22.999482: Epoch time: 110.56 s\n",
      "2025-01-17 21:24:23.557878: \n",
      "2025-01-17 21:24:23.558059: Epoch 468\n",
      "2025-01-17 21:24:23.558125: Current learning rate: 0.00567\n",
      "2025-01-17 21:26:14.202978: train_loss -0.8197\n",
      "2025-01-17 21:26:14.203111: val_loss -0.6988\n",
      "2025-01-17 21:26:14.203153: Pseudo dice [np.float32(0.741)]\n",
      "2025-01-17 21:26:14.203188: Epoch time: 110.65 s\n",
      "2025-01-17 21:26:14.758066: \n",
      "2025-01-17 21:26:14.758157: Epoch 469\n",
      "2025-01-17 21:26:14.758220: Current learning rate: 0.00566\n",
      "2025-01-17 21:28:05.341127: train_loss -0.8052\n",
      "2025-01-17 21:28:05.341408: val_loss -0.5835\n",
      "2025-01-17 21:28:05.341551: Pseudo dice [np.float32(0.6164)]\n",
      "2025-01-17 21:28:05.341620: Epoch time: 110.58 s\n",
      "2025-01-17 21:28:06.145110: \n",
      "2025-01-17 21:28:06.145213: Epoch 470\n",
      "2025-01-17 21:28:06.145342: Current learning rate: 0.00565\n",
      "2025-01-17 21:29:56.717909: train_loss -0.825\n",
      "2025-01-17 21:29:56.718099: val_loss -0.5723\n",
      "2025-01-17 21:29:56.718132: Pseudo dice [np.float32(0.6891)]\n",
      "2025-01-17 21:29:56.718166: Epoch time: 110.57 s\n",
      "2025-01-17 21:29:57.279301: \n",
      "2025-01-17 21:29:57.279407: Epoch 471\n",
      "2025-01-17 21:29:57.279471: Current learning rate: 0.00564\n",
      "2025-01-17 21:31:47.834478: train_loss -0.8295\n",
      "2025-01-17 21:31:47.834697: val_loss -0.6532\n",
      "2025-01-17 21:31:47.834861: Pseudo dice [np.float32(0.7484)]\n",
      "2025-01-17 21:31:47.834942: Epoch time: 110.56 s\n",
      "2025-01-17 21:31:48.398202: \n",
      "2025-01-17 21:31:48.398523: Epoch 472\n",
      "2025-01-17 21:31:48.398681: Current learning rate: 0.00563\n",
      "2025-01-17 21:33:38.954995: train_loss -0.8297\n",
      "2025-01-17 21:33:38.955123: val_loss -0.6592\n",
      "2025-01-17 21:33:38.955157: Pseudo dice [np.float32(0.7344)]\n",
      "2025-01-17 21:33:38.955189: Epoch time: 110.56 s\n",
      "2025-01-17 21:33:39.524371: \n",
      "2025-01-17 21:33:39.524461: Epoch 473\n",
      "2025-01-17 21:33:39.524522: Current learning rate: 0.00562\n",
      "2025-01-17 21:35:30.265512: train_loss -0.8379\n",
      "2025-01-17 21:35:30.265637: val_loss -0.7374\n",
      "2025-01-17 21:35:30.265671: Pseudo dice [np.float32(0.7526)]\n",
      "2025-01-17 21:35:30.265704: Epoch time: 110.74 s\n",
      "2025-01-17 21:35:30.828362: \n",
      "2025-01-17 21:35:30.828587: Epoch 474\n",
      "2025-01-17 21:35:30.828764: Current learning rate: 0.00561\n",
      "2025-01-17 21:37:21.428462: train_loss -0.8043\n",
      "2025-01-17 21:37:21.428805: val_loss -0.5539\n",
      "2025-01-17 21:37:21.428885: Pseudo dice [np.float32(0.7304)]\n",
      "2025-01-17 21:37:21.428926: Epoch time: 110.6 s\n",
      "2025-01-17 21:37:21.996621: \n",
      "2025-01-17 21:37:21.996709: Epoch 475\n",
      "2025-01-17 21:37:21.996770: Current learning rate: 0.0056\n",
      "2025-01-17 21:39:12.592273: train_loss -0.8048\n",
      "2025-01-17 21:39:12.592400: val_loss -0.665\n",
      "2025-01-17 21:39:12.592433: Pseudo dice [np.float32(0.693)]\n",
      "2025-01-17 21:39:12.592467: Epoch time: 110.6 s\n",
      "2025-01-17 21:39:13.156833: \n",
      "2025-01-17 21:39:13.157063: Epoch 476\n",
      "2025-01-17 21:39:13.157265: Current learning rate: 0.00559\n",
      "2025-01-17 21:41:03.896003: train_loss -0.803\n",
      "2025-01-17 21:41:03.896126: val_loss -0.6294\n",
      "2025-01-17 21:41:03.896158: Pseudo dice [np.float32(0.7203)]\n",
      "2025-01-17 21:41:03.896192: Epoch time: 110.74 s\n",
      "2025-01-17 21:41:04.455342: \n",
      "2025-01-17 21:41:04.455439: Epoch 477\n",
      "2025-01-17 21:41:04.455514: Current learning rate: 0.00558\n",
      "2025-01-17 21:42:55.023185: train_loss -0.7966\n",
      "2025-01-17 21:42:55.023342: val_loss -0.553\n",
      "2025-01-17 21:42:55.023407: Pseudo dice [np.float32(0.2397)]\n",
      "2025-01-17 21:42:55.023448: Epoch time: 110.57 s\n",
      "2025-01-17 21:42:55.591694: \n",
      "2025-01-17 21:42:55.592124: Epoch 478\n",
      "2025-01-17 21:42:55.592410: Current learning rate: 0.00557\n",
      "2025-01-17 21:44:46.115558: train_loss -0.7933\n",
      "2025-01-17 21:44:46.115705: val_loss -0.6511\n",
      "2025-01-17 21:44:46.115741: Pseudo dice [np.float32(0.7275)]\n",
      "2025-01-17 21:44:46.115774: Epoch time: 110.53 s\n",
      "2025-01-17 21:44:46.685690: \n",
      "2025-01-17 21:44:46.685828: Epoch 479\n",
      "2025-01-17 21:44:46.686009: Current learning rate: 0.00556\n",
      "2025-01-17 21:46:37.269799: train_loss -0.7599\n",
      "2025-01-17 21:46:37.270011: val_loss -0.5505\n",
      "2025-01-17 21:46:37.270048: Pseudo dice [np.float32(0.4184)]\n",
      "2025-01-17 21:46:37.270080: Epoch time: 110.58 s\n",
      "2025-01-17 21:46:37.840190: \n",
      "2025-01-17 21:46:37.840350: Epoch 480\n",
      "2025-01-17 21:46:37.840424: Current learning rate: 0.00555\n",
      "2025-01-17 21:48:28.476543: train_loss -0.7925\n",
      "2025-01-17 21:48:28.476724: val_loss -0.6532\n",
      "2025-01-17 21:48:28.476757: Pseudo dice [np.float32(0.7004)]\n",
      "2025-01-17 21:48:28.476789: Epoch time: 110.64 s\n",
      "2025-01-17 21:48:29.048601: \n",
      "2025-01-17 21:48:29.048988: Epoch 481\n",
      "2025-01-17 21:48:29.049084: Current learning rate: 0.00554\n",
      "2025-01-17 21:50:19.603329: train_loss -0.8016\n",
      "2025-01-17 21:50:19.603482: val_loss -0.705\n",
      "2025-01-17 21:50:19.603810: Pseudo dice [np.float32(0.7789)]\n",
      "2025-01-17 21:50:19.603970: Epoch time: 110.56 s\n",
      "2025-01-17 21:50:20.192661: \n",
      "2025-01-17 21:50:20.193016: Epoch 482\n",
      "2025-01-17 21:50:20.193139: Current learning rate: 0.00553\n",
      "2025-01-17 21:52:10.755623: train_loss -0.8288\n",
      "2025-01-17 21:52:10.755820: val_loss -0.6127\n",
      "2025-01-17 21:52:10.755876: Pseudo dice [np.float32(0.6778)]\n",
      "2025-01-17 21:52:10.755986: Epoch time: 110.56 s\n",
      "2025-01-17 21:52:11.583121: \n",
      "2025-01-17 21:52:11.583227: Epoch 483\n",
      "2025-01-17 21:52:11.583297: Current learning rate: 0.00552\n",
      "2025-01-17 21:54:02.114318: train_loss -0.8239\n",
      "2025-01-17 21:54:02.114550: val_loss -0.7053\n",
      "2025-01-17 21:54:02.114593: Pseudo dice [np.float32(0.7615)]\n",
      "2025-01-17 21:54:02.114627: Epoch time: 110.53 s\n",
      "2025-01-17 21:54:02.679851: \n",
      "2025-01-17 21:54:02.679951: Epoch 484\n",
      "2025-01-17 21:54:02.680014: Current learning rate: 0.00551\n",
      "2025-01-17 21:55:53.191450: train_loss -0.8327\n",
      "2025-01-17 21:55:53.191586: val_loss -0.6836\n",
      "2025-01-17 21:55:53.191619: Pseudo dice [np.float32(0.7744)]\n",
      "2025-01-17 21:55:53.191652: Epoch time: 110.51 s\n",
      "2025-01-17 21:55:53.760461: \n",
      "2025-01-17 21:55:53.760849: Epoch 485\n",
      "2025-01-17 21:55:53.760917: Current learning rate: 0.0055\n",
      "2025-01-17 21:57:44.441201: train_loss -0.7944\n",
      "2025-01-17 21:57:44.441330: val_loss -0.7226\n",
      "2025-01-17 21:57:44.441361: Pseudo dice [np.float32(0.786)]\n",
      "2025-01-17 21:57:44.441395: Epoch time: 110.68 s\n",
      "2025-01-17 21:57:45.009991: \n",
      "2025-01-17 21:57:45.010140: Epoch 486\n",
      "2025-01-17 21:57:45.010213: Current learning rate: 0.00549\n",
      "2025-01-17 21:59:35.570828: train_loss -0.8016\n",
      "2025-01-17 21:59:35.571027: val_loss -0.6542\n",
      "2025-01-17 21:59:35.571060: Pseudo dice [np.float32(0.7411)]\n",
      "2025-01-17 21:59:35.571093: Epoch time: 110.56 s\n",
      "2025-01-17 21:59:36.140505: \n",
      "2025-01-17 21:59:36.140673: Epoch 487\n",
      "2025-01-17 21:59:36.140738: Current learning rate: 0.00548\n",
      "2025-01-17 22:01:26.719015: train_loss -0.8043\n",
      "2025-01-17 22:01:26.719145: val_loss -0.6873\n",
      "2025-01-17 22:01:26.719180: Pseudo dice [np.float32(0.7739)]\n",
      "2025-01-17 22:01:26.719212: Epoch time: 110.58 s\n",
      "2025-01-17 22:01:27.285197: \n",
      "2025-01-17 22:01:27.285553: Epoch 488\n",
      "2025-01-17 22:01:27.285717: Current learning rate: 0.00547\n",
      "2025-01-17 22:03:17.808884: train_loss -0.8192\n",
      "2025-01-17 22:03:17.809068: val_loss -0.5868\n",
      "2025-01-17 22:03:17.809200: Pseudo dice [np.float32(0.7126)]\n",
      "2025-01-17 22:03:17.809313: Epoch time: 110.52 s\n",
      "2025-01-17 22:03:18.383959: \n",
      "2025-01-17 22:03:18.384373: Epoch 489\n",
      "2025-01-17 22:03:18.384439: Current learning rate: 0.00546\n",
      "2025-01-17 22:05:09.134953: train_loss -0.8086\n",
      "2025-01-17 22:05:09.135095: val_loss -0.6514\n",
      "2025-01-17 22:05:09.135136: Pseudo dice [np.float32(0.6519)]\n",
      "2025-01-17 22:05:09.135187: Epoch time: 110.75 s\n",
      "2025-01-17 22:05:09.699565: \n",
      "2025-01-17 22:05:09.699710: Epoch 490\n",
      "2025-01-17 22:05:09.699784: Current learning rate: 0.00546\n",
      "2025-01-17 22:07:00.257593: train_loss -0.7853\n",
      "2025-01-17 22:07:00.257916: val_loss -0.612\n",
      "2025-01-17 22:07:00.258077: Pseudo dice [np.float32(0.7056)]\n",
      "2025-01-17 22:07:00.258125: Epoch time: 110.56 s\n",
      "2025-01-17 22:07:00.819028: \n",
      "2025-01-17 22:07:00.819312: Epoch 491\n",
      "2025-01-17 22:07:00.819386: Current learning rate: 0.00545\n",
      "2025-01-17 22:08:51.390558: train_loss -0.7859\n",
      "2025-01-17 22:08:51.390690: val_loss -0.7135\n",
      "2025-01-17 22:08:51.390723: Pseudo dice [np.float32(0.7511)]\n",
      "2025-01-17 22:08:51.390757: Epoch time: 110.57 s\n",
      "2025-01-17 22:08:51.959252: \n",
      "2025-01-17 22:08:51.959620: Epoch 492\n",
      "2025-01-17 22:08:51.959762: Current learning rate: 0.00544\n",
      "2025-01-17 22:10:42.570883: train_loss -0.8006\n",
      "2025-01-17 22:10:42.571012: val_loss -0.6474\n",
      "2025-01-17 22:10:42.571045: Pseudo dice [np.float32(0.7118)]\n",
      "2025-01-17 22:10:42.571078: Epoch time: 110.61 s\n",
      "2025-01-17 22:10:43.134784: \n",
      "2025-01-17 22:10:43.135068: Epoch 493\n",
      "2025-01-17 22:10:43.135148: Current learning rate: 0.00543\n",
      "2025-01-17 22:12:33.672502: train_loss -0.8107\n",
      "2025-01-17 22:12:33.672942: val_loss -0.6243\n",
      "2025-01-17 22:12:33.673003: Pseudo dice [np.float32(0.6627)]\n",
      "2025-01-17 22:12:33.673043: Epoch time: 110.54 s\n",
      "2025-01-17 22:12:34.234226: \n",
      "2025-01-17 22:12:34.234574: Epoch 494\n",
      "2025-01-17 22:12:34.234879: Current learning rate: 0.00542\n",
      "2025-01-17 22:14:24.794847: train_loss -0.7751\n",
      "2025-01-17 22:14:24.794983: val_loss -0.6784\n",
      "2025-01-17 22:14:24.795022: Pseudo dice [np.float32(0.7054)]\n",
      "2025-01-17 22:14:24.795063: Epoch time: 110.56 s\n",
      "2025-01-17 22:14:25.362529: \n",
      "2025-01-17 22:14:25.362638: Epoch 495\n",
      "2025-01-17 22:14:25.362708: Current learning rate: 0.00541\n",
      "2025-01-17 22:16:15.939312: train_loss -0.7842\n",
      "2025-01-17 22:16:15.939451: val_loss -0.6369\n",
      "2025-01-17 22:16:15.939486: Pseudo dice [np.float32(0.7899)]\n",
      "2025-01-17 22:16:15.939519: Epoch time: 110.58 s\n",
      "2025-01-17 22:16:16.749716: \n",
      "2025-01-17 22:16:16.749912: Epoch 496\n",
      "2025-01-17 22:16:16.750025: Current learning rate: 0.0054\n",
      "2025-01-17 22:18:07.316492: train_loss -0.8061\n",
      "2025-01-17 22:18:07.316626: val_loss -0.6291\n",
      "2025-01-17 22:18:07.316657: Pseudo dice [np.float32(0.6479)]\n",
      "2025-01-17 22:18:07.316688: Epoch time: 110.57 s\n",
      "2025-01-17 22:18:07.890653: \n",
      "2025-01-17 22:18:07.891035: Epoch 497\n",
      "2025-01-17 22:18:07.891120: Current learning rate: 0.00539\n",
      "2025-01-17 22:19:58.432635: train_loss -0.8313\n",
      "2025-01-17 22:19:58.432779: val_loss -0.6641\n",
      "2025-01-17 22:19:58.432819: Pseudo dice [np.float32(0.6893)]\n",
      "2025-01-17 22:19:58.432856: Epoch time: 110.54 s\n",
      "2025-01-17 22:19:59.004474: \n",
      "2025-01-17 22:19:59.004569: Epoch 498\n",
      "2025-01-17 22:19:59.004630: Current learning rate: 0.00538\n",
      "2025-01-17 22:21:49.571132: train_loss -0.7444\n",
      "2025-01-17 22:21:49.571259: val_loss -0.6298\n",
      "2025-01-17 22:21:49.571291: Pseudo dice [np.float32(0.6582)]\n",
      "2025-01-17 22:21:49.571327: Epoch time: 110.57 s\n",
      "2025-01-17 22:21:50.140550: \n",
      "2025-01-17 22:21:50.140955: Epoch 499\n",
      "2025-01-17 22:21:50.141029: Current learning rate: 0.00537\n",
      "2025-01-17 22:23:40.713328: train_loss -0.7674\n",
      "2025-01-17 22:23:40.713452: val_loss -0.6145\n",
      "2025-01-17 22:23:40.713485: Pseudo dice [np.float32(0.6858)]\n",
      "2025-01-17 22:23:40.713519: Epoch time: 110.57 s\n",
      "2025-01-17 22:23:41.508860: \n",
      "2025-01-17 22:23:41.509197: Epoch 500\n",
      "2025-01-17 22:23:41.509334: Current learning rate: 0.00536\n",
      "2025-01-17 22:25:32.239079: train_loss -0.7835\n",
      "2025-01-17 22:25:32.239276: val_loss -0.588\n",
      "2025-01-17 22:25:32.239322: Pseudo dice [np.float32(0.7351)]\n",
      "2025-01-17 22:25:32.239396: Epoch time: 110.73 s\n",
      "2025-01-17 22:25:32.808957: \n",
      "2025-01-17 22:25:32.809371: Epoch 501\n",
      "2025-01-17 22:25:32.809448: Current learning rate: 0.00535\n",
      "2025-01-17 22:27:23.392644: train_loss -0.8182\n",
      "2025-01-17 22:27:23.392799: val_loss -0.6882\n",
      "2025-01-17 22:27:23.392838: Pseudo dice [np.float32(0.734)]\n",
      "2025-01-17 22:27:23.392874: Epoch time: 110.58 s\n",
      "2025-01-17 22:27:23.970857: \n",
      "2025-01-17 22:27:23.970954: Epoch 502\n",
      "2025-01-17 22:27:23.971015: Current learning rate: 0.00534\n",
      "2025-01-17 22:29:14.529521: train_loss -0.8293\n",
      "2025-01-17 22:29:14.529651: val_loss -0.635\n",
      "2025-01-17 22:29:14.529683: Pseudo dice [np.float32(0.6724)]\n",
      "2025-01-17 22:29:14.529714: Epoch time: 110.56 s\n",
      "2025-01-17 22:29:15.095444: \n",
      "2025-01-17 22:29:15.095663: Epoch 503\n",
      "2025-01-17 22:29:15.095755: Current learning rate: 0.00533\n",
      "2025-01-17 22:31:05.815006: train_loss -0.8196\n",
      "2025-01-17 22:31:05.815196: val_loss -0.6149\n",
      "2025-01-17 22:31:05.815231: Pseudo dice [np.float32(0.5733)]\n",
      "2025-01-17 22:31:05.815262: Epoch time: 110.72 s\n",
      "2025-01-17 22:31:06.379371: \n",
      "2025-01-17 22:31:06.379464: Epoch 504\n",
      "2025-01-17 22:31:06.379528: Current learning rate: 0.00532\n",
      "2025-01-17 22:32:56.941714: train_loss -0.8207\n",
      "2025-01-17 22:32:56.941837: val_loss -0.6036\n",
      "2025-01-17 22:32:56.941869: Pseudo dice [np.float32(0.6755)]\n",
      "2025-01-17 22:32:56.941901: Epoch time: 110.56 s\n",
      "2025-01-17 22:32:57.512763: \n",
      "2025-01-17 22:32:57.512948: Epoch 505\n",
      "2025-01-17 22:32:57.513031: Current learning rate: 0.00531\n",
      "2025-01-17 22:34:48.088396: train_loss -0.8361\n",
      "2025-01-17 22:34:48.088555: val_loss -0.6215\n",
      "2025-01-17 22:34:48.088622: Pseudo dice [np.float32(0.6882)]\n",
      "2025-01-17 22:34:48.088665: Epoch time: 110.58 s\n",
      "2025-01-17 22:34:48.658633: \n",
      "2025-01-17 22:34:48.658915: Epoch 506\n",
      "2025-01-17 22:34:48.659045: Current learning rate: 0.0053\n",
      "2025-01-17 22:36:39.219004: train_loss -0.8497\n",
      "2025-01-17 22:36:39.219206: val_loss -0.6814\n",
      "2025-01-17 22:36:39.219251: Pseudo dice [np.float32(0.7236)]\n",
      "2025-01-17 22:36:39.219285: Epoch time: 110.56 s\n",
      "2025-01-17 22:36:39.783304: \n",
      "2025-01-17 22:36:39.783506: Epoch 507\n",
      "2025-01-17 22:36:39.783702: Current learning rate: 0.00529\n",
      "2025-01-17 22:38:30.511605: train_loss -0.8325\n",
      "2025-01-17 22:38:30.511733: val_loss -0.6245\n",
      "2025-01-17 22:38:30.511767: Pseudo dice [np.float32(0.7209)]\n",
      "2025-01-17 22:38:30.511799: Epoch time: 110.73 s\n",
      "2025-01-17 22:38:31.324262: \n",
      "2025-01-17 22:38:31.324524: Epoch 508\n",
      "2025-01-17 22:38:31.324724: Current learning rate: 0.00528\n",
      "2025-01-17 22:40:21.962979: train_loss -0.8358\n",
      "2025-01-17 22:40:21.963101: val_loss -0.6804\n",
      "2025-01-17 22:40:21.963134: Pseudo dice [np.float32(0.7247)]\n",
      "2025-01-17 22:40:21.963167: Epoch time: 110.64 s\n",
      "2025-01-17 22:40:22.549286: \n",
      "2025-01-17 22:40:22.549488: Epoch 509\n",
      "2025-01-17 22:40:22.549564: Current learning rate: 0.00527\n",
      "2025-01-17 22:42:13.295263: train_loss -0.8385\n",
      "2025-01-17 22:42:13.295453: val_loss -0.6796\n",
      "2025-01-17 22:42:13.295497: Pseudo dice [np.float32(0.7189)]\n",
      "2025-01-17 22:42:13.295532: Epoch time: 110.75 s\n",
      "2025-01-17 22:42:13.858220: \n",
      "2025-01-17 22:42:13.858520: Epoch 510\n",
      "2025-01-17 22:42:13.858594: Current learning rate: 0.00526\n",
      "2025-01-17 22:44:04.604483: train_loss -0.8126\n",
      "2025-01-17 22:44:04.604611: val_loss -0.5737\n",
      "2025-01-17 22:44:04.604645: Pseudo dice [np.float32(0.7236)]\n",
      "2025-01-17 22:44:04.604679: Epoch time: 110.75 s\n",
      "2025-01-17 22:44:05.172416: \n",
      "2025-01-17 22:44:05.172595: Epoch 511\n",
      "2025-01-17 22:44:05.172737: Current learning rate: 0.00525\n",
      "2025-01-17 22:45:55.939939: train_loss -0.8089\n",
      "2025-01-17 22:45:55.940073: val_loss -0.6025\n",
      "2025-01-17 22:45:55.940108: Pseudo dice [np.float32(0.7005)]\n",
      "2025-01-17 22:45:55.940143: Epoch time: 110.77 s\n",
      "2025-01-17 22:45:56.508346: \n",
      "2025-01-17 22:45:56.508527: Epoch 512\n",
      "2025-01-17 22:45:56.508600: Current learning rate: 0.00524\n",
      "2025-01-17 22:47:47.349826: train_loss -0.8087\n",
      "2025-01-17 22:47:47.350064: val_loss -0.5756\n",
      "2025-01-17 22:47:47.350108: Pseudo dice [np.float32(0.7279)]\n",
      "2025-01-17 22:47:47.350170: Epoch time: 110.84 s\n",
      "2025-01-17 22:47:47.925079: \n",
      "2025-01-17 22:47:47.925179: Epoch 513\n",
      "Current learning rate: 0.00523\n",
      "2025-01-17 22:49:38.637507: train_loss -0.7684\n",
      "2025-01-17 22:49:38.637643: val_loss -0.6331\n",
      "2025-01-17 22:49:38.637679: Pseudo dice [np.float32(0.7054)]\n",
      "2025-01-17 22:49:38.637711: Epoch time: 110.71 s\n",
      "2025-01-17 22:49:39.211416: \n",
      "2025-01-17 22:49:39.211506: Epoch 514\n",
      "2025-01-17 22:49:39.211567: Current learning rate: 0.00522\n",
      "2025-01-17 22:51:29.975532: train_loss -0.7557\n",
      "2025-01-17 22:51:29.975778: val_loss -0.666\n",
      "2025-01-17 22:51:29.975868: Pseudo dice [np.float32(0.7194)]\n",
      "2025-01-17 22:51:29.975925: Epoch time: 110.76 s\n",
      "2025-01-17 22:51:30.551230: \n",
      "2025-01-17 22:51:30.551333: Epoch 515\n",
      "2025-01-17 22:51:30.551395: Current learning rate: 0.00521\n",
      "2025-01-17 22:53:21.295640: train_loss -0.7984\n",
      "2025-01-17 22:53:21.295803: val_loss -0.652\n",
      "2025-01-17 22:53:21.295877: Pseudo dice [np.float32(0.6758)]\n",
      "2025-01-17 22:53:21.295920: Epoch time: 110.74 s\n",
      "2025-01-17 22:53:21.870003: \n",
      "2025-01-17 22:53:21.870097: Epoch 516\n",
      "2025-01-17 22:53:21.870321: Current learning rate: 0.0052\n",
      "2025-01-17 22:55:12.525878: train_loss -0.7967\n",
      "2025-01-17 22:55:12.526012: val_loss -0.6227\n",
      "2025-01-17 22:55:12.526047: Pseudo dice [np.float32(0.7361)]\n",
      "2025-01-17 22:55:12.526081: Epoch time: 110.66 s\n",
      "2025-01-17 22:55:13.093612: \n",
      "2025-01-17 22:55:13.093808: Epoch 517\n",
      "2025-01-17 22:55:13.093875: Current learning rate: 0.00519\n",
      "2025-01-17 22:57:03.610211: train_loss -0.7994\n",
      "2025-01-17 22:57:03.610427: val_loss -0.6025\n",
      "2025-01-17 22:57:03.610470: Pseudo dice [np.float32(0.7108)]\n",
      "2025-01-17 22:57:03.610504: Epoch time: 110.52 s\n",
      "2025-01-17 22:57:04.185176: \n",
      "2025-01-17 22:57:04.185355: Epoch 518\n",
      "2025-01-17 22:57:04.185430: Current learning rate: 0.00518\n",
      "2025-01-17 22:58:54.953236: train_loss -0.8156\n",
      "2025-01-17 22:58:54.953362: val_loss -0.6627\n",
      "2025-01-17 22:58:54.953397: Pseudo dice [np.float32(0.6751)]\n",
      "2025-01-17 22:58:54.953431: Epoch time: 110.77 s\n",
      "2025-01-17 22:58:55.528277: \n",
      "2025-01-17 22:58:55.528492: Epoch 519\n",
      "2025-01-17 22:58:55.528655: Current learning rate: 0.00518\n",
      "2025-01-17 23:00:46.238653: train_loss -0.8347\n",
      "2025-01-17 23:00:46.238776: val_loss -0.5898\n",
      "2025-01-17 23:00:46.238808: Pseudo dice [np.float32(0.6598)]\n",
      "2025-01-17 23:00:46.238840: Epoch time: 110.71 s\n",
      "2025-01-17 23:00:46.808059: \n",
      "2025-01-17 23:00:46.808143: Epoch 520\n",
      "2025-01-17 23:00:46.808205: Current learning rate: 0.00517\n",
      "2025-01-17 23:02:37.360700: train_loss -0.7364\n",
      "2025-01-17 23:02:37.360838: val_loss -0.6559\n",
      "2025-01-17 23:02:37.360870: Pseudo dice [np.float32(0.7004)]\n",
      "2025-01-17 23:02:37.360904: Epoch time: 110.55 s\n",
      "2025-01-17 23:02:38.196749: \n",
      "2025-01-17 23:02:38.196935: Epoch 521\n",
      "2025-01-17 23:02:38.197035: Current learning rate: 0.00516\n",
      "2025-01-17 23:04:28.869546: train_loss -0.7441\n",
      "2025-01-17 23:04:28.869746: val_loss -0.631\n",
      "2025-01-17 23:04:28.869795: Pseudo dice [np.float32(0.7151)]\n",
      "2025-01-17 23:04:28.869828: Epoch time: 110.67 s\n",
      "2025-01-17 23:04:29.450075: \n",
      "2025-01-17 23:04:29.450175: Epoch 522\n",
      "2025-01-17 23:04:29.450238: Current learning rate: 0.00515\n",
      "2025-01-17 23:06:20.190351: train_loss -0.7941\n",
      "2025-01-17 23:06:20.190496: val_loss -0.6081\n",
      "2025-01-17 23:06:20.190538: Pseudo dice [np.float32(0.7033)]\n",
      "2025-01-17 23:06:20.190578: Epoch time: 110.74 s\n",
      "2025-01-17 23:06:20.768697: \n",
      "2025-01-17 23:06:20.769024: Epoch 523\n",
      "2025-01-17 23:06:20.769159: Current learning rate: 0.00514\n",
      "2025-01-17 23:08:11.511308: train_loss -0.8094\n",
      "2025-01-17 23:08:11.511436: val_loss -0.6325\n",
      "2025-01-17 23:08:11.511472: Pseudo dice [np.float32(0.7505)]\n",
      "2025-01-17 23:08:11.511504: Epoch time: 110.74 s\n",
      "2025-01-17 23:08:12.075654: \n",
      "2025-01-17 23:08:12.075818: Epoch 524\n",
      "2025-01-17 23:08:12.075879: Current learning rate: 0.00513\n",
      "2025-01-17 23:10:02.656652: train_loss -0.8225\n",
      "2025-01-17 23:10:02.656982: val_loss -0.6626\n",
      "2025-01-17 23:10:02.657084: Pseudo dice [np.float32(0.7422)]\n",
      "2025-01-17 23:10:02.657133: Epoch time: 110.58 s\n",
      "2025-01-17 23:10:03.241037: \n",
      "2025-01-17 23:10:03.241132: Epoch 525\n",
      "2025-01-17 23:10:03.241194: Current learning rate: 0.00512\n",
      "2025-01-17 23:11:53.973946: train_loss -0.833\n",
      "2025-01-17 23:11:53.974092: val_loss -0.6507\n",
      "2025-01-17 23:11:53.974127: Pseudo dice [np.float32(0.7615)]\n",
      "2025-01-17 23:11:53.974160: Epoch time: 110.73 s\n",
      "2025-01-17 23:11:54.556322: \n",
      "2025-01-17 23:11:54.556479: Epoch 526\n",
      "2025-01-17 23:11:54.556551: Current learning rate: 0.00511\n",
      "2025-01-17 23:13:45.274529: train_loss -0.8\n",
      "2025-01-17 23:13:45.274650: val_loss -0.652\n",
      "2025-01-17 23:13:45.274682: Pseudo dice [np.float32(0.7601)]\n",
      "2025-01-17 23:13:45.274715: Epoch time: 110.72 s\n",
      "2025-01-17 23:13:45.847485: \n",
      "2025-01-17 23:13:45.847578: Epoch 527\n",
      "2025-01-17 23:13:45.847640: Current learning rate: 0.0051\n",
      "2025-01-17 23:15:36.440199: train_loss -0.8015\n",
      "2025-01-17 23:15:36.440348: val_loss -0.5973\n",
      "2025-01-17 23:15:36.440389: Pseudo dice [np.float32(0.684)]\n",
      "2025-01-17 23:15:36.440428: Epoch time: 110.59 s\n",
      "2025-01-17 23:15:37.024679: \n",
      "2025-01-17 23:15:37.024773: Epoch 528\n",
      "2025-01-17 23:15:37.024837: Current learning rate: 0.00509\n",
      "2025-01-17 23:17:27.611635: train_loss -0.8268\n",
      "2025-01-17 23:17:27.611763: val_loss -0.7224\n",
      "2025-01-17 23:17:27.611797: Pseudo dice [np.float32(0.7425)]\n",
      "2025-01-17 23:17:27.611832: Epoch time: 110.59 s\n",
      "2025-01-17 23:17:28.188422: \n",
      "2025-01-17 23:17:28.188514: Epoch 529\n",
      "2025-01-17 23:17:28.188576: Current learning rate: 0.00508\n",
      "2025-01-17 23:19:18.944319: train_loss -0.8325\n",
      "2025-01-17 23:19:18.944613: val_loss -0.6867\n",
      "2025-01-17 23:19:18.944676: Pseudo dice [np.float32(0.7278)]\n",
      "2025-01-17 23:19:18.944715: Epoch time: 110.76 s\n",
      "2025-01-17 23:19:19.514772: \n",
      "2025-01-17 23:19:19.515040: Epoch 530\n",
      "2025-01-17 23:19:19.515210: Current learning rate: 0.00507\n",
      "2025-01-17 23:21:10.065387: train_loss -0.8308\n",
      "2025-01-17 23:21:10.065598: val_loss -0.6901\n",
      "2025-01-17 23:21:10.065637: Pseudo dice [np.float32(0.683)]\n",
      "2025-01-17 23:21:10.065672: Epoch time: 110.55 s\n",
      "2025-01-17 23:21:10.649561: \n",
      "2025-01-17 23:21:10.649656: Epoch 531\n",
      "2025-01-17 23:21:10.649719: Current learning rate: 0.00506\n",
      "2025-01-17 23:23:01.262791: train_loss -0.8142\n",
      "2025-01-17 23:23:01.263000: val_loss -0.6759\n",
      "2025-01-17 23:23:01.263044: Pseudo dice [np.float32(0.6821)]\n",
      "2025-01-17 23:23:01.263076: Epoch time: 110.61 s\n",
      "2025-01-17 23:23:01.838429: \n",
      "2025-01-17 23:23:01.838769: Epoch 532\n",
      "2025-01-17 23:23:01.838842: Current learning rate: 0.00505\n",
      "2025-01-17 23:24:52.560660: train_loss -0.8403\n",
      "2025-01-17 23:24:52.560783: val_loss -0.6465\n",
      "2025-01-17 23:24:52.560815: Pseudo dice [np.float32(0.735)]\n",
      "2025-01-17 23:24:52.560848: Epoch time: 110.72 s\n",
      "2025-01-17 23:24:53.132865: \n",
      "2025-01-17 23:24:53.132999: Epoch 533\n",
      "2025-01-17 23:24:53.133072: Current learning rate: 0.00504\n",
      "2025-01-17 23:26:43.747396: train_loss -0.8253\n",
      "2025-01-17 23:26:43.747598: val_loss -0.7119\n",
      "2025-01-17 23:26:43.747644: Pseudo dice [np.float32(0.7422)]\n",
      "2025-01-17 23:26:43.747726: Epoch time: 110.62 s\n",
      "2025-01-17 23:26:44.564315: \n",
      "2025-01-17 23:26:44.564494: Epoch 534\n",
      "2025-01-17 23:26:44.564568: Current learning rate: 0.00503\n",
      "2025-01-17 23:28:35.128720: train_loss -0.8077\n",
      "2025-01-17 23:28:35.128851: val_loss -0.6882\n",
      "2025-01-17 23:28:35.128885: Pseudo dice [np.float32(0.8007)]\n",
      "2025-01-17 23:28:35.128917: Epoch time: 110.56 s\n",
      "2025-01-17 23:28:35.701660: \n",
      "2025-01-17 23:28:35.701827: Epoch 535\n",
      "2025-01-17 23:28:35.702059: Current learning rate: 0.00502\n",
      "2025-01-17 23:30:26.243166: train_loss -0.8225\n",
      "2025-01-17 23:30:26.243364: val_loss -0.6896\n",
      "2025-01-17 23:30:26.243402: Pseudo dice [np.float32(0.8526)]\n",
      "2025-01-17 23:30:26.243436: Epoch time: 110.54 s\n",
      "2025-01-17 23:30:26.809703: \n",
      "2025-01-17 23:30:26.809800: Epoch 536\n",
      "2025-01-17 23:30:26.809860: Current learning rate: 0.00501\n",
      "2025-01-17 23:32:17.379842: train_loss -0.8246\n",
      "2025-01-17 23:32:17.379968: val_loss -0.6931\n",
      "2025-01-17 23:32:17.380002: Pseudo dice [np.float32(0.7878)]\n",
      "2025-01-17 23:32:17.380036: Epoch time: 110.57 s\n",
      "2025-01-17 23:32:17.947961: \n",
      "2025-01-17 23:32:17.948137: Epoch 537\n",
      "2025-01-17 23:32:17.948212: Current learning rate: 0.005\n",
      "2025-01-17 23:34:08.510245: train_loss -0.8339\n",
      "2025-01-17 23:34:08.510604: val_loss -0.6576\n",
      "2025-01-17 23:34:08.510667: Pseudo dice [np.float32(0.741)]\n",
      "2025-01-17 23:34:08.510715: Epoch time: 110.56 s\n",
      "2025-01-17 23:34:09.108545: \n",
      "2025-01-17 23:34:09.108786: Epoch 538\n",
      "Current learning rate: 0.00499\n",
      "2025-01-17 23:35:59.649518: train_loss -0.8415\n",
      "2025-01-17 23:35:59.649652: val_loss -0.6636\n",
      "2025-01-17 23:35:59.649684: Pseudo dice [np.float32(0.7466)]\n",
      "2025-01-17 23:35:59.649718: Epoch time: 110.54 s\n",
      "2025-01-17 23:36:00.219706: \n",
      "2025-01-17 23:36:00.220069: Epoch 539\n",
      "2025-01-17 23:36:00.220161: Current learning rate: 0.00498\n",
      "2025-01-17 23:37:50.789865: train_loss -0.8071\n",
      "2025-01-17 23:37:50.790060: val_loss -0.6999\n",
      "2025-01-17 23:37:50.790093: Pseudo dice [np.float32(0.7632)]\n",
      "2025-01-17 23:37:50.790127: Epoch time: 110.57 s\n",
      "2025-01-17 23:37:51.356321: \n",
      "2025-01-17 23:37:51.356505: Epoch 540\n",
      "2025-01-17 23:37:51.356580: Current learning rate: 0.00497\n",
      "2025-01-17 23:39:41.962076: train_loss -0.7784\n",
      "2025-01-17 23:39:41.962466: val_loss -0.6912\n",
      "2025-01-17 23:39:41.962506: Pseudo dice [np.float32(0.7366)]\n",
      "2025-01-17 23:39:41.962539: Epoch time: 110.61 s\n",
      "2025-01-17 23:39:42.537213: \n",
      "2025-01-17 23:39:42.537317: Epoch 541\n",
      "2025-01-17 23:39:42.537380: Current learning rate: 0.00496\n",
      "2025-01-17 23:41:33.120949: train_loss -0.8133\n",
      "2025-01-17 23:41:33.121084: val_loss -0.6122\n",
      "2025-01-17 23:41:33.121115: Pseudo dice [np.float32(0.6212)]\n",
      "2025-01-17 23:41:33.121147: Epoch time: 110.58 s\n",
      "2025-01-17 23:41:33.697438: \n",
      "2025-01-17 23:41:33.697727: Epoch 542\n",
      "2025-01-17 23:41:33.697877: Current learning rate: 0.00495\n",
      "2025-01-17 23:43:24.254675: train_loss -0.7821\n",
      "2025-01-17 23:43:24.254806: val_loss -0.6188\n",
      "2025-01-17 23:43:24.254839: Pseudo dice [np.float32(0.7132)]\n",
      "2025-01-17 23:43:24.254873: Epoch time: 110.56 s\n",
      "2025-01-17 23:43:24.826092: \n",
      "2025-01-17 23:43:24.826321: Epoch 543\n",
      "2025-01-17 23:43:24.826413: Current learning rate: 0.00494\n",
      "2025-01-17 23:45:15.349278: train_loss -0.8166\n",
      "2025-01-17 23:45:15.349427: val_loss -0.6142\n",
      "2025-01-17 23:45:15.349462: Pseudo dice [np.float32(0.7624)]\n",
      "2025-01-17 23:45:15.349496: Epoch time: 110.52 s\n",
      "2025-01-17 23:45:15.921630: \n",
      "2025-01-17 23:45:15.921902: Epoch 544\n",
      "2025-01-17 23:45:15.922045: Current learning rate: 0.00493\n",
      "2025-01-17 23:47:06.507038: train_loss -0.8001\n",
      "2025-01-17 23:47:06.507205: val_loss -0.6485\n",
      "2025-01-17 23:47:06.507258: Pseudo dice [np.float32(0.7312)]\n",
      "2025-01-17 23:47:06.507312: Epoch time: 110.59 s\n",
      "2025-01-17 23:47:07.092349: \n",
      "2025-01-17 23:47:07.092576: Epoch 545\n",
      "2025-01-17 23:47:07.092646: Current learning rate: 0.00492\n",
      "2025-01-17 23:48:57.751749: train_loss -0.8087\n",
      "2025-01-17 23:48:57.751885: val_loss -0.6072\n",
      "2025-01-17 23:48:57.751919: Pseudo dice [np.float32(0.6894)]\n",
      "2025-01-17 23:48:57.751952: Epoch time: 110.66 s\n",
      "2025-01-17 23:48:58.325993: \n",
      "2025-01-17 23:48:58.326357: Epoch 546\n",
      "2025-01-17 23:48:58.326466: Current learning rate: 0.00491\n",
      "2025-01-17 23:50:48.893684: train_loss -0.8317\n",
      "2025-01-17 23:50:48.893882: val_loss -0.6222\n",
      "2025-01-17 23:50:48.893925: Pseudo dice [np.float32(0.7123)]\n",
      "2025-01-17 23:50:48.893959: Epoch time: 110.57 s\n",
      "2025-01-17 23:50:49.733438: \n",
      "2025-01-17 23:50:49.733746: Epoch 547\n",
      "2025-01-17 23:50:49.734035: Current learning rate: 0.0049\n",
      "2025-01-17 23:52:40.343105: train_loss -0.818\n",
      "2025-01-17 23:52:40.343249: val_loss -0.6788\n",
      "2025-01-17 23:52:40.343285: Pseudo dice [np.float32(0.7901)]\n",
      "2025-01-17 23:52:40.343319: Epoch time: 110.61 s\n",
      "2025-01-17 23:52:40.915130: \n",
      "2025-01-17 23:52:40.915230: Epoch 548\n",
      "2025-01-17 23:52:40.915294: Current learning rate: 0.00489\n",
      "2025-01-17 23:54:31.463664: train_loss -0.8008\n",
      "2025-01-17 23:54:31.463989: val_loss -0.602\n",
      "2025-01-17 23:54:31.464160: Pseudo dice [np.float32(0.6724)]\n",
      "2025-01-17 23:54:31.464234: Epoch time: 110.55 s\n",
      "2025-01-17 23:54:32.041139: \n",
      "2025-01-17 23:54:32.041250: Epoch 549\n",
      "2025-01-17 23:54:32.041325: Current learning rate: 0.00488\n",
      "2025-01-17 23:56:22.684705: train_loss -0.755\n",
      "2025-01-17 23:56:22.684831: val_loss -0.5584\n",
      "2025-01-17 23:56:22.684864: Pseudo dice [np.float32(0.7263)]\n",
      "2025-01-17 23:56:22.684898: Epoch time: 110.64 s\n",
      "2025-01-17 23:56:23.478897: \n",
      "2025-01-17 23:56:23.479352: Epoch 550\n",
      "2025-01-17 23:56:23.479487: Current learning rate: 0.00487\n",
      "2025-01-17 23:58:14.268574: train_loss -0.7662\n",
      "2025-01-17 23:58:14.268758: val_loss -0.6295\n",
      "2025-01-17 23:58:14.268793: Pseudo dice [np.float32(0.7035)]\n",
      "2025-01-17 23:58:14.268827: Epoch time: 110.79 s\n",
      "2025-01-17 23:58:14.839981: \n",
      "2025-01-17 23:58:14.840311: Epoch 551\n",
      "2025-01-17 23:58:14.840416: Current learning rate: 0.00486\n",
      "2025-01-18 00:00:05.450998: train_loss -0.8089\n",
      "2025-01-18 00:00:05.451149: val_loss -0.7008\n",
      "2025-01-18 00:00:05.451185: Pseudo dice [np.float32(0.7368)]\n",
      "2025-01-18 00:00:05.451218: Epoch time: 110.61 s\n",
      "2025-01-18 00:00:06.024804: \n",
      "2025-01-18 00:00:06.025165: Epoch 552\n",
      "2025-01-18 00:00:06.025253: Current learning rate: 0.00485\n",
      "2025-01-18 00:01:56.581048: train_loss -0.8285\n",
      "2025-01-18 00:01:56.581177: val_loss -0.6312\n",
      "2025-01-18 00:01:56.581212: Pseudo dice [np.float32(0.7314)]\n",
      "2025-01-18 00:01:56.581261: Epoch time: 110.56 s\n",
      "2025-01-18 00:01:57.155452: \n",
      "2025-01-18 00:01:57.155552: Epoch 553\n",
      "2025-01-18 00:01:57.155615: Current learning rate: 0.00484\n",
      "2025-01-18 00:03:47.859300: train_loss -0.7976\n",
      "2025-01-18 00:03:47.859518: val_loss -0.6909\n",
      "2025-01-18 00:03:47.859562: Pseudo dice [np.float32(0.7633)]\n",
      "2025-01-18 00:03:47.859596: Epoch time: 110.7 s\n",
      "2025-01-18 00:03:48.444761: \n",
      "2025-01-18 00:03:48.444857: Epoch 554\n",
      "2025-01-18 00:03:48.444921: Current learning rate: 0.00484\n",
      "2025-01-18 00:05:39.021020: train_loss -0.8138\n",
      "2025-01-18 00:05:39.021156: val_loss -0.6705\n",
      "2025-01-18 00:05:39.021190: Pseudo dice [np.float32(0.6588)]\n",
      "2025-01-18 00:05:39.021222: Epoch time: 110.58 s\n",
      "2025-01-18 00:05:39.590878: \n",
      "2025-01-18 00:05:39.591035: Epoch 555\n",
      "2025-01-18 00:05:39.591099: Current learning rate: 0.00483\n",
      "2025-01-18 00:07:30.355066: train_loss -0.7911\n",
      "2025-01-18 00:07:30.355191: val_loss -0.6205\n",
      "2025-01-18 00:07:30.355223: Pseudo dice [np.float32(0.713)]\n",
      "2025-01-18 00:07:30.355257: Epoch time: 110.76 s\n",
      "2025-01-18 00:07:30.928947: \n",
      "2025-01-18 00:07:30.929360: Epoch 556\n",
      "2025-01-18 00:07:30.929452: Current learning rate: 0.00482\n",
      "2025-01-18 00:09:21.708764: train_loss -0.8199\n",
      "2025-01-18 00:09:21.708955: val_loss -0.7085\n",
      "2025-01-18 00:09:21.708988: Pseudo dice [np.float32(0.7507)]\n",
      "2025-01-18 00:09:21.709020: Epoch time: 110.78 s\n",
      "2025-01-18 00:09:22.296641: \n",
      "2025-01-18 00:09:22.296961: Epoch 557\n",
      "2025-01-18 00:09:22.297141: Current learning rate: 0.00481\n",
      "2025-01-18 00:11:13.052201: train_loss -0.727\n",
      "2025-01-18 00:11:13.052428: val_loss -0.7056\n",
      "2025-01-18 00:11:13.052475: Pseudo dice [np.float32(0.7523)]\n",
      "2025-01-18 00:11:13.052515: Epoch time: 110.76 s\n",
      "2025-01-18 00:11:13.629801: \n",
      "2025-01-18 00:11:13.630019: Epoch 558\n",
      "2025-01-18 00:11:13.630091: Current learning rate: 0.0048\n",
      "2025-01-18 00:13:04.343807: train_loss -0.7551\n",
      "2025-01-18 00:13:04.343939: val_loss -0.5133\n",
      "2025-01-18 00:13:04.343986: Pseudo dice [np.float32(0.4515)]\n",
      "2025-01-18 00:13:04.344023: Epoch time: 110.71 s\n",
      "2025-01-18 00:13:04.917204: \n",
      "2025-01-18 00:13:04.917302: Epoch 559\n",
      "2025-01-18 00:13:04.917367: Current learning rate: 0.00479\n",
      "2025-01-18 00:14:55.667642: train_loss -0.7838\n",
      "2025-01-18 00:14:55.667770: val_loss -0.64\n",
      "2025-01-18 00:14:55.668071: Pseudo dice [np.float32(0.7439)]\n",
      "2025-01-18 00:14:55.668144: Epoch time: 110.75 s\n",
      "2025-01-18 00:14:56.504669: \n",
      "2025-01-18 00:14:56.504846: Epoch 560\n",
      "2025-01-18 00:14:56.504918: Current learning rate: 0.00478\n",
      "2025-01-18 00:16:47.270977: train_loss -0.8137\n",
      "2025-01-18 00:16:47.271116: val_loss -0.661\n",
      "2025-01-18 00:16:47.271162: Pseudo dice [np.float32(0.6819)]\n",
      "2025-01-18 00:16:47.271196: Epoch time: 110.77 s\n",
      "2025-01-18 00:16:47.840062: \n",
      "2025-01-18 00:16:47.840166: Epoch 561\n",
      "2025-01-18 00:16:47.840229: Current learning rate: 0.00477\n",
      "2025-01-18 00:18:38.349571: train_loss -0.8388\n",
      "2025-01-18 00:18:38.349716: val_loss -0.6517\n",
      "2025-01-18 00:18:38.349749: Pseudo dice [np.float32(0.703)]\n",
      "2025-01-18 00:18:38.349782: Epoch time: 110.51 s\n",
      "2025-01-18 00:18:38.930323: \n",
      "2025-01-18 00:18:38.930576: Epoch 562\n",
      "2025-01-18 00:18:38.930669: Current learning rate: 0.00476\n",
      "2025-01-18 00:20:29.588212: train_loss -0.7678\n",
      "2025-01-18 00:20:29.588336: val_loss -0.6076\n",
      "2025-01-18 00:20:29.588477: Pseudo dice [np.float32(0.7203)]\n",
      "2025-01-18 00:20:29.588563: Epoch time: 110.66 s\n",
      "2025-01-18 00:20:30.159171: \n",
      "2025-01-18 00:20:30.159327: Epoch 563\n",
      "Current learning rate: 0.00475\n",
      "2025-01-18 00:22:20.731442: train_loss -0.7973\n",
      "2025-01-18 00:22:20.731569: val_loss -0.6094\n",
      "2025-01-18 00:22:20.731603: Pseudo dice [np.float32(0.689)]\n",
      "2025-01-18 00:22:20.731637: Epoch time: 110.57 s\n",
      "2025-01-18 00:22:21.306722: \n",
      "2025-01-18 00:22:21.307005: Epoch 564\n",
      "2025-01-18 00:22:21.307157: Current learning rate: 0.00474\n",
      "2025-01-18 00:24:12.055281: train_loss -0.8117\n",
      "2025-01-18 00:24:12.055428: val_loss -0.6858\n",
      "2025-01-18 00:24:12.055511: Pseudo dice [np.float32(0.6879)]\n",
      "2025-01-18 00:24:12.055580: Epoch time: 110.75 s\n",
      "2025-01-18 00:24:12.635956: \n",
      "2025-01-18 00:24:12.636134: Epoch 565\n",
      "2025-01-18 00:24:12.636207: Current learning rate: 0.00473\n",
      "2025-01-18 00:26:03.332963: train_loss -0.8192\n",
      "2025-01-18 00:26:03.333107: val_loss -0.6613\n",
      "2025-01-18 00:26:03.333147: Pseudo dice [np.float32(0.6693)]\n",
      "2025-01-18 00:26:03.333185: Epoch time: 110.7 s\n",
      "2025-01-18 00:26:03.907690: \n",
      "2025-01-18 00:26:03.907844: Epoch 566\n",
      "2025-01-18 00:26:03.907923: Current learning rate: 0.00472\n",
      "2025-01-18 00:27:54.665829: train_loss -0.8414\n",
      "2025-01-18 00:27:54.665965: val_loss -0.6312\n",
      "2025-01-18 00:27:54.666006: Pseudo dice [np.float32(0.7096)]\n",
      "2025-01-18 00:27:54.666042: Epoch time: 110.76 s\n",
      "2025-01-18 00:27:55.237797: \n",
      "2025-01-18 00:27:55.238212: Epoch 567\n",
      "2025-01-18 00:27:55.238284: Current learning rate: 0.00471\n",
      "2025-01-18 00:29:45.988571: train_loss -0.8391\n",
      "2025-01-18 00:29:45.988694: val_loss -0.6919\n",
      "2025-01-18 00:29:45.988728: Pseudo dice [np.float32(0.7642)]\n",
      "2025-01-18 00:29:45.988760: Epoch time: 110.75 s\n",
      "2025-01-18 00:29:46.572729: \n",
      "2025-01-18 00:29:46.572927: Epoch 568\n",
      "2025-01-18 00:29:46.573029: Current learning rate: 0.0047\n",
      "2025-01-18 00:31:37.159880: train_loss -0.8374\n",
      "2025-01-18 00:31:37.160016: val_loss -0.6809\n",
      "2025-01-18 00:31:37.160051: Pseudo dice [np.float32(0.7287)]\n",
      "2025-01-18 00:31:37.160085: Epoch time: 110.59 s\n",
      "2025-01-18 00:31:37.733051: \n",
      "2025-01-18 00:31:37.733147: Epoch 569\n",
      "2025-01-18 00:31:37.733211: Current learning rate: 0.00469\n",
      "2025-01-18 00:33:28.305436: train_loss -0.8432\n",
      "2025-01-18 00:33:28.305564: val_loss -0.5946\n",
      "2025-01-18 00:33:28.305597: Pseudo dice [np.float32(0.7083)]\n",
      "2025-01-18 00:33:28.305634: Epoch time: 110.57 s\n",
      "2025-01-18 00:33:28.874515: \n",
      "2025-01-18 00:33:28.874601: Epoch 570\n",
      "2025-01-18 00:33:28.874663: Current learning rate: 0.00468\n",
      "2025-01-18 00:35:19.696905: train_loss -0.8328\n",
      "2025-01-18 00:35:19.697107: val_loss -0.7104\n",
      "2025-01-18 00:35:19.697371: Pseudo dice [np.float32(0.7583)]\n",
      "2025-01-18 00:35:19.697445: Epoch time: 110.82 s\n",
      "2025-01-18 00:35:20.276589: \n",
      "2025-01-18 00:35:20.276685: Epoch 571\n",
      "2025-01-18 00:35:20.276747: Current learning rate: 0.00467\n",
      "2025-01-18 00:37:10.994944: train_loss -0.8051\n",
      "2025-01-18 00:37:10.995087: val_loss -0.615\n",
      "2025-01-18 00:37:10.995127: Pseudo dice [np.float32(0.7184)]\n",
      "2025-01-18 00:37:10.995166: Epoch time: 110.72 s\n",
      "2025-01-18 00:37:11.566686: \n",
      "2025-01-18 00:37:11.566774: Epoch 572\n",
      "2025-01-18 00:37:11.566834: Current learning rate: 0.00466\n",
      "2025-01-18 00:39:02.178838: train_loss -0.7911\n",
      "2025-01-18 00:39:02.178986: val_loss -0.6912\n",
      "2025-01-18 00:39:02.179027: Pseudo dice [np.float32(0.7565)]\n",
      "2025-01-18 00:39:02.179068: Epoch time: 110.61 s\n",
      "2025-01-18 00:39:03.009679: \n",
      "2025-01-18 00:39:03.010030: Epoch 573\n",
      "2025-01-18 00:39:03.010164: Current learning rate: 0.00465\n",
      "2025-01-18 00:40:53.807946: train_loss -0.8182\n",
      "2025-01-18 00:40:53.808095: val_loss -0.6784\n",
      "2025-01-18 00:40:53.808132: Pseudo dice [np.float32(0.7677)]\n",
      "2025-01-18 00:40:53.808167: Epoch time: 110.8 s\n",
      "2025-01-18 00:40:54.387791: \n",
      "2025-01-18 00:40:54.387900: Epoch 574\n",
      "2025-01-18 00:40:54.388030: Current learning rate: 0.00464\n",
      "2025-01-18 00:42:44.953080: train_loss -0.8156\n",
      "2025-01-18 00:42:44.953267: val_loss -0.6631\n",
      "2025-01-18 00:42:44.953305: Pseudo dice [np.float32(0.716)]\n",
      "2025-01-18 00:42:44.953339: Epoch time: 110.57 s\n",
      "2025-01-18 00:42:45.532371: \n",
      "2025-01-18 00:42:45.532558: Epoch 575\n",
      "2025-01-18 00:42:45.532631: Current learning rate: 0.00463\n",
      "2025-01-18 00:44:36.156794: train_loss -0.826\n",
      "2025-01-18 00:44:36.157016: val_loss -0.6848\n",
      "2025-01-18 00:44:36.157051: Pseudo dice [np.float32(0.7332)]\n",
      "2025-01-18 00:44:36.157083: Epoch time: 110.62 s\n",
      "2025-01-18 00:44:36.743130: \n",
      "2025-01-18 00:44:36.743633: Epoch 576\n",
      "2025-01-18 00:44:36.743729: Current learning rate: 0.00462\n",
      "2025-01-18 00:46:27.513096: train_loss -0.8106\n",
      "2025-01-18 00:46:27.513236: val_loss -0.6822\n",
      "2025-01-18 00:46:27.513275: Pseudo dice [np.float32(0.7239)]\n",
      "2025-01-18 00:46:27.513307: Epoch time: 110.77 s\n",
      "2025-01-18 00:46:28.100923: \n",
      "2025-01-18 00:46:28.101204: Epoch 577\n",
      "2025-01-18 00:46:28.101300: Current learning rate: 0.00461\n",
      "2025-01-18 00:48:18.934556: train_loss -0.8391\n",
      "2025-01-18 00:48:18.934724: val_loss -0.6631\n",
      "2025-01-18 00:48:18.934796: Pseudo dice [np.float32(0.7074)]\n",
      "2025-01-18 00:48:18.934839: Epoch time: 110.83 s\n",
      "2025-01-18 00:48:19.514232: \n",
      "2025-01-18 00:48:19.514585: Epoch 578\n",
      "2025-01-18 00:48:19.514657: Current learning rate: 0.0046\n",
      "2025-01-18 00:50:10.104677: train_loss -0.8479\n",
      "2025-01-18 00:50:10.104804: val_loss -0.6736\n",
      "2025-01-18 00:50:10.104839: Pseudo dice [np.float32(0.7369)]\n",
      "2025-01-18 00:50:10.104874: Epoch time: 110.59 s\n",
      "2025-01-18 00:50:10.682531: \n",
      "2025-01-18 00:50:10.682893: Epoch 579\n",
      "2025-01-18 00:50:10.683070: Current learning rate: 0.00459\n",
      "2025-01-18 00:52:01.250379: train_loss -0.8338\n",
      "2025-01-18 00:52:01.250506: val_loss -0.7136\n",
      "2025-01-18 00:52:01.250539: Pseudo dice [np.float32(0.7501)]\n",
      "2025-01-18 00:52:01.250583: Epoch time: 110.57 s\n",
      "2025-01-18 00:52:01.854083: \n",
      "2025-01-18 00:52:01.854461: Epoch 580\n",
      "2025-01-18 00:52:01.854736: Current learning rate: 0.00458\n",
      "2025-01-18 00:53:52.623106: train_loss -0.832\n",
      "2025-01-18 00:53:52.623245: val_loss -0.6281\n",
      "2025-01-18 00:53:52.623358: Pseudo dice [np.float32(0.757)]\n",
      "2025-01-18 00:53:52.623435: Epoch time: 110.77 s\n",
      "2025-01-18 00:53:53.208251: \n",
      "2025-01-18 00:53:53.208642: Epoch 581\n",
      "2025-01-18 00:53:53.208743: Current learning rate: 0.00457\n",
      "2025-01-18 00:55:43.760436: train_loss -0.83\n",
      "2025-01-18 00:55:43.760561: val_loss -0.7224\n",
      "2025-01-18 00:55:43.760596: Pseudo dice [np.float32(0.6826)]\n",
      "2025-01-18 00:55:43.760629: Epoch time: 110.55 s\n",
      "2025-01-18 00:55:44.341584: \n",
      "2025-01-18 00:55:44.341781: Epoch 582\n",
      "2025-01-18 00:55:44.341862: Current learning rate: 0.00456\n",
      "2025-01-18 00:57:34.877063: train_loss -0.8232\n",
      "2025-01-18 00:57:34.877210: val_loss -0.6155\n",
      "2025-01-18 00:57:34.877358: Pseudo dice [np.float32(0.7228)]\n",
      "2025-01-18 00:57:34.877518: Epoch time: 110.54 s\n",
      "2025-01-18 00:57:35.456730: \n",
      "2025-01-18 00:57:35.456825: Epoch 583\n",
      "2025-01-18 00:57:35.456887: Current learning rate: 0.00455\n",
      "2025-01-18 00:59:26.160681: train_loss -0.8304\n",
      "2025-01-18 00:59:26.160852: val_loss -0.6717\n",
      "2025-01-18 00:59:26.160889: Pseudo dice [np.float32(0.7332)]\n",
      "2025-01-18 00:59:26.160924: Epoch time: 110.7 s\n",
      "2025-01-18 00:59:26.748978: \n",
      "2025-01-18 00:59:26.749209: Epoch 584\n",
      "2025-01-18 00:59:26.749300: Current learning rate: 0.00454\n",
      "2025-01-18 01:01:17.447767: train_loss -0.8418\n",
      "2025-01-18 01:01:17.447993: val_loss -0.6417\n",
      "2025-01-18 01:01:17.448038: Pseudo dice [np.float32(0.7369)]\n",
      "2025-01-18 01:01:17.448072: Epoch time: 110.7 s\n",
      "2025-01-18 01:01:18.270949: \n",
      "2025-01-18 01:01:18.271106: Epoch 585\n",
      "2025-01-18 01:01:18.271198: Current learning rate: 0.00453\n",
      "2025-01-18 01:03:09.024666: train_loss -0.815\n",
      "2025-01-18 01:03:09.024798: val_loss -0.6362\n",
      "2025-01-18 01:03:09.024834: Pseudo dice [np.float32(0.7384)]\n",
      "2025-01-18 01:03:09.024868: Epoch time: 110.75 s\n",
      "2025-01-18 01:03:09.620760: \n",
      "2025-01-18 01:03:09.620956: Epoch 586\n",
      "2025-01-18 01:03:09.621029: Current learning rate: 0.00452\n",
      "2025-01-18 01:05:00.178102: train_loss -0.8339\n",
      "2025-01-18 01:05:00.178294: val_loss -0.6738\n",
      "2025-01-18 01:05:00.178328: Pseudo dice [np.float32(0.7703)]\n",
      "2025-01-18 01:05:00.178360: Epoch time: 110.56 s\n",
      "2025-01-18 01:05:00.757857: \n",
      "2025-01-18 01:05:00.757984: Epoch 587\n",
      "2025-01-18 01:05:00.758083: Current learning rate: 0.00451\n",
      "2025-01-18 01:06:51.268570: train_loss -0.8422\n",
      "2025-01-18 01:06:51.268700: val_loss -0.674\n",
      "2025-01-18 01:06:51.268755: Pseudo dice [np.float32(0.7209)]\n",
      "2025-01-18 01:06:51.268791: Epoch time: 110.51 s\n",
      "2025-01-18 01:06:51.857090: \n",
      "2025-01-18 01:06:51.857182: Epoch 588\n",
      "Current learning rate: 0.0045\n",
      "2025-01-18 01:08:42.433316: train_loss -0.8532\n",
      "2025-01-18 01:08:42.433446: val_loss -0.5863\n",
      "2025-01-18 01:08:42.433479: Pseudo dice [np.float32(0.7422)]\n",
      "2025-01-18 01:08:42.433513: Epoch time: 110.58 s\n",
      "2025-01-18 01:08:43.018671: \n",
      "2025-01-18 01:08:43.018815: Epoch 589\n",
      "2025-01-18 01:08:43.019090: Current learning rate: 0.00449\n",
      "2025-01-18 01:10:33.652958: train_loss -0.8429\n",
      "2025-01-18 01:10:33.653106: val_loss -0.6678\n",
      "2025-01-18 01:10:33.653148: Pseudo dice [np.float32(0.7297)]\n",
      "2025-01-18 01:10:33.653191: Epoch time: 110.63 s\n",
      "2025-01-18 01:10:34.236717: \n",
      "2025-01-18 01:10:34.236906: Epoch 590\n",
      "2025-01-18 01:10:34.236980: Current learning rate: 0.00448\n",
      "2025-01-18 01:12:24.827158: train_loss -0.8291\n",
      "2025-01-18 01:12:24.827370: val_loss -0.6454\n",
      "2025-01-18 01:12:24.827403: Pseudo dice [np.float32(0.6805)]\n",
      "2025-01-18 01:12:24.827435: Epoch time: 110.59 s\n",
      "2025-01-18 01:12:25.414318: \n",
      "2025-01-18 01:12:25.414420: Epoch 591\n",
      "2025-01-18 01:12:25.414488: Current learning rate: 0.00447\n",
      "2025-01-18 01:14:16.142183: train_loss -0.8436\n",
      "2025-01-18 01:14:16.142353: val_loss -0.7191\n",
      "2025-01-18 01:14:16.142423: Pseudo dice [np.float32(0.7953)]\n",
      "2025-01-18 01:14:16.142529: Epoch time: 110.73 s\n",
      "2025-01-18 01:14:16.729500: \n",
      "2025-01-18 01:14:16.729594: Epoch 592\n",
      "2025-01-18 01:14:16.729654: Current learning rate: 0.00446\n",
      "2025-01-18 01:16:07.522526: train_loss -0.8422\n",
      "2025-01-18 01:16:07.522759: val_loss -0.6965\n",
      "2025-01-18 01:16:07.522803: Pseudo dice [np.float32(0.7363)]\n",
      "2025-01-18 01:16:07.522863: Epoch time: 110.79 s\n",
      "2025-01-18 01:16:08.098785: \n",
      "2025-01-18 01:16:08.099015: Epoch 593\n",
      "2025-01-18 01:16:08.099162: Current learning rate: 0.00445\n",
      "2025-01-18 01:17:58.825844: train_loss -0.8471\n",
      "2025-01-18 01:17:58.826020: val_loss -0.6674\n",
      "2025-01-18 01:17:58.826136: Pseudo dice [np.float32(0.6892)]\n",
      "2025-01-18 01:17:58.826319: Epoch time: 110.73 s\n",
      "2025-01-18 01:17:59.411079: \n",
      "2025-01-18 01:17:59.411230: Epoch 594\n",
      "2025-01-18 01:17:59.411295: Current learning rate: 0.00444\n",
      "2025-01-18 01:19:49.996670: train_loss -0.8297\n",
      "2025-01-18 01:19:49.996809: val_loss -0.7338\n",
      "2025-01-18 01:19:49.996842: Pseudo dice [np.float32(0.7758)]\n",
      "2025-01-18 01:19:49.996875: Epoch time: 110.59 s\n",
      "2025-01-18 01:19:50.580914: \n",
      "2025-01-18 01:19:50.581147: Epoch 595\n",
      "2025-01-18 01:19:50.581324: Current learning rate: 0.00443\n",
      "2025-01-18 01:21:41.310423: train_loss -0.8237\n",
      "2025-01-18 01:21:41.310662: val_loss -0.7008\n",
      "2025-01-18 01:21:41.310707: Pseudo dice [np.float32(0.7977)]\n",
      "2025-01-18 01:21:41.310744: Epoch time: 110.73 s\n",
      "2025-01-18 01:21:41.890412: \n",
      "2025-01-18 01:21:41.890561: Epoch 596\n",
      "2025-01-18 01:21:41.890634: Current learning rate: 0.00442\n",
      "2025-01-18 01:23:32.634086: train_loss -0.801\n",
      "2025-01-18 01:23:32.634240: val_loss -0.6175\n",
      "2025-01-18 01:23:32.634278: Pseudo dice [np.float32(0.649)]\n",
      "2025-01-18 01:23:32.634317: Epoch time: 110.74 s\n",
      "2025-01-18 01:23:33.459255: \n",
      "2025-01-18 01:23:33.459357: Epoch 597\n",
      "2025-01-18 01:23:33.459428: Current learning rate: 0.00441\n",
      "2025-01-18 01:25:23.999740: train_loss -0.8463\n",
      "2025-01-18 01:25:23.999969: val_loss -0.611\n",
      "2025-01-18 01:25:24.000011: Pseudo dice [np.float32(0.7357)]\n",
      "2025-01-18 01:25:24.000096: Epoch time: 110.54 s\n",
      "2025-01-18 01:25:24.583933: \n",
      "2025-01-18 01:25:24.584120: Epoch 598\n",
      "2025-01-18 01:25:24.584190: Current learning rate: 0.0044\n",
      "2025-01-18 01:27:15.294228: train_loss -0.8557\n",
      "2025-01-18 01:27:15.294374: val_loss -0.7025\n",
      "2025-01-18 01:27:15.294405: Pseudo dice [np.float32(0.7954)]\n",
      "2025-01-18 01:27:15.294438: Epoch time: 110.71 s\n",
      "2025-01-18 01:27:15.866595: \n",
      "2025-01-18 01:27:15.866923: Epoch 599\n",
      "2025-01-18 01:27:15.866997: Current learning rate: 0.00439\n",
      "2025-01-18 01:29:06.599395: train_loss -0.837\n",
      "2025-01-18 01:29:06.599545: val_loss -0.6459\n",
      "2025-01-18 01:29:06.599576: Pseudo dice [np.float32(0.7709)]\n",
      "2025-01-18 01:29:06.599610: Epoch time: 110.73 s\n",
      "2025-01-18 01:29:07.405133: \n",
      "2025-01-18 01:29:07.405298: Epoch 600\n",
      "2025-01-18 01:29:07.405442: Current learning rate: 0.00438\n",
      "2025-01-18 01:30:57.970588: train_loss -0.8203\n",
      "2025-01-18 01:30:57.970733: val_loss -0.6636\n",
      "2025-01-18 01:30:57.970769: Pseudo dice [np.float32(0.6154)]\n",
      "2025-01-18 01:30:57.970801: Epoch time: 110.57 s\n",
      "2025-01-18 01:30:58.552504: \n",
      "2025-01-18 01:30:58.552598: Epoch 601\n",
      "2025-01-18 01:30:58.552663: Current learning rate: 0.00437\n",
      "2025-01-18 01:32:49.256620: train_loss -0.8286\n",
      "2025-01-18 01:32:49.256774: val_loss -0.6987\n",
      "2025-01-18 01:32:49.256884: Pseudo dice [np.float32(0.6791)]\n",
      "2025-01-18 01:32:49.257051: Epoch time: 110.7 s\n",
      "2025-01-18 01:32:49.845561: \n",
      "2025-01-18 01:32:49.846073: Epoch 602\n",
      "2025-01-18 01:32:49.846172: Current learning rate: 0.00436\n",
      "2025-01-18 01:34:40.424323: train_loss -0.8385\n",
      "2025-01-18 01:34:40.424716: val_loss -0.6239\n",
      "2025-01-18 01:34:40.424784: Pseudo dice [np.float32(0.7413)]\n",
      "2025-01-18 01:34:40.424823: Epoch time: 110.58 s\n",
      "2025-01-18 01:34:41.010067: \n",
      "2025-01-18 01:34:41.010277: Epoch 603\n",
      "2025-01-18 01:34:41.010438: Current learning rate: 0.00435\n",
      "2025-01-18 01:36:31.691861: train_loss -0.8483\n",
      "2025-01-18 01:36:31.692001: val_loss -0.6833\n",
      "2025-01-18 01:36:31.692040: Pseudo dice [np.float32(0.704)]\n",
      "2025-01-18 01:36:31.692073: Epoch time: 110.68 s\n",
      "2025-01-18 01:36:32.278333: \n",
      "2025-01-18 01:36:32.278499: Epoch 604\n",
      "2025-01-18 01:36:32.278595: Current learning rate: 0.00434\n",
      "2025-01-18 01:38:22.965384: train_loss -0.8553\n",
      "2025-01-18 01:38:22.965632: val_loss -0.6434\n",
      "2025-01-18 01:38:22.965675: Pseudo dice [np.float32(0.7606)]\n",
      "2025-01-18 01:38:22.965711: Epoch time: 110.69 s\n",
      "2025-01-18 01:38:23.548247: \n",
      "2025-01-18 01:38:23.548564: Epoch 605\n",
      "2025-01-18 01:38:23.548668: Current learning rate: 0.00433\n",
      "2025-01-18 01:40:14.117432: train_loss -0.8537\n",
      "2025-01-18 01:40:14.117896: val_loss -0.6545\n",
      "2025-01-18 01:40:14.118081: Pseudo dice [np.float32(0.6711)]\n",
      "2025-01-18 01:40:14.118151: Epoch time: 110.57 s\n",
      "2025-01-18 01:40:14.701460: \n",
      "2025-01-18 01:40:14.701554: Epoch 606\n",
      "2025-01-18 01:40:14.701614: Current learning rate: 0.00432\n",
      "2025-01-18 01:42:05.302244: train_loss -0.8204\n",
      "2025-01-18 01:42:05.302384: val_loss -0.6537\n",
      "2025-01-18 01:42:05.302430: Pseudo dice [np.float32(0.7168)]\n",
      "2025-01-18 01:42:05.302466: Epoch time: 110.6 s\n",
      "2025-01-18 01:42:05.900426: \n",
      "2025-01-18 01:42:05.900865: Epoch 607\n",
      "2025-01-18 01:42:05.900955: Current learning rate: 0.00431\n",
      "2025-01-18 01:43:56.501420: train_loss -0.8466\n",
      "2025-01-18 01:43:56.501545: val_loss -0.7313\n",
      "2025-01-18 01:43:56.501578: Pseudo dice [np.float32(0.7603)]\n",
      "2025-01-18 01:43:56.501610: Epoch time: 110.6 s\n",
      "2025-01-18 01:43:57.080271: \n",
      "2025-01-18 01:43:57.080428: Epoch 608\n",
      "2025-01-18 01:43:57.080500: Current learning rate: 0.0043\n",
      "2025-01-18 01:45:47.803421: train_loss -0.8357\n",
      "2025-01-18 01:45:47.803562: val_loss -0.6641\n",
      "2025-01-18 01:45:47.803595: Pseudo dice [np.float32(0.7532)]\n",
      "2025-01-18 01:45:47.803629: Epoch time: 110.72 s\n",
      "2025-01-18 01:45:48.385885: \n",
      "2025-01-18 01:45:48.385973: Epoch 609\n",
      "2025-01-18 01:45:48.386134: Current learning rate: 0.00429\n",
      "2025-01-18 01:47:39.137000: train_loss -0.8377\n",
      "2025-01-18 01:47:39.137137: val_loss -0.6424\n",
      "2025-01-18 01:47:39.137171: Pseudo dice [np.float32(0.7221)]\n",
      "2025-01-18 01:47:39.137207: Epoch time: 110.75 s\n",
      "2025-01-18 01:47:39.994322: \n",
      "2025-01-18 01:47:39.994428: Epoch 610\n",
      "2025-01-18 01:47:39.994507: Current learning rate: 0.00429\n",
      "2025-01-18 01:49:30.719423: train_loss -0.8469\n",
      "2025-01-18 01:49:30.719555: val_loss -0.5815\n",
      "2025-01-18 01:49:30.719596: Pseudo dice [np.float32(0.6732)]\n",
      "2025-01-18 01:49:30.719629: Epoch time: 110.73 s\n",
      "2025-01-18 01:49:31.301750: \n",
      "2025-01-18 01:49:31.301977: Epoch 611\n",
      "2025-01-18 01:49:31.302099: Current learning rate: 0.00428\n",
      "2025-01-18 01:51:22.072088: train_loss -0.8476\n",
      "2025-01-18 01:51:22.072208: val_loss -0.6908\n",
      "2025-01-18 01:51:22.072241: Pseudo dice [np.float32(0.7824)]\n",
      "2025-01-18 01:51:22.072275: Epoch time: 110.77 s\n",
      "2025-01-18 01:51:22.661293: \n",
      "2025-01-18 01:51:22.661389: Epoch 612\n",
      "2025-01-18 01:51:22.661453: Current learning rate: 0.00427\n",
      "2025-01-18 01:53:13.174903: train_loss -0.8555\n",
      "2025-01-18 01:53:13.175015: val_loss -0.7045\n",
      "2025-01-18 01:53:13.175045: Pseudo dice [np.float32(0.737)]\n",
      "2025-01-18 01:53:13.175076: Epoch time: 110.51 s\n",
      "2025-01-18 01:53:13.740592: \n",
      "2025-01-18 01:53:13.740680: Epoch 613\n",
      "Current learning rate: 0.00426\n",
      "2025-01-18 01:55:04.371236: train_loss -0.8425\n",
      "2025-01-18 01:55:04.371359: val_loss -0.6139\n",
      "2025-01-18 01:55:04.371392: Pseudo dice [np.float32(0.6959)]\n",
      "2025-01-18 01:55:04.371423: Epoch time: 110.63 s\n",
      "2025-01-18 01:55:04.939409: \n",
      "2025-01-18 01:55:04.939572: Epoch 614\n",
      "2025-01-18 01:55:04.939636: Current learning rate: 0.00425\n",
      "2025-01-18 01:56:55.562124: train_loss -0.8271\n",
      "2025-01-18 01:56:55.562273: val_loss -0.6405\n",
      "2025-01-18 01:56:55.562349: Pseudo dice [np.float32(0.6914)]\n",
      "2025-01-18 01:56:55.562392: Epoch time: 110.62 s\n",
      "2025-01-18 01:56:56.137873: \n",
      "2025-01-18 01:56:56.138045: Epoch 615\n",
      "2025-01-18 01:56:56.138118: Current learning rate: 0.00424\n",
      "2025-01-18 01:58:46.806072: train_loss -0.8409\n",
      "2025-01-18 01:58:46.806196: val_loss -0.6872\n",
      "2025-01-18 01:58:46.806235: Pseudo dice [np.float32(0.7428)]\n",
      "2025-01-18 01:58:46.806268: Epoch time: 110.67 s\n",
      "2025-01-18 01:58:47.374693: \n",
      "2025-01-18 01:58:47.374781: Epoch 616\n",
      "2025-01-18 01:58:47.374840: Current learning rate: 0.00423\n",
      "2025-01-18 02:00:37.833793: train_loss -0.8401\n",
      "2025-01-18 02:00:37.834100: val_loss -0.6776\n",
      "2025-01-18 02:00:37.834311: Pseudo dice [np.float32(0.8132)]\n",
      "2025-01-18 02:00:37.834362: Epoch time: 110.46 s\n",
      "2025-01-18 02:00:38.406576: \n",
      "2025-01-18 02:00:38.406708: Epoch 617\n",
      "2025-01-18 02:00:38.406768: Current learning rate: 0.00422\n",
      "2025-01-18 02:02:29.046031: train_loss -0.846\n",
      "2025-01-18 02:02:29.046148: val_loss -0.6956\n",
      "2025-01-18 02:02:29.046180: Pseudo dice [np.float32(0.7274)]\n",
      "2025-01-18 02:02:29.046211: Epoch time: 110.64 s\n",
      "2025-01-18 02:02:29.618647: \n",
      "2025-01-18 02:02:29.618815: Epoch 618\n",
      "2025-01-18 02:02:29.618884: Current learning rate: 0.00421\n",
      "2025-01-18 02:04:20.071768: train_loss -0.8431\n",
      "2025-01-18 02:04:20.071886: val_loss -0.6575\n",
      "2025-01-18 02:04:20.071918: Pseudo dice [np.float32(0.725)]\n",
      "2025-01-18 02:04:20.071979: Epoch time: 110.45 s\n",
      "2025-01-18 02:04:20.639541: \n",
      "2025-01-18 02:04:20.639702: Epoch 619\n",
      "2025-01-18 02:04:20.639776: Current learning rate: 0.0042\n",
      "2025-01-18 02:06:11.094507: train_loss -0.8167\n",
      "2025-01-18 02:06:11.094707: val_loss -0.6763\n",
      "2025-01-18 02:06:11.094742: Pseudo dice [np.float32(0.6696)]\n",
      "2025-01-18 02:06:11.094774: Epoch time: 110.46 s\n",
      "2025-01-18 02:06:11.660472: \n",
      "2025-01-18 02:06:11.660631: Epoch 620\n",
      "2025-01-18 02:06:11.660702: Current learning rate: 0.00419\n",
      "2025-01-18 02:08:02.146124: train_loss -0.8343\n",
      "2025-01-18 02:08:02.146250: val_loss -0.7542\n",
      "2025-01-18 02:08:02.146285: Pseudo dice [np.float32(0.742)]\n",
      "2025-01-18 02:08:02.146316: Epoch time: 110.49 s\n",
      "2025-01-18 02:08:02.719897: \n",
      "2025-01-18 02:08:02.720026: Epoch 621\n",
      "2025-01-18 02:08:02.720096: Current learning rate: 0.00418\n",
      "2025-01-18 02:09:53.211800: train_loss -0.8193\n",
      "2025-01-18 02:09:53.211920: val_loss -0.7132\n",
      "2025-01-18 02:09:53.211952: Pseudo dice [np.float32(0.7828)]\n",
      "2025-01-18 02:09:53.211985: Epoch time: 110.49 s\n",
      "2025-01-18 02:09:53.991787: \n",
      "2025-01-18 02:09:53.992125: Epoch 622\n",
      "2025-01-18 02:09:53.992192: Current learning rate: 0.00417\n",
      "2025-01-18 02:11:44.639023: train_loss -0.7958\n",
      "2025-01-18 02:11:44.639160: val_loss -0.5983\n",
      "2025-01-18 02:11:44.639195: Pseudo dice [np.float32(0.6955)]\n",
      "2025-01-18 02:11:44.639241: Epoch time: 110.65 s\n",
      "2025-01-18 02:11:45.208849: \n",
      "2025-01-18 02:11:45.209183: Epoch 623\n",
      "2025-01-18 02:11:45.209256: Current learning rate: 0.00416\n",
      "2025-01-18 02:13:35.676078: train_loss -0.7906\n",
      "2025-01-18 02:13:35.676196: val_loss -0.6743\n",
      "2025-01-18 02:13:35.676229: Pseudo dice [np.float32(0.7725)]\n",
      "2025-01-18 02:13:35.676261: Epoch time: 110.47 s\n",
      "2025-01-18 02:13:36.251019: \n",
      "2025-01-18 02:13:36.251195: Epoch 624\n",
      "2025-01-18 02:13:36.251263: Current learning rate: 0.00415\n",
      "2025-01-18 02:15:26.926081: train_loss -0.7753\n",
      "2025-01-18 02:15:26.926265: val_loss -0.5771\n",
      "2025-01-18 02:15:26.926300: Pseudo dice [np.float32(0.7015)]\n",
      "2025-01-18 02:15:26.926337: Epoch time: 110.68 s\n",
      "2025-01-18 02:15:27.497732: \n",
      "2025-01-18 02:15:27.497822: Epoch 625\n",
      "2025-01-18 02:15:27.497883: Current learning rate: 0.00414\n",
      "2025-01-18 02:17:18.108407: train_loss -0.8182\n",
      "2025-01-18 02:17:18.108536: val_loss -0.7151\n",
      "2025-01-18 02:17:18.108569: Pseudo dice [np.float32(0.7668)]\n",
      "2025-01-18 02:17:18.108600: Epoch time: 110.61 s\n",
      "2025-01-18 02:17:18.675275: \n",
      "2025-01-18 02:17:18.675409: Epoch 626\n",
      "2025-01-18 02:17:18.675479: Current learning rate: 0.00413\n",
      "2025-01-18 02:19:09.061601: train_loss -0.8328\n",
      "2025-01-18 02:19:09.061820: val_loss -0.6792\n",
      "2025-01-18 02:19:09.061892: Pseudo dice [np.float32(0.6958)]\n",
      "2025-01-18 02:19:09.061933: Epoch time: 110.39 s\n",
      "2025-01-18 02:19:09.632054: \n",
      "2025-01-18 02:19:09.632148: Epoch 627\n",
      "2025-01-18 02:19:09.632210: Current learning rate: 0.00412\n",
      "2025-01-18 02:21:00.064356: train_loss -0.8378\n",
      "2025-01-18 02:21:00.064475: val_loss -0.6813\n",
      "2025-01-18 02:21:00.064508: Pseudo dice [np.float32(0.7147)]\n",
      "2025-01-18 02:21:00.064549: Epoch time: 110.43 s\n",
      "2025-01-18 02:21:00.635870: \n",
      "2025-01-18 02:21:00.636051: Epoch 628\n",
      "2025-01-18 02:21:00.636118: Current learning rate: 0.00411\n",
      "2025-01-18 02:22:51.119612: train_loss -0.8329\n",
      "2025-01-18 02:22:51.119757: val_loss -0.6921\n",
      "2025-01-18 02:22:51.119794: Pseudo dice [np.float32(0.7665)]\n",
      "2025-01-18 02:22:51.119828: Epoch time: 110.48 s\n",
      "2025-01-18 02:22:51.681675: \n",
      "2025-01-18 02:22:51.681871: Epoch 629\n",
      "2025-01-18 02:22:51.682019: Current learning rate: 0.0041\n",
      "2025-01-18 02:24:42.163734: train_loss -0.7747\n",
      "2025-01-18 02:24:42.163859: val_loss -0.594\n",
      "2025-01-18 02:24:42.163893: Pseudo dice [np.float32(0.6916)]\n",
      "2025-01-18 02:24:42.163925: Epoch time: 110.48 s\n",
      "2025-01-18 02:24:42.744316: \n",
      "2025-01-18 02:24:42.744458: Epoch 630\n",
      "2025-01-18 02:24:42.744529: Current learning rate: 0.00409\n",
      "2025-01-18 02:26:33.231301: train_loss -0.8093\n",
      "2025-01-18 02:26:33.231424: val_loss -0.6827\n",
      "2025-01-18 02:26:33.231607: Pseudo dice [np.float32(0.7232)]\n",
      "2025-01-18 02:26:33.231707: Epoch time: 110.49 s\n",
      "2025-01-18 02:26:33.805950: \n",
      "2025-01-18 02:26:33.806249: Epoch 631\n",
      "2025-01-18 02:26:33.806365: Current learning rate: 0.00408\n",
      "2025-01-18 02:28:24.268803: train_loss -0.8312\n",
      "2025-01-18 02:28:24.268988: val_loss -0.7141\n",
      "2025-01-18 02:28:24.269020: Pseudo dice [np.float32(0.7674)]\n",
      "2025-01-18 02:28:24.269051: Epoch time: 110.46 s\n",
      "2025-01-18 02:28:24.833215: \n",
      "2025-01-18 02:28:24.833307: Epoch 632\n",
      "2025-01-18 02:28:24.833395: Current learning rate: 0.00407\n",
      "2025-01-18 02:30:15.252377: train_loss -0.8371\n",
      "2025-01-18 02:30:15.252507: val_loss -0.7096\n",
      "2025-01-18 02:30:15.252540: Pseudo dice [np.float32(0.7228)]\n",
      "2025-01-18 02:30:15.252573: Epoch time: 110.42 s\n",
      "2025-01-18 02:30:15.822456: \n",
      "2025-01-18 02:30:15.822625: Epoch 633\n",
      "2025-01-18 02:30:15.822707: Current learning rate: 0.00406\n",
      "2025-01-18 02:32:06.264479: train_loss -0.8341\n",
      "2025-01-18 02:32:06.264610: val_loss -0.6486\n",
      "2025-01-18 02:32:06.264643: Pseudo dice [np.float32(0.7334)]\n",
      "2025-01-18 02:32:06.264675: Epoch time: 110.44 s\n",
      "2025-01-18 02:32:06.841212: \n",
      "2025-01-18 02:32:06.841432: Epoch 634\n",
      "2025-01-18 02:32:06.841508: Current learning rate: 0.00405\n",
      "2025-01-18 02:33:57.502032: train_loss -0.839\n",
      "2025-01-18 02:33:57.502158: val_loss -0.7007\n",
      "2025-01-18 02:33:57.502192: Pseudo dice [np.float32(0.7766)]\n",
      "2025-01-18 02:33:57.502225: Epoch time: 110.66 s\n",
      "2025-01-18 02:33:58.312362: \n",
      "2025-01-18 02:33:58.312532: Epoch 635\n",
      "2025-01-18 02:33:58.312601: Current learning rate: 0.00404\n",
      "2025-01-18 02:35:48.910889: train_loss -0.8248\n",
      "2025-01-18 02:35:48.911020: val_loss -0.6864\n",
      "2025-01-18 02:35:48.911057: Pseudo dice [np.float32(0.8005)]\n",
      "2025-01-18 02:35:48.911092: Epoch time: 110.6 s\n",
      "2025-01-18 02:35:49.490578: \n",
      "2025-01-18 02:35:49.490731: Epoch 636\n",
      "2025-01-18 02:35:49.490798: Current learning rate: 0.00403\n",
      "2025-01-18 02:37:39.948680: train_loss -0.8496\n",
      "2025-01-18 02:37:39.948811: val_loss -0.6175\n",
      "2025-01-18 02:37:39.948844: Pseudo dice [np.float32(0.7483)]\n",
      "2025-01-18 02:37:39.948877: Epoch time: 110.46 s\n",
      "2025-01-18 02:37:40.513117: \n",
      "2025-01-18 02:37:40.513207: Epoch 637\n",
      "2025-01-18 02:37:40.513268: Current learning rate: 0.00402\n",
      "2025-01-18 02:39:31.006902: train_loss -0.7829\n",
      "2025-01-18 02:39:31.007025: val_loss -0.6839\n",
      "2025-01-18 02:39:31.007059: Pseudo dice [np.float32(0.7088)]\n",
      "2025-01-18 02:39:31.007094: Epoch time: 110.49 s\n",
      "2025-01-18 02:39:31.580756: \n",
      "2025-01-18 02:39:31.581014: Epoch 638\n",
      "Current learning rate: 0.00401\n",
      "2025-01-18 02:41:22.037557: train_loss -0.8188\n",
      "2025-01-18 02:41:22.037757: val_loss -0.6435\n",
      "2025-01-18 02:41:22.037801: Pseudo dice [np.float32(0.7172)]\n",
      "2025-01-18 02:41:22.037834: Epoch time: 110.46 s\n",
      "2025-01-18 02:41:22.604128: \n",
      "2025-01-18 02:41:22.604237: Epoch 639\n",
      "2025-01-18 02:41:22.604296: Current learning rate: 0.004\n",
      "2025-01-18 02:43:13.094523: train_loss -0.8289\n",
      "2025-01-18 02:43:13.094632: val_loss -0.7184\n",
      "2025-01-18 02:43:13.094663: Pseudo dice [np.float32(0.7633)]\n",
      "2025-01-18 02:43:13.094700: Epoch time: 110.49 s\n",
      "2025-01-18 02:43:13.666825: \n",
      "2025-01-18 02:43:13.666996: Epoch 640\n",
      "2025-01-18 02:43:13.667065: Current learning rate: 0.00399\n",
      "2025-01-18 02:45:04.262438: train_loss -0.8354\n",
      "2025-01-18 02:45:04.262567: val_loss -0.6538\n",
      "2025-01-18 02:45:04.262603: Pseudo dice [np.float32(0.7181)]\n",
      "2025-01-18 02:45:04.262638: Epoch time: 110.6 s\n",
      "2025-01-18 02:45:04.830307: \n",
      "2025-01-18 02:45:04.830474: Epoch 641\n",
      "2025-01-18 02:45:04.830544: Current learning rate: 0.00398\n",
      "2025-01-18 02:46:55.313367: train_loss -0.7996\n",
      "2025-01-18 02:46:55.313491: val_loss -0.6621\n",
      "2025-01-18 02:46:55.313524: Pseudo dice [np.float32(0.7588)]\n",
      "2025-01-18 02:46:55.313557: Epoch time: 110.48 s\n",
      "2025-01-18 02:46:55.876904: \n",
      "2025-01-18 02:46:55.877005: Epoch 642\n",
      "2025-01-18 02:46:55.877067: Current learning rate: 0.00397\n",
      "2025-01-18 02:48:46.502884: train_loss -0.8446\n",
      "2025-01-18 02:48:46.503003: val_loss -0.6707\n",
      "2025-01-18 02:48:46.503037: Pseudo dice [np.float32(0.7535)]\n",
      "2025-01-18 02:48:46.503071: Epoch time: 110.63 s\n",
      "2025-01-18 02:48:47.073488: \n",
      "2025-01-18 02:48:47.073571: Epoch 643\n",
      "2025-01-18 02:48:47.073630: Current learning rate: 0.00396\n",
      "2025-01-18 02:50:37.550799: train_loss -0.8368\n",
      "2025-01-18 02:50:37.551013: val_loss -0.6534\n",
      "2025-01-18 02:50:37.551057: Pseudo dice [np.float32(0.7747)]\n",
      "2025-01-18 02:50:37.551097: Epoch time: 110.48 s\n",
      "2025-01-18 02:50:38.120049: \n",
      "2025-01-18 02:50:38.120136: Epoch 644\n",
      "2025-01-18 02:50:38.120198: Current learning rate: 0.00395\n",
      "2025-01-18 02:52:28.716035: train_loss -0.8481\n",
      "2025-01-18 02:52:28.716271: val_loss -0.6427\n",
      "2025-01-18 02:52:28.716316: Pseudo dice [np.float32(0.7767)]\n",
      "2025-01-18 02:52:28.716362: Epoch time: 110.6 s\n",
      "2025-01-18 02:52:29.291597: \n",
      "2025-01-18 02:52:29.291811: Epoch 645\n",
      "2025-01-18 02:52:29.291896: Current learning rate: 0.00394\n",
      "2025-01-18 02:54:19.824703: train_loss -0.8603\n",
      "2025-01-18 02:54:19.824828: val_loss -0.7241\n",
      "2025-01-18 02:54:19.824860: Pseudo dice [np.float32(0.7464)]\n",
      "2025-01-18 02:54:19.824900: Epoch time: 110.53 s\n",
      "2025-01-18 02:54:20.410479: \n",
      "2025-01-18 02:54:20.410564: Epoch 646\n",
      "2025-01-18 02:54:20.410625: Current learning rate: 0.00393\n",
      "2025-01-18 02:56:11.139917: train_loss -0.8603\n",
      "2025-01-18 02:56:11.140280: val_loss -0.6724\n",
      "2025-01-18 02:56:11.140566: Pseudo dice [np.float32(0.7788)]\n",
      "2025-01-18 02:56:11.140641: Epoch time: 110.73 s\n",
      "2025-01-18 02:56:11.974477: \n",
      "2025-01-18 02:56:11.974879: Epoch 647\n",
      "2025-01-18 02:56:11.974977: Current learning rate: 0.00392\n",
      "2025-01-18 02:58:02.535421: train_loss -0.8409\n",
      "2025-01-18 02:58:02.535601: val_loss -0.6528\n",
      "2025-01-18 02:58:02.535636: Pseudo dice [np.float32(0.774)]\n",
      "2025-01-18 02:58:02.535668: Epoch time: 110.56 s\n",
      "2025-01-18 02:58:03.124122: \n",
      "2025-01-18 02:58:03.124464: Epoch 648\n",
      "2025-01-18 02:58:03.124614: Current learning rate: 0.00391\n",
      "2025-01-18 02:59:53.875947: train_loss -0.8555\n",
      "2025-01-18 02:59:53.876148: val_loss -0.6201\n",
      "2025-01-18 02:59:53.876184: Pseudo dice [np.float32(0.7325)]\n",
      "2025-01-18 02:59:53.876221: Epoch time: 110.75 s\n",
      "2025-01-18 02:59:54.459064: \n",
      "2025-01-18 02:59:54.459155: Epoch 649\n",
      "2025-01-18 02:59:54.459213: Current learning rate: 0.0039\n",
      "2025-01-18 03:01:45.189506: train_loss -0.8494\n",
      "2025-01-18 03:01:45.189699: val_loss -0.6693\n",
      "2025-01-18 03:01:45.189742: Pseudo dice [np.float32(0.7296)]\n",
      "2025-01-18 03:01:45.189775: Epoch time: 110.73 s\n",
      "2025-01-18 03:01:46.016737: \n",
      "2025-01-18 03:01:46.017116: Epoch 650\n",
      "2025-01-18 03:01:46.017191: Current learning rate: 0.00389\n",
      "2025-01-18 03:03:36.753613: train_loss -0.8465\n",
      "2025-01-18 03:03:36.753752: val_loss -0.6993\n",
      "2025-01-18 03:03:36.753901: Pseudo dice [np.float32(0.7212)]\n",
      "2025-01-18 03:03:36.753970: Epoch time: 110.74 s\n",
      "2025-01-18 03:03:37.339131: \n",
      "2025-01-18 03:03:37.339335: Epoch 651\n",
      "2025-01-18 03:03:37.339409: Current learning rate: 0.00388\n",
      "2025-01-18 03:05:28.091245: train_loss -0.8335\n",
      "2025-01-18 03:05:28.091378: val_loss -0.6659\n",
      "2025-01-18 03:05:28.091412: Pseudo dice [np.float32(0.7517)]\n",
      "2025-01-18 03:05:28.091447: Epoch time: 110.75 s\n",
      "2025-01-18 03:05:28.665611: \n",
      "2025-01-18 03:05:28.665768: Epoch 652\n",
      "2025-01-18 03:05:28.665840: Current learning rate: 0.00387\n",
      "2025-01-18 03:07:19.378369: train_loss -0.8452\n",
      "2025-01-18 03:07:19.378506: val_loss -0.7009\n",
      "2025-01-18 03:07:19.378539: Pseudo dice [np.float32(0.7353)]\n",
      "2025-01-18 03:07:19.378571: Epoch time: 110.71 s\n",
      "2025-01-18 03:07:19.965513: \n",
      "2025-01-18 03:07:19.965722: Epoch 653\n",
      "2025-01-18 03:07:19.965792: Current learning rate: 0.00386\n",
      "2025-01-18 03:09:10.747886: train_loss -0.8322\n",
      "2025-01-18 03:09:10.748280: val_loss -0.6349\n",
      "2025-01-18 03:09:10.748331: Pseudo dice [np.float32(0.7136)]\n",
      "2025-01-18 03:09:10.748379: Epoch time: 110.78 s\n",
      "2025-01-18 03:09:11.350740: \n",
      "2025-01-18 03:09:11.350837: Epoch 654\n",
      "2025-01-18 03:09:11.350897: Current learning rate: 0.00385\n",
      "2025-01-18 03:11:02.095534: train_loss -0.8498\n",
      "2025-01-18 03:11:02.095679: val_loss -0.6798\n",
      "2025-01-18 03:11:02.095712: Pseudo dice [np.float32(0.7189)]\n",
      "2025-01-18 03:11:02.095746: Epoch time: 110.75 s\n",
      "2025-01-18 03:11:02.685021: \n",
      "2025-01-18 03:11:02.685188: Epoch 655\n",
      "2025-01-18 03:11:02.685340: Current learning rate: 0.00384\n",
      "2025-01-18 03:12:53.191400: train_loss -0.8565\n",
      "2025-01-18 03:12:53.191536: val_loss -0.676\n",
      "2025-01-18 03:12:53.191569: Pseudo dice [np.float32(0.7695)]\n",
      "2025-01-18 03:12:53.191610: Epoch time: 110.51 s\n",
      "2025-01-18 03:12:53.775385: \n",
      "2025-01-18 03:12:53.775718: Epoch 656\n",
      "2025-01-18 03:12:53.775798: Current learning rate: 0.00383\n",
      "2025-01-18 03:14:44.479846: train_loss -0.8483\n",
      "2025-01-18 03:14:44.480028: val_loss -0.6759\n",
      "2025-01-18 03:14:44.480061: Pseudo dice [np.float32(0.7942)]\n",
      "2025-01-18 03:14:44.480094: Epoch time: 110.71 s\n",
      "2025-01-18 03:14:45.067367: \n",
      "2025-01-18 03:14:45.067627: Epoch 657\n",
      "2025-01-18 03:14:45.067799: Current learning rate: 0.00382\n",
      "2025-01-18 03:16:35.827566: train_loss -0.8357\n",
      "2025-01-18 03:16:35.827700: val_loss -0.6844\n",
      "2025-01-18 03:16:35.827732: Pseudo dice [np.float32(0.7822)]\n",
      "2025-01-18 03:16:35.827764: Epoch time: 110.76 s\n",
      "2025-01-18 03:16:36.409771: \n",
      "2025-01-18 03:16:36.410147: Epoch 658\n",
      "2025-01-18 03:16:36.410283: Current learning rate: 0.00381\n",
      "2025-01-18 03:18:27.163255: train_loss -0.8379\n",
      "2025-01-18 03:18:27.163398: val_loss -0.6742\n",
      "2025-01-18 03:18:27.163435: Pseudo dice [np.float32(0.7016)]\n",
      "2025-01-18 03:18:27.163469: Epoch time: 110.75 s\n",
      "2025-01-18 03:18:27.746568: \n",
      "2025-01-18 03:18:27.746656: Epoch 659\n",
      "2025-01-18 03:18:27.746718: Current learning rate: 0.0038\n",
      "2025-01-18 03:20:18.308051: train_loss -0.8308\n",
      "2025-01-18 03:20:18.308251: val_loss -0.6563\n",
      "2025-01-18 03:20:18.308327: Pseudo dice [np.float32(0.7444)]\n",
      "2025-01-18 03:20:18.308370: Epoch time: 110.56 s\n",
      "2025-01-18 03:20:19.139461: \n",
      "2025-01-18 03:20:19.139688: Epoch 660\n",
      "2025-01-18 03:20:19.139758: Current learning rate: 0.00379\n",
      "2025-01-18 03:22:09.719320: train_loss -0.8335\n",
      "2025-01-18 03:22:09.719457: val_loss -0.6425\n",
      "2025-01-18 03:22:09.719491: Pseudo dice [np.float32(0.7076)]\n",
      "2025-01-18 03:22:09.719524: Epoch time: 110.58 s\n",
      "2025-01-18 03:22:10.312903: \n",
      "2025-01-18 03:22:10.313156: Epoch 661\n",
      "2025-01-18 03:22:10.313226: Current learning rate: 0.00378\n",
      "2025-01-18 03:24:01.047001: train_loss -0.8265\n",
      "2025-01-18 03:24:01.047228: val_loss -0.5957\n",
      "2025-01-18 03:24:01.047275: Pseudo dice [np.float32(0.796)]\n",
      "2025-01-18 03:24:01.047311: Epoch time: 110.73 s\n",
      "2025-01-18 03:24:01.637461: \n",
      "2025-01-18 03:24:01.637604: Epoch 662\n",
      "2025-01-18 03:24:01.637731: Current learning rate: 0.00377\n",
      "2025-01-18 03:25:52.213461: train_loss -0.8407\n",
      "2025-01-18 03:25:52.213595: val_loss -0.6549\n",
      "2025-01-18 03:25:52.213628: Pseudo dice [np.float32(0.7605)]\n",
      "2025-01-18 03:25:52.213661: Epoch time: 110.58 s\n",
      "2025-01-18 03:25:52.800483: \n",
      "2025-01-18 03:25:52.800586: Epoch 663\n",
      "Current learning rate: 0.00376\n",
      "2025-01-18 03:27:43.371612: train_loss -0.8478\n",
      "2025-01-18 03:27:43.371761: val_loss -0.6744\n",
      "2025-01-18 03:27:43.371830: Pseudo dice [np.float32(0.8128)]\n",
      "2025-01-18 03:27:43.371872: Epoch time: 110.57 s\n",
      "2025-01-18 03:27:43.371896: Yayy! New best EMA pseudo Dice: 0.7554000020027161\n",
      "2025-01-18 03:27:44.225317: \n",
      "2025-01-18 03:27:44.225576: Epoch 664\n",
      "2025-01-18 03:27:44.225679: Current learning rate: 0.00375\n",
      "2025-01-18 03:29:34.957450: train_loss -0.8356\n",
      "2025-01-18 03:29:34.957593: val_loss -0.6326\n",
      "2025-01-18 03:29:34.957627: Pseudo dice [np.float32(0.5869)]\n",
      "2025-01-18 03:29:34.957659: Epoch time: 110.73 s\n",
      "2025-01-18 03:29:35.535594: \n",
      "2025-01-18 03:29:35.535733: Epoch 665\n",
      "2025-01-18 03:29:35.535796: Current learning rate: 0.00374\n",
      "2025-01-18 03:31:26.247437: train_loss -0.8203\n",
      "2025-01-18 03:31:26.247623: val_loss -0.6532\n",
      "2025-01-18 03:31:26.247756: Pseudo dice [np.float32(0.6859)]\n",
      "2025-01-18 03:31:26.247913: Epoch time: 110.71 s\n",
      "2025-01-18 03:31:26.837564: \n",
      "2025-01-18 03:31:26.837642: Epoch 666\n",
      "2025-01-18 03:31:26.837701: Current learning rate: 0.00373\n",
      "2025-01-18 03:33:17.555640: train_loss -0.82\n",
      "2025-01-18 03:33:17.555846: val_loss -0.649\n",
      "2025-01-18 03:33:17.556030: Pseudo dice [np.float32(0.7316)]\n",
      "2025-01-18 03:33:17.556098: Epoch time: 110.72 s\n",
      "2025-01-18 03:33:18.138675: \n",
      "2025-01-18 03:33:18.138772: Epoch 667\n",
      "2025-01-18 03:33:18.138832: Current learning rate: 0.00372\n",
      "2025-01-18 03:35:08.891231: train_loss -0.8404\n",
      "2025-01-18 03:35:08.891354: val_loss -0.6456\n",
      "2025-01-18 03:35:08.891387: Pseudo dice [np.float32(0.7283)]\n",
      "2025-01-18 03:35:08.891419: Epoch time: 110.75 s\n",
      "2025-01-18 03:35:09.487552: \n",
      "2025-01-18 03:35:09.487703: Epoch 668\n",
      "2025-01-18 03:35:09.487767: Current learning rate: 0.00371\n",
      "2025-01-18 03:37:00.180446: train_loss -0.8548\n",
      "2025-01-18 03:37:00.180589: val_loss -0.6592\n",
      "2025-01-18 03:37:00.180623: Pseudo dice [np.float32(0.7085)]\n",
      "2025-01-18 03:37:00.180662: Epoch time: 110.69 s\n",
      "2025-01-18 03:37:00.777840: \n",
      "2025-01-18 03:37:00.778184: Epoch 669\n",
      "2025-01-18 03:37:00.778499: Current learning rate: 0.0037\n",
      "2025-01-18 03:38:51.516909: train_loss -0.8025\n",
      "2025-01-18 03:38:51.517044: val_loss -0.6838\n",
      "2025-01-18 03:38:51.517434: Pseudo dice [np.float32(0.7502)]\n",
      "2025-01-18 03:38:51.517504: Epoch time: 110.74 s\n",
      "2025-01-18 03:38:52.109990: \n",
      "2025-01-18 03:38:52.110193: Epoch 670\n",
      "2025-01-18 03:38:52.110306: Current learning rate: 0.00369\n",
      "2025-01-18 03:40:42.882492: train_loss -0.8175\n",
      "2025-01-18 03:40:42.882660: val_loss -0.7254\n",
      "2025-01-18 03:40:42.882712: Pseudo dice [np.float32(0.7458)]\n",
      "2025-01-18 03:40:42.882767: Epoch time: 110.77 s\n",
      "2025-01-18 03:40:43.699134: \n",
      "2025-01-18 03:40:43.699397: Epoch 671\n",
      "2025-01-18 03:40:43.699521: Current learning rate: 0.00368\n",
      "2025-01-18 03:42:34.331119: train_loss -0.8153\n",
      "2025-01-18 03:42:34.331248: val_loss -0.6364\n",
      "2025-01-18 03:42:34.331286: Pseudo dice [np.float32(0.7119)]\n",
      "2025-01-18 03:42:34.331319: Epoch time: 110.63 s\n",
      "2025-01-18 03:42:34.935452: \n",
      "2025-01-18 03:42:34.935833: Epoch 672\n",
      "2025-01-18 03:42:34.935904: Current learning rate: 0.00367\n",
      "2025-01-18 03:44:25.474141: train_loss -0.8422\n",
      "2025-01-18 03:44:25.474335: val_loss -0.6338\n",
      "2025-01-18 03:44:25.474378: Pseudo dice [np.float32(0.7356)]\n",
      "2025-01-18 03:44:25.474427: Epoch time: 110.54 s\n",
      "2025-01-18 03:44:26.076024: \n",
      "2025-01-18 03:44:26.076299: Epoch 673\n",
      "2025-01-18 03:44:26.076482: Current learning rate: 0.00366\n",
      "2025-01-18 03:46:16.660860: train_loss -0.843\n",
      "2025-01-18 03:46:16.660984: val_loss -0.6423\n",
      "2025-01-18 03:46:16.661018: Pseudo dice [np.float32(0.7427)]\n",
      "2025-01-18 03:46:16.661050: Epoch time: 110.59 s\n",
      "2025-01-18 03:46:17.262480: \n",
      "2025-01-18 03:46:17.262572: Epoch 674\n",
      "2025-01-18 03:46:17.262633: Current learning rate: 0.00365\n",
      "2025-01-18 03:48:08.028029: train_loss -0.8291\n",
      "2025-01-18 03:48:08.028166: val_loss -0.5751\n",
      "2025-01-18 03:48:08.028200: Pseudo dice [np.float32(0.6186)]\n",
      "2025-01-18 03:48:08.028234: Epoch time: 110.77 s\n",
      "2025-01-18 03:48:08.624838: \n",
      "2025-01-18 03:48:08.625104: Epoch 675\n",
      "2025-01-18 03:48:08.625176: Current learning rate: 0.00364\n",
      "2025-01-18 03:49:59.371744: train_loss -0.8294\n",
      "2025-01-18 03:49:59.371875: val_loss -0.6082\n",
      "2025-01-18 03:49:59.372011: Pseudo dice [np.float32(0.7151)]\n",
      "2025-01-18 03:49:59.372126: Epoch time: 110.75 s\n",
      "2025-01-18 03:49:59.962789: \n",
      "2025-01-18 03:49:59.962954: Epoch 676\n",
      "2025-01-18 03:49:59.963019: Current learning rate: 0.00363\n",
      "2025-01-18 03:51:50.729702: train_loss -0.8171\n",
      "2025-01-18 03:51:50.729904: val_loss -0.6818\n",
      "2025-01-18 03:51:50.729946: Pseudo dice [np.float32(0.6863)]\n",
      "2025-01-18 03:51:50.729980: Epoch time: 110.77 s\n",
      "2025-01-18 03:51:51.329254: \n",
      "2025-01-18 03:51:51.329441: Epoch 677\n",
      "2025-01-18 03:51:51.329600: Current learning rate: 0.00362\n",
      "2025-01-18 03:53:41.912528: train_loss -0.8214\n",
      "2025-01-18 03:53:41.912665: val_loss -0.5999\n",
      "2025-01-18 03:53:41.912697: Pseudo dice [np.float32(0.7399)]\n",
      "2025-01-18 03:53:41.912729: Epoch time: 110.58 s\n",
      "2025-01-18 03:53:42.513319: \n",
      "2025-01-18 03:53:42.513505: Epoch 678\n",
      "2025-01-18 03:53:42.513577: Current learning rate: 0.00361\n",
      "2025-01-18 03:55:33.052492: train_loss -0.8427\n",
      "2025-01-18 03:55:33.052632: val_loss -0.6561\n",
      "2025-01-18 03:55:33.052666: Pseudo dice [np.float32(0.7528)]\n",
      "2025-01-18 03:55:33.052699: Epoch time: 110.54 s\n",
      "2025-01-18 03:55:33.642008: \n",
      "2025-01-18 03:55:33.642342: Epoch 679\n",
      "2025-01-18 03:55:33.642586: Current learning rate: 0.0036\n",
      "2025-01-18 03:57:24.195529: train_loss -0.8479\n",
      "2025-01-18 03:57:24.195767: val_loss -0.6624\n",
      "2025-01-18 03:57:24.195810: Pseudo dice [np.float32(0.7815)]\n",
      "2025-01-18 03:57:24.195845: Epoch time: 110.55 s\n",
      "2025-01-18 03:57:24.783927: \n",
      "2025-01-18 03:57:24.784326: Epoch 680\n",
      "2025-01-18 03:57:24.784481: Current learning rate: 0.00359\n",
      "2025-01-18 03:59:15.468276: train_loss -0.8614\n",
      "2025-01-18 03:59:15.468474: val_loss -0.6404\n",
      "2025-01-18 03:59:15.468519: Pseudo dice [np.float32(0.7285)]\n",
      "2025-01-18 03:59:15.468554: Epoch time: 110.68 s\n",
      "2025-01-18 03:59:16.068517: \n",
      "2025-01-18 03:59:16.068889: Epoch 681\n",
      "2025-01-18 03:59:16.069003: Current learning rate: 0.00358\n",
      "2025-01-18 04:01:06.599859: train_loss -0.8482\n",
      "2025-01-18 04:01:06.599995: val_loss -0.6789\n",
      "2025-01-18 04:01:06.600027: Pseudo dice [np.float32(0.6887)]\n",
      "2025-01-18 04:01:06.600060: Epoch time: 110.53 s\n",
      "2025-01-18 04:01:07.190759: \n",
      "2025-01-18 04:01:07.190847: Epoch 682\n",
      "2025-01-18 04:01:07.190907: Current learning rate: 0.00357\n",
      "2025-01-18 04:02:57.968788: train_loss -0.8211\n",
      "2025-01-18 04:02:57.968921: val_loss -0.6536\n",
      "2025-01-18 04:02:57.968956: Pseudo dice [np.float32(0.7174)]\n",
      "2025-01-18 04:02:57.968988: Epoch time: 110.78 s\n",
      "2025-01-18 04:02:58.815517: \n",
      "2025-01-18 04:02:58.815719: Epoch 683\n",
      "2025-01-18 04:02:58.815859: Current learning rate: 0.00356\n",
      "2025-01-18 04:04:49.557148: train_loss -0.8419\n",
      "2025-01-18 04:04:49.557299: val_loss -0.7258\n",
      "2025-01-18 04:04:49.557340: Pseudo dice [np.float32(0.7279)]\n",
      "2025-01-18 04:04:49.557378: Epoch time: 110.74 s\n",
      "2025-01-18 04:04:50.251122: \n",
      "2025-01-18 04:04:50.251543: Epoch 684\n",
      "2025-01-18 04:04:50.251631: Current learning rate: 0.00355\n",
      "2025-01-18 04:06:40.977738: train_loss -0.8568\n",
      "2025-01-18 04:06:40.977864: val_loss -0.7164\n",
      "2025-01-18 04:06:40.978029: Pseudo dice [np.float32(0.7707)]\n",
      "2025-01-18 04:06:40.978109: Epoch time: 110.73 s\n",
      "2025-01-18 04:06:41.570219: \n",
      "2025-01-18 04:06:41.570443: Epoch 685\n",
      "2025-01-18 04:06:41.570625: Current learning rate: 0.00354\n",
      "2025-01-18 04:08:32.150223: train_loss -0.843\n",
      "2025-01-18 04:08:32.150360: val_loss -0.6557\n",
      "2025-01-18 04:08:32.150405: Pseudo dice [np.float32(0.7396)]\n",
      "2025-01-18 04:08:32.150463: Epoch time: 110.58 s\n",
      "2025-01-18 04:08:32.758887: \n",
      "2025-01-18 04:08:32.759035: Epoch 686\n",
      "2025-01-18 04:08:32.759106: Current learning rate: 0.00353\n",
      "2025-01-18 04:10:23.531126: train_loss -0.852\n",
      "2025-01-18 04:10:23.531251: val_loss -0.6634\n",
      "2025-01-18 04:10:23.531284: Pseudo dice [np.float32(0.736)]\n",
      "2025-01-18 04:10:23.531317: Epoch time: 110.77 s\n",
      "2025-01-18 04:10:24.126020: \n",
      "2025-01-18 04:10:24.126191: Epoch 687\n",
      "2025-01-18 04:10:24.126266: Current learning rate: 0.00352\n",
      "2025-01-18 04:12:14.736332: train_loss -0.8393\n",
      "2025-01-18 04:12:14.736686: val_loss -0.6916\n",
      "2025-01-18 04:12:14.736860: Pseudo dice [np.float32(0.7825)]\n",
      "2025-01-18 04:12:14.736940: Epoch time: 110.61 s\n",
      "2025-01-18 04:12:15.338103: \n",
      "2025-01-18 04:12:15.338562: Epoch 688\n",
      "2025-01-18 04:12:15.338659: Current learning rate: 0.00351\n",
      "2025-01-18 04:14:06.089780: train_loss -0.8377\n",
      "2025-01-18 04:14:06.089914: val_loss -0.5882\n",
      "2025-01-18 04:14:06.089947: Pseudo dice [np.float32(0.7724)]\n",
      "2025-01-18 04:14:06.089986: Epoch time: 110.75 s\n",
      "2025-01-18 04:14:06.681542: \n",
      "2025-01-18 04:14:06.681889: Epoch 689\n",
      "2025-01-18 04:14:06.682027: Current learning rate: 0.0035\n",
      "2025-01-18 04:15:57.455947: train_loss -0.8428\n",
      "2025-01-18 04:15:57.456084: val_loss -0.6759\n",
      "2025-01-18 04:15:57.456120: Pseudo dice [np.float32(0.736)]\n",
      "2025-01-18 04:15:57.456152: Epoch time: 110.77 s\n",
      "2025-01-18 04:15:58.042289: \n",
      "2025-01-18 04:15:58.042451: Epoch 690\n",
      "2025-01-18 04:15:58.042524: Current learning rate: 0.00349\n",
      "2025-01-18 04:17:48.752760: train_loss -0.8475\n",
      "2025-01-18 04:17:48.752884: val_loss -0.7069\n",
      "2025-01-18 04:17:48.752919: Pseudo dice [np.float32(0.756)]\n",
      "2025-01-18 04:17:48.752954: Epoch time: 110.71 s\n",
      "2025-01-18 04:17:49.343407: \n",
      "2025-01-18 04:17:49.343809: Epoch 691\n",
      "2025-01-18 04:17:49.344035: Current learning rate: 0.00348\n",
      "2025-01-18 04:19:40.101082: train_loss -0.8615\n",
      "2025-01-18 04:19:40.101438: val_loss -0.6535\n",
      "2025-01-18 04:19:40.101507: Pseudo dice [np.float32(0.7756)]\n",
      "2025-01-18 04:19:40.101549: Epoch time: 110.76 s\n",
      "2025-01-18 04:19:40.703381: \n",
      "2025-01-18 04:19:40.703840: Epoch 692\n",
      "2025-01-18 04:19:40.703935: Current learning rate: 0.00346\n",
      "2025-01-18 04:21:31.244785: train_loss -0.8594\n",
      "2025-01-18 04:21:31.244910: val_loss -0.6428\n",
      "2025-01-18 04:21:31.244944: Pseudo dice [np.float32(0.7263)]\n",
      "2025-01-18 04:21:31.244977: Epoch time: 110.54 s\n",
      "2025-01-18 04:21:31.839692: \n",
      "2025-01-18 04:21:31.840130: Epoch 693\n",
      "2025-01-18 04:21:31.840262: Current learning rate: 0.00345\n",
      "2025-01-18 04:23:22.460573: train_loss -0.847\n",
      "2025-01-18 04:23:22.460808: val_loss -0.6434\n",
      "2025-01-18 04:23:22.460857: Pseudo dice [np.float32(0.7786)]\n",
      "2025-01-18 04:23:22.460891: Epoch time: 110.62 s\n",
      "2025-01-18 04:23:23.055367: \n",
      "2025-01-18 04:23:23.055707: Epoch 694\n",
      "2025-01-18 04:23:23.055771: Current learning rate: 0.00344\n",
      "2025-01-18 04:25:13.803869: train_loss -0.8551\n",
      "2025-01-18 04:25:13.804079: val_loss -0.6576\n",
      "2025-01-18 04:25:13.804114: Pseudo dice [np.float32(0.8053)]\n",
      "2025-01-18 04:25:13.804148: Epoch time: 110.75 s\n",
      "2025-01-18 04:25:14.642905: \n",
      "2025-01-18 04:25:14.643014: Epoch 695\n",
      "2025-01-18 04:25:14.643080: Current learning rate: 0.00343\n",
      "2025-01-18 04:27:05.229102: train_loss -0.8519\n",
      "2025-01-18 04:27:05.229438: val_loss -0.5918\n",
      "2025-01-18 04:27:05.229501: Pseudo dice [np.float32(0.7177)]\n",
      "2025-01-18 04:27:05.229553: Epoch time: 110.59 s\n",
      "2025-01-18 04:27:05.829201: \n",
      "2025-01-18 04:27:05.829294: Epoch 696\n",
      "2025-01-18 04:27:05.829355: Current learning rate: 0.00342\n",
      "2025-01-18 04:28:56.449236: train_loss -0.8501\n",
      "2025-01-18 04:28:56.449443: val_loss -0.6911\n",
      "2025-01-18 04:28:56.449476: Pseudo dice [np.float32(0.6712)]\n",
      "2025-01-18 04:28:56.449508: Epoch time: 110.62 s\n",
      "2025-01-18 04:28:57.044429: \n",
      "2025-01-18 04:28:57.044618: Epoch 697\n",
      "2025-01-18 04:28:57.044692: Current learning rate: 0.00341\n",
      "2025-01-18 04:30:47.772206: train_loss -0.8573\n",
      "2025-01-18 04:30:47.772353: val_loss -0.5833\n",
      "2025-01-18 04:30:47.772389: Pseudo dice [np.float32(0.7129)]\n",
      "2025-01-18 04:30:47.772421: Epoch time: 110.73 s\n",
      "2025-01-18 04:30:48.378911: \n",
      "2025-01-18 04:30:48.379277: Epoch 698\n",
      "2025-01-18 04:30:48.379409: Current learning rate: 0.0034\n",
      "2025-01-18 04:32:39.108875: train_loss -0.8379\n",
      "2025-01-18 04:32:39.109010: val_loss -0.6785\n",
      "2025-01-18 04:32:39.109044: Pseudo dice [np.float32(0.7603)]\n",
      "2025-01-18 04:32:39.109077: Epoch time: 110.73 s\n",
      "2025-01-18 04:32:39.702633: \n",
      "2025-01-18 04:32:39.702758: Epoch 699\n",
      "2025-01-18 04:32:39.702831: Current learning rate: 0.00339\n",
      "2025-01-18 04:34:30.325809: train_loss -0.8232\n",
      "2025-01-18 04:34:30.325947: val_loss -0.6136\n",
      "2025-01-18 04:34:30.326093: Pseudo dice [np.float32(0.7015)]\n",
      "2025-01-18 04:34:30.326161: Epoch time: 110.62 s\n",
      "2025-01-18 04:34:31.166365: \n",
      "2025-01-18 04:34:31.166606: Epoch 700\n",
      "2025-01-18 04:34:31.166751: Current learning rate: 0.00338\n",
      "2025-01-18 04:36:21.905121: train_loss -0.8444\n",
      "2025-01-18 04:36:21.905259: val_loss -0.6275\n",
      "2025-01-18 04:36:21.905296: Pseudo dice [np.float32(0.7224)]\n",
      "2025-01-18 04:36:21.905328: Epoch time: 110.74 s\n",
      "2025-01-18 04:36:22.501304: \n",
      "2025-01-18 04:36:22.501627: Epoch 701\n",
      "2025-01-18 04:36:22.501778: Current learning rate: 0.00337\n",
      "2025-01-18 04:38:13.140836: train_loss -0.8481\n",
      "2025-01-18 04:38:13.140963: val_loss -0.672\n",
      "2025-01-18 04:38:13.140994: Pseudo dice [np.float32(0.7147)]\n",
      "2025-01-18 04:38:13.141026: Epoch time: 110.64 s\n",
      "2025-01-18 04:38:13.727340: \n",
      "2025-01-18 04:38:13.727483: Epoch 702\n",
      "2025-01-18 04:38:13.727585: Current learning rate: 0.00336\n",
      "2025-01-18 04:40:04.501072: train_loss -0.8499\n",
      "2025-01-18 04:40:04.501255: val_loss -0.7041\n",
      "2025-01-18 04:40:04.501303: Pseudo dice [np.float32(0.7312)]\n",
      "2025-01-18 04:40:04.501341: Epoch time: 110.77 s\n",
      "2025-01-18 04:40:05.102912: \n",
      "2025-01-18 04:40:05.103204: Epoch 703\n",
      "2025-01-18 04:40:05.103424: Current learning rate: 0.00335\n",
      "2025-01-18 04:41:55.667427: train_loss -0.8533\n",
      "2025-01-18 04:41:55.667571: val_loss -0.697\n",
      "2025-01-18 04:41:55.667639: Pseudo dice [np.float32(0.7254)]\n",
      "2025-01-18 04:41:55.667678: Epoch time: 110.57 s\n",
      "2025-01-18 04:41:56.267781: \n",
      "2025-01-18 04:41:56.267865: Epoch 704\n",
      "2025-01-18 04:41:56.267925: Current learning rate: 0.00334\n",
      "2025-01-18 04:43:46.989351: train_loss -0.8735\n",
      "2025-01-18 04:43:46.989611: val_loss -0.6563\n",
      "2025-01-18 04:43:46.989651: Pseudo dice [np.float32(0.7083)]\n",
      "2025-01-18 04:43:46.989685: Epoch time: 110.72 s\n",
      "2025-01-18 04:43:47.593722: \n",
      "2025-01-18 04:43:47.594043: Epoch 705\n",
      "2025-01-18 04:43:47.594115: Current learning rate: 0.00333\n",
      "2025-01-18 04:45:38.318914: train_loss -0.862\n",
      "2025-01-18 04:45:38.319048: val_loss -0.6302\n",
      "2025-01-18 04:45:38.319082: Pseudo dice [np.float32(0.7314)]\n",
      "2025-01-18 04:45:38.319114: Epoch time: 110.73 s\n",
      "2025-01-18 04:45:38.915655: \n",
      "2025-01-18 04:45:38.915971: Epoch 706\n",
      "2025-01-18 04:45:38.916102: Current learning rate: 0.00332\n",
      "2025-01-18 04:47:29.686399: train_loss -0.8596\n",
      "2025-01-18 04:47:29.686607: val_loss -0.6694\n",
      "2025-01-18 04:47:29.686651: Pseudo dice [np.float32(0.7302)]\n",
      "2025-01-18 04:47:29.686684: Epoch time: 110.77 s\n",
      "2025-01-18 04:47:30.505508: \n",
      "2025-01-18 04:47:30.505902: Epoch 707\n",
      "2025-01-18 04:47:30.505973: Current learning rate: 0.00331\n",
      "2025-01-18 04:49:21.060855: train_loss -0.845\n",
      "2025-01-18 04:49:21.060993: val_loss -0.6443\n",
      "2025-01-18 04:49:21.061031: Pseudo dice [np.float32(0.7247)]\n",
      "2025-01-18 04:49:21.061066: Epoch time: 110.56 s\n",
      "2025-01-18 04:49:21.657325: \n",
      "2025-01-18 04:49:21.657426: Epoch 708\n",
      "2025-01-18 04:49:21.657489: Current learning rate: 0.0033\n",
      "2025-01-18 04:51:12.266793: train_loss -0.8499\n",
      "2025-01-18 04:51:12.266937: val_loss -0.6328\n",
      "2025-01-18 04:51:12.266972: Pseudo dice [np.float32(0.693)]\n",
      "2025-01-18 04:51:12.267004: Epoch time: 110.61 s\n",
      "2025-01-18 04:51:12.861743: \n",
      "2025-01-18 04:51:12.861869: Epoch 709\n",
      "2025-01-18 04:51:12.862068: Current learning rate: 0.00329\n",
      "2025-01-18 04:53:03.636281: train_loss -0.8558\n",
      "2025-01-18 04:53:03.636414: val_loss -0.6548\n",
      "2025-01-18 04:53:03.636499: Pseudo dice [np.float32(0.6814)]\n",
      "2025-01-18 04:53:03.636568: Epoch time: 110.78 s\n",
      "2025-01-18 04:53:04.232260: \n",
      "2025-01-18 04:53:04.232442: Epoch 710\n",
      "2025-01-18 04:53:04.232516: Current learning rate: 0.00328\n",
      "2025-01-18 04:54:54.967559: train_loss -0.8513\n",
      "2025-01-18 04:54:54.967687: val_loss -0.6687\n",
      "2025-01-18 04:54:54.967720: Pseudo dice [np.float32(0.7768)]\n",
      "2025-01-18 04:54:54.967752: Epoch time: 110.74 s\n",
      "2025-01-18 04:54:55.570052: \n",
      "2025-01-18 04:54:55.570250: Epoch 711\n",
      "2025-01-18 04:54:55.570386: Current learning rate: 0.00327\n",
      "2025-01-18 04:56:46.315372: train_loss -0.8247\n",
      "2025-01-18 04:56:46.315505: val_loss -0.6568\n",
      "2025-01-18 04:56:46.315539: Pseudo dice [np.float32(0.7444)]\n",
      "2025-01-18 04:56:46.315573: Epoch time: 110.75 s\n",
      "2025-01-18 04:56:46.916266: \n",
      "2025-01-18 04:56:46.916365: Epoch 712\n",
      "2025-01-18 04:56:46.916428: Current learning rate: 0.00326\n",
      "2025-01-18 04:58:37.655410: train_loss -0.86\n",
      "2025-01-18 04:58:37.655555: val_loss -0.6667\n",
      "2025-01-18 04:58:37.655594: Pseudo dice [np.float32(0.7591)]\n",
      "2025-01-18 04:58:37.655628: Epoch time: 110.74 s\n",
      "2025-01-18 04:58:38.250517: \n",
      "2025-01-18 04:58:38.250771: Epoch 713\n",
      "2025-01-18 04:58:38.250926: Current learning rate: 0.00325\n",
      "2025-01-18 05:00:29.028845: train_loss -0.8546\n",
      "2025-01-18 05:00:29.029001: val_loss -0.7216\n",
      "2025-01-18 05:00:29.029037: Pseudo dice [np.float32(0.7278)]\n",
      "2025-01-18 05:00:29.029070: Epoch time: 110.78 s\n",
      "2025-01-18 05:00:29.642704: \n",
      "2025-01-18 05:00:29.642847: Epoch 714\n",
      "2025-01-18 05:00:29.642918: Current learning rate: 0.00324\n",
      "2025-01-18 05:02:20.369453: train_loss -0.8452\n",
      "2025-01-18 05:02:20.369583: val_loss -0.6142\n",
      "2025-01-18 05:02:20.369616: Pseudo dice [np.float32(0.676)]\n",
      "2025-01-18 05:02:20.369649: Epoch time: 110.73 s\n",
      "2025-01-18 05:02:20.973570: \n",
      "2025-01-18 05:02:20.973667: Epoch 715\n",
      "2025-01-18 05:02:20.973729: Current learning rate: 0.00323\n",
      "2025-01-18 05:04:11.777304: train_loss -0.8513\n",
      "2025-01-18 05:04:11.777825: val_loss -0.6546\n",
      "2025-01-18 05:04:11.777965: Pseudo dice [np.float32(0.7566)]\n",
      "2025-01-18 05:04:11.778018: Epoch time: 110.8 s\n",
      "2025-01-18 05:04:12.378523: \n",
      "2025-01-18 05:04:12.378727: Epoch 716\n",
      "2025-01-18 05:04:12.378828: Current learning rate: 0.00322\n",
      "2025-01-18 05:06:02.983440: train_loss -0.836\n",
      "2025-01-18 05:06:02.983930: val_loss -0.6518\n",
      "2025-01-18 05:06:02.984003: Pseudo dice [np.float32(0.766)]\n",
      "2025-01-18 05:06:02.984044: Epoch time: 110.61 s\n",
      "2025-01-18 05:06:03.587204: \n",
      "2025-01-18 05:06:03.587381: Epoch 717\n",
      "2025-01-18 05:06:03.587453: Current learning rate: 0.00321\n",
      "2025-01-18 05:07:54.137351: train_loss -0.8475\n",
      "2025-01-18 05:07:54.137500: val_loss -0.7117\n",
      "2025-01-18 05:07:54.137539: Pseudo dice [np.float32(0.7682)]\n",
      "2025-01-18 05:07:54.137574: Epoch time: 110.55 s\n",
      "2025-01-18 05:07:54.742530: \n",
      "2025-01-18 05:07:54.742872: Epoch 718\n",
      "2025-01-18 05:07:54.742951: Current learning rate: 0.0032\n",
      "2025-01-18 05:09:45.450746: train_loss -0.846\n",
      "2025-01-18 05:09:45.450920: val_loss -0.7096\n",
      "2025-01-18 05:09:45.450992: Pseudo dice [np.float32(0.7623)]\n",
      "2025-01-18 05:09:45.451034: Epoch time: 110.71 s\n",
      "2025-01-18 05:09:46.318910: \n",
      "2025-01-18 05:09:46.319016: Epoch 719\n",
      "2025-01-18 05:09:46.319098: Current learning rate: 0.00319\n",
      "2025-01-18 05:11:37.120921: train_loss -0.8286\n",
      "2025-01-18 05:11:37.121085: val_loss -0.7368\n",
      "2025-01-18 05:11:37.121150: Pseudo dice [np.float32(0.7949)]\n",
      "2025-01-18 05:11:37.121193: Epoch time: 110.8 s\n",
      "2025-01-18 05:11:37.716726: \n",
      "2025-01-18 05:11:37.717125: Epoch 720\n",
      "2025-01-18 05:11:37.717219: Current learning rate: 0.00318\n",
      "2025-01-18 05:13:28.264447: train_loss -0.8244\n",
      "2025-01-18 05:13:28.264801: val_loss -0.6908\n",
      "2025-01-18 05:13:28.264839: Pseudo dice [np.float32(0.771)]\n",
      "2025-01-18 05:13:28.264872: Epoch time: 110.55 s\n",
      "2025-01-18 05:13:28.860359: \n",
      "2025-01-18 05:13:28.860636: Epoch 721\n",
      "2025-01-18 05:13:28.860708: Current learning rate: 0.00317\n",
      "2025-01-18 05:15:19.637861: train_loss -0.8347\n",
      "2025-01-18 05:15:19.637980: val_loss -0.6819\n",
      "2025-01-18 05:15:19.638010: Pseudo dice [np.float32(0.7525)]\n",
      "2025-01-18 05:15:19.638042: Epoch time: 110.78 s\n",
      "2025-01-18 05:15:20.237340: \n",
      "2025-01-18 05:15:20.237464: Epoch 722\n",
      "2025-01-18 05:15:20.237536: Current learning rate: 0.00316\n",
      "2025-01-18 05:17:10.839752: train_loss -0.795\n",
      "2025-01-18 05:17:10.839890: val_loss -0.6765\n",
      "2025-01-18 05:17:10.839924: Pseudo dice [np.float32(0.8253)]\n",
      "2025-01-18 05:17:10.839957: Epoch time: 110.6 s\n",
      "2025-01-18 05:17:11.441175: \n",
      "2025-01-18 05:17:11.441305: Epoch 723\n",
      "2025-01-18 05:17:11.441378: Current learning rate: 0.00315\n",
      "2025-01-18 05:19:02.133386: train_loss -0.8493\n",
      "2025-01-18 05:19:02.133508: val_loss -0.6874\n",
      "2025-01-18 05:19:02.133541: Pseudo dice [np.float32(0.7859)]\n",
      "2025-01-18 05:19:02.133574: Epoch time: 110.69 s\n",
      "2025-01-18 05:19:02.133594: Yayy! New best EMA pseudo Dice: 0.758400022983551\n",
      "2025-01-18 05:19:02.957675: \n",
      "2025-01-18 05:19:02.958068: Epoch 724\n",
      "2025-01-18 05:19:02.958244: Current learning rate: 0.00314\n",
      "2025-01-18 05:20:53.555708: train_loss -0.8456\n",
      "2025-01-18 05:20:53.556121: val_loss -0.7041\n",
      "2025-01-18 05:20:53.556194: Pseudo dice [np.float32(0.7454)]\n",
      "2025-01-18 05:20:53.556234: Epoch time: 110.6 s\n",
      "2025-01-18 05:20:54.161242: \n",
      "2025-01-18 05:20:54.161345: Epoch 725\n",
      "2025-01-18 05:20:54.161406: Current learning rate: 0.00313\n",
      "2025-01-18 05:22:44.927549: train_loss -0.8603\n",
      "2025-01-18 05:22:44.928022: val_loss -0.6899\n",
      "2025-01-18 05:22:44.928063: Pseudo dice [np.float32(0.7291)]\n",
      "2025-01-18 05:22:44.928097: Epoch time: 110.77 s\n",
      "2025-01-18 05:22:45.537628: \n",
      "2025-01-18 05:22:45.537934: Epoch 726\n",
      "2025-01-18 05:22:45.538002: Current learning rate: 0.00312\n",
      "2025-01-18 05:24:36.241127: train_loss -0.8701\n",
      "2025-01-18 05:24:36.241252: val_loss -0.6582\n",
      "2025-01-18 05:24:36.241284: Pseudo dice [np.float32(0.7764)]\n",
      "2025-01-18 05:24:36.241315: Epoch time: 110.7 s\n",
      "2025-01-18 05:24:36.837100: \n",
      "2025-01-18 05:24:36.837194: Epoch 727\n",
      "2025-01-18 05:24:36.837255: Current learning rate: 0.00311\n",
      "2025-01-18 05:26:27.383711: train_loss -0.857\n",
      "2025-01-18 05:26:27.383849: val_loss -0.7229\n",
      "2025-01-18 05:26:27.383882: Pseudo dice [np.float32(0.7806)]\n",
      "2025-01-18 05:26:27.383915: Epoch time: 110.55 s\n",
      "2025-01-18 05:26:27.383936: Yayy! New best EMA pseudo Dice: 0.758899986743927\n",
      "2025-01-18 05:26:28.210283: \n",
      "2025-01-18 05:26:28.210378: Epoch 728\n",
      "2025-01-18 05:26:28.210439: Current learning rate: 0.0031\n",
      "2025-01-18 05:28:18.913192: train_loss -0.8387\n",
      "2025-01-18 05:28:18.913380: val_loss -0.6547\n",
      "2025-01-18 05:28:18.913415: Pseudo dice [np.float32(0.7254)]\n",
      "2025-01-18 05:28:18.913449: Epoch time: 110.7 s\n",
      "2025-01-18 05:28:19.507265: \n",
      "2025-01-18 05:28:19.507630: Epoch 729\n",
      "2025-01-18 05:28:19.507783: Current learning rate: 0.00309\n",
      "2025-01-18 05:30:10.260261: train_loss -0.8338\n",
      "2025-01-18 05:30:10.260389: val_loss -0.7152\n",
      "2025-01-18 05:30:10.260422: Pseudo dice [np.float32(0.8178)]\n",
      "2025-01-18 05:30:10.260460: Epoch time: 110.75 s\n",
      "2025-01-18 05:30:10.260486: Yayy! New best EMA pseudo Dice: 0.7617999911308289\n",
      "2025-01-18 05:30:11.091664: \n",
      "2025-01-18 05:30:11.091979: Epoch 730\n",
      "2025-01-18 05:30:11.092116: Current learning rate: 0.00308\n",
      "2025-01-18 05:32:01.657203: train_loss -0.8602\n",
      "2025-01-18 05:32:01.657334: val_loss -0.6606\n",
      "2025-01-18 05:32:01.657366: Pseudo dice [np.float32(0.6597)]\n",
      "2025-01-18 05:32:01.657400: Epoch time: 110.57 s\n",
      "2025-01-18 05:32:02.491157: \n",
      "2025-01-18 05:32:02.491279: Epoch 731\n",
      "2025-01-18 05:32:02.491342: Current learning rate: 0.00307\n",
      "2025-01-18 05:33:53.217907: train_loss -0.8639\n",
      "2025-01-18 05:33:53.218046: val_loss -0.6626\n",
      "2025-01-18 05:33:53.218079: Pseudo dice [np.float32(0.7905)]\n",
      "2025-01-18 05:33:53.218112: Epoch time: 110.73 s\n",
      "2025-01-18 05:33:53.818593: \n",
      "2025-01-18 05:33:53.818694: Epoch 732\n",
      "2025-01-18 05:33:53.818778: Current learning rate: 0.00306\n",
      "2025-01-18 05:35:44.515197: train_loss -0.8647\n",
      "2025-01-18 05:35:44.515338: val_loss -0.727\n",
      "2025-01-18 05:35:44.515371: Pseudo dice [np.float32(0.7851)]\n",
      "2025-01-18 05:35:44.515404: Epoch time: 110.7 s\n",
      "2025-01-18 05:35:45.113879: \n",
      "2025-01-18 05:35:45.114061: Epoch 733\n",
      "2025-01-18 05:35:45.114130: Current learning rate: 0.00305\n",
      "2025-01-18 05:37:35.827936: train_loss -0.8398\n",
      "2025-01-18 05:37:35.828085: val_loss -0.6622\n",
      "2025-01-18 05:37:35.828115: Pseudo dice [np.float32(0.7399)]\n",
      "2025-01-18 05:37:35.828147: Epoch time: 110.71 s\n",
      "2025-01-18 05:37:36.428652: \n",
      "2025-01-18 05:37:36.428763: Epoch 734\n",
      "2025-01-18 05:37:36.428830: Current learning rate: 0.00304\n",
      "2025-01-18 05:39:27.183108: train_loss -0.8535\n",
      "2025-01-18 05:39:27.183249: val_loss -0.7093\n",
      "2025-01-18 05:39:27.183283: Pseudo dice [np.float32(0.7871)]\n",
      "2025-01-18 05:39:27.183314: Epoch time: 110.76 s\n",
      "2025-01-18 05:39:27.788019: \n",
      "2025-01-18 05:39:27.788130: Epoch 735\n",
      "2025-01-18 05:39:27.788198: Current learning rate: 0.00303\n",
      "2025-01-18 05:41:18.408921: train_loss -0.8624\n",
      "2025-01-18 05:41:18.409276: val_loss -0.7351\n",
      "2025-01-18 05:41:18.409382: Pseudo dice [np.float32(0.7779)]\n",
      "2025-01-18 05:41:18.409423: Epoch time: 110.62 s\n",
      "2025-01-18 05:41:19.017282: \n",
      "2025-01-18 05:41:19.017469: Epoch 736\n",
      "2025-01-18 05:41:19.017641: Current learning rate: 0.00302\n",
      "2025-01-18 05:43:09.754118: train_loss -0.8588\n",
      "2025-01-18 05:43:09.754271: val_loss -0.6708\n",
      "2025-01-18 05:43:09.754303: Pseudo dice [np.float32(0.7829)]\n",
      "2025-01-18 05:43:09.754334: Epoch time: 110.74 s\n",
      "2025-01-18 05:43:09.754354: Yayy! New best EMA pseudo Dice: 0.7635999917984009\n",
      "2025-01-18 05:43:10.583704: \n",
      "2025-01-18 05:43:10.584044: Epoch 737\n",
      "2025-01-18 05:43:10.584231: Current learning rate: 0.00301\n",
      "2025-01-18 05:45:01.332043: train_loss -0.8619\n",
      "2025-01-18 05:45:01.332168: val_loss -0.7377\n",
      "2025-01-18 05:45:01.332202: Pseudo dice [np.float32(0.8053)]\n",
      "2025-01-18 05:45:01.332236: Epoch time: 110.75 s\n",
      "2025-01-18 05:45:01.332257: Yayy! New best EMA pseudo Dice: 0.767799973487854\n",
      "2025-01-18 05:45:02.172516: \n",
      "2025-01-18 05:45:02.172621: Epoch 738\n",
      "2025-01-18 05:45:02.172686: Current learning rate: 0.003\n",
      "2025-01-18 05:46:52.797631: train_loss -0.8645\n",
      "2025-01-18 05:46:52.797761: val_loss -0.7365\n",
      "2025-01-18 05:46:52.797794: Pseudo dice [np.float32(0.7838)]\n",
      "2025-01-18 05:46:52.797850: Epoch time: 110.63 s\n",
      "2025-01-18 05:46:52.797978: Yayy! New best EMA pseudo Dice: 0.7694000005722046\n",
      "2025-01-18 05:46:53.632788: \n",
      "2025-01-18 05:46:53.632879: Epoch 739\n",
      "2025-01-18 05:46:53.632940: Current learning rate: 0.00299\n",
      "2025-01-18 05:48:44.390861: train_loss -0.864\n",
      "2025-01-18 05:48:44.391076: val_loss -0.6785\n",
      "2025-01-18 05:48:44.391150: Pseudo dice [np.float32(0.7511)]\n",
      "2025-01-18 05:48:44.391193: Epoch time: 110.76 s\n",
      "2025-01-18 05:48:44.992082: \n",
      "2025-01-18 05:48:44.992492: Epoch 740\n",
      "2025-01-18 05:48:44.992591: Current learning rate: 0.00297\n",
      "2025-01-18 05:50:35.789053: train_loss -0.8682\n",
      "2025-01-18 05:50:35.789365: val_loss -0.6607\n",
      "2025-01-18 05:50:35.789409: Pseudo dice [np.float32(0.7695)]\n",
      "2025-01-18 05:50:35.789443: Epoch time: 110.8 s\n",
      "2025-01-18 05:50:36.381657: \n",
      "2025-01-18 05:50:36.381759: Epoch 741\n",
      "2025-01-18 05:50:36.381824: Current learning rate: 0.00296\n",
      "2025-01-18 05:52:27.163025: train_loss -0.8699\n",
      "2025-01-18 05:52:27.163158: val_loss -0.6545\n",
      "2025-01-18 05:52:27.163189: Pseudo dice [np.float32(0.8009)]\n",
      "2025-01-18 05:52:27.163223: Epoch time: 110.78 s\n",
      "2025-01-18 05:52:27.163243: Yayy! New best EMA pseudo Dice: 0.7710999846458435\n",
      "2025-01-18 05:52:28.223513: \n",
      "2025-01-18 05:52:28.223878: Epoch 742\n",
      "2025-01-18 05:52:28.224022: Current learning rate: 0.00295\n",
      "2025-01-18 05:54:18.797562: train_loss -0.8619\n",
      "2025-01-18 05:54:18.797689: val_loss -0.6727\n",
      "2025-01-18 05:54:18.797723: Pseudo dice [np.float32(0.7824)]\n",
      "2025-01-18 05:54:18.797756: Epoch time: 110.57 s\n",
      "2025-01-18 05:54:18.797777: Yayy! New best EMA pseudo Dice: 0.7721999883651733\n",
      "2025-01-18 05:54:19.635679: \n",
      "2025-01-18 05:54:19.635986: Epoch 743\n",
      "2025-01-18 05:54:19.636115: Current learning rate: 0.00294\n",
      "2025-01-18 05:56:10.380704: train_loss -0.8471\n",
      "2025-01-18 05:56:10.380843: val_loss -0.672\n",
      "2025-01-18 05:56:10.380877: Pseudo dice [np.float32(0.7643)]\n",
      "2025-01-18 05:56:10.380911: Epoch time: 110.75 s\n",
      "2025-01-18 05:56:10.985255: \n",
      "2025-01-18 05:56:10.985382: Epoch 744\n",
      "2025-01-18 05:56:10.985450: Current learning rate: 0.00293\n",
      "2025-01-18 05:58:01.756399: train_loss -0.8544\n",
      "2025-01-18 05:58:01.756734: val_loss -0.6652\n",
      "2025-01-18 05:58:01.756784: Pseudo dice [np.float32(0.7038)]\n",
      "2025-01-18 05:58:01.756824: Epoch time: 110.77 s\n",
      "2025-01-18 05:58:02.360379: \n",
      "2025-01-18 05:58:02.360613: Epoch 745\n",
      "2025-01-18 05:58:02.360696: Current learning rate: 0.00292\n",
      "2025-01-18 05:59:53.147084: train_loss -0.8664\n",
      "2025-01-18 05:59:53.147326: val_loss -0.651\n",
      "2025-01-18 05:59:53.147363: Pseudo dice [np.float32(0.7463)]\n",
      "2025-01-18 05:59:53.147399: Epoch time: 110.79 s\n",
      "2025-01-18 05:59:53.750219: \n",
      "2025-01-18 05:59:53.750601: Epoch 746\n",
      "2025-01-18 05:59:53.750756: Current learning rate: 0.00291\n",
      "2025-01-18 06:01:44.351093: train_loss -0.8587\n",
      "2025-01-18 06:01:44.351239: val_loss -0.6663\n",
      "2025-01-18 06:01:44.351273: Pseudo dice [np.float32(0.7413)]\n",
      "2025-01-18 06:01:44.351307: Epoch time: 110.6 s\n",
      "2025-01-18 06:01:44.942430: \n",
      "2025-01-18 06:01:44.942803: Epoch 747\n",
      "2025-01-18 06:01:44.942876: Current learning rate: 0.0029\n",
      "2025-01-18 06:03:35.601184: train_loss -0.8685\n",
      "2025-01-18 06:03:35.601321: val_loss -0.6228\n",
      "2025-01-18 06:03:35.601421: Pseudo dice [np.float32(0.7878)]\n",
      "2025-01-18 06:03:35.601856: Epoch time: 110.66 s\n",
      "2025-01-18 06:03:36.210133: \n",
      "2025-01-18 06:03:36.210235: Epoch 748\n",
      "2025-01-18 06:03:36.210334: Current learning rate: 0.00289\n",
      "2025-01-18 06:05:26.820963: train_loss -0.8551\n",
      "2025-01-18 06:05:26.821242: val_loss -0.6403\n",
      "2025-01-18 06:05:26.821288: Pseudo dice [np.float32(0.6985)]\n",
      "2025-01-18 06:05:26.821322: Epoch time: 110.61 s\n",
      "2025-01-18 06:05:27.428638: \n",
      "2025-01-18 06:05:27.428891: Epoch 749\n",
      "2025-01-18 06:05:27.428966: Current learning rate: 0.00288\n",
      "2025-01-18 06:07:18.184314: train_loss -0.8456\n",
      "2025-01-18 06:07:18.184534: val_loss -0.7194\n",
      "2025-01-18 06:07:18.184604: Pseudo dice [np.float32(0.7822)]\n",
      "2025-01-18 06:07:18.184646: Epoch time: 110.76 s\n",
      "2025-01-18 06:07:19.010950: \n",
      "2025-01-18 06:07:19.011135: Epoch 750\n",
      "2025-01-18 06:07:19.011204: Current learning rate: 0.00287\n",
      "2025-01-18 06:09:09.763612: train_loss -0.8582\n",
      "2025-01-18 06:09:09.763804: val_loss -0.6581\n",
      "2025-01-18 06:09:09.763848: Pseudo dice [np.float32(0.7273)]\n",
      "2025-01-18 06:09:09.763887: Epoch time: 110.75 s\n",
      "2025-01-18 06:09:10.365458: \n",
      "2025-01-18 06:09:10.365702: Epoch 751\n",
      "2025-01-18 06:09:10.365840: Current learning rate: 0.00286\n",
      "2025-01-18 06:11:00.916355: train_loss -0.8389\n",
      "2025-01-18 06:11:00.916496: val_loss -0.6617\n",
      "2025-01-18 06:11:00.916532: Pseudo dice [np.float32(0.7548)]\n",
      "2025-01-18 06:11:00.916567: Epoch time: 110.55 s\n",
      "2025-01-18 06:11:01.516344: \n",
      "2025-01-18 06:11:01.516745: Epoch 752\n",
      "2025-01-18 06:11:01.516842: Current learning rate: 0.00285\n",
      "2025-01-18 06:12:52.234781: train_loss -0.8539\n",
      "2025-01-18 06:12:52.234907: val_loss -0.623\n",
      "2025-01-18 06:12:52.234940: Pseudo dice [np.float32(0.7158)]\n",
      "2025-01-18 06:12:52.234973: Epoch time: 110.72 s\n",
      "2025-01-18 06:12:52.835586: \n",
      "2025-01-18 06:12:52.836033: Epoch 753\n",
      "2025-01-18 06:12:52.836101: Current learning rate: 0.00284\n",
      "2025-01-18 06:14:43.594087: train_loss -0.8404\n",
      "2025-01-18 06:14:43.594227: val_loss -0.533\n",
      "2025-01-18 06:14:43.594261: Pseudo dice [np.float32(0.6251)]\n",
      "2025-01-18 06:14:43.594295: Epoch time: 110.76 s\n",
      "2025-01-18 06:14:44.450099: \n",
      "2025-01-18 06:14:44.450468: Epoch 754\n",
      "2025-01-18 06:14:44.450587: Current learning rate: 0.00283\n",
      "2025-01-18 06:16:35.016890: train_loss -0.8606\n",
      "2025-01-18 06:16:35.017038: val_loss -0.6059\n",
      "2025-01-18 06:16:35.017086: Pseudo dice [np.float32(0.7032)]\n",
      "2025-01-18 06:16:35.017123: Epoch time: 110.57 s\n",
      "2025-01-18 06:16:35.620520: \n",
      "2025-01-18 06:16:35.620730: Epoch 755\n",
      "2025-01-18 06:16:35.620808: Current learning rate: 0.00282\n",
      "2025-01-18 06:18:26.380990: train_loss -0.8595\n",
      "2025-01-18 06:18:26.381170: val_loss -0.6741\n",
      "2025-01-18 06:18:26.381206: Pseudo dice [np.float32(0.7454)]\n",
      "2025-01-18 06:18:26.381244: Epoch time: 110.76 s\n",
      "2025-01-18 06:18:26.988023: \n",
      "2025-01-18 06:18:26.988391: Epoch 756\n",
      "2025-01-18 06:18:26.988513: Current learning rate: 0.00281\n",
      "2025-01-18 06:20:17.762625: train_loss -0.8508\n",
      "2025-01-18 06:20:17.762766: val_loss -0.7254\n",
      "2025-01-18 06:20:17.762801: Pseudo dice [np.float32(0.7776)]\n",
      "2025-01-18 06:20:17.762835: Epoch time: 110.78 s\n",
      "2025-01-18 06:20:18.362230: \n",
      "2025-01-18 06:20:18.362436: Epoch 757\n",
      "2025-01-18 06:20:18.362521: Current learning rate: 0.0028\n",
      "2025-01-18 06:22:08.948824: train_loss -0.865\n",
      "2025-01-18 06:22:08.949314: val_loss -0.701\n",
      "2025-01-18 06:22:08.949375: Pseudo dice [np.float32(0.7785)]\n",
      "2025-01-18 06:22:08.949415: Epoch time: 110.59 s\n",
      "2025-01-18 06:22:09.560601: \n",
      "2025-01-18 06:22:09.560719: Epoch 758\n",
      "2025-01-18 06:22:09.560790: Current learning rate: 0.00279\n",
      "2025-01-18 06:24:00.324027: train_loss -0.8607\n",
      "2025-01-18 06:24:00.324153: val_loss -0.7272\n",
      "2025-01-18 06:24:00.324210: Pseudo dice [np.float32(0.7906)]\n",
      "2025-01-18 06:24:00.324249: Epoch time: 110.76 s\n",
      "2025-01-18 06:24:00.928135: \n",
      "2025-01-18 06:24:00.928334: Epoch 759\n",
      "2025-01-18 06:24:00.928407: Current learning rate: 0.00278\n",
      "2025-01-18 06:25:51.529172: train_loss -0.8727\n",
      "2025-01-18 06:25:51.529554: val_loss -0.6901\n",
      "2025-01-18 06:25:51.529683: Pseudo dice [np.float32(0.7691)]\n",
      "2025-01-18 06:25:51.529855: Epoch time: 110.6 s\n",
      "2025-01-18 06:25:52.131666: \n",
      "2025-01-18 06:25:52.132017: Epoch 760\n",
      "2025-01-18 06:25:52.132264: Current learning rate: 0.00277\n",
      "2025-01-18 06:27:42.740076: train_loss -0.8675\n",
      "2025-01-18 06:27:42.740451: val_loss -0.6233\n",
      "2025-01-18 06:27:42.740581: Pseudo dice [np.float32(0.7115)]\n",
      "2025-01-18 06:27:42.740631: Epoch time: 110.61 s\n",
      "2025-01-18 06:27:43.337620: \n",
      "2025-01-18 06:27:43.337866: Epoch 761\n",
      "2025-01-18 06:27:43.337967: Current learning rate: 0.00276\n",
      "2025-01-18 06:29:34.070299: train_loss -0.8557\n",
      "2025-01-18 06:29:34.070451: val_loss -0.6244\n",
      "2025-01-18 06:29:34.070485: Pseudo dice [np.float32(0.6979)]\n",
      "2025-01-18 06:29:34.070519: Epoch time: 110.73 s\n",
      "2025-01-18 06:29:34.668953: \n",
      "2025-01-18 06:29:34.669149: Epoch 762\n",
      "2025-01-18 06:29:34.669250: Current learning rate: 0.00275\n",
      "2025-01-18 06:31:25.410346: train_loss -0.8369\n",
      "2025-01-18 06:31:25.410528: val_loss -0.6779\n",
      "2025-01-18 06:31:25.410583: Pseudo dice [np.float32(0.7147)]\n",
      "2025-01-18 06:31:25.410621: Epoch time: 110.74 s\n",
      "2025-01-18 06:31:26.013769: \n",
      "2025-01-18 06:31:26.013911: Epoch 763\n",
      "2025-01-18 06:31:26.013984: Current learning rate: 0.00274\n",
      "2025-01-18 06:33:16.799542: train_loss -0.8195\n",
      "2025-01-18 06:33:16.799763: val_loss -0.5964\n",
      "2025-01-18 06:33:16.799867: Pseudo dice [np.float32(0.7232)]\n",
      "2025-01-18 06:33:16.799974: Epoch time: 110.79 s\n",
      "2025-01-18 06:33:17.414838: \n",
      "2025-01-18 06:33:17.415165: Epoch 764\n",
      "2025-01-18 06:33:17.415295: Current learning rate: 0.00273\n",
      "2025-01-18 06:35:08.163846: train_loss -0.8448\n",
      "2025-01-18 06:35:08.163971: val_loss -0.706\n",
      "2025-01-18 06:35:08.164003: Pseudo dice [np.float32(0.7069)]\n",
      "2025-01-18 06:35:08.164040: Epoch time: 110.75 s\n",
      "2025-01-18 06:35:08.775859: \n",
      "2025-01-18 06:35:08.775945: Epoch 765\n",
      "2025-01-18 06:35:08.776007: Current learning rate: 0.00272\n",
      "2025-01-18 06:36:59.498812: train_loss -0.8551\n",
      "2025-01-18 06:36:59.498970: val_loss -0.6784\n",
      "2025-01-18 06:36:59.499008: Pseudo dice [np.float32(0.749)]\n",
      "2025-01-18 06:36:59.499041: Epoch time: 110.72 s\n",
      "2025-01-18 06:37:00.329830: \n",
      "2025-01-18 06:37:00.330213: Epoch 766\n",
      "2025-01-18 06:37:00.330383: Current learning rate: 0.00271\n",
      "2025-01-18 06:38:51.048772: train_loss -0.8602\n",
      "2025-01-18 06:38:51.048905: val_loss -0.6605\n",
      "2025-01-18 06:38:51.048940: Pseudo dice [np.float32(0.7178)]\n",
      "2025-01-18 06:38:51.048974: Epoch time: 110.72 s\n",
      "2025-01-18 06:38:51.659598: \n",
      "2025-01-18 06:38:51.659808: Epoch 767\n",
      "2025-01-18 06:38:51.659892: Current learning rate: 0.0027\n",
      "2025-01-18 06:40:42.238568: train_loss -0.871\n",
      "2025-01-18 06:40:42.238696: val_loss -0.6634\n",
      "2025-01-18 06:40:42.238730: Pseudo dice [np.float32(0.7725)]\n",
      "2025-01-18 06:40:42.238763: Epoch time: 110.58 s\n",
      "2025-01-18 06:40:42.853988: \n",
      "2025-01-18 06:40:42.854083: Epoch 768\n",
      "2025-01-18 06:40:42.854143: Current learning rate: 0.00268\n",
      "2025-01-18 06:42:33.569202: train_loss -0.8585\n",
      "2025-01-18 06:42:33.569333: val_loss -0.6796\n",
      "2025-01-18 06:42:33.569365: Pseudo dice [np.float32(0.7684)]\n",
      "2025-01-18 06:42:33.569399: Epoch time: 110.72 s\n",
      "2025-01-18 06:42:34.175161: \n",
      "2025-01-18 06:42:34.175370: Epoch 769\n",
      "2025-01-18 06:42:34.175472: Current learning rate: 0.00267\n",
      "2025-01-18 06:44:24.708137: train_loss -0.8655\n",
      "2025-01-18 06:44:24.708266: val_loss -0.7173\n",
      "2025-01-18 06:44:24.708298: Pseudo dice [np.float32(0.771)]\n",
      "2025-01-18 06:44:24.708330: Epoch time: 110.53 s\n",
      "2025-01-18 06:44:25.327866: \n",
      "2025-01-18 06:44:25.327962: Epoch 770\n",
      "2025-01-18 06:44:25.328025: Current learning rate: 0.00266\n",
      "2025-01-18 06:46:16.066306: train_loss -0.8399\n",
      "2025-01-18 06:46:16.066718: val_loss -0.7585\n",
      "2025-01-18 06:46:16.066766: Pseudo dice [np.float32(0.6503)]\n",
      "2025-01-18 06:46:16.066831: Epoch time: 110.74 s\n",
      "2025-01-18 06:46:16.686267: \n",
      "2025-01-18 06:46:16.686466: Epoch 771\n",
      "2025-01-18 06:46:16.686540: Current learning rate: 0.00265\n",
      "2025-01-18 06:48:07.235850: train_loss -0.8441\n",
      "2025-01-18 06:48:07.236040: val_loss -0.6957\n",
      "2025-01-18 06:48:07.236073: Pseudo dice [np.float32(0.7458)]\n",
      "2025-01-18 06:48:07.236115: Epoch time: 110.55 s\n",
      "2025-01-18 06:48:07.842632: \n",
      "2025-01-18 06:48:07.842723: Epoch 772\n",
      "2025-01-18 06:48:07.842785: Current learning rate: 0.00264\n",
      "2025-01-18 06:49:58.601938: train_loss -0.8543\n",
      "2025-01-18 06:49:58.602071: val_loss -0.6826\n",
      "2025-01-18 06:49:58.602113: Pseudo dice [np.float32(0.7184)]\n",
      "2025-01-18 06:49:58.602150: Epoch time: 110.76 s\n",
      "2025-01-18 06:49:59.209386: \n",
      "2025-01-18 06:49:59.209747: Epoch 773\n",
      "2025-01-18 06:49:59.209830: Current learning rate: 0.00263\n",
      "2025-01-18 06:51:49.957089: train_loss -0.8575\n",
      "2025-01-18 06:51:49.957259: val_loss -0.6554\n",
      "2025-01-18 06:51:49.957329: Pseudo dice [np.float32(0.7678)]\n",
      "2025-01-18 06:51:49.957369: Epoch time: 110.75 s\n",
      "2025-01-18 06:51:50.565988: \n",
      "2025-01-18 06:51:50.566393: Epoch 774\n",
      "2025-01-18 06:51:50.566467: Current learning rate: 0.00262\n",
      "2025-01-18 06:53:41.166159: train_loss -0.8512\n",
      "2025-01-18 06:53:41.166290: val_loss -0.6428\n",
      "2025-01-18 06:53:41.166326: Pseudo dice [np.float32(0.7001)]\n",
      "2025-01-18 06:53:41.166358: Epoch time: 110.6 s\n",
      "2025-01-18 06:53:41.779621: \n",
      "2025-01-18 06:53:41.779957: Epoch 775\n",
      "2025-01-18 06:53:41.780066: Current learning rate: 0.00261\n",
      "2025-01-18 06:55:32.361661: train_loss -0.8561\n",
      "2025-01-18 06:55:32.361807: val_loss -0.6697\n",
      "2025-01-18 06:55:32.361840: Pseudo dice [np.float32(0.7834)]\n",
      "2025-01-18 06:55:32.361872: Epoch time: 110.58 s\n",
      "2025-01-18 06:55:32.962190: \n",
      "2025-01-18 06:55:32.962486: Epoch 776\n",
      "2025-01-18 06:55:32.962712: Current learning rate: 0.0026\n",
      "2025-01-18 06:57:23.709807: train_loss -0.836\n",
      "2025-01-18 06:57:23.709951: val_loss -0.6386\n",
      "2025-01-18 06:57:23.709984: Pseudo dice [np.float32(0.692)]\n",
      "2025-01-18 06:57:23.710017: Epoch time: 110.75 s\n",
      "2025-01-18 06:57:24.547353: \n",
      "2025-01-18 06:57:24.547599: Epoch 777\n",
      "2025-01-18 06:57:24.547732: Current learning rate: 0.00259\n",
      "2025-01-18 06:59:15.159937: train_loss -0.8453\n",
      "2025-01-18 06:59:15.160058: val_loss -0.6724\n",
      "2025-01-18 06:59:15.160090: Pseudo dice [np.float32(0.7099)]\n",
      "2025-01-18 06:59:15.160123: Epoch time: 110.61 s\n",
      "2025-01-18 06:59:15.778944: \n",
      "2025-01-18 06:59:15.779315: Epoch 778\n",
      "2025-01-18 06:59:15.779414: Current learning rate: 0.00258\n",
      "2025-01-18 07:01:06.450739: train_loss -0.8667\n",
      "2025-01-18 07:01:06.450887: val_loss -0.661\n",
      "2025-01-18 07:01:06.450922: Pseudo dice [np.float32(0.7802)]\n",
      "2025-01-18 07:01:06.450956: Epoch time: 110.67 s\n",
      "2025-01-18 07:01:07.066941: \n",
      "2025-01-18 07:01:07.067292: Epoch 779\n",
      "2025-01-18 07:01:07.067405: Current learning rate: 0.00257\n",
      "2025-01-18 07:02:57.790924: train_loss -0.8477\n",
      "2025-01-18 07:02:57.791049: val_loss -0.6371\n",
      "2025-01-18 07:02:57.791082: Pseudo dice [np.float32(0.7105)]\n",
      "2025-01-18 07:02:57.791116: Epoch time: 110.72 s\n",
      "2025-01-18 07:02:58.395851: \n",
      "2025-01-18 07:02:58.395944: Epoch 780\n",
      "2025-01-18 07:02:58.396007: Current learning rate: 0.00256\n",
      "2025-01-18 07:04:49.155572: train_loss -0.8393\n",
      "2025-01-18 07:04:49.155723: val_loss -0.7013\n",
      "2025-01-18 07:04:49.155759: Pseudo dice [np.float32(0.7648)]\n",
      "2025-01-18 07:04:49.155815: Epoch time: 110.76 s\n",
      "2025-01-18 07:04:49.773656: \n",
      "2025-01-18 07:04:49.773746: Epoch 781\n",
      "2025-01-18 07:04:49.773807: Current learning rate: 0.00255\n",
      "2025-01-18 07:06:40.487187: train_loss -0.874\n",
      "2025-01-18 07:06:40.487428: val_loss -0.6493\n",
      "2025-01-18 07:06:40.487562: Pseudo dice [np.float32(0.7162)]\n",
      "2025-01-18 07:06:40.487622: Epoch time: 110.71 s\n",
      "2025-01-18 07:06:41.093759: \n",
      "2025-01-18 07:06:41.093962: Epoch 782\n",
      "2025-01-18 07:06:41.094038: Current learning rate: 0.00254\n",
      "2025-01-18 07:08:31.842193: train_loss -0.8811\n",
      "2025-01-18 07:08:31.842578: val_loss -0.6934\n",
      "2025-01-18 07:08:31.842714: Pseudo dice [np.float32(0.7821)]\n",
      "2025-01-18 07:08:31.842779: Epoch time: 110.75 s\n",
      "2025-01-18 07:08:32.451425: \n",
      "2025-01-18 07:08:32.451784: Epoch 783\n",
      "2025-01-18 07:08:32.451853: Current learning rate: 0.00253\n",
      "2025-01-18 07:10:23.209710: train_loss -0.867\n",
      "2025-01-18 07:10:23.209839: val_loss -0.6954\n",
      "2025-01-18 07:10:23.209872: Pseudo dice [np.float32(0.7677)]\n",
      "2025-01-18 07:10:23.209905: Epoch time: 110.76 s\n",
      "2025-01-18 07:10:23.824756: \n",
      "2025-01-18 07:10:23.824856: Epoch 784\n",
      "2025-01-18 07:10:23.824918: Current learning rate: 0.00252\n",
      "2025-01-18 07:12:14.590719: train_loss -0.842\n",
      "2025-01-18 07:12:14.590882: val_loss -0.695\n",
      "2025-01-18 07:12:14.590955: Pseudo dice [np.float32(0.8351)]\n",
      "2025-01-18 07:12:14.590998: Epoch time: 110.77 s\n",
      "2025-01-18 07:12:15.194672: \n",
      "2025-01-18 07:12:15.195057: Epoch 785\n",
      "2025-01-18 07:12:15.195132: Current learning rate: 0.00251\n",
      "2025-01-18 07:14:05.768928: train_loss -0.8585\n",
      "2025-01-18 07:14:05.769134: val_loss -0.684\n",
      "2025-01-18 07:14:05.769167: Pseudo dice [np.float32(0.8469)]\n",
      "2025-01-18 07:14:05.769198: Epoch time: 110.57 s\n",
      "2025-01-18 07:14:06.380076: \n",
      "2025-01-18 07:14:06.380332: Epoch 786\n",
      "2025-01-18 07:14:06.380503: Current learning rate: 0.0025\n",
      "2025-01-18 07:15:57.119850: train_loss -0.8411\n",
      "2025-01-18 07:15:57.120018: val_loss -0.695\n",
      "2025-01-18 07:15:57.120090: Pseudo dice [np.float32(0.7275)]\n",
      "2025-01-18 07:15:57.120131: Epoch time: 110.74 s\n",
      "2025-01-18 07:15:57.734637: \n",
      "2025-01-18 07:15:57.734918: Epoch 787\n",
      "2025-01-18 07:15:57.734995: Current learning rate: 0.00249\n",
      "2025-01-18 07:17:48.473409: train_loss -0.8374\n",
      "2025-01-18 07:17:48.473539: val_loss -0.6352\n",
      "2025-01-18 07:17:48.473572: Pseudo dice [np.float32(0.7248)]\n",
      "2025-01-18 07:17:48.473606: Epoch time: 110.74 s\n",
      "2025-01-18 07:17:49.087564: \n",
      "2025-01-18 07:17:49.087651: Epoch 788\n",
      "2025-01-18 07:17:49.087712: Current learning rate: 0.00248\n",
      "2025-01-18 07:19:39.793154: train_loss -0.8305\n",
      "2025-01-18 07:19:39.793279: val_loss -0.646\n",
      "2025-01-18 07:19:39.793312: Pseudo dice [np.float32(0.7396)]\n",
      "2025-01-18 07:19:39.793346: Epoch time: 110.71 s\n",
      "2025-01-18 07:19:40.669026: \n",
      "2025-01-18 07:19:40.669217: Epoch 789\n",
      "2025-01-18 07:19:40.669308: Current learning rate: 0.00247\n",
      "2025-01-18 07:21:31.431909: train_loss -0.8539\n",
      "2025-01-18 07:21:31.432061: val_loss -0.6415\n",
      "2025-01-18 07:21:31.432093: Pseudo dice [np.float32(0.7437)]\n",
      "2025-01-18 07:21:31.432127: Epoch time: 110.76 s\n",
      "2025-01-18 07:21:32.043379: \n",
      "2025-01-18 07:21:32.043616: Epoch 790\n",
      "2025-01-18 07:21:32.043720: Current learning rate: 0.00245\n",
      "2025-01-18 07:23:22.752993: train_loss -0.8618\n",
      "2025-01-18 07:23:22.753117: val_loss -0.6347\n",
      "2025-01-18 07:23:22.753150: Pseudo dice [np.float32(0.7666)]\n",
      "2025-01-18 07:23:22.753186: Epoch time: 110.71 s\n",
      "2025-01-18 07:23:23.374117: \n",
      "2025-01-18 07:23:23.374228: Epoch 791\n",
      "2025-01-18 07:23:23.374291: Current learning rate: 0.00244\n",
      "2025-01-18 07:25:14.087328: train_loss -0.8495\n",
      "2025-01-18 07:25:14.087448: val_loss -0.7024\n",
      "2025-01-18 07:25:14.087480: Pseudo dice [np.float32(0.776)]\n",
      "2025-01-18 07:25:14.087512: Epoch time: 110.71 s\n",
      "2025-01-18 07:25:14.703783: \n",
      "2025-01-18 07:25:14.704196: Epoch 792\n",
      "2025-01-18 07:25:14.704325: Current learning rate: 0.00243\n",
      "2025-01-18 07:27:05.269120: train_loss -0.8413\n",
      "2025-01-18 07:27:05.269341: val_loss -0.7033\n",
      "2025-01-18 07:27:05.269375: Pseudo dice [np.float32(0.7648)]\n",
      "2025-01-18 07:27:05.269411: Epoch time: 110.57 s\n",
      "2025-01-18 07:27:05.872810: \n",
      "2025-01-18 07:27:05.872913: Epoch 793\n",
      "2025-01-18 07:27:05.872974: Current learning rate: 0.00242\n",
      "2025-01-18 07:28:56.636578: train_loss -0.8489\n",
      "2025-01-18 07:28:56.636993: val_loss -0.7043\n",
      "2025-01-18 07:28:56.637064: Pseudo dice [np.float32(0.7987)]\n",
      "2025-01-18 07:28:56.637105: Epoch time: 110.76 s\n",
      "2025-01-18 07:28:57.258210: \n",
      "2025-01-18 07:28:57.258542: Epoch 794\n",
      "2025-01-18 07:28:57.258628: Current learning rate: 0.00241\n",
      "2025-01-18 07:30:48.002299: train_loss -0.8563\n",
      "2025-01-18 07:30:48.002428: val_loss -0.6704\n",
      "2025-01-18 07:30:48.002462: Pseudo dice [np.float32(0.7235)]\n",
      "2025-01-18 07:30:48.002496: Epoch time: 110.74 s\n",
      "2025-01-18 07:30:48.612859: \n",
      "2025-01-18 07:30:48.612955: Epoch 795\n",
      "2025-01-18 07:30:48.613018: Current learning rate: 0.0024\n",
      "2025-01-18 07:32:39.187348: train_loss -0.8633\n",
      "2025-01-18 07:32:39.187529: val_loss -0.6353\n",
      "2025-01-18 07:32:39.187562: Pseudo dice [np.float32(0.7597)]\n",
      "2025-01-18 07:32:39.187596: Epoch time: 110.58 s\n",
      "2025-01-18 07:32:39.799616: \n",
      "2025-01-18 07:32:39.799765: Epoch 796\n",
      "2025-01-18 07:32:39.799839: Current learning rate: 0.00239\n",
      "2025-01-18 07:34:30.543638: train_loss -0.8739\n",
      "2025-01-18 07:34:30.544044: val_loss -0.7322\n",
      "2025-01-18 07:34:30.544083: Pseudo dice [np.float32(0.7747)]\n",
      "2025-01-18 07:34:30.544116: Epoch time: 110.74 s\n",
      "2025-01-18 07:34:31.158646: \n",
      "2025-01-18 07:34:31.158918: Epoch 797\n",
      "2025-01-18 07:34:31.159069: Current learning rate: 0.00238\n",
      "2025-01-18 07:36:21.723593: train_loss -0.8575\n",
      "2025-01-18 07:36:21.723727: val_loss -0.6718\n",
      "2025-01-18 07:36:21.723763: Pseudo dice [np.float32(0.6723)]\n",
      "2025-01-18 07:36:21.723796: Epoch time: 110.57 s\n",
      "2025-01-18 07:36:22.340971: \n",
      "2025-01-18 07:36:22.341069: Epoch 798\n",
      "2025-01-18 07:36:22.341131: Current learning rate: 0.00237\n",
      "2025-01-18 07:38:12.908489: train_loss -0.8645\n",
      "2025-01-18 07:38:12.908616: val_loss -0.6551\n",
      "2025-01-18 07:38:12.908649: Pseudo dice [np.float32(0.7152)]\n",
      "2025-01-18 07:38:12.908687: Epoch time: 110.57 s\n",
      "2025-01-18 07:38:13.525184: \n",
      "2025-01-18 07:38:13.525277: Epoch 799\n",
      "2025-01-18 07:38:13.525339: Current learning rate: 0.00236\n",
      "2025-01-18 07:40:04.106452: train_loss -0.8731\n",
      "2025-01-18 07:40:04.106587: val_loss -0.6422\n",
      "2025-01-18 07:40:04.106626: Pseudo dice [np.float32(0.734)]\n",
      "2025-01-18 07:40:04.106658: Epoch time: 110.58 s\n",
      "2025-01-18 07:40:04.958940: \n",
      "2025-01-18 07:40:04.959235: Epoch 800\n",
      "2025-01-18 07:40:04.959362: Current learning rate: 0.00235\n",
      "2025-01-18 07:41:55.467708: train_loss -0.8763\n",
      "2025-01-18 07:41:55.467839: val_loss -0.6894\n",
      "2025-01-18 07:41:55.467874: Pseudo dice [np.float32(0.7541)]\n",
      "2025-01-18 07:41:55.467907: Epoch time: 110.51 s\n",
      "2025-01-18 07:41:56.352042: \n",
      "2025-01-18 07:41:56.352237: Epoch 801\n",
      "2025-01-18 07:41:56.352314: Current learning rate: 0.00234\n",
      "2025-01-18 07:43:46.937398: train_loss -0.8778\n",
      "2025-01-18 07:43:46.937590: val_loss -0.6765\n",
      "2025-01-18 07:43:46.937678: Pseudo dice [np.float32(0.7902)]\n",
      "2025-01-18 07:43:46.937722: Epoch time: 110.59 s\n",
      "2025-01-18 07:43:47.555212: \n",
      "2025-01-18 07:43:47.555314: Epoch 802\n",
      "2025-01-18 07:43:47.555374: Current learning rate: 0.00233\n",
      "2025-01-18 07:45:38.295910: train_loss -0.8739\n",
      "2025-01-18 07:45:38.296037: val_loss -0.6545\n",
      "2025-01-18 07:45:38.296070: Pseudo dice [np.float32(0.7779)]\n",
      "2025-01-18 07:45:38.296118: Epoch time: 110.74 s\n",
      "2025-01-18 07:45:38.904198: \n",
      "2025-01-18 07:45:38.904395: Epoch 803\n",
      "2025-01-18 07:45:38.904472: Current learning rate: 0.00232\n",
      "2025-01-18 07:47:29.651324: train_loss -0.8844\n",
      "2025-01-18 07:47:29.651456: val_loss -0.6888\n",
      "2025-01-18 07:47:29.651620: Pseudo dice [np.float32(0.821)]\n",
      "2025-01-18 07:47:29.651687: Epoch time: 110.75 s\n",
      "2025-01-18 07:47:30.266670: \n",
      "2025-01-18 07:47:30.267032: Epoch 804\n",
      "2025-01-18 07:47:30.267102: Current learning rate: 0.00231\n",
      "2025-01-18 07:49:20.901104: train_loss -0.8576\n",
      "2025-01-18 07:49:20.901225: val_loss -0.6868\n",
      "2025-01-18 07:49:20.901256: Pseudo dice [np.float32(0.7455)]\n",
      "2025-01-18 07:49:20.901289: Epoch time: 110.63 s\n",
      "2025-01-18 07:49:21.519372: \n",
      "2025-01-18 07:49:21.519664: Epoch 805\n",
      "2025-01-18 07:49:21.519737: Current learning rate: 0.0023\n",
      "2025-01-18 07:51:12.136877: train_loss -0.8501\n",
      "2025-01-18 07:51:12.136992: val_loss -0.5856\n",
      "2025-01-18 07:51:12.137024: Pseudo dice [np.float32(0.4608)]\n",
      "2025-01-18 07:51:12.137057: Epoch time: 110.62 s\n",
      "2025-01-18 07:51:12.756052: \n",
      "2025-01-18 07:51:12.756244: Epoch 806\n",
      "2025-01-18 07:51:12.756422: Current learning rate: 0.00229\n",
      "2025-01-18 07:53:03.525477: train_loss -0.852\n",
      "2025-01-18 07:53:03.525616: val_loss -0.4923\n",
      "2025-01-18 07:53:03.525647: Pseudo dice [np.float32(0.5024)]\n",
      "2025-01-18 07:53:03.525679: Epoch time: 110.77 s\n",
      "2025-01-18 07:53:04.130102: \n",
      "2025-01-18 07:53:04.130199: Epoch 807\n",
      "2025-01-18 07:53:04.130261: Current learning rate: 0.00228\n",
      "2025-01-18 07:54:54.917079: train_loss -0.8394\n",
      "2025-01-18 07:54:54.917207: val_loss -0.6617\n",
      "2025-01-18 07:54:54.917241: Pseudo dice [np.float32(0.7782)]\n",
      "2025-01-18 07:54:54.917274: Epoch time: 110.79 s\n",
      "2025-01-18 07:54:55.536258: \n",
      "2025-01-18 07:54:55.536577: Epoch 808\n",
      "2025-01-18 07:54:55.536658: Current learning rate: 0.00226\n",
      "2025-01-18 07:56:46.101872: train_loss -0.8557\n",
      "2025-01-18 07:56:46.102176: val_loss -0.6495\n",
      "2025-01-18 07:56:46.102292: Pseudo dice [np.float32(0.7064)]\n",
      "2025-01-18 07:56:46.102345: Epoch time: 110.57 s\n",
      "2025-01-18 07:56:46.719569: \n",
      "2025-01-18 07:56:46.719980: Epoch 809\n",
      "2025-01-18 07:56:46.720138: Current learning rate: 0.00225\n",
      "2025-01-18 07:58:37.536209: train_loss -0.8418\n",
      "2025-01-18 07:58:37.536346: val_loss -0.6198\n",
      "2025-01-18 07:58:37.536381: Pseudo dice [np.float32(0.7459)]\n",
      "2025-01-18 07:58:37.536414: Epoch time: 110.82 s\n",
      "2025-01-18 07:58:38.154623: \n",
      "2025-01-18 07:58:38.154715: Epoch 810\n",
      "2025-01-18 07:58:38.154777: Current learning rate: 0.00224\n",
      "2025-01-18 08:00:28.894555: train_loss -0.8407\n",
      "2025-01-18 08:00:28.894757: val_loss -0.6337\n",
      "2025-01-18 08:00:28.894823: Pseudo dice [np.float32(0.7722)]\n",
      "2025-01-18 08:00:28.894865: Epoch time: 110.74 s\n",
      "2025-01-18 08:00:29.510474: \n",
      "2025-01-18 08:00:29.510607: Epoch 811\n",
      "2025-01-18 08:00:29.510685: Current learning rate: 0.00223\n",
      "2025-01-18 08:02:20.067019: train_loss -0.8491\n",
      "2025-01-18 08:02:20.067159: val_loss -0.5953\n",
      "2025-01-18 08:02:20.067192: Pseudo dice [np.float32(0.7646)]\n",
      "2025-01-18 08:02:20.067224: Epoch time: 110.56 s\n",
      "2025-01-18 08:02:20.913203: \n",
      "2025-01-18 08:02:20.913564: Epoch 812\n",
      "2025-01-18 08:02:20.913819: Current learning rate: 0.00222\n",
      "2025-01-18 08:04:11.637955: train_loss -0.8575\n",
      "2025-01-18 08:04:11.638095: val_loss -0.6813\n",
      "2025-01-18 08:04:11.638129: Pseudo dice [np.float32(0.7596)]\n",
      "2025-01-18 08:04:11.638162: Epoch time: 110.73 s\n",
      "2025-01-18 08:04:12.263133: \n",
      "2025-01-18 08:04:12.263504: Epoch 813\n",
      "2025-01-18 08:04:12.263655: Current learning rate: 0.00221\n",
      "2025-01-18 08:06:02.812935: train_loss -0.833\n",
      "2025-01-18 08:06:02.813068: val_loss -0.6908\n",
      "2025-01-18 08:06:02.813102: Pseudo dice [np.float32(0.7966)]\n",
      "2025-01-18 08:06:02.813136: Epoch time: 110.55 s\n",
      "2025-01-18 08:06:03.422905: \n",
      "2025-01-18 08:06:03.423057: Epoch 814\n",
      "2025-01-18 08:06:03.423132: Current learning rate: 0.0022\n",
      "2025-01-18 08:07:54.096388: train_loss -0.8516\n",
      "2025-01-18 08:07:54.096624: val_loss -0.7003\n",
      "2025-01-18 08:07:54.096760: Pseudo dice [np.float32(0.7772)]\n",
      "2025-01-18 08:07:54.096910: Epoch time: 110.67 s\n",
      "2025-01-18 08:07:54.704866: \n",
      "2025-01-18 08:07:54.704965: Epoch 815\n",
      "2025-01-18 08:07:54.705185: Current learning rate: 0.00219\n",
      "2025-01-18 08:09:45.265651: train_loss -0.8697\n",
      "2025-01-18 08:09:45.265783: val_loss -0.6601\n",
      "2025-01-18 08:09:45.265816: Pseudo dice [np.float32(0.7478)]\n",
      "2025-01-18 08:09:45.265847: Epoch time: 110.56 s\n",
      "2025-01-18 08:09:45.880200: \n",
      "2025-01-18 08:09:45.880546: Epoch 816\n",
      "2025-01-18 08:09:45.880666: Current learning rate: 0.00218\n",
      "2025-01-18 08:11:36.642812: train_loss -0.8638\n",
      "2025-01-18 08:11:36.642933: val_loss -0.654\n",
      "2025-01-18 08:11:36.642967: Pseudo dice [np.float32(0.7478)]\n",
      "2025-01-18 08:11:36.643000: Epoch time: 110.76 s\n",
      "2025-01-18 08:11:37.250788: \n",
      "2025-01-18 08:11:37.251108: Epoch 817\n",
      "2025-01-18 08:11:37.251175: Current learning rate: 0.00217\n",
      "2025-01-18 08:13:27.850355: train_loss -0.8701\n",
      "2025-01-18 08:13:27.850492: val_loss -0.6592\n",
      "2025-01-18 08:13:27.850528: Pseudo dice [np.float32(0.7698)]\n",
      "2025-01-18 08:13:27.850561: Epoch time: 110.6 s\n",
      "2025-01-18 08:13:28.459076: \n",
      "2025-01-18 08:13:28.459469: Epoch 818\n",
      "2025-01-18 08:13:28.459615: Current learning rate: 0.00216\n",
      "2025-01-18 08:15:19.170669: train_loss -0.8629\n",
      "2025-01-18 08:15:19.170869: val_loss -0.6759\n",
      "2025-01-18 08:15:19.170903: Pseudo dice [np.float32(0.7384)]\n",
      "2025-01-18 08:15:19.170935: Epoch time: 110.71 s\n",
      "2025-01-18 08:15:19.780645: \n",
      "2025-01-18 08:15:19.780802: Epoch 819\n",
      "2025-01-18 08:15:19.780867: Current learning rate: 0.00215\n",
      "2025-01-18 08:17:10.567040: train_loss -0.8456\n",
      "2025-01-18 08:17:10.567168: val_loss -0.7007\n",
      "2025-01-18 08:17:10.567202: Pseudo dice [np.float32(0.7355)]\n",
      "2025-01-18 08:17:10.567236: Epoch time: 110.79 s\n",
      "2025-01-18 08:17:11.161614: \n",
      "2025-01-18 08:17:11.161930: Epoch 820\n",
      "2025-01-18 08:17:11.162015: Current learning rate: 0.00214\n",
      "2025-01-18 08:19:01.923463: train_loss -0.8549\n",
      "2025-01-18 08:19:01.923587: val_loss -0.7101\n",
      "2025-01-18 08:19:01.923619: Pseudo dice [np.float32(0.7551)]\n",
      "2025-01-18 08:19:01.923653: Epoch time: 110.76 s\n",
      "2025-01-18 08:19:02.509707: \n",
      "2025-01-18 08:19:02.510046: Epoch 821\n",
      "2025-01-18 08:19:02.510123: Current learning rate: 0.00213\n",
      "2025-01-18 08:20:53.214869: train_loss -0.8586\n",
      "2025-01-18 08:20:53.215185: val_loss -0.6836\n",
      "2025-01-18 08:20:53.215330: Pseudo dice [np.float32(0.7767)]\n",
      "2025-01-18 08:20:53.215384: Epoch time: 110.71 s\n",
      "2025-01-18 08:20:53.813674: \n",
      "2025-01-18 08:20:53.813810: Epoch 822\n",
      "2025-01-18 08:20:53.813922: Current learning rate: 0.00212\n",
      "2025-01-18 08:22:44.386736: train_loss -0.854\n",
      "2025-01-18 08:22:44.386882: val_loss -0.6324\n",
      "2025-01-18 08:22:44.386966: Pseudo dice [np.float32(0.7331)]\n",
      "2025-01-18 08:22:44.387012: Epoch time: 110.57 s\n",
      "2025-01-18 08:22:44.982223: \n",
      "2025-01-18 08:22:44.982560: Epoch 823\n",
      "2025-01-18 08:22:44.982693: Current learning rate: 0.0021\n",
      "2025-01-18 08:24:35.519709: train_loss -0.8565\n",
      "2025-01-18 08:24:35.519832: val_loss -0.6905\n",
      "2025-01-18 08:24:35.519863: Pseudo dice [np.float32(0.7264)]\n",
      "2025-01-18 08:24:35.519899: Epoch time: 110.54 s\n",
      "2025-01-18 08:24:36.370986: \n",
      "2025-01-18 08:24:36.371344: Epoch 824\n",
      "2025-01-18 08:24:36.371444: Current learning rate: 0.00209\n",
      "2025-01-18 08:26:26.940140: train_loss -0.8588\n",
      "2025-01-18 08:26:26.940283: val_loss -0.6593\n",
      "2025-01-18 08:26:26.940318: Pseudo dice [np.float32(0.7304)]\n",
      "2025-01-18 08:26:26.940351: Epoch time: 110.57 s\n",
      "2025-01-18 08:26:27.537631: \n",
      "2025-01-18 08:26:27.537896: Epoch 825\n",
      "2025-01-18 08:26:27.538037: Current learning rate: 0.00208\n",
      "2025-01-18 08:28:18.296621: train_loss -0.8475\n",
      "2025-01-18 08:28:18.296763: val_loss -0.6414\n",
      "2025-01-18 08:28:18.296911: Pseudo dice [np.float32(0.812)]\n",
      "2025-01-18 08:28:18.296997: Epoch time: 110.76 s\n",
      "2025-01-18 08:28:18.898858: \n",
      "2025-01-18 08:28:18.898962: Epoch 826\n",
      "2025-01-18 08:28:18.899027: Current learning rate: 0.00207\n",
      "2025-01-18 08:30:09.457643: train_loss -0.8643\n",
      "2025-01-18 08:30:09.457815: val_loss -0.715\n",
      "2025-01-18 08:30:09.457891: Pseudo dice [np.float32(0.7595)]\n",
      "2025-01-18 08:30:09.457933: Epoch time: 110.56 s\n",
      "2025-01-18 08:30:10.048003: \n",
      "2025-01-18 08:30:10.048240: Epoch 827\n",
      "2025-01-18 08:30:10.048307: Current learning rate: 0.00206\n",
      "2025-01-18 08:32:00.675466: train_loss -0.8549\n",
      "2025-01-18 08:32:00.675751: val_loss -0.7012\n",
      "2025-01-18 08:32:00.675814: Pseudo dice [np.float32(0.8098)]\n",
      "2025-01-18 08:32:00.675853: Epoch time: 110.63 s\n",
      "2025-01-18 08:32:01.278583: \n",
      "2025-01-18 08:32:01.278926: Epoch 828\n",
      "2025-01-18 08:32:01.279054: Current learning rate: 0.00205\n",
      "2025-01-18 08:33:51.850538: train_loss -0.8532\n",
      "2025-01-18 08:33:51.850696: val_loss -0.6402\n",
      "2025-01-18 08:33:51.850737: Pseudo dice [np.float32(0.6819)]\n",
      "2025-01-18 08:33:51.850774: Epoch time: 110.57 s\n",
      "2025-01-18 08:33:52.446414: \n",
      "2025-01-18 08:33:52.446757: Epoch 829\n",
      "2025-01-18 08:33:52.446847: Current learning rate: 0.00204\n",
      "2025-01-18 08:35:43.050002: train_loss -0.8325\n",
      "2025-01-18 08:35:43.050179: val_loss -0.6303\n",
      "2025-01-18 08:35:43.050279: Pseudo dice [np.float32(0.6765)]\n",
      "2025-01-18 08:35:43.050346: Epoch time: 110.6 s\n",
      "2025-01-18 08:35:43.645790: \n",
      "2025-01-18 08:35:43.645898: Epoch 830\n",
      "2025-01-18 08:35:43.645963: Current learning rate: 0.00203\n",
      "2025-01-18 08:37:34.232933: train_loss -0.8509\n",
      "2025-01-18 08:37:34.233099: val_loss -0.6834\n",
      "2025-01-18 08:37:34.233146: Pseudo dice [np.float32(0.7779)]\n",
      "2025-01-18 08:37:34.233183: Epoch time: 110.59 s\n",
      "2025-01-18 08:37:34.827024: \n",
      "2025-01-18 08:37:34.827263: Epoch 831\n",
      "2025-01-18 08:37:34.827437: Current learning rate: 0.00202\n",
      "2025-01-18 08:39:25.604760: train_loss -0.8391\n",
      "2025-01-18 08:39:25.604955: val_loss -0.6132\n",
      "2025-01-18 08:39:25.605002: Pseudo dice [np.float32(0.6394)]\n",
      "2025-01-18 08:39:25.605037: Epoch time: 110.78 s\n",
      "2025-01-18 08:39:26.192231: \n",
      "2025-01-18 08:39:26.192688: Epoch 832\n",
      "2025-01-18 08:39:26.192758: Current learning rate: 0.00201\n",
      "2025-01-18 08:41:16.895473: train_loss -0.8493\n",
      "2025-01-18 08:41:16.895615: val_loss -0.7054\n",
      "2025-01-18 08:41:16.895649: Pseudo dice [np.float32(0.735)]\n",
      "2025-01-18 08:41:16.895682: Epoch time: 110.7 s\n",
      "2025-01-18 08:41:17.490096: \n",
      "2025-01-18 08:41:17.490480: Epoch 833\n",
      "2025-01-18 08:41:17.490577: Current learning rate: 0.002\n",
      "2025-01-18 08:43:08.040170: train_loss -0.8399\n",
      "2025-01-18 08:43:08.040301: val_loss -0.6666\n",
      "2025-01-18 08:43:08.040336: Pseudo dice [np.float32(0.7498)]\n",
      "2025-01-18 08:43:08.040628: Epoch time: 110.55 s\n",
      "2025-01-18 08:43:08.629801: \n",
      "2025-01-18 08:43:08.630117: Epoch 834\n",
      "2025-01-18 08:43:08.630298: Current learning rate: 0.00199\n",
      "2025-01-18 08:44:59.201388: train_loss -0.8487\n",
      "2025-01-18 08:44:59.201517: val_loss -0.6381\n",
      "2025-01-18 08:44:59.201548: Pseudo dice [np.float32(0.7881)]\n",
      "2025-01-18 08:44:59.201579: Epoch time: 110.57 s\n",
      "2025-01-18 08:44:59.797870: \n",
      "2025-01-18 08:44:59.798252: Epoch 835\n",
      "2025-01-18 08:44:59.798323: Current learning rate: 0.00198\n",
      "2025-01-18 08:46:50.408530: train_loss -0.8576\n",
      "2025-01-18 08:46:50.408665: val_loss -0.6544\n",
      "2025-01-18 08:46:50.408701: Pseudo dice [np.float32(0.7452)]\n",
      "2025-01-18 08:46:50.408748: Epoch time: 110.61 s\n",
      "2025-01-18 08:46:50.997857: \n",
      "2025-01-18 08:46:50.998270: Epoch 836\n",
      "2025-01-18 08:46:50.998378: Current learning rate: 0.00196\n",
      "2025-01-18 08:48:41.759025: train_loss -0.8649\n",
      "2025-01-18 08:48:41.759381: val_loss -0.6283\n",
      "2025-01-18 08:48:41.759420: Pseudo dice [np.float32(0.7694)]\n",
      "2025-01-18 08:48:41.759454: Epoch time: 110.76 s\n",
      "2025-01-18 08:48:42.634006: \n",
      "2025-01-18 08:48:42.634249: Epoch 837\n",
      "2025-01-18 08:48:42.634354: Current learning rate: 0.00195\n",
      "2025-01-18 08:50:33.373566: train_loss -0.8624\n",
      "2025-01-18 08:50:33.373701: val_loss -0.6423\n",
      "2025-01-18 08:50:33.373745: Pseudo dice [np.float32(0.7907)]\n",
      "2025-01-18 08:50:33.373776: Epoch time: 110.74 s\n",
      "2025-01-18 08:50:33.966347: \n",
      "2025-01-18 08:50:33.966663: Epoch 838\n",
      "2025-01-18 08:50:33.966809: Current learning rate: 0.00194\n",
      "2025-01-18 08:52:24.739686: train_loss -0.8653\n",
      "2025-01-18 08:52:24.739823: val_loss -0.7106\n",
      "2025-01-18 08:52:24.739856: Pseudo dice [np.float32(0.7524)]\n",
      "2025-01-18 08:52:24.739889: Epoch time: 110.77 s\n",
      "2025-01-18 08:52:25.334416: \n",
      "2025-01-18 08:52:25.334518: Epoch 839\n",
      "2025-01-18 08:52:25.334581: Current learning rate: 0.00193\n",
      "2025-01-18 08:54:16.086591: train_loss -0.8695\n",
      "2025-01-18 08:54:16.086828: val_loss -0.6378\n",
      "2025-01-18 08:54:16.086873: Pseudo dice [np.float32(0.7276)]\n",
      "2025-01-18 08:54:16.086908: Epoch time: 110.75 s\n",
      "2025-01-18 08:54:16.681095: \n",
      "2025-01-18 08:54:16.681481: Epoch 840\n",
      "2025-01-18 08:54:16.681578: Current learning rate: 0.00192\n",
      "2025-01-18 08:56:07.418936: train_loss -0.8371\n",
      "2025-01-18 08:56:07.419051: val_loss -0.6649\n",
      "2025-01-18 08:56:07.419083: Pseudo dice [np.float32(0.7273)]\n",
      "2025-01-18 08:56:07.419115: Epoch time: 110.74 s\n",
      "2025-01-18 08:56:08.003132: \n",
      "2025-01-18 08:56:08.003229: Epoch 841\n",
      "2025-01-18 08:56:08.003291: Current learning rate: 0.00191\n",
      "2025-01-18 08:57:58.638147: train_loss -0.8663\n",
      "2025-01-18 08:57:58.638273: val_loss -0.6882\n",
      "2025-01-18 08:57:58.638305: Pseudo dice [np.float32(0.7503)]\n",
      "2025-01-18 08:57:58.638337: Epoch time: 110.64 s\n",
      "2025-01-18 08:57:59.229750: \n",
      "2025-01-18 08:57:59.230206: Epoch 842\n",
      "2025-01-18 08:57:59.230342: Current learning rate: 0.0019\n",
      "2025-01-18 08:59:50.009535: train_loss -0.8645\n",
      "2025-01-18 08:59:50.009740: val_loss -0.6471\n",
      "2025-01-18 08:59:50.009859: Pseudo dice [np.float32(0.7508)]\n",
      "2025-01-18 08:59:50.010014: Epoch time: 110.78 s\n",
      "2025-01-18 08:59:50.598320: \n",
      "2025-01-18 08:59:50.598421: Epoch 843\n",
      "2025-01-18 08:59:50.598484: Current learning rate: 0.00189\n",
      "2025-01-18 09:01:41.344096: train_loss -0.8773\n",
      "2025-01-18 09:01:41.344254: val_loss -0.6298\n",
      "2025-01-18 09:01:41.344290: Pseudo dice [np.float32(0.7763)]\n",
      "2025-01-18 09:01:41.344323: Epoch time: 110.75 s\n",
      "2025-01-18 09:01:41.928280: \n",
      "2025-01-18 09:01:41.928438: Epoch 844\n",
      "2025-01-18 09:01:41.928543: Current learning rate: 0.00188\n",
      "2025-01-18 09:03:32.699063: train_loss -0.856\n",
      "2025-01-18 09:03:32.699227: val_loss -0.7307\n",
      "2025-01-18 09:03:32.699367: Pseudo dice [np.float32(0.8154)]\n",
      "2025-01-18 09:03:32.699422: Epoch time: 110.77 s\n",
      "2025-01-18 09:03:33.291109: \n",
      "2025-01-18 09:03:33.291197: Epoch 845\n",
      "2025-01-18 09:03:33.291257: Current learning rate: 0.00187\n",
      "2025-01-18 09:05:24.029488: train_loss -0.8474\n",
      "2025-01-18 09:05:24.029617: val_loss -0.7033\n",
      "2025-01-18 09:05:24.029649: Pseudo dice [np.float32(0.7717)]\n",
      "2025-01-18 09:05:24.029681: Epoch time: 110.74 s\n",
      "2025-01-18 09:05:24.629992: \n",
      "2025-01-18 09:05:24.630301: Epoch 846\n",
      "2025-01-18 09:05:24.630534: Current learning rate: 0.00186\n",
      "2025-01-18 09:07:15.370653: train_loss -0.8655\n",
      "2025-01-18 09:07:15.370862: val_loss -0.6915\n",
      "2025-01-18 09:07:15.370931: Pseudo dice [np.float32(0.7591)]\n",
      "2025-01-18 09:07:15.370970: Epoch time: 110.74 s\n",
      "2025-01-18 09:07:15.963027: \n",
      "2025-01-18 09:07:15.963396: Epoch 847\n",
      "2025-01-18 09:07:15.963466: Current learning rate: 0.00185\n",
      "2025-01-18 09:09:06.742491: train_loss -0.8569\n",
      "2025-01-18 09:09:06.742617: val_loss -0.66\n",
      "2025-01-18 09:09:06.742648: Pseudo dice [np.float32(0.6864)]\n",
      "2025-01-18 09:09:06.742683: Epoch time: 110.78 s\n",
      "2025-01-18 09:09:07.346048: \n",
      "2025-01-18 09:09:07.346505: Epoch 848\n",
      "2025-01-18 09:09:07.346665: Current learning rate: 0.00184\n",
      "2025-01-18 09:10:58.080415: train_loss -0.8636\n",
      "2025-01-18 09:10:58.080763: val_loss -0.6142\n",
      "2025-01-18 09:10:58.080956: Pseudo dice [np.float32(0.8065)]\n",
      "2025-01-18 09:10:58.081086: Epoch time: 110.73 s\n",
      "2025-01-18 09:10:58.675460: \n",
      "2025-01-18 09:10:58.675786: Epoch 849\n",
      "2025-01-18 09:10:58.675853: Current learning rate: 0.00182\n",
      "2025-01-18 09:12:49.234790: train_loss -0.8656\n",
      "2025-01-18 09:12:49.234929: val_loss -0.6689\n",
      "2025-01-18 09:12:49.234969: Pseudo dice [np.float32(0.749)]\n",
      "2025-01-18 09:12:49.235007: Epoch time: 110.56 s\n",
      "2025-01-18 09:12:50.328808: \n",
      "2025-01-18 09:12:50.329141: Epoch 850\n",
      "2025-01-18 09:12:50.329268: Current learning rate: 0.00181\n",
      "2025-01-18 09:14:40.906208: train_loss -0.8707\n",
      "2025-01-18 09:14:40.906666: val_loss -0.7276\n",
      "2025-01-18 09:14:40.906748: Pseudo dice [np.float32(0.8088)]\n",
      "2025-01-18 09:14:40.906793: Epoch time: 110.58 s\n",
      "2025-01-18 09:14:41.488173: \n",
      "2025-01-18 09:14:41.488280: Epoch 851\n",
      "2025-01-18 09:14:41.488347: Current learning rate: 0.0018\n",
      "2025-01-18 09:16:32.212317: train_loss -0.8608\n",
      "2025-01-18 09:16:32.212466: val_loss -0.6513\n",
      "2025-01-18 09:16:32.212506: Pseudo dice [np.float32(0.7907)]\n",
      "2025-01-18 09:16:32.212543: Epoch time: 110.72 s\n",
      "2025-01-18 09:16:32.795110: \n",
      "2025-01-18 09:16:32.795390: Epoch 852\n",
      "2025-01-18 09:16:32.795455: Current learning rate: 0.00179\n",
      "2025-01-18 09:18:23.356824: train_loss -0.8528\n",
      "2025-01-18 09:18:23.356958: val_loss -0.6902\n",
      "2025-01-18 09:18:23.356994: Pseudo dice [np.float32(0.7609)]\n",
      "2025-01-18 09:18:23.357028: Epoch time: 110.56 s\n",
      "2025-01-18 09:18:23.946169: \n",
      "2025-01-18 09:18:23.946529: Epoch 853\n",
      "2025-01-18 09:18:23.946626: Current learning rate: 0.00178\n",
      "2025-01-18 09:20:14.656961: train_loss -0.867\n",
      "2025-01-18 09:20:14.657102: val_loss -0.6304\n",
      "2025-01-18 09:20:14.657135: Pseudo dice [np.float32(0.7553)]\n",
      "2025-01-18 09:20:14.657168: Epoch time: 110.71 s\n",
      "2025-01-18 09:20:15.261498: \n",
      "2025-01-18 09:20:15.261948: Epoch 854\n",
      "2025-01-18 09:20:15.262079: Current learning rate: 0.00177\n",
      "2025-01-18 09:22:06.033642: train_loss -0.8798\n",
      "2025-01-18 09:22:06.033803: val_loss -0.7278\n",
      "2025-01-18 09:22:06.033877: Pseudo dice [np.float32(0.7691)]\n",
      "2025-01-18 09:22:06.033918: Epoch time: 110.77 s\n",
      "2025-01-18 09:22:06.615590: \n",
      "2025-01-18 09:22:06.615875: Epoch 855\n",
      "2025-01-18 09:22:06.616046: Current learning rate: 0.00176\n",
      "2025-01-18 09:23:57.394023: train_loss -0.8684\n",
      "2025-01-18 09:23:57.394155: val_loss -0.7384\n",
      "2025-01-18 09:23:57.394217: Pseudo dice [np.float32(0.8384)]\n",
      "2025-01-18 09:23:57.394257: Epoch time: 110.78 s\n",
      "2025-01-18 09:23:57.987423: \n",
      "2025-01-18 09:23:57.987521: Epoch 856\n",
      "2025-01-18 09:23:57.987587: Current learning rate: 0.00175\n",
      "2025-01-18 09:25:48.539361: train_loss -0.8777\n",
      "2025-01-18 09:25:48.539500: val_loss -0.6787\n",
      "2025-01-18 09:25:48.539535: Pseudo dice [np.float32(0.7953)]\n",
      "2025-01-18 09:25:48.539572: Epoch time: 110.55 s\n",
      "2025-01-18 09:25:48.539597: Yayy! New best EMA pseudo Dice: 0.7732999920845032\n",
      "2025-01-18 09:25:49.350684: \n",
      "2025-01-18 09:25:49.350865: Epoch 857\n",
      "2025-01-18 09:25:49.350995: Current learning rate: 0.00174\n",
      "2025-01-18 09:27:40.108939: train_loss -0.8773\n",
      "2025-01-18 09:27:40.109082: val_loss -0.6778\n",
      "2025-01-18 09:27:40.109123: Pseudo dice [np.float32(0.8371)]\n",
      "2025-01-18 09:27:40.109162: Epoch time: 110.76 s\n",
      "2025-01-18 09:27:40.109184: Yayy! New best EMA pseudo Dice: 0.7796000242233276\n",
      "2025-01-18 09:27:40.926008: \n",
      "2025-01-18 09:27:40.926408: Epoch 858\n",
      "2025-01-18 09:27:40.926533: Current learning rate: 0.00173\n",
      "2025-01-18 09:29:31.642792: train_loss -0.8685\n",
      "2025-01-18 09:29:31.643158: val_loss -0.6652\n",
      "2025-01-18 09:29:31.643202: Pseudo dice [np.float32(0.8399)]\n",
      "2025-01-18 09:29:31.643234: Epoch time: 110.72 s\n",
      "2025-01-18 09:29:31.643255: Yayy! New best EMA pseudo Dice: 0.7857000231742859\n",
      "2025-01-18 09:29:32.458014: \n",
      "2025-01-18 09:29:32.458366: Epoch 859\n",
      "2025-01-18 09:29:32.458441: Current learning rate: 0.00172\n",
      "2025-01-18 09:31:23.031907: train_loss -0.8802\n",
      "2025-01-18 09:31:23.032049: val_loss -0.723\n",
      "2025-01-18 09:31:23.032098: Pseudo dice [np.float32(0.7978)]\n",
      "2025-01-18 09:31:23.032133: Epoch time: 110.57 s\n",
      "2025-01-18 09:31:23.032154: Yayy! New best EMA pseudo Dice: 0.786899983882904\n",
      "2025-01-18 09:31:23.852084: \n",
      "2025-01-18 09:31:23.852486: Epoch 860\n",
      "2025-01-18 09:31:23.852581: Current learning rate: 0.0017\n",
      "2025-01-18 09:33:14.636078: train_loss -0.8706\n",
      "2025-01-18 09:33:14.636196: val_loss -0.7122\n",
      "2025-01-18 09:33:14.636227: Pseudo dice [np.float32(0.7507)]\n",
      "2025-01-18 09:33:14.636259: Epoch time: 110.78 s\n",
      "2025-01-18 09:33:15.216007: \n",
      "2025-01-18 09:33:15.216095: Epoch 861\n",
      "2025-01-18 09:33:15.216158: Current learning rate: 0.00169\n",
      "2025-01-18 09:35:05.805034: train_loss -0.8779\n",
      "2025-01-18 09:35:05.805157: val_loss -0.7042\n",
      "2025-01-18 09:35:05.805189: Pseudo dice [np.float32(0.7967)]\n",
      "2025-01-18 09:35:05.805222: Epoch time: 110.59 s\n",
      "2025-01-18 09:35:06.629358: \n",
      "2025-01-18 09:35:06.629738: Epoch 862\n",
      "2025-01-18 09:35:06.629967: Current learning rate: 0.00168\n",
      "2025-01-18 09:36:57.230715: train_loss -0.8777\n",
      "2025-01-18 09:36:57.230917: val_loss -0.7122\n",
      "2025-01-18 09:36:57.230952: Pseudo dice [np.float32(0.7928)]\n",
      "2025-01-18 09:36:57.230984: Epoch time: 110.6 s\n",
      "2025-01-18 09:36:57.814229: \n",
      "2025-01-18 09:36:57.814331: Epoch 863\n",
      "2025-01-18 09:36:57.814392: Current learning rate: 0.00167\n",
      "2025-01-18 09:38:48.353161: train_loss -0.8765\n",
      "2025-01-18 09:38:48.353304: val_loss -0.6957\n",
      "2025-01-18 09:38:48.353344: Pseudo dice [np.float32(0.7905)]\n",
      "2025-01-18 09:38:48.353384: Epoch time: 110.54 s\n",
      "2025-01-18 09:38:48.939276: \n",
      "2025-01-18 09:38:48.939371: Epoch 864\n",
      "2025-01-18 09:38:48.939435: Current learning rate: 0.00166\n",
      "2025-01-18 09:40:39.665160: train_loss -0.8754\n",
      "2025-01-18 09:40:39.665285: val_loss -0.6853\n",
      "2025-01-18 09:40:39.665317: Pseudo dice [np.float32(0.6997)]\n",
      "2025-01-18 09:40:39.665350: Epoch time: 110.73 s\n",
      "2025-01-18 09:40:40.245217: \n",
      "2025-01-18 09:40:40.245312: Epoch 865\n",
      "2025-01-18 09:40:40.245373: Current learning rate: 0.00165\n",
      "2025-01-18 09:42:30.849505: train_loss -0.8636\n",
      "2025-01-18 09:42:30.849751: val_loss -0.6492\n",
      "2025-01-18 09:42:30.849798: Pseudo dice [np.float32(0.7846)]\n",
      "2025-01-18 09:42:30.849831: Epoch time: 110.6 s\n",
      "2025-01-18 09:42:31.436821: \n",
      "2025-01-18 09:42:31.437180: Epoch 866\n",
      "2025-01-18 09:42:31.437267: Current learning rate: 0.00164\n",
      "2025-01-18 09:44:22.022939: train_loss -0.8612\n",
      "2025-01-18 09:44:22.023492: val_loss -0.6626\n",
      "2025-01-18 09:44:22.023637: Pseudo dice [np.float32(0.7734)]\n",
      "2025-01-18 09:44:22.023699: Epoch time: 110.59 s\n",
      "2025-01-18 09:44:22.604744: \n",
      "2025-01-18 09:44:22.604833: Epoch 867\n",
      "2025-01-18 09:44:22.604911: Current learning rate: 0.00163\n",
      "2025-01-18 09:46:13.336941: train_loss -0.8734\n",
      "2025-01-18 09:46:13.337076: val_loss -0.6619\n",
      "2025-01-18 09:46:13.337108: Pseudo dice [np.float32(0.7839)]\n",
      "2025-01-18 09:46:13.337141: Epoch time: 110.73 s\n",
      "2025-01-18 09:46:13.926110: \n",
      "2025-01-18 09:46:13.926496: Epoch 868\n",
      "2025-01-18 09:46:13.926584: Current learning rate: 0.00162\n",
      "2025-01-18 09:48:04.586331: train_loss -0.8678\n",
      "2025-01-18 09:48:04.586459: val_loss -0.6841\n",
      "2025-01-18 09:48:04.586492: Pseudo dice [np.float32(0.7674)]\n",
      "2025-01-18 09:48:04.586526: Epoch time: 110.66 s\n",
      "2025-01-18 09:48:05.176090: \n",
      "2025-01-18 09:48:05.176488: Epoch 869\n",
      "2025-01-18 09:48:05.176608: Current learning rate: 0.00161\n",
      "2025-01-18 09:49:55.846386: train_loss -0.8742\n",
      "2025-01-18 09:49:55.846510: val_loss -0.7157\n",
      "2025-01-18 09:49:55.846547: Pseudo dice [np.float32(0.7854)]\n",
      "2025-01-18 09:49:55.846581: Epoch time: 110.67 s\n",
      "2025-01-18 09:49:56.429788: \n",
      "2025-01-18 09:49:56.430156: Epoch 870\n",
      "2025-01-18 09:49:56.430239: Current learning rate: 0.00159\n",
      "2025-01-18 09:51:47.042503: train_loss -0.875\n",
      "2025-01-18 09:51:47.042628: val_loss -0.685\n",
      "2025-01-18 09:51:47.042662: Pseudo dice [np.float32(0.7929)]\n",
      "2025-01-18 09:51:47.042708: Epoch time: 110.61 s\n",
      "2025-01-18 09:51:47.625624: \n",
      "2025-01-18 09:51:47.625987: Epoch 871\n",
      "2025-01-18 09:51:47.626059: Current learning rate: 0.00158\n",
      "2025-01-18 09:53:38.254883: train_loss -0.8798\n",
      "2025-01-18 09:53:38.255018: val_loss -0.6982\n",
      "2025-01-18 09:53:38.255057: Pseudo dice [np.float32(0.773)]\n",
      "2025-01-18 09:53:38.255095: Epoch time: 110.63 s\n",
      "2025-01-18 09:53:38.840457: \n",
      "2025-01-18 09:53:38.840885: Epoch 872\n",
      "2025-01-18 09:53:38.841054: Current learning rate: 0.00157\n",
      "2025-01-18 09:55:29.381134: train_loss -0.8831\n",
      "2025-01-18 09:55:29.381282: val_loss -0.6661\n",
      "2025-01-18 09:55:29.381317: Pseudo dice [np.float32(0.7624)]\n",
      "2025-01-18 09:55:29.381350: Epoch time: 110.54 s\n",
      "2025-01-18 09:55:29.967297: \n",
      "2025-01-18 09:55:29.967389: Epoch 873\n",
      "2025-01-18 09:55:29.967450: Current learning rate: 0.00156\n",
      "2025-01-18 09:57:20.545826: train_loss -0.8739\n",
      "2025-01-18 09:57:20.545967: val_loss -0.6492\n",
      "2025-01-18 09:57:20.546002: Pseudo dice [np.float32(0.7819)]\n",
      "2025-01-18 09:57:20.546034: Epoch time: 110.58 s\n",
      "2025-01-18 09:57:21.127039: \n",
      "2025-01-18 09:57:21.127379: Epoch 874\n",
      "2025-01-18 09:57:21.127454: Current learning rate: 0.00155\n",
      "2025-01-18 09:59:11.846301: train_loss -0.8771\n",
      "2025-01-18 09:59:11.846532: val_loss -0.6834\n",
      "2025-01-18 09:59:11.846568: Pseudo dice [np.float32(0.8175)]\n",
      "2025-01-18 09:59:11.846601: Epoch time: 110.72 s\n",
      "2025-01-18 09:59:12.727149: \n",
      "2025-01-18 09:59:12.727506: Epoch 875\n",
      "2025-01-18 09:59:12.727645: Current learning rate: 0.00154\n",
      "2025-01-18 10:01:03.465621: train_loss -0.8774\n",
      "2025-01-18 10:01:03.465789: val_loss -0.6636\n",
      "2025-01-18 10:01:03.465826: Pseudo dice [np.float32(0.7855)]\n",
      "2025-01-18 10:01:03.465864: Epoch time: 110.74 s\n",
      "2025-01-18 10:01:04.049390: \n",
      "2025-01-18 10:01:04.049484: Epoch 876\n",
      "2025-01-18 10:01:04.049545: Current learning rate: 0.00153\n",
      "2025-01-18 10:02:54.829174: train_loss -0.8627\n",
      "2025-01-18 10:02:54.829313: val_loss -0.7033\n",
      "2025-01-18 10:02:54.829345: Pseudo dice [np.float32(0.6974)]\n",
      "2025-01-18 10:02:54.829379: Epoch time: 110.78 s\n",
      "2025-01-18 10:02:55.417883: \n",
      "2025-01-18 10:02:55.417979: Epoch 877\n",
      "2025-01-18 10:02:55.418041: Current learning rate: 0.00152\n",
      "2025-01-18 10:04:46.057559: train_loss -0.8628\n",
      "2025-01-18 10:04:46.057746: val_loss -0.6994\n",
      "2025-01-18 10:04:46.057781: Pseudo dice [np.float32(0.8048)]\n",
      "2025-01-18 10:04:46.057813: Epoch time: 110.64 s\n",
      "2025-01-18 10:04:46.645010: \n",
      "2025-01-18 10:04:46.645362: Epoch 878\n",
      "2025-01-18 10:04:46.645470: Current learning rate: 0.00151\n",
      "2025-01-18 10:06:37.188933: train_loss -0.8751\n",
      "2025-01-18 10:06:37.189056: val_loss -0.6145\n",
      "2025-01-18 10:06:37.189089: Pseudo dice [np.float32(0.757)]\n",
      "2025-01-18 10:06:37.189122: Epoch time: 110.54 s\n",
      "2025-01-18 10:06:37.783546: \n",
      "2025-01-18 10:06:37.783904: Epoch 879\n",
      "2025-01-18 10:06:37.784018: Current learning rate: 0.00149\n",
      "2025-01-18 10:08:28.405796: train_loss -0.8833\n",
      "2025-01-18 10:08:28.405920: val_loss -0.7046\n",
      "2025-01-18 10:08:28.405953: Pseudo dice [np.float32(0.7142)]\n",
      "2025-01-18 10:08:28.405987: Epoch time: 110.62 s\n",
      "2025-01-18 10:08:28.996970: \n",
      "2025-01-18 10:08:28.997383: Epoch 880\n",
      "2025-01-18 10:08:28.997477: Current learning rate: 0.00148\n",
      "2025-01-18 10:10:19.755118: train_loss -0.8776\n",
      "2025-01-18 10:10:19.755259: val_loss -0.6692\n",
      "2025-01-18 10:10:19.755297: Pseudo dice [np.float32(0.722)]\n",
      "2025-01-18 10:10:19.755331: Epoch time: 110.76 s\n",
      "2025-01-18 10:10:20.347114: \n",
      "2025-01-18 10:10:20.347219: Epoch 881\n",
      "2025-01-18 10:10:20.347287: Current learning rate: 0.00147\n",
      "2025-01-18 10:12:10.949613: train_loss -0.8771\n",
      "2025-01-18 10:12:10.949826: val_loss -0.6673\n",
      "2025-01-18 10:12:10.949860: Pseudo dice [np.float32(0.772)]\n",
      "2025-01-18 10:12:10.949892: Epoch time: 110.6 s\n",
      "2025-01-18 10:12:11.536212: \n",
      "2025-01-18 10:12:11.536309: Epoch 882\n",
      "2025-01-18 10:12:11.536371: Current learning rate: 0.00146\n",
      "2025-01-18 10:14:02.325911: train_loss -0.8721\n",
      "2025-01-18 10:14:02.326063: val_loss -0.7233\n",
      "2025-01-18 10:14:02.326104: Pseudo dice [np.float32(0.7716)]\n",
      "2025-01-18 10:14:02.326137: Epoch time: 110.79 s\n",
      "2025-01-18 10:14:02.905512: \n",
      "2025-01-18 10:14:02.905944: Epoch 883\n",
      "2025-01-18 10:14:02.906059: Current learning rate: 0.00145\n",
      "2025-01-18 10:15:53.683590: train_loss -0.8784\n",
      "2025-01-18 10:15:53.683797: val_loss -0.6917\n",
      "2025-01-18 10:15:53.683841: Pseudo dice [np.float32(0.764)]\n",
      "2025-01-18 10:15:53.683875: Epoch time: 110.78 s\n",
      "2025-01-18 10:15:54.275288: \n",
      "2025-01-18 10:15:54.275379: Epoch 884\n",
      "2025-01-18 10:15:54.275441: Current learning rate: 0.00144\n",
      "2025-01-18 10:17:44.865923: train_loss -0.8802\n",
      "2025-01-18 10:17:44.866051: val_loss -0.6474\n",
      "2025-01-18 10:17:44.866084: Pseudo dice [np.float32(0.7918)]\n",
      "2025-01-18 10:17:44.866117: Epoch time: 110.59 s\n",
      "2025-01-18 10:17:45.457591: \n",
      "2025-01-18 10:17:45.457873: Epoch 885\n",
      "2025-01-18 10:17:45.458232: Current learning rate: 0.00143\n",
      "2025-01-18 10:19:36.045001: train_loss -0.8761\n",
      "2025-01-18 10:19:36.045136: val_loss -0.6904\n",
      "2025-01-18 10:19:36.045169: Pseudo dice [np.float32(0.6934)]\n",
      "2025-01-18 10:19:36.045202: Epoch time: 110.59 s\n",
      "2025-01-18 10:19:36.624362: \n",
      "2025-01-18 10:19:36.624453: Epoch 886\n",
      "2025-01-18 10:19:36.624512: Current learning rate: 0.00142\n",
      "2025-01-18 10:21:27.178575: train_loss -0.8829\n",
      "2025-01-18 10:21:27.178743: val_loss -0.6452\n",
      "2025-01-18 10:21:27.178789: Pseudo dice [np.float32(0.8189)]\n",
      "2025-01-18 10:21:27.178825: Epoch time: 110.55 s\n",
      "2025-01-18 10:21:27.761352: \n",
      "2025-01-18 10:21:27.761441: Epoch 887\n",
      "2025-01-18 10:21:27.761502: Current learning rate: 0.00141\n",
      "2025-01-18 10:23:18.339911: train_loss -0.8812\n",
      "2025-01-18 10:23:18.340047: val_loss -0.6952\n",
      "2025-01-18 10:23:18.340082: Pseudo dice [np.float32(0.7881)]\n",
      "2025-01-18 10:23:18.340115: Epoch time: 110.58 s\n",
      "2025-01-18 10:23:19.214030: \n",
      "2025-01-18 10:23:19.214134: Epoch 888\n",
      "2025-01-18 10:23:19.214234: Current learning rate: 0.00139\n",
      "2025-01-18 10:25:09.938751: train_loss -0.8603\n",
      "2025-01-18 10:25:09.938904: val_loss -0.6469\n",
      "2025-01-18 10:25:09.938944: Pseudo dice [np.float32(0.7799)]\n",
      "2025-01-18 10:25:09.938980: Epoch time: 110.73 s\n",
      "2025-01-18 10:25:10.522117: \n",
      "2025-01-18 10:25:10.522495: Epoch 889\n",
      "2025-01-18 10:25:10.522678: Current learning rate: 0.00138\n",
      "2025-01-18 10:27:01.111185: train_loss -0.8752\n",
      "2025-01-18 10:27:01.111314: val_loss -0.6768\n",
      "2025-01-18 10:27:01.111349: Pseudo dice [np.float32(0.7688)]\n",
      "2025-01-18 10:27:01.111403: Epoch time: 110.59 s\n",
      "2025-01-18 10:27:01.695776: \n",
      "2025-01-18 10:27:01.695954: Epoch 890\n",
      "2025-01-18 10:27:01.696027: Current learning rate: 0.00137\n",
      "2025-01-18 10:28:52.330656: train_loss -0.872\n",
      "2025-01-18 10:28:52.330792: val_loss -0.6343\n",
      "2025-01-18 10:28:52.330824: Pseudo dice [np.float32(0.657)]\n",
      "2025-01-18 10:28:52.330856: Epoch time: 110.64 s\n",
      "2025-01-18 10:28:52.921224: \n",
      "2025-01-18 10:28:52.921554: Epoch 891\n",
      "2025-01-18 10:28:52.921639: Current learning rate: 0.00136\n",
      "2025-01-18 10:30:43.685889: train_loss -0.8708\n",
      "2025-01-18 10:30:43.686015: val_loss -0.7247\n",
      "2025-01-18 10:30:43.686048: Pseudo dice [np.float32(0.7072)]\n",
      "2025-01-18 10:30:43.686200: Epoch time: 110.77 s\n",
      "2025-01-18 10:30:44.269161: \n",
      "2025-01-18 10:30:44.269266: Epoch 892\n",
      "2025-01-18 10:30:44.269330: Current learning rate: 0.00135\n",
      "2025-01-18 10:32:35.061091: train_loss -0.8705\n",
      "2025-01-18 10:32:35.061226: val_loss -0.7638\n",
      "2025-01-18 10:32:35.061259: Pseudo dice [np.float32(0.8214)]\n",
      "2025-01-18 10:32:35.061291: Epoch time: 110.79 s\n",
      "2025-01-18 10:32:35.653094: \n",
      "2025-01-18 10:32:35.653453: Epoch 893\n",
      "2025-01-18 10:32:35.653541: Current learning rate: 0.00134\n",
      "2025-01-18 10:34:26.483867: train_loss -0.8705\n",
      "2025-01-18 10:34:26.483993: val_loss -0.6968\n",
      "2025-01-18 10:34:26.484028: Pseudo dice [np.float32(0.7356)]\n",
      "2025-01-18 10:34:26.484061: Epoch time: 110.83 s\n",
      "2025-01-18 10:34:27.076253: \n",
      "2025-01-18 10:34:27.076350: Epoch 894\n",
      "2025-01-18 10:34:27.076418: Current learning rate: 0.00133\n",
      "2025-01-18 10:36:17.834304: train_loss -0.8763\n",
      "2025-01-18 10:36:17.834424: val_loss -0.6097\n",
      "2025-01-18 10:36:17.834456: Pseudo dice [np.float32(0.8019)]\n",
      "2025-01-18 10:36:17.834489: Epoch time: 110.76 s\n",
      "2025-01-18 10:36:18.424390: \n",
      "2025-01-18 10:36:18.424753: Epoch 895\n",
      "2025-01-18 10:36:18.425014: Current learning rate: 0.00132\n",
      "2025-01-18 10:38:09.188496: train_loss -0.8762\n",
      "2025-01-18 10:38:09.188618: val_loss -0.6704\n",
      "2025-01-18 10:38:09.188651: Pseudo dice [np.float32(0.7728)]\n",
      "2025-01-18 10:38:09.188683: Epoch time: 110.76 s\n",
      "2025-01-18 10:38:09.784627: \n",
      "2025-01-18 10:38:09.784954: Epoch 896\n",
      "2025-01-18 10:38:09.785094: Current learning rate: 0.0013\n",
      "2025-01-18 10:40:00.551741: train_loss -0.8673\n",
      "2025-01-18 10:40:00.551873: val_loss -0.7248\n",
      "2025-01-18 10:40:00.551906: Pseudo dice [np.float32(0.8041)]\n",
      "2025-01-18 10:40:00.551939: Epoch time: 110.77 s\n",
      "2025-01-18 10:40:01.136788: \n",
      "2025-01-18 10:40:01.136989: Epoch 897\n",
      "2025-01-18 10:40:01.137058: Current learning rate: 0.00129\n",
      "2025-01-18 10:41:51.919343: train_loss -0.88\n",
      "2025-01-18 10:41:51.919493: val_loss -0.6954\n",
      "2025-01-18 10:41:51.919594: Pseudo dice [np.float32(0.7505)]\n",
      "2025-01-18 10:41:51.919650: Epoch time: 110.78 s\n",
      "2025-01-18 10:41:52.501333: \n",
      "2025-01-18 10:41:52.501707: Epoch 898\n",
      "2025-01-18 10:41:52.501891: Current learning rate: 0.00128\n",
      "2025-01-18 10:43:43.131416: train_loss -0.8773\n",
      "2025-01-18 10:43:43.131554: val_loss -0.6289\n",
      "2025-01-18 10:43:43.131588: Pseudo dice [np.float32(0.8157)]\n",
      "2025-01-18 10:43:43.131620: Epoch time: 110.63 s\n",
      "2025-01-18 10:43:43.718382: \n",
      "2025-01-18 10:43:43.718470: Epoch 899\n",
      "2025-01-18 10:43:43.718535: Current learning rate: 0.00127\n",
      "2025-01-18 10:45:34.475396: train_loss -0.8664\n",
      "2025-01-18 10:45:34.475533: val_loss -0.7172\n",
      "2025-01-18 10:45:34.475568: Pseudo dice [np.float32(0.8212)]\n",
      "2025-01-18 10:45:34.475602: Epoch time: 110.76 s\n",
      "2025-01-18 10:45:35.302931: \n",
      "2025-01-18 10:45:35.303019: Epoch 900\n",
      "2025-01-18 10:45:35.303082: Current learning rate: 0.00126\n",
      "2025-01-18 10:47:26.092537: train_loss -0.8682\n",
      "2025-01-18 10:47:26.092669: val_loss -0.7359\n",
      "2025-01-18 10:47:26.092703: Pseudo dice [np.float32(0.7964)]\n",
      "2025-01-18 10:47:26.092736: Epoch time: 110.79 s\n",
      "2025-01-18 10:47:26.950321: \n",
      "2025-01-18 10:47:26.950527: Epoch 901\n",
      "2025-01-18 10:47:26.950603: Current learning rate: 0.00125\n",
      "2025-01-18 10:49:17.698247: train_loss -0.8696\n",
      "2025-01-18 10:49:17.698388: val_loss -0.6722\n",
      "2025-01-18 10:49:17.698421: Pseudo dice [np.float32(0.7606)]\n",
      "2025-01-18 10:49:17.698453: Epoch time: 110.75 s\n",
      "2025-01-18 10:49:18.285895: \n",
      "2025-01-18 10:49:18.286299: Epoch 902\n",
      "2025-01-18 10:49:18.286394: Current learning rate: 0.00124\n",
      "2025-01-18 10:51:09.066419: train_loss -0.8722\n",
      "2025-01-18 10:51:09.066614: val_loss -0.7276\n",
      "2025-01-18 10:51:09.066648: Pseudo dice [np.float32(0.7715)]\n",
      "2025-01-18 10:51:09.066680: Epoch time: 110.78 s\n",
      "2025-01-18 10:51:09.649944: \n",
      "2025-01-18 10:51:09.650038: Epoch 903\n",
      "2025-01-18 10:51:09.650101: Current learning rate: 0.00122\n",
      "2025-01-18 10:53:00.440774: train_loss -0.8767\n",
      "2025-01-18 10:53:00.440924: val_loss -0.6807\n",
      "2025-01-18 10:53:00.440966: Pseudo dice [np.float32(0.8068)]\n",
      "2025-01-18 10:53:00.441005: Epoch time: 110.79 s\n",
      "2025-01-18 10:53:01.030793: \n",
      "2025-01-18 10:53:01.031121: Epoch 904\n",
      "2025-01-18 10:53:01.031260: Current learning rate: 0.00121\n",
      "2025-01-18 10:54:51.618943: train_loss -0.8754\n",
      "2025-01-18 10:54:51.619169: val_loss -0.6984\n",
      "2025-01-18 10:54:51.619213: Pseudo dice [np.float32(0.8012)]\n",
      "2025-01-18 10:54:51.619248: Epoch time: 110.59 s\n",
      "2025-01-18 10:54:52.207479: \n",
      "2025-01-18 10:54:52.207661: Epoch 905\n",
      "2025-01-18 10:54:52.207734: Current learning rate: 0.0012\n",
      "2025-01-18 10:56:42.983449: train_loss -0.885\n",
      "2025-01-18 10:56:42.983604: val_loss -0.693\n",
      "2025-01-18 10:56:42.983637: Pseudo dice [np.float32(0.8285)]\n",
      "2025-01-18 10:56:42.983671: Epoch time: 110.78 s\n",
      "2025-01-18 10:56:43.569719: \n",
      "2025-01-18 10:56:43.570101: Epoch 906\n",
      "2025-01-18 10:56:43.570201: Current learning rate: 0.00119\n",
      "2025-01-18 10:58:34.323364: train_loss -0.8753\n",
      "2025-01-18 10:58:34.323569: val_loss -0.633\n",
      "2025-01-18 10:58:34.323602: Pseudo dice [np.float32(0.7836)]\n",
      "2025-01-18 10:58:34.323640: Epoch time: 110.75 s\n",
      "2025-01-18 10:58:34.911508: \n",
      "2025-01-18 10:58:34.911679: Epoch 907\n",
      "2025-01-18 10:58:34.911857: Current learning rate: 0.00118\n",
      "2025-01-18 11:00:25.503688: train_loss -0.8744\n",
      "2025-01-18 11:00:25.503811: val_loss -0.7175\n",
      "2025-01-18 11:00:25.503859: Pseudo dice [np.float32(0.7702)]\n",
      "2025-01-18 11:00:25.503896: Epoch time: 110.59 s\n",
      "2025-01-18 11:00:26.092224: \n",
      "2025-01-18 11:00:26.092565: Epoch 908\n",
      "2025-01-18 11:00:26.092635: Current learning rate: 0.00117\n",
      "2025-01-18 11:02:16.759039: train_loss -0.8722\n",
      "2025-01-18 11:02:16.759153: val_loss -0.6213\n",
      "2025-01-18 11:02:16.759185: Pseudo dice [np.float32(0.7383)]\n",
      "2025-01-18 11:02:16.759222: Epoch time: 110.67 s\n",
      "2025-01-18 11:02:17.346949: \n",
      "2025-01-18 11:02:17.347257: Epoch 909\n",
      "2025-01-18 11:02:17.347344: Current learning rate: 0.00116\n",
      "2025-01-18 11:04:07.926219: train_loss -0.8712\n",
      "2025-01-18 11:04:07.926357: val_loss -0.7404\n",
      "2025-01-18 11:04:07.926390: Pseudo dice [np.float32(0.8174)]\n",
      "2025-01-18 11:04:07.926423: Epoch time: 110.58 s\n",
      "2025-01-18 11:04:08.507515: \n",
      "2025-01-18 11:04:08.507830: Epoch 910\n",
      "2025-01-18 11:04:08.508022: Current learning rate: 0.00115\n",
      "2025-01-18 11:05:59.258661: train_loss -0.8835\n",
      "2025-01-18 11:05:59.258791: val_loss -0.7038\n",
      "2025-01-18 11:05:59.258826: Pseudo dice [np.float32(0.7998)]\n",
      "2025-01-18 11:05:59.258860: Epoch time: 110.75 s\n",
      "2025-01-18 11:05:59.846017: \n",
      "2025-01-18 11:05:59.846349: Epoch 911\n",
      "2025-01-18 11:05:59.846426: Current learning rate: 0.00113\n",
      "2025-01-18 11:07:50.541487: train_loss -0.8861\n",
      "2025-01-18 11:07:50.541788: val_loss -0.7003\n",
      "2025-01-18 11:07:50.541919: Pseudo dice [np.float32(0.8275)]\n",
      "2025-01-18 11:07:50.541972: Epoch time: 110.7 s\n",
      "2025-01-18 11:07:50.541997: Yayy! New best EMA pseudo Dice: 0.7890999913215637\n",
      "2025-01-18 11:07:51.367959: \n",
      "2025-01-18 11:07:51.368049: Epoch 912\n",
      "2025-01-18 11:07:51.368111: Current learning rate: 0.00112\n",
      "2025-01-18 11:09:41.997022: train_loss -0.8772\n",
      "2025-01-18 11:09:41.997163: val_loss -0.7361\n",
      "2025-01-18 11:09:41.997203: Pseudo dice [np.float32(0.807)]\n",
      "2025-01-18 11:09:41.997241: Epoch time: 110.63 s\n",
      "2025-01-18 11:09:41.997266: Yayy! New best EMA pseudo Dice: 0.7908999919891357\n",
      "2025-01-18 11:09:42.815212: \n",
      "2025-01-18 11:09:42.815489: Epoch 913\n",
      "2025-01-18 11:09:42.815630: Current learning rate: 0.00111\n",
      "2025-01-18 11:11:33.529814: train_loss -0.8814\n",
      "2025-01-18 11:11:33.529935: val_loss -0.7368\n",
      "2025-01-18 11:11:33.529969: Pseudo dice [np.float32(0.8067)]\n",
      "2025-01-18 11:11:33.530005: Epoch time: 110.72 s\n",
      "2025-01-18 11:11:33.530029: Yayy! New best EMA pseudo Dice: 0.7925000190734863\n",
      "2025-01-18 11:11:34.612199: \n",
      "2025-01-18 11:11:34.612311: Epoch 914\n",
      "2025-01-18 11:11:34.612375: Current learning rate: 0.0011\n",
      "2025-01-18 11:13:25.349490: train_loss -0.8813\n",
      "2025-01-18 11:13:25.349623: val_loss -0.6896\n",
      "2025-01-18 11:13:25.349796: Pseudo dice [np.float32(0.7954)]\n",
      "2025-01-18 11:13:25.350067: Epoch time: 110.74 s\n",
      "2025-01-18 11:13:25.350122: Yayy! New best EMA pseudo Dice: 0.7928000092506409\n",
      "2025-01-18 11:13:26.172105: \n",
      "2025-01-18 11:13:26.172278: Epoch 915\n",
      "2025-01-18 11:13:26.172351: Current learning rate: 0.00109\n",
      "2025-01-18 11:15:16.912739: train_loss -0.8665\n",
      "2025-01-18 11:15:16.912868: val_loss -0.7257\n",
      "2025-01-18 11:15:16.912900: Pseudo dice [np.float32(0.8323)]\n",
      "2025-01-18 11:15:16.912933: Epoch time: 110.74 s\n",
      "2025-01-18 11:15:16.912953: Yayy! New best EMA pseudo Dice: 0.7967000007629395\n",
      "2025-01-18 11:15:17.736952: \n",
      "2025-01-18 11:15:17.737339: Epoch 916\n",
      "2025-01-18 11:15:17.737412: Current learning rate: 0.00108\n",
      "2025-01-18 11:17:08.335400: train_loss -0.8676\n",
      "2025-01-18 11:17:08.335527: val_loss -0.7029\n",
      "2025-01-18 11:17:08.335557: Pseudo dice [np.float32(0.7895)]\n",
      "2025-01-18 11:17:08.335590: Epoch time: 110.6 s\n",
      "2025-01-18 11:17:08.926233: \n",
      "2025-01-18 11:17:08.926390: Epoch 917\n",
      "2025-01-18 11:17:08.926457: Current learning rate: 0.00106\n",
      "2025-01-18 11:18:59.687535: train_loss -0.8759\n",
      "2025-01-18 11:18:59.687671: val_loss -0.7085\n",
      "2025-01-18 11:18:59.687842: Pseudo dice [np.float32(0.7571)]\n",
      "2025-01-18 11:18:59.687899: Epoch time: 110.76 s\n",
      "2025-01-18 11:19:00.277480: \n",
      "2025-01-18 11:19:00.277583: Epoch 918\n",
      "2025-01-18 11:19:00.277664: Current learning rate: 0.00105\n",
      "2025-01-18 11:20:50.893955: train_loss -0.8699\n",
      "2025-01-18 11:20:50.894156: val_loss -0.7307\n",
      "2025-01-18 11:20:50.894218: Pseudo dice [np.float32(0.8173)]\n",
      "2025-01-18 11:20:50.894256: Epoch time: 110.62 s\n",
      "2025-01-18 11:20:51.474528: \n",
      "2025-01-18 11:20:51.474818: Epoch 919\n",
      "2025-01-18 11:20:51.475009: Current learning rate: 0.00104\n",
      "2025-01-18 11:22:42.093861: train_loss -0.875\n",
      "2025-01-18 11:22:42.093990: val_loss -0.7173\n",
      "2025-01-18 11:22:42.094118: Pseudo dice [np.float32(0.8161)]\n",
      "2025-01-18 11:22:42.094293: Epoch time: 110.62 s\n",
      "2025-01-18 11:22:42.094356: Yayy! New best EMA pseudo Dice: 0.7968000173568726\n",
      "2025-01-18 11:22:43.000329: \n",
      "2025-01-18 11:22:43.000430: Epoch 920\n",
      "2025-01-18 11:22:43.000500: Current learning rate: 0.00103\n",
      "2025-01-18 11:24:33.749852: train_loss -0.8798\n",
      "2025-01-18 11:24:33.750030: val_loss -0.7143\n",
      "2025-01-18 11:24:33.750372: Pseudo dice [np.float32(0.8286)]\n",
      "2025-01-18 11:24:33.750445: Epoch time: 110.75 s\n",
      "2025-01-18 11:24:33.750478: Yayy! New best EMA pseudo Dice: 0.800000011920929\n",
      "2025-01-18 11:24:34.569617: \n",
      "2025-01-18 11:24:34.570032: Epoch 921\n",
      "2025-01-18 11:24:34.570171: Current learning rate: 0.00102\n",
      "2025-01-18 11:26:25.343942: train_loss -0.8782\n",
      "2025-01-18 11:26:25.344339: val_loss -0.7242\n",
      "2025-01-18 11:26:25.344386: Pseudo dice [np.float32(0.7958)]\n",
      "2025-01-18 11:26:25.344423: Epoch time: 110.77 s\n",
      "2025-01-18 11:26:25.932148: \n",
      "2025-01-18 11:26:25.932513: Epoch 922\n",
      "2025-01-18 11:26:25.932582: Current learning rate: 0.00101\n",
      "2025-01-18 11:28:16.732763: train_loss -0.8767\n",
      "2025-01-18 11:28:16.732929: val_loss -0.6961\n",
      "2025-01-18 11:28:16.733072: Pseudo dice [np.float32(0.788)]\n",
      "2025-01-18 11:28:16.733139: Epoch time: 110.8 s\n",
      "2025-01-18 11:28:17.315511: \n",
      "2025-01-18 11:28:17.315697: Epoch 923\n",
      "2025-01-18 11:28:17.315771: Current learning rate: 0.001\n",
      "2025-01-18 11:30:08.103851: train_loss -0.877\n",
      "2025-01-18 11:30:08.103979: val_loss -0.6876\n",
      "2025-01-18 11:30:08.104127: Pseudo dice [np.float32(0.79)]\n",
      "2025-01-18 11:30:08.104206: Epoch time: 110.79 s\n",
      "2025-01-18 11:30:08.685817: \n",
      "2025-01-18 11:30:08.685913: Epoch 924\n",
      "2025-01-18 11:30:08.685975: Current learning rate: 0.00098\n",
      "2025-01-18 11:31:59.282393: train_loss -0.8751\n",
      "2025-01-18 11:31:59.282772: val_loss -0.7397\n",
      "2025-01-18 11:31:59.282810: Pseudo dice [np.float32(0.83)]\n",
      "2025-01-18 11:31:59.282842: Epoch time: 110.6 s\n",
      "2025-01-18 11:31:59.282863: Yayy! New best EMA pseudo Dice: 0.8008000254631042\n",
      "2025-01-18 11:32:00.108007: \n",
      "2025-01-18 11:32:00.108264: Epoch 925\n",
      "2025-01-18 11:32:00.108338: Current learning rate: 0.00097\n",
      "2025-01-18 11:33:51.034747: train_loss -0.8784\n",
      "2025-01-18 11:33:51.034871: val_loss -0.7113\n",
      "2025-01-18 11:33:51.034905: Pseudo dice [np.float32(0.8012)]\n",
      "2025-01-18 11:33:51.034939: Epoch time: 110.93 s\n",
      "2025-01-18 11:33:51.034961: Yayy! New best EMA pseudo Dice: 0.8008000254631042\n",
      "2025-01-18 11:33:51.855030: \n",
      "2025-01-18 11:33:51.855501: Epoch 926\n",
      "2025-01-18 11:33:51.855674: Current learning rate: 0.00096\n",
      "2025-01-18 11:35:42.661043: train_loss -0.8851\n",
      "2025-01-18 11:35:42.661184: val_loss -0.6812\n",
      "2025-01-18 11:35:42.661218: Pseudo dice [np.float32(0.8152)]\n",
      "2025-01-18 11:35:42.661251: Epoch time: 110.81 s\n",
      "2025-01-18 11:35:42.661271: Yayy! New best EMA pseudo Dice: 0.802299976348877\n",
      "2025-01-18 11:35:43.480422: \n",
      "2025-01-18 11:35:43.480713: Epoch 927\n",
      "2025-01-18 11:35:43.480870: Current learning rate: 0.00095\n",
      "2025-01-18 11:37:34.227884: train_loss -0.8781\n",
      "2025-01-18 11:37:34.228020: val_loss -0.6805\n",
      "2025-01-18 11:37:34.228051: Pseudo dice [np.float32(0.783)]\n",
      "2025-01-18 11:37:34.228084: Epoch time: 110.75 s\n",
      "2025-01-18 11:37:34.810796: \n",
      "2025-01-18 11:37:34.811128: Epoch 928\n",
      "2025-01-18 11:37:34.811223: Current learning rate: 0.00094\n",
      "2025-01-18 11:39:25.453439: train_loss -0.8768\n",
      "2025-01-18 11:39:25.453577: val_loss -0.6097\n",
      "2025-01-18 11:39:25.453610: Pseudo dice [np.float32(0.803)]\n",
      "2025-01-18 11:39:25.453644: Epoch time: 110.64 s\n",
      "2025-01-18 11:39:26.032842: \n",
      "2025-01-18 11:39:26.032964: Epoch 929\n",
      "2025-01-18 11:39:26.033037: Current learning rate: 0.00092\n",
      "2025-01-18 11:41:16.817159: train_loss -0.8744\n",
      "2025-01-18 11:41:16.817292: val_loss -0.6638\n",
      "2025-01-18 11:41:16.817325: Pseudo dice [np.float32(0.8106)]\n",
      "2025-01-18 11:41:16.817357: Epoch time: 110.78 s\n",
      "2025-01-18 11:41:17.399605: \n",
      "2025-01-18 11:41:17.399701: Epoch 930\n",
      "2025-01-18 11:41:17.399765: Current learning rate: 0.00091\n",
      "2025-01-18 11:43:08.187478: train_loss -0.8892\n",
      "2025-01-18 11:43:08.187605: val_loss -0.6995\n",
      "2025-01-18 11:43:08.187639: Pseudo dice [np.float32(0.7982)]\n",
      "2025-01-18 11:43:08.187677: Epoch time: 110.79 s\n",
      "2025-01-18 11:43:08.774793: \n",
      "2025-01-18 11:43:08.775253: Epoch 931\n",
      "2025-01-18 11:43:08.775380: Current learning rate: 0.0009\n",
      "2025-01-18 11:44:59.507548: train_loss -0.8812\n",
      "2025-01-18 11:44:59.507788: val_loss -0.7157\n",
      "2025-01-18 11:44:59.507849: Pseudo dice [np.float32(0.7989)]\n",
      "2025-01-18 11:44:59.507888: Epoch time: 110.73 s\n",
      "2025-01-18 11:45:00.099807: \n",
      "2025-01-18 11:45:00.099922: Epoch 932\n",
      "2025-01-18 11:45:00.099991: Current learning rate: 0.00089\n",
      "2025-01-18 11:46:50.768582: train_loss -0.8743\n",
      "val_loss -0.7053:50.768726: \n",
      "2025-01-18 11:46:50.768901: Pseudo dice [np.float32(0.8242)]\n",
      "2025-01-18 11:46:50.769018: Epoch time: 110.67 s\n",
      "2025-01-18 11:46:50.769102: Yayy! New best EMA pseudo Dice: 0.8033000230789185\n",
      "2025-01-18 11:46:51.593135: \n",
      "2025-01-18 11:46:51.593565: Epoch 933\n",
      "2025-01-18 11:46:51.593702: Current learning rate: 0.00088\n",
      "2025-01-18 11:48:42.342769: train_loss -0.8811\n",
      "2025-01-18 11:48:42.342899: val_loss -0.6854\n",
      "2025-01-18 11:48:42.342932: Pseudo dice [np.float32(0.8357)]\n",
      "2025-01-18 11:48:42.342983: Epoch time: 110.75 s\n",
      "2025-01-18 11:48:42.343006: Yayy! New best EMA pseudo Dice: 0.8065999746322632\n",
      "2025-01-18 11:48:43.161461: \n",
      "2025-01-18 11:48:43.161781: Epoch 934\n",
      "2025-01-18 11:48:43.161852: Current learning rate: 0.00087\n",
      "2025-01-18 11:50:33.933003: train_loss -0.8872\n",
      "2025-01-18 11:50:33.933234: val_loss -0.699\n",
      "2025-01-18 11:50:33.933280: Pseudo dice [np.float32(0.7781)]\n",
      "2025-01-18 11:50:33.933387: Epoch time: 110.77 s\n",
      "2025-01-18 11:50:34.514587: \n",
      "2025-01-18 11:50:34.514675: Epoch 935\n",
      "2025-01-18 11:50:34.514736: Current learning rate: 0.00085\n",
      "2025-01-18 11:52:25.262622: train_loss -0.8773\n",
      "2025-01-18 11:52:25.262762: val_loss -0.7322\n",
      "2025-01-18 11:52:25.262795: Pseudo dice [np.float32(0.8006)]\n",
      "2025-01-18 11:52:25.262828: Epoch time: 110.75 s\n",
      "2025-01-18 11:52:25.856866: \n",
      "2025-01-18 11:52:25.856957: Epoch 936\n",
      "2025-01-18 11:52:25.857019: Current learning rate: 0.00084\n",
      "2025-01-18 11:54:16.587925: train_loss -0.8831\n",
      "2025-01-18 11:54:16.588054: val_loss -0.739\n",
      "2025-01-18 11:54:16.588087: Pseudo dice [np.float32(0.829)]\n",
      "2025-01-18 11:54:16.588119: Epoch time: 110.73 s\n",
      "2025-01-18 11:54:17.180485: \n",
      "2025-01-18 11:54:17.180585: Epoch 937\n",
      "2025-01-18 11:54:17.180660: Current learning rate: 0.00083\n",
      "2025-01-18 11:56:10.385533: train_loss -0.8876\n",
      "2025-01-18 11:56:10.385675: val_loss -0.7301\n",
      "2025-01-18 11:56:10.385708: Pseudo dice [np.float32(0.7798)]\n",
      "2025-01-18 11:56:10.385741: Epoch time: 113.21 s\n",
      "2025-01-18 11:56:11.258605: \n",
      "2025-01-18 11:56:11.259018: Epoch 938\n",
      "2025-01-18 11:56:11.259125: Current learning rate: 0.00082\n",
      "2025-01-18 11:58:03.060059: train_loss -0.8811\n",
      "2025-01-18 11:58:03.060180: val_loss -0.6658\n",
      "2025-01-18 11:58:03.060212: Pseudo dice [np.float32(0.7569)]\n",
      "2025-01-18 11:58:03.060244: Epoch time: 111.8 s\n",
      "2025-01-18 11:58:03.651191: \n",
      "2025-01-18 11:58:03.651290: Epoch 939\n",
      "2025-01-18 11:58:03.651353: Current learning rate: 0.00081\n",
      "2025-01-18 11:59:54.833272: train_loss -0.8775\n",
      "2025-01-18 11:59:54.833402: val_loss -0.6747\n",
      "2025-01-18 11:59:54.833438: Pseudo dice [np.float32(0.7567)]\n",
      "2025-01-18 11:59:54.833472: Epoch time: 111.18 s\n",
      "2025-01-18 11:59:55.424357: \n",
      "2025-01-18 11:59:55.424456: Epoch 940\n",
      "2025-01-18 11:59:55.424518: Current learning rate: 0.00079\n",
      "2025-01-18 12:01:46.453981: train_loss -0.8888\n",
      "2025-01-18 12:01:46.454109: val_loss -0.7263\n",
      "2025-01-18 12:01:46.454142: Pseudo dice [np.float32(0.7905)]\n",
      "2025-01-18 12:01:46.454184: Epoch time: 111.03 s\n",
      "2025-01-18 12:01:47.044875: \n",
      "2025-01-18 12:01:47.045033: Epoch 941\n",
      "2025-01-18 12:01:47.045100: Current learning rate: 0.00078\n",
      "2025-01-18 12:03:38.132783: train_loss -0.8824\n",
      "2025-01-18 12:03:38.133014: val_loss -0.6685\n",
      "2025-01-18 12:03:38.133076: Pseudo dice [np.float32(0.8233)]\n",
      "2025-01-18 12:03:38.133117: Epoch time: 111.09 s\n",
      "2025-01-18 12:03:38.723978: \n",
      "2025-01-18 12:03:38.724080: Epoch 942\n",
      "2025-01-18 12:03:38.724141: Current learning rate: 0.00077\n",
      "2025-01-18 12:05:29.882529: train_loss -0.8828\n",
      "2025-01-18 12:05:29.882706: val_loss -0.6915\n",
      "2025-01-18 12:05:29.882782: Pseudo dice [np.float32(0.7744)]\n",
      "2025-01-18 12:05:29.882826: Epoch time: 111.16 s\n",
      "2025-01-18 12:05:30.468555: \n",
      "2025-01-18 12:05:30.468920: Epoch 943\n",
      "2025-01-18 12:05:30.468991: Current learning rate: 0.00076\n",
      "2025-01-18 12:07:21.580338: train_loss -0.8788\n",
      "2025-01-18 12:07:21.580461: val_loss -0.7595\n",
      "2025-01-18 12:07:21.580493: Pseudo dice [np.float32(0.8176)]\n",
      "2025-01-18 12:07:21.580525: Epoch time: 111.11 s\n",
      "2025-01-18 12:07:22.171443: \n",
      "2025-01-18 12:07:22.171763: Epoch 944\n",
      "2025-01-18 12:07:22.171894: Current learning rate: 0.00075\n",
      "2025-01-18 12:09:12.961694: train_loss -0.8852\n",
      "2025-01-18 12:09:12.961966: val_loss -0.7007\n",
      "2025-01-18 12:09:12.962077: Pseudo dice [np.float32(0.7813)]\n",
      "2025-01-18 12:09:12.962123: Epoch time: 110.79 s\n",
      "2025-01-18 12:09:13.550422: \n",
      "2025-01-18 12:09:13.550707: Epoch 945\n",
      "2025-01-18 12:09:13.550901: Current learning rate: 0.00074\n",
      "2025-01-18 12:11:04.340557: train_loss -0.882\n",
      "2025-01-18 12:11:04.340773: val_loss -0.6857\n",
      "2025-01-18 12:11:04.340813: Pseudo dice [np.float32(0.8067)]\n",
      "2025-01-18 12:11:04.340847: Epoch time: 110.79 s\n",
      "2025-01-18 12:11:04.930388: \n",
      "2025-01-18 12:11:04.930479: Epoch 946\n",
      "2025-01-18 12:11:04.930539: Current learning rate: 0.00072\n",
      "2025-01-18 12:12:55.677556: train_loss -0.8788\n",
      "2025-01-18 12:12:55.677700: val_loss -0.6582\n",
      "2025-01-18 12:12:55.677743: Pseudo dice [np.float32(0.8243)]\n",
      "2025-01-18 12:12:55.677783: Epoch time: 110.75 s\n",
      "2025-01-18 12:12:56.265825: \n",
      "2025-01-18 12:12:56.265926: Epoch 947\n",
      "2025-01-18 12:12:56.265986: Current learning rate: 0.00071\n",
      "2025-01-18 12:14:47.048789: train_loss -0.8866\n",
      "2025-01-18 12:14:47.048934: val_loss -0.6844\n",
      "2025-01-18 12:14:47.048970: Pseudo dice [np.float32(0.8029)]\n",
      "2025-01-18 12:14:47.049002: Epoch time: 110.78 s\n",
      "2025-01-18 12:14:47.631983: \n",
      "2025-01-18 12:14:47.632354: Epoch 948\n",
      "2025-01-18 12:14:47.632672: Current learning rate: 0.0007\n",
      "2025-01-18 12:16:38.482138: train_loss -0.8908\n",
      "2025-01-18 12:16:38.482286: val_loss -0.6952\n",
      "2025-01-18 12:16:38.482324: Pseudo dice [np.float32(0.7599)]\n",
      "2025-01-18 12:16:38.482358: Epoch time: 110.85 s\n",
      "2025-01-18 12:16:39.064722: \n",
      "2025-01-18 12:16:39.065075: Epoch 949\n",
      "2025-01-18 12:16:39.065157: Current learning rate: 0.00069\n",
      "2025-01-18 12:18:29.827255: train_loss -0.875\n",
      "2025-01-18 12:18:29.827492: val_loss -0.7177\n",
      "2025-01-18 12:18:29.827527: Pseudo dice [np.float32(0.8213)]\n",
      "2025-01-18 12:18:29.827564: Epoch time: 110.76 s\n",
      "2025-01-18 12:18:30.658938: \n",
      "2025-01-18 12:18:30.659207: Epoch 950\n",
      "2025-01-18 12:18:30.659346: Current learning rate: 0.00067\n",
      "2025-01-18 12:20:21.400075: train_loss -0.8897\n",
      "2025-01-18 12:20:21.400248: val_loss -0.6882\n",
      "2025-01-18 12:20:21.400283: Pseudo dice [np.float32(0.8086)]\n",
      "2025-01-18 12:20:21.400316: Epoch time: 110.74 s\n",
      "2025-01-18 12:20:22.231244: \n",
      "2025-01-18 12:20:22.231547: Epoch 951\n",
      "2025-01-18 12:20:22.231680: Current learning rate: 0.00066\n",
      "2025-01-18 12:22:12.961297: train_loss -0.8876\n",
      "2025-01-18 12:22:12.961532: val_loss -0.6161\n",
      "2025-01-18 12:22:12.961626: Pseudo dice [np.float32(0.7941)]\n",
      "2025-01-18 12:22:12.961667: Epoch time: 110.73 s\n",
      "2025-01-18 12:22:13.542337: \n",
      "2025-01-18 12:22:13.542514: Epoch 952\n",
      "2025-01-18 12:22:13.542584: Current learning rate: 0.00065\n",
      "2025-01-18 12:24:04.334885: train_loss -0.8856\n",
      "2025-01-18 12:24:04.335016: val_loss -0.7628\n",
      "2025-01-18 12:24:04.335049: Pseudo dice [np.float32(0.8066)]\n",
      "2025-01-18 12:24:04.335083: Epoch time: 110.79 s\n",
      "2025-01-18 12:24:04.927529: \n",
      "2025-01-18 12:24:04.927866: Epoch 953\n",
      "2025-01-18 12:24:04.927941: Current learning rate: 0.00064\n",
      "2025-01-18 12:25:55.615487: train_loss -0.8852\n",
      "2025-01-18 12:25:55.615658: val_loss -0.66\n",
      "2025-01-18 12:25:55.615732: Pseudo dice [np.float32(0.8204)]\n",
      "2025-01-18 12:25:55.615774: Epoch time: 110.69 s\n",
      "2025-01-18 12:25:56.206985: \n",
      "2025-01-18 12:25:56.207402: Epoch 954\n",
      "2025-01-18 12:25:56.207505: Current learning rate: 0.00063\n",
      "2025-01-18 12:27:47.032098: train_loss -0.8874\n",
      "2025-01-18 12:27:47.032228: val_loss -0.6807\n",
      "2025-01-18 12:27:47.032369: Pseudo dice [np.float32(0.8243)]\n",
      "2025-01-18 12:27:47.032442: Epoch time: 110.83 s\n",
      "2025-01-18 12:27:47.623968: \n",
      "2025-01-18 12:27:47.624382: Epoch 955\n",
      "2025-01-18 12:27:47.624538: Current learning rate: 0.00061\n",
      "2025-01-18 12:29:38.436130: train_loss -0.8929\n",
      "2025-01-18 12:29:38.436415: val_loss -0.6818\n",
      "2025-01-18 12:29:38.436460: Pseudo dice [np.float32(0.7981)]\n",
      "2025-01-18 12:29:38.436508: Epoch time: 110.81 s\n",
      "2025-01-18 12:29:39.040084: \n",
      "2025-01-18 12:29:39.040461: Epoch 956\n",
      "2025-01-18 12:29:39.040537: Current learning rate: 0.0006\n",
      "2025-01-18 12:31:29.824896: train_loss -0.8933\n",
      "2025-01-18 12:31:29.825024: val_loss -0.6987\n",
      "2025-01-18 12:31:29.825058: Pseudo dice [np.float32(0.8052)]\n",
      "2025-01-18 12:31:29.825093: Epoch time: 110.79 s\n",
      "2025-01-18 12:31:30.416383: \n",
      "2025-01-18 12:31:30.416722: Epoch 957\n",
      "2025-01-18 12:31:30.416905: Current learning rate: 0.00059\n",
      "2025-01-18 12:33:21.240862: train_loss -0.8868\n",
      "2025-01-18 12:33:21.241110: val_loss -0.7093\n",
      "2025-01-18 12:33:21.241157: Pseudo dice [np.float32(0.8122)]\n",
      "2025-01-18 12:33:21.241192: Epoch time: 110.83 s\n",
      "2025-01-18 12:33:21.832882: \n",
      "2025-01-18 12:33:21.832983: Epoch 958\n",
      "2025-01-18 12:33:21.833048: Current learning rate: 0.00058\n",
      "2025-01-18 12:35:12.486143: train_loss -0.8866\n",
      "2025-01-18 12:35:12.486330: val_loss -0.6977\n",
      "2025-01-18 12:35:12.486364: Pseudo dice [np.float32(0.761)]\n",
      "2025-01-18 12:35:12.486396: Epoch time: 110.65 s\n",
      "2025-01-18 12:35:13.072644: \n",
      "2025-01-18 12:35:13.072990: Epoch 959\n",
      "2025-01-18 12:35:13.073125: Current learning rate: 0.00056\n",
      "2025-01-18 12:37:03.670803: train_loss -0.8862\n",
      "2025-01-18 12:37:03.671011: val_loss -0.7323\n",
      "2025-01-18 12:37:03.671064: Pseudo dice [np.float32(0.7909)]\n",
      "2025-01-18 12:37:03.671103: Epoch time: 110.6 s\n",
      "2025-01-18 12:37:04.262337: \n",
      "2025-01-18 12:37:04.262471: Epoch 960\n",
      "2025-01-18 12:37:04.262535: Current learning rate: 0.00055\n",
      "2025-01-18 12:38:54.994069: train_loss -0.8869\n",
      "2025-01-18 12:38:54.994236: val_loss -0.7243\n",
      "2025-01-18 12:38:54.994304: Pseudo dice [np.float32(0.7482)]\n",
      "2025-01-18 12:38:54.994345: Epoch time: 110.73 s\n",
      "2025-01-18 12:38:55.578506: \n",
      "2025-01-18 12:38:55.578664: Epoch 961\n",
      "2025-01-18 12:38:55.578728: Current learning rate: 0.00054\n",
      "2025-01-18 12:40:46.319716: train_loss -0.8854\n",
      "2025-01-18 12:40:46.319904: val_loss -0.6465\n",
      "2025-01-18 12:40:46.319949: Pseudo dice [np.float32(0.7893)]\n",
      "2025-01-18 12:40:46.320007: Epoch time: 110.74 s\n",
      "2025-01-18 12:40:46.908387: \n",
      "2025-01-18 12:40:46.908688: Epoch 962\n",
      "2025-01-18 12:40:46.908818: Current learning rate: 0.00053\n",
      "2025-01-18 12:42:37.691319: train_loss -0.883\n",
      "2025-01-18 12:42:37.691544: val_loss -0.6934\n",
      "2025-01-18 12:42:37.691587: Pseudo dice [np.float32(0.8187)]\n",
      "2025-01-18 12:42:37.691623: Epoch time: 110.78 s\n",
      "2025-01-18 12:42:38.287973: \n",
      "2025-01-18 12:42:38.288358: Epoch 963\n",
      "2025-01-18 12:42:38.288449: Current learning rate: 0.00051\n",
      "2025-01-18 12:44:29.033171: train_loss -0.8821\n",
      "2025-01-18 12:44:29.033313: val_loss -0.6878\n",
      "2025-01-18 12:44:29.033346: Pseudo dice [np.float32(0.8013)]\n",
      "2025-01-18 12:44:29.033379: Epoch time: 110.75 s\n",
      "2025-01-18 12:44:29.876421: \n",
      "2025-01-18 12:44:29.876527: Epoch 964\n",
      "2025-01-18 12:44:29.876607: Current learning rate: 0.0005\n",
      "2025-01-18 12:46:20.643026: train_loss -0.8895\n",
      "2025-01-18 12:46:20.643141: val_loss -0.7238\n",
      "2025-01-18 12:46:20.643171: Pseudo dice [np.float32(0.8037)]\n",
      "2025-01-18 12:46:20.643209: Epoch time: 110.77 s\n",
      "2025-01-18 12:46:21.239589: \n",
      "2025-01-18 12:46:21.239727: Epoch 965\n",
      "2025-01-18 12:46:21.239812: Current learning rate: 0.00049\n",
      "2025-01-18 12:48:11.988938: train_loss -0.8828\n",
      "2025-01-18 12:48:11.989136: val_loss -0.5941\n",
      "2025-01-18 12:48:11.989166: Pseudo dice [np.float32(0.7985)]\n",
      "2025-01-18 12:48:11.989198: Epoch time: 110.75 s\n",
      "2025-01-18 12:48:12.582326: \n",
      "2025-01-18 12:48:12.582524: Epoch 966\n",
      "2025-01-18 12:48:12.582612: Current learning rate: 0.00048\n",
      "2025-01-18 12:50:03.384693: train_loss -0.8879\n",
      "2025-01-18 12:50:03.384854: val_loss -0.7313\n",
      "2025-01-18 12:50:03.384886: Pseudo dice [np.float32(0.7944)]\n",
      "2025-01-18 12:50:03.384919: Epoch time: 110.8 s\n",
      "2025-01-18 12:50:03.977116: \n",
      "2025-01-18 12:50:03.977294: Epoch 967\n",
      "2025-01-18 12:50:03.977441: Current learning rate: 0.00046\n",
      "2025-01-18 12:51:54.751262: train_loss -0.8866\n",
      "2025-01-18 12:51:54.751395: val_loss -0.7204\n",
      "2025-01-18 12:51:54.751429: Pseudo dice [np.float32(0.8045)]\n",
      "2025-01-18 12:51:54.751462: Epoch time: 110.77 s\n",
      "2025-01-18 12:51:55.341813: \n",
      "2025-01-18 12:51:55.341974: Epoch 968\n",
      "2025-01-18 12:51:55.342049: Current learning rate: 0.00045\n",
      "2025-01-18 12:53:45.949904: train_loss -0.8858\n",
      "2025-01-18 12:53:45.950239: val_loss -0.715\n",
      "2025-01-18 12:53:45.950418: Pseudo dice [np.float32(0.7988)]\n",
      "2025-01-18 12:53:45.950538: Epoch time: 110.61 s\n",
      "2025-01-18 12:53:46.544522: \n",
      "2025-01-18 12:53:46.544902: Epoch 969\n",
      "2025-01-18 12:53:46.544969: Current learning rate: 0.00044\n",
      "2025-01-18 12:55:37.315140: train_loss -0.8865\n",
      "2025-01-18 12:55:37.315281: val_loss -0.6949\n",
      "2025-01-18 12:55:37.315318: Pseudo dice [np.float32(0.781)]\n",
      "2025-01-18 12:55:37.315357: Epoch time: 110.77 s\n",
      "2025-01-18 12:55:37.909274: \n",
      "2025-01-18 12:55:37.909619: Epoch 970\n",
      "2025-01-18 12:55:37.909761: Current learning rate: 0.00043\n",
      "2025-01-18 12:57:28.633052: train_loss -0.8896\n",
      "2025-01-18 12:57:28.633180: val_loss -0.669\n",
      "2025-01-18 12:57:28.633213: Pseudo dice [np.float32(0.8161)]\n",
      "2025-01-18 12:57:28.633245: Epoch time: 110.72 s\n",
      "2025-01-18 12:57:29.226503: \n",
      "2025-01-18 12:57:29.226638: Epoch 971\n",
      "2025-01-18 12:57:29.226840: Current learning rate: 0.00041\n",
      "2025-01-18 12:59:19.980296: train_loss -0.8847\n",
      "2025-01-18 12:59:19.980498: val_loss -0.7394\n",
      "2025-01-18 12:59:19.980629: Pseudo dice [np.float32(0.7989)]\n",
      "2025-01-18 12:59:19.980680: Epoch time: 110.75 s\n",
      "2025-01-18 12:59:20.572137: \n",
      "2025-01-18 12:59:20.572462: Epoch 972\n",
      "2025-01-18 12:59:20.572582: Current learning rate: 0.0004\n",
      "2025-01-18 13:01:11.336788: train_loss -0.8882\n",
      "2025-01-18 13:01:11.336951: val_loss -0.6614\n",
      "2025-01-18 13:01:11.336989: Pseudo dice [np.float32(0.8162)]\n",
      "2025-01-18 13:01:11.337025: Epoch time: 110.77 s\n",
      "2025-01-18 13:01:11.934580: \n",
      "2025-01-18 13:01:11.934843: Epoch 973\n",
      "2025-01-18 13:01:11.934921: Current learning rate: 0.00039\n",
      "2025-01-18 13:03:02.614356: train_loss -0.8939\n",
      "2025-01-18 13:03:02.614795: val_loss -0.6757\n",
      "2025-01-18 13:03:02.614867: Pseudo dice [np.float32(0.8011)]\n",
      "2025-01-18 13:03:02.614908: Epoch time: 110.68 s\n",
      "2025-01-18 13:03:03.204064: \n",
      "2025-01-18 13:03:03.204224: Epoch 974\n",
      "2025-01-18 13:03:03.204296: Current learning rate: 0.00037\n",
      "2025-01-18 13:04:53.850499: train_loss -0.8858\n",
      "2025-01-18 13:04:53.850637: val_loss -0.7227\n",
      "2025-01-18 13:04:53.850672: Pseudo dice [np.float32(0.8067)]\n",
      "2025-01-18 13:04:53.850721: Epoch time: 110.65 s\n",
      "2025-01-18 13:04:54.446580: \n",
      "2025-01-18 13:04:54.446978: Epoch 975\n",
      "2025-01-18 13:04:54.447070: Current learning rate: 0.00036\n",
      "2025-01-18 13:06:45.050435: train_loss -0.8867\n",
      "2025-01-18 13:06:45.050568: val_loss -0.7255\n",
      "2025-01-18 13:06:45.050601: Pseudo dice [np.float32(0.8069)]\n",
      "2025-01-18 13:06:45.050636: Epoch time: 110.6 s\n",
      "2025-01-18 13:06:45.648337: \n",
      "2025-01-18 13:06:45.648522: Epoch 976\n",
      "2025-01-18 13:06:45.648595: Current learning rate: 0.00035\n",
      "2025-01-18 13:08:36.492939: train_loss -0.8883\n",
      "2025-01-18 13:08:36.493064: val_loss -0.6669\n",
      "2025-01-18 13:08:36.493096: Pseudo dice [np.float32(0.7169)]\n",
      "2025-01-18 13:08:36.493127: Epoch time: 110.85 s\n",
      "2025-01-18 13:08:37.327040: \n",
      "2025-01-18 13:08:37.327461: Epoch 977\n",
      "2025-01-18 13:08:37.327563: Current learning rate: 0.00034\n",
      "2025-01-18 13:10:28.126829: train_loss -0.8912\n",
      "2025-01-18 13:10:28.126974: val_loss -0.6641\n",
      "2025-01-18 13:10:28.127006: Pseudo dice [np.float32(0.7749)]\n",
      "2025-01-18 13:10:28.127039: Epoch time: 110.8 s\n",
      "2025-01-18 13:10:28.725134: \n",
      "2025-01-18 13:10:28.725333: Epoch 978\n",
      "2025-01-18 13:10:28.725491: Current learning rate: 0.00032\n",
      "2025-01-18 13:12:19.493182: train_loss -0.8952\n",
      "2025-01-18 13:12:19.493531: val_loss -0.7462\n",
      "2025-01-18 13:12:19.493647: Pseudo dice [np.float32(0.8209)]\n",
      "2025-01-18 13:12:19.493695: Epoch time: 110.77 s\n",
      "2025-01-18 13:12:20.094955: \n",
      "2025-01-18 13:12:20.095249: Epoch 979\n",
      "2025-01-18 13:12:20.095400: Current learning rate: 0.00031\n",
      "2025-01-18 13:14:10.874028: train_loss -0.8931\n",
      "2025-01-18 13:14:10.874160: val_loss -0.711\n",
      "2025-01-18 13:14:10.874217: Pseudo dice [np.float32(0.8022)]\n",
      "2025-01-18 13:14:10.874254: Epoch time: 110.78 s\n",
      "2025-01-18 13:14:11.477476: \n",
      "2025-01-18 13:14:11.477845: Epoch 980\n",
      "2025-01-18 13:14:11.477919: Current learning rate: 0.0003\n",
      "2025-01-18 13:16:02.285687: train_loss -0.8966\n",
      "2025-01-18 13:16:02.285897: val_loss -0.7526\n",
      "2025-01-18 13:16:02.285932: Pseudo dice [np.float32(0.8232)]\n",
      "2025-01-18 13:16:02.285968: Epoch time: 110.81 s\n",
      "2025-01-18 13:16:02.885971: \n",
      "2025-01-18 13:16:02.886184: Epoch 981\n",
      "2025-01-18 13:16:02.886350: Current learning rate: 0.00028\n",
      "2025-01-18 13:17:53.688101: train_loss -0.8907\n",
      "2025-01-18 13:17:53.688251: val_loss -0.7195\n",
      "2025-01-18 13:17:53.688287: Pseudo dice [np.float32(0.8073)]\n",
      "2025-01-18 13:17:53.688318: Epoch time: 110.8 s\n",
      "2025-01-18 13:17:54.281862: \n",
      "2025-01-18 13:17:54.282169: Epoch 982\n",
      "2025-01-18 13:17:54.282352: Current learning rate: 0.00027\n",
      "2025-01-18 13:19:45.085137: train_loss -0.8931\n",
      "2025-01-18 13:19:45.085352: val_loss -0.7\n",
      "2025-01-18 13:19:45.085386: Pseudo dice [np.float32(0.8)]\n",
      "2025-01-18 13:19:45.085419: Epoch time: 110.8 s\n",
      "2025-01-18 13:19:45.669485: \n",
      "2025-01-18 13:19:45.669629: Epoch 983\n",
      "2025-01-18 13:19:45.669702: Current learning rate: 0.00026\n",
      "2025-01-18 13:21:36.491500: train_loss -0.8871\n",
      "2025-01-18 13:21:36.491646: val_loss -0.6859\n",
      "2025-01-18 13:21:36.492023: Pseudo dice [np.float32(0.8024)]\n",
      "2025-01-18 13:21:36.492091: Epoch time: 110.82 s\n",
      "2025-01-18 13:21:37.086080: \n",
      "2025-01-18 13:21:37.086266: Epoch 984\n",
      "2025-01-18 13:21:37.086384: Current learning rate: 0.00024\n",
      "2025-01-18 13:23:27.862037: train_loss -0.8852\n",
      "2025-01-18 13:23:27.862162: val_loss -0.6101\n",
      "2025-01-18 13:23:27.862195: Pseudo dice [np.float32(0.7702)]\n",
      "2025-01-18 13:23:27.862233: Epoch time: 110.78 s\n",
      "2025-01-18 13:23:28.457215: \n",
      "2025-01-18 13:23:28.457337: Epoch 985\n",
      "2025-01-18 13:23:28.457405: Current learning rate: 0.00023\n",
      "2025-01-18 13:25:19.258521: train_loss -0.8864\n",
      "2025-01-18 13:25:19.258654: val_loss -0.6702\n",
      "2025-01-18 13:25:19.258686: Pseudo dice [np.float32(0.816)]\n",
      "2025-01-18 13:25:19.258732: Epoch time: 110.8 s\n",
      "2025-01-18 13:25:19.854832: \n",
      "2025-01-18 13:25:19.854926: Epoch 986\n",
      "2025-01-18 13:25:19.854988: Current learning rate: 0.00021\n",
      "2025-01-18 13:27:10.690653: train_loss -0.8904\n",
      "2025-01-18 13:27:10.690786: val_loss -0.7247\n",
      "2025-01-18 13:27:10.690820: Pseudo dice [np.float32(0.8067)]\n",
      "2025-01-18 13:27:10.690866: Epoch time: 110.84 s\n",
      "2025-01-18 13:27:11.287645: \n",
      "2025-01-18 13:27:11.288009: Epoch 987\n",
      "2025-01-18 13:27:11.288216: Current learning rate: 0.0002\n",
      "2025-01-18 13:29:02.091831: train_loss -0.8873\n",
      "2025-01-18 13:29:02.091955: val_loss -0.7591\n",
      "2025-01-18 13:29:02.091990: Pseudo dice [np.float32(0.8001)]\n",
      "2025-01-18 13:29:02.092023: Epoch time: 110.8 s\n",
      "2025-01-18 13:29:02.686896: \n",
      "2025-01-18 13:29:02.687093: Epoch 988\n",
      "2025-01-18 13:29:02.687176: Current learning rate: 0.00019\n",
      "2025-01-18 13:30:53.432751: train_loss -0.8862\n",
      "2025-01-18 13:30:53.432917: val_loss -0.6908\n",
      "2025-01-18 13:30:53.432953: Pseudo dice [np.float32(0.8228)]\n",
      "2025-01-18 13:30:53.432988: Epoch time: 110.75 s\n",
      "2025-01-18 13:30:54.023801: \n",
      "2025-01-18 13:30:54.024133: Epoch 989\n",
      "2025-01-18 13:30:54.024268: Current learning rate: 0.00017\n",
      "2025-01-18 13:32:44.714056: train_loss -0.8893\n",
      "2025-01-18 13:32:44.714266: val_loss -0.6778\n",
      "2025-01-18 13:32:44.714341: Pseudo dice [np.float32(0.8117)]\n",
      "2025-01-18 13:32:44.714382: Epoch time: 110.69 s\n",
      "2025-01-18 13:32:45.545046: \n",
      "2025-01-18 13:32:45.545157: Epoch 990\n",
      "2025-01-18 13:32:45.545219: Current learning rate: 0.00016\n",
      "2025-01-18 13:34:36.368280: train_loss -0.8924\n",
      "2025-01-18 13:34:36.368409: val_loss -0.7501\n",
      "2025-01-18 13:34:36.368441: Pseudo dice [np.float32(0.8184)]\n",
      "2025-01-18 13:34:36.368473: Epoch time: 110.82 s\n",
      "2025-01-18 13:34:36.962434: \n",
      "2025-01-18 13:34:36.962538: Epoch 991\n",
      "2025-01-18 13:34:36.962599: Current learning rate: 0.00014\n",
      "2025-01-18 13:36:27.781822: train_loss -0.8894\n",
      "2025-01-18 13:36:27.781974: val_loss -0.7064\n",
      "2025-01-18 13:36:27.782010: Pseudo dice [np.float32(0.8025)]\n",
      "2025-01-18 13:36:27.782042: Epoch time: 110.82 s\n",
      "2025-01-18 13:36:28.371140: \n",
      "2025-01-18 13:36:28.371349: Epoch 992\n",
      "2025-01-18 13:36:28.371422: Current learning rate: 0.00013\n",
      "2025-01-18 13:38:19.151487: train_loss -0.889\n",
      "2025-01-18 13:38:19.151622: val_loss -0.6771\n",
      "2025-01-18 13:38:19.151656: Pseudo dice [np.float32(0.7965)]\n",
      "2025-01-18 13:38:19.151688: Epoch time: 110.78 s\n",
      "2025-01-18 13:38:19.743603: \n",
      "2025-01-18 13:38:19.743709: Epoch 993\n",
      "2025-01-18 13:38:19.743772: Current learning rate: 0.00011\n",
      "2025-01-18 13:40:10.533544: train_loss -0.8919\n",
      "2025-01-18 13:40:10.533675: val_loss -0.7388\n",
      "2025-01-18 13:40:10.533717: Pseudo dice [np.float32(0.8278)]\n",
      "2025-01-18 13:40:10.533754: Epoch time: 110.79 s\n",
      "2025-01-18 13:40:11.133245: \n",
      "2025-01-18 13:40:11.133347: Epoch 994\n",
      "2025-01-18 13:40:11.133410: Current learning rate: 0.0001\n",
      "2025-01-18 13:42:01.868789: train_loss -0.8965\n",
      "2025-01-18 13:42:01.868913: val_loss -0.6696\n",
      "2025-01-18 13:42:01.868945: Pseudo dice [np.float32(0.8086)]\n",
      "2025-01-18 13:42:01.868980: Epoch time: 110.74 s\n",
      "2025-01-18 13:42:02.462729: \n",
      "2025-01-18 13:42:02.462976: Epoch 995\n",
      "2025-01-18 13:42:02.463176: Current learning rate: 8e-05\n",
      "2025-01-18 13:43:53.279752: train_loss -0.8954\n",
      "2025-01-18 13:43:53.279879: val_loss -0.71\n",
      "2025-01-18 13:43:53.279912: Pseudo dice [np.float32(0.7879)]\n",
      "2025-01-18 13:43:53.280004: Epoch time: 110.82 s\n",
      "2025-01-18 13:43:53.872497: \n",
      "2025-01-18 13:43:53.872844: Epoch 996\n",
      "2025-01-18 13:43:53.873048: Current learning rate: 7e-05\n",
      "2025-01-18 13:45:44.630444: train_loss -0.8847\n",
      "2025-01-18 13:45:44.630928: val_loss -0.7219\n",
      "2025-01-18 13:45:44.631028: Pseudo dice [np.float32(0.8033)]\n",
      "2025-01-18 13:45:44.631071: Epoch time: 110.76 s\n",
      "2025-01-18 13:45:45.224337: \n",
      "2025-01-18 13:45:45.224748: Epoch 997\n",
      "2025-01-18 13:45:45.224849: Current learning rate: 5e-05\n",
      "2025-01-18 13:47:36.005507: train_loss -0.8927\n",
      "2025-01-18 13:47:36.005703: val_loss -0.6977\n",
      "2025-01-18 13:47:36.005751: Pseudo dice [np.float32(0.8002)]\n",
      "2025-01-18 13:47:36.005865: Epoch time: 110.78 s\n",
      "2025-01-18 13:47:36.613076: \n",
      "2025-01-18 13:47:36.613446: Epoch 998\n",
      "2025-01-18 13:47:36.613554: Current learning rate: 4e-05\n",
      "2025-01-18 13:49:27.212506: train_loss -0.8892\n",
      "2025-01-18 13:49:27.212660: val_loss -0.7399\n",
      "2025-01-18 13:49:27.212700: Pseudo dice [np.float32(0.7568)]\n",
      "2025-01-18 13:49:27.212737: Epoch time: 110.6 s\n",
      "2025-01-18 13:49:27.813761: \n",
      "2025-01-18 13:49:27.814041: Epoch 999\n",
      "2025-01-18 13:49:27.814129: Current learning rate: 2e-05\n",
      "2025-01-18 13:51:18.593068: train_loss -0.8896\n",
      "2025-01-18 13:51:18.593194: val_loss -0.717\n",
      "2025-01-18 13:51:18.593228: Pseudo dice [np.float32(0.8108)]\n",
      "2025-01-18 13:51:18.593261: Epoch time: 110.78 s\n",
      "2025-01-18 13:51:19.472818: Training done.\n",
      "2025-01-18 13:51:19.478910: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset002_Lung_split/splits_final.json\n",
      "2025-01-18 13:51:19.479042: The split file contains 5 splits.\n",
      "2025-01-18 13:51:19.479063: Desired fold for training: 4\n",
      "2025-01-18 13:51:19.479076: This split has 40 training and 10 validation cases.\n",
      "2025-01-18 13:51:19.479146: predicting lung_004\n",
      "2025-01-18 13:51:19.479670: lung_004, shape torch.Size([1, 276, 535, 535]), rank 0\n",
      "2025-01-18 13:52:52.320924: predicting lung_054\n",
      "2025-01-18 13:52:52.324732: lung_054, shape torch.Size([1, 241, 490, 490]), rank 0\n",
      "2025-01-18 13:54:18.900177: predicting lung_064\n",
      "2025-01-18 13:54:18.902693: lung_064, shape torch.Size([1, 248, 455, 455]), rank 0\n",
      "2025-01-18 13:55:16.668305: predicting lung_066\n",
      "2025-01-18 13:55:16.671154: lung_066, shape torch.Size([1, 241, 573, 573]), rank 0\n",
      "2025-01-18 13:56:57.759736: predicting lung_070\n",
      "2025-01-18 13:56:57.763303: lung_070, shape torch.Size([1, 266, 492, 492]), rank 0\n",
      "2025-01-18 13:58:24.354903: predicting lung_079\n",
      "2025-01-18 13:58:24.357558: lung_079, shape torch.Size([1, 251, 601, 601]), rank 0\n",
      "2025-01-18 14:00:25.619077: predicting lung_092\n",
      "2025-01-18 14:00:25.622818: lung_092, shape torch.Size([1, 245, 614, 614]), rank 0\n",
      "2025-01-18 14:02:26.927033: predicting lung_093\n",
      "2025-01-18 14:02:26.931830: lung_093, shape torch.Size([1, 228, 505, 505]), rank 0\n",
      "2025-01-18 14:03:39.134708: predicting lung_095\n",
      "2025-01-18 14:03:39.137551: lung_095, shape torch.Size([1, 231, 499, 499]), rank 0\n",
      "2025-01-18 14:04:51.356124: predicting lung_096\n",
      "2025-01-18 14:04:51.358758: lung_096, shape torch.Size([1, 251, 506, 506]), rank 0\n",
      "2025-01-18 14:06:44.150032: Validation complete\n",
      "2025-01-18 14:06:44.150811: Mean Validation Dice:  0.6794226273200477\n",
      "Standard U-Net training completed for 3d_fullres fold 4 -epoch 1000\n",
      "Standard U-Net training completed for all 5 folds of 3d_fullres\n"
     ]
    }
   ],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self, dataset_id: int = 2, epoch: int = 1000):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.epoch = epoch\n",
    "        self.configurations = ['2d', '3d_fullres', '3d_lowres', '3d_cascade_fullres']\n",
    "        self.folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    #first approach - 3 fold training\n",
    "    def train_standard_unet_3folds(self, config: str, epoch):\n",
    "        \"\"\"Train with standard U-Net configuration for all 5 folds\"\"\"\n",
    "        num_folds = 3 #testing purpose, train with 3 folds instead of 5, 500 epochs instead of 1000\n",
    "        for fold in range(num_folds):\n",
    "            try:\n",
    "                # to continue training after paused (Ctrl C) addd  --continue_training flag at the end of command\n",
    "                command = f'nnUNetv2_train {self.dataset_id} {config} {fold}'\n",
    "                print(f\"Starting standard U-Net training: {command}\")\n",
    "                os.system(command)\n",
    "                print(f\"Standard U-Net training completed for {config} fold {fold} -epoch {epoch}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Standard U-Net training failed for fold {fold}: {str(e)}\")\n",
    "                raise\n",
    "        print(f\"Standard U-Net training completed for all {num_folds} folds of {config}\")\n",
    "    #second approach: train all posible models with default 5 fold cross validation. but only with 250 epochs. see which performs the best for final training\n",
    "    def train_all_configurations(self, epoch=250):\n",
    "        \"\"\"Train all available configurations with default folds for specified epochs\"\"\"\n",
    "        print(f\"Starting training for all configurations with {epoch} epochs...\")\n",
    "        \n",
    "        for config in self.configurations:\n",
    "            try:\n",
    "                # Construct the path for this configuration\n",
    "                config_path = os.path.join(os.environ[\"nnUNet_results\"], \n",
    "                                         f\"Dataset{self.dataset_id:03d}_Lung/nnUNetTrainer_{epoch}epochs__nnUNetPlans__{config}\")\n",
    "                \n",
    "                # Check if this configuration is already fully trained\n",
    "                all_folds_trained = True\n",
    "                for fold in self.folds:\n",
    "                    fold_path = os.path.join(config_path, f\"fold_{fold}\")\n",
    "                    checkpoint_path = os.path.join(fold_path, \"checkpoint_final.pth\")\n",
    "                    if not os.path.exists(checkpoint_path):\n",
    "                        all_folds_trained = False\n",
    "                        break\n",
    "                \n",
    "                if all_folds_trained:\n",
    "                    print(f\"Skipping configuration {config} as it has already been trained for all folds\")\n",
    "                    continue\n",
    "                    \n",
    "                # For cascade, check if 3d_lowres exists\n",
    "                if config == '3d_cascade_fullres':\n",
    "                    lowres_path = os.path.join(os.environ[\"nnUNet_results\"], \n",
    "                                             f\"Dataset{self.dataset_id:03d}_Lung/nnUNetTrainer_{epoch}epochs__nnUNetPlans__3d_lowres\")\n",
    "                    if not os.path.exists(lowres_path):\n",
    "                        print(\"Skipping 3d_cascade_fullres as 3d_lowres is not trained yet\")\n",
    "                        continue\n",
    "                \n",
    "                # Train for each fold\n",
    "                for fold in self.folds:\n",
    "                    # Check if this specific fold is already trained\n",
    "                    fold_path = os.path.join(config_path, f\"fold_{fold}\")\n",
    "                    checkpoint_path = os.path.join(fold_path, \"checkpoint_final.pth\")\n",
    "                    \n",
    "                    if os.path.exists(checkpoint_path):\n",
    "                        print(f\"Skipping {config} fold {fold} as it has already been trained\")\n",
    "                        continue\n",
    "                    \n",
    "                    command = f'nnUNetv2_train {self.dataset_id} {config} {fold} -tr nnUNetTrainer_{epoch}epochs --npz'\n",
    "                    print(f\"Starting training for configuration: {config}, fold: {fold}\")\n",
    "                    print(f\"Command: {command}\")\n",
    "                    os.system(command)\n",
    "                    print(f\"Training completed for configuration: {config}, fold: {fold}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Training failed for configuration {config}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"Training completed for all configurations\")\n",
    "    \n",
    "    #based on approach 2:\n",
    "    #Evaluate all possible 2-configuration ensembles\n",
    "    #nnUNetv2_find_best_configuration will also automatically determine the postprocessing that should be used\n",
    "    def find_best_configuration(self, epoch=250):\n",
    "        \"\"\"Find the best configuration or ensemble among trained models\"\"\"\n",
    "        trained_configs = ['2d', '3d_fullres', '3d_lowres']  # Excluding 3d_cascade_fullres\n",
    "        \n",
    "        # Convert list of configs to string for command\n",
    "        config_str = ' '.join(trained_configs)\n",
    "        \n",
    "        try:\n",
    "            command = f'nnUNetv2_find_best_configuration {self.dataset_id} -c {config_str} -tr nnUNetTrainer_{epoch}epochs'\n",
    "            print(f\"Finding best configuration with command: {command}\")\n",
    "            os.system(command)\n",
    "            \n",
    "            # Print location of results\n",
    "            results_path = os.path.join(os.environ[\"nnUNet_results\"], f\"Dataset{self.dataset_id:03d}_Lung\")\n",
    "            print(f\"\\nResults available at:\")\n",
    "            print(f\"1. {os.path.join(results_path, 'inference_instructions.txt')}\")\n",
    "            print(f\"2. {os.path.join(results_path, 'inference_information.json')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding best configuration: {str(e)}\")\n",
    "            raise    \n",
    "    #final approach -> default: train 3d fullress 1000 epoch 5 folds -> with splitted dataset (all cancerous)\n",
    "    def train_standard_unet_default_splitted(self, config: str, epoch):\n",
    "        \"\"\"Train with standard U-Net configuration for all 5 folds\"\"\"\n",
    "        num_folds = 5 \n",
    "        for fold in range(num_folds):\n",
    "            try:\n",
    "                # to continue training after paused (Ctrl C) add  --c flag at the end of command\n",
    "                command = f'nnUNetv2_train {self.dataset_id} {config} {fold} --npz --c'\n",
    "                print(f\"Starting standard U-Net training: {command}\")\n",
    "                os.system(command)\n",
    "                print(f\"Standard U-Net training completed for {config} fold {fold} -epoch {epoch}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Standard U-Net training failed for fold {fold}: {str(e)}\")\n",
    "                raise\n",
    "        print(f\"Standard U-Net training completed for all {num_folds} folds of {config}\")\n",
    "           \n",
    "\n",
    "# Example usage:\n",
    "trainer = TrainingConfig()\n",
    "\n",
    "trainer.train_standard_unet_default_splitted('3d_fullres', 1000)     # Full training with standard U-Net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running inference on single image: lung_001_0000.nii.gz ===\n",
      "Running command: nnUNetv2_predict -d Dataset002_Lung_split -i /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/temp_single_image_input -o /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/temp_single_image_output -f 0 1 2 3 4 -c 3d_fullres -p nnUNetPlans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/inference/predict_from_raw_data.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(join(model_training_output_dir, f'fold_{f}', checkpoint_name),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting lung_001:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [01:12<00:00,  1.65it/s]\n",
      "100%|██████████| 120/120 [01:10<00:00,  1.71it/s]\n",
      "100%|██████████| 120/120 [01:10<00:00,  1.71it/s]\n",
      "100%|██████████| 120/120 [01:10<00:00,  1.71it/s]\n",
      "100%|██████████| 120/120 [01:10<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with lung_001\n",
      "Running command: nnUNetv2_apply_postprocessing -i /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/temp_single_image_output -o /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/temp_single_image_postproc -pp_pkl_file /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/nnUNetTrainer__nnUNetPlans__3d_fullres/crossval_results_folds_0_1_2_3_4/postprocessing.pkl -plans_json /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/nnUNetTrainer__nnUNetPlans__3d_fullres/crossval_results_folds_0_1_2_3_4/plans.json -np 8\n",
      "\n",
      "=== Single image inference completed ===\n",
      "Time taken: 399.54 seconds\n",
      "Result saved at: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/temp_single_image_postproc/lung_001.nii.gz\n",
      "Inference completed in 399.54 seconds. Result saved at: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/temp_single_image_postproc/lung_001.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import shutil\n",
    "\n",
    "\n",
    "class InferencePipeline:\n",
    "    \"\"\"\n",
    "    Inference pipeline with automatic best configuration finding:\n",
    "    - Supports finding best configuration and postprocessing\n",
    "    - Can run single configuration or ensemble predictions\n",
    "    - Applies determined postprocessing\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_id: int = 1, epoch: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize InferencePipeline\n",
    "        Args:\n",
    "            dataset_id: Dataset identifier (default: 1)\n",
    "            epoch: Number of epochs used in training (default: 250)\n",
    "        \"\"\"\n",
    "        self.dataset_id = dataset_id\n",
    "        self.epoch = epoch\n",
    "        self.dataset_name = f\"Dataset{dataset_id:03d}_Lung_split\"\n",
    "        \n",
    "        # Setup base directories\n",
    "        self.results_dir = os.environ[\"nnUNet_results\"]\n",
    "        self.base_output = os.path.join(self.results_dir, f\"{self.dataset_name}/predictions\")\n",
    "        os.makedirs(self.base_output, exist_ok=True)\n",
    "        \n",
    "        # Setup output paths for each step\n",
    "        self.fullres_output = os.path.join(self.base_output, \"3d_fullres_output\")\n",
    "        self.lowres_output = os.path.join(self.base_output, \"3d_lowres_output\")\n",
    "        self.ensemble_output = os.path.join(self.base_output, \"ensemble_output\")\n",
    "        self.final_output = os.path.join(self.base_output, \"final_output_with_postprocessing\")\n",
    "        \n",
    "        # Best configuration info will be set after finding it\n",
    "        self.best_configuration = None\n",
    "        self.use_ensemble = False\n",
    "        self.postprocessing_file = None\n",
    "        self.plans_file = None\n",
    "\n",
    "    def find_best_configuration(self):\n",
    "        \"\"\"Find the best configuration and set up postprocessing\"\"\"\n",
    "        print(\"\\n=== Finding Best Configuration ===\")\n",
    "        try:\n",
    "            # Run find_best_configuration for the trained configuration\n",
    "            command = (f'nnUNetv2_find_best_configuration {self.dataset_id} '\n",
    "                      f'-c 3d_fullres')\n",
    "            print(f\"Running command: {command}\")\n",
    "            if os.system(command) != 0:\n",
    "                raise RuntimeError(\"Finding best configuration failed\")\n",
    "\n",
    "            # Read the inference_information.json to get configuration details\n",
    "            info_file = os.path.join(self.results_dir, self.dataset_name, \"inference_information.json\")\n",
    "            import json\n",
    "            with open(info_file, 'r') as f:\n",
    "                info = json.load(f)\n",
    "            \n",
    "            # Set configuration based on results\n",
    "            self.best_configuration = info.get(\"best_configuration\", \"3d_fullres\")\n",
    "            \n",
    "            # Get postprocessing and plans file paths\n",
    "            model_folder = os.path.join(self.results_dir, self.dataset_name,\n",
    "                                      f\"nnUNetTrainer__nnUNetPlans__{self.best_configuration}\")\n",
    "            self.postprocessing_file = os.path.join(model_folder, \"postprocessing.pkl\")\n",
    "            self.plans_file = os.path.join(model_folder, \"plans.json\")\n",
    "            \n",
    "            print(f\"Best configuration determined: {self.best_configuration}\")\n",
    "            print(f\"Postprocessing file: {self.postprocessing_file}\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding best configuration: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def predict_3d_fullres(self, input_folder: str):\n",
    "        \"\"\"Run predictions for 3D fullres model\"\"\"\n",
    "        print(\"\\n=== Running 3D Fullres Predictions ===\")\n",
    "        command = (f'nnUNetv2_predict -d {self.dataset_name} '\n",
    "                  f'-i {input_folder} -o {self.fullres_output} '\n",
    "                  f'-f 0 1 2 3 4 '\n",
    "                  f'-c 3d_fullres -p nnUNetPlans')\n",
    "        print(f\"Running command: {command}\")\n",
    "        return os.system(command)\n",
    "\n",
    "    def apply_postprocessing(self, input_folder: str, output_folder: str):\n",
    "        \"\"\"Apply postprocessing to results\"\"\"\n",
    "        print(\"\\n=== Applying Postprocessing ===\")\n",
    "                \n",
    "        # Update path to use crossval folder\n",
    "        crossval_folder = os.path.join(self.results_dir, self.dataset_name,\n",
    "                                   \"nnUNetTrainer__nnUNetPlans__3d_fullres\",\n",
    "                                   \"crossval_results_folds_0_1_2_3_4\")\n",
    "        \n",
    "        postprocessing_file = os.path.join(crossval_folder, \"postprocessing.pkl\")\n",
    "        plans_file = os.path.join(crossval_folder, \"plans.json\")\n",
    "    \n",
    "        print(f\"Using postprocessing file from: {postprocessing_file}\")\n",
    "        \n",
    "        if not os.path.exists(postprocessing_file):\n",
    "            raise RuntimeError(f\"Postprocessing file not found at {postprocessing_file}\")\n",
    "                \n",
    "        command = (f'nnUNetv2_apply_postprocessing -i {input_folder} '\n",
    "                  f'-o {output_folder} '\n",
    "                  f'-pp_pkl_file {postprocessing_file} '\n",
    "                  f'-plans_json {plans_file} '\n",
    "                  f'-np 8')\n",
    "        print(f\"Running command: {command}\")\n",
    "        return os.system(command)\n",
    "\n",
    "    def run_full_pipeline(self, input_folder: str):\n",
    "        \"\"\"Run complete inference pipeline including finding best configuration and postprocessing\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nStarting inference pipeline for {self.dataset_name}\")\n",
    "            print(f\"Input folder: {input_folder}\")\n",
    "            print(f\"Results will be saved in: {self.base_output}\")\n",
    "            \n",
    "            # # First find best configuration\n",
    "            # if not self.find_best_configuration():\n",
    "            #     raise RuntimeError(\"Could not determine best configuration\")\n",
    "            \n",
    "            # Run prediction\n",
    "            if self.predict_3d_fullres(input_folder) != 0:\n",
    "                raise RuntimeError(\"3D Fullres prediction failed\")\n",
    "                \n",
    "            # Apply postprocessing\n",
    "            if self.apply_postprocessing(self.fullres_output, self.final_output) != 0:\n",
    "                raise RuntimeError(\"Postprocessing failed\")\n",
    "\n",
    "            print(\"\\n=== Inference Pipeline Completed Successfully ===\")\n",
    "            print(f\"Final results with postprocessing can be found in: {self.final_output}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference pipeline: {str(e)}\")\n",
    "            raise\n",
    "    def predict_single_image(self, image_file_path: str):\n",
    "        \"\"\"\n",
    "        Run inference on a single image and measure the time taken\n",
    "        \n",
    "        Args:\n",
    "            image_file_path: Full path to the image file\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (time_taken, output_path)\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Running inference on single image: {os.path.basename(image_file_path)} ===\")\n",
    "        \n",
    "        # Create temporary directory for the single image\n",
    "        temp_input_dir = os.path.join(self.base_output, \"temp_single_image_input\")\n",
    "        temp_output_dir = os.path.join(self.base_output, \"temp_single_image_output\")\n",
    "        temp_postproc_dir = os.path.join(self.base_output, \"temp_single_image_postproc\")\n",
    "        \n",
    "        os.makedirs(temp_input_dir, exist_ok=True)\n",
    "        os.makedirs(temp_output_dir, exist_ok=True)\n",
    "        os.makedirs(temp_postproc_dir, exist_ok=True)\n",
    "        \n",
    "        # Get filename and copy to temp input dir\n",
    "        filename = os.path.basename(image_file_path)\n",
    "        destination = os.path.join(temp_input_dir, filename)\n",
    "        shutil.copy(image_file_path, destination)\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Run prediction on single image\n",
    "            command = (f'nnUNetv2_predict -d {self.dataset_name} '\n",
    "                      f'-i {temp_input_dir} -o {temp_output_dir} '\n",
    "                      f'-f 0 1 2 3 4 '\n",
    "                      f'-c 3d_fullres -p nnUNetPlans')\n",
    "            print(f\"Running command: {command}\")\n",
    "            if os.system(command) != 0:\n",
    "                raise RuntimeError(\"Prediction failed for single image\")\n",
    "            \n",
    "            # Apply postprocessing\n",
    "            crossval_folder = os.path.join(self.results_dir, self.dataset_name,\n",
    "                                       \"nnUNetTrainer__nnUNetPlans__3d_fullres\",\n",
    "                                       \"crossval_results_folds_0_1_2_3_4\")\n",
    "            \n",
    "            postprocessing_file = os.path.join(crossval_folder, \"postprocessing.pkl\")\n",
    "            plans_file = os.path.join(crossval_folder, \"plans.json\")\n",
    "            \n",
    "            if not os.path.exists(postprocessing_file):\n",
    "                raise RuntimeError(f\"Postprocessing file not found at {postprocessing_file}\")\n",
    "            \n",
    "            command = (f'nnUNetv2_apply_postprocessing -i {temp_output_dir} '\n",
    "                      f'-o {temp_postproc_dir} '\n",
    "                      f'-pp_pkl_file {postprocessing_file} '\n",
    "                      f'-plans_json {plans_file} '\n",
    "                      f'-np 8')\n",
    "            print(f\"Running command: {command}\")\n",
    "            if os.system(command) != 0:\n",
    "                raise RuntimeError(\"Postprocessing failed for single image\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            time_taken = end_time - start_time\n",
    "            \n",
    "            # Get output filename (remove the _0000 suffix that might be in the input filename)\n",
    "            base_name = filename.split('_0000')[0] if '_0000' in filename else os.path.splitext(filename)[0]\n",
    "            output_file = os.path.join(temp_postproc_dir, f\"{base_name}.nii.gz\")\n",
    "            \n",
    "            print(f\"\\n=== Single image inference completed ===\")\n",
    "            print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "            print(f\"Result saved at: {output_file}\")\n",
    "            \n",
    "            return time_taken, output_file\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during single image inference: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Keep the final result but clean up intermediate files\n",
    "            # Uncomment these if you want to clean up temp folders\n",
    "            shutil.rmtree(temp_input_dir, ignore_errors=True)\n",
    "            shutil.rmtree(temp_output_dir, ignore_errors=True)\n",
    "            pass\n",
    "\n",
    "\n",
    "# Initialize inference pipeline\n",
    "inference = InferencePipeline(dataset_id=2)\n",
    "\n",
    "single_image_path = \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/imagesTs/lung_001_0000.nii.gz\"\n",
    "time_taken, output_path = inference.predict_single_image(single_image_path)\n",
    "print(f\"Inference completed in {time_taken:.2f} seconds. Result saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed lung_012.nii.gz to lung_012_0000.nii.gz\n",
      "Renamed lung_007.nii.gz to lung_007_0000.nii.gz\n",
      "Renamed lung_056.nii.gz to lung_056_0000.nii.gz\n",
      "Renamed lung_011.nii.gz to lung_011_0000.nii.gz\n",
      "Renamed lung_072.nii.gz to lung_072_0000.nii.gz\n",
      "Renamed lung_067.nii.gz to lung_067_0000.nii.gz\n",
      "Renamed lung_076.nii.gz to lung_076_0000.nii.gz\n",
      "Renamed lung_088.nii.gz to lung_088_0000.nii.gz\n",
      "Renamed lung_013.nii.gz to lung_013_0000.nii.gz\n",
      "Renamed lung_035.nii.gz to lung_035_0000.nii.gz\n",
      "Renamed lung_068.nii.gz to lung_068_0000.nii.gz\n",
      "Renamed lung_024.nii.gz to lung_024_0000.nii.gz\n",
      "Renamed lung_017.nii.gz to lung_017_0000.nii.gz\n",
      "Renamed lung_002.nii.gz to lung_002_0000.nii.gz\n",
      "Renamed lung_077.nii.gz to lung_077_0000.nii.gz\n",
      "Renamed lung_060.nii.gz to lung_060_0000.nii.gz\n",
      "Renamed lung_021.nii.gz to lung_021_0000.nii.gz\n",
      "Renamed lung_032.nii.gz to lung_032_0000.nii.gz\n",
      "Renamed lung_089.nii.gz to lung_089_0000.nii.gz\n",
      "Renamed lung_019.nii.gz to lung_019_0000.nii.gz\n",
      "Renamed lung_090.nii.gz to lung_090_0000.nii.gz\n",
      "Renamed lung_085.nii.gz to lung_085_0000.nii.gz\n",
      "Renamed lung_082.nii.gz to lung_082_0000.nii.gz\n",
      "Renamed lung_091.nii.gz to lung_091_0000.nii.gz\n",
      "Renamed lung_008.nii.gz to lung_008_0000.nii.gz\n",
      "Renamed lung_040.nii.gz to lung_040_0000.nii.gz\n",
      "Renamed lung_030.nii.gz to lung_030_0000.nii.gz\n",
      "Renamed lung_052.nii.gz to lung_052_0000.nii.gz\n",
      "Renamed lung_050.nii.gz to lung_050_0000.nii.gz\n",
      "Renamed lung_087.nii.gz to lung_087_0000.nii.gz\n",
      "Renamed lung_039.nii.gz to lung_039_0000.nii.gz\n",
      "Renamed lung_063.nii.gz to lung_063_0000.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Rename the testing images before inference it so that it follows the correct format -> run this only when needed\n",
    "test_folder = '/home/doodledaron/FYP/nnUnet/FYP-file/RawData/Task001_Lung/imagesTs'\n",
    "\n",
    "for filename in os.listdir(test_folder):\n",
    "    if filename.endswith('.nii.gz'):\n",
    "        old_path = os.path.join(test_folder, filename)\n",
    "        new_name = filename.replace('.nii.gz', '_0000.nii.gz')\n",
    "        new_path = os.path.join(test_folder, new_name)\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed {filename} to {new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running folder-based evaluation on raw predictions...\n",
      "\n",
      "=== Running Folder-based Evaluation ===\n",
      "Using predictions from: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output\n",
      "Using ground truth from: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs\n",
      "Using dataset.json from: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/dataset.json\n",
      "Using plans.json from: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json\n",
      "Results will be saved to: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/evaluation_results_raw.json\n",
      "Running command: nnUNetv2_evaluate_folder /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output -djfile /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/dataset.json -pfile /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json -o /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/evaluation_results_raw.json\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "Evaluation Results:\n",
      "{\n",
      "  \"foreground_mean\": {\n",
      "    \"Dice\": 0.7807254323319224,\n",
      "    \"FN\": 8220.384615384615,\n",
      "    \"FP\": 4758.923076923077,\n",
      "    \"IoU\": 0.6486048387770675,\n",
      "    \"TN\": 79338346.61538461,\n",
      "    \"TP\": 17811.30769230769,\n",
      "    \"n_pred\": 22570.23076923077,\n",
      "    \"n_ref\": 26031.69230769231\n",
      "  },\n",
      "  \"mean\": {\n",
      "    \"1\": {\n",
      "      \"Dice\": 0.7807254323319224,\n",
      "      \"FN\": 8220.384615384615,\n",
      "      \"FP\": 4758.923076923077,\n",
      "      \"IoU\": 0.6486048387770675,\n",
      "      \"TN\": 79338346.61538461,\n",
      "      \"TP\": 17811.30769230769,\n",
      "      \"n_pred\": 22570.23076923077,\n",
      "      \"n_ref\": 26031.69230769231\n",
      "    }\n",
      "  },\n",
      "  \"metric_per_case\": [\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8073229291716687,\n",
      "          \"FN\": 29,\n",
      "          \"FP\": 1255,\n",
      "          \"IoU\": 0.676899849018621,\n",
      "          \"TN\": 79687802,\n",
      "          \"TP\": 2690,\n",
      "          \"n_pred\": 3945,\n",
      "          \"n_ref\": 2719\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_001.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_001.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8626617732277381,\n",
      "          \"FN\": 351,\n",
      "          \"FP\": 1782,\n",
      "          \"IoU\": 0.7584918478260869,\n",
      "          \"TN\": 148626816,\n",
      "          \"TP\": 6699,\n",
      "          \"n_pred\": 8481,\n",
      "          \"n_ref\": 7050\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_006.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_006.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8422535211267606,\n",
      "          \"FN\": 1302,\n",
      "          \"FP\": 490,\n",
      "          \"IoU\": 0.7274939172749392,\n",
      "          \"TN\": 82306640,\n",
      "          \"TP\": 4784,\n",
      "          \"n_pred\": 5274,\n",
      "          \"n_ref\": 6086\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_020.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_020.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8608374606033319,\n",
      "          \"FN\": 5379,\n",
      "          \"FP\": 2348,\n",
      "          \"IoU\": 0.7556757098589768,\n",
      "          \"TN\": 59475062,\n",
      "          \"TP\": 23899,\n",
      "          \"n_pred\": 26247,\n",
      "          \"n_ref\": 29278\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_026.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_026.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8876805921286741,\n",
      "          \"FN\": 650,\n",
      "          \"FP\": 7241,\n",
      "          \"IoU\": 0.7980446855885138,\n",
      "          \"TN\": 69166943,\n",
      "          \"TP\": 31182,\n",
      "          \"n_pred\": 38423,\n",
      "          \"n_ref\": 31832\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_027.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_027.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8313568842841094,\n",
      "          \"FN\": 594,\n",
      "          \"FP\": 757,\n",
      "          \"IoU\": 0.7113864558854945,\n",
      "          \"TN\": 66842039,\n",
      "          \"TP\": 3330,\n",
      "          \"n_pred\": 4087,\n",
      "          \"n_ref\": 3924\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_029.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_029.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.598692259017085,\n",
      "          \"FN\": 65235,\n",
      "          \"FP\": 1356,\n",
      "          \"IoU\": 0.4272382443253658,\n",
      "          \"TN\": 79575513,\n",
      "          \"TP\": 49672,\n",
      "          \"n_pred\": 51028,\n",
      "          \"n_ref\": 114907\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_031.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_031.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.861640169998426,\n",
      "          \"FN\": 391,\n",
      "          \"FP\": 488,\n",
      "          \"IoU\": 0.7569137168141593,\n",
      "          \"TN\": 29356512,\n",
      "          \"TP\": 2737,\n",
      "          \"n_pred\": 3225,\n",
      "          \"n_ref\": 3128\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_057.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_057.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.7523056653491436,\n",
      "          \"FN\": 152,\n",
      "          \"FP\": 976,\n",
      "          \"IoU\": 0.6029567053854277,\n",
      "          \"TN\": 57144551,\n",
      "          \"TP\": 1713,\n",
      "          \"n_pred\": 2689,\n",
      "          \"n_ref\": 1865\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_059.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_059.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.7188235294117648,\n",
      "          \"FN\": 468,\n",
      "          \"FP\": 10,\n",
      "          \"IoU\": 0.5610651974288338,\n",
      "          \"TN\": 63437759,\n",
      "          \"TP\": 611,\n",
      "          \"n_pred\": 621,\n",
      "          \"n_ref\": 1079\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_069.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_069.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.6627689508180149,\n",
      "          \"FN\": 4657,\n",
      "          \"FP\": 15358,\n",
      "          \"IoU\": 0.49562785071693166,\n",
      "          \"TN\": 129197309,\n",
      "          \"TP\": 19668,\n",
      "          \"n_pred\": 35026,\n",
      "          \"n_ref\": 24325\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_073.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_073.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.7896446043974582,\n",
      "          \"FN\": 14907,\n",
      "          \"FP\": 15019,\n",
      "          \"IoU\": 0.6524072245775016,\n",
      "          \"TN\": 96645041,\n",
      "          \"TP\": 56169,\n",
      "          \"n_pred\": 71188,\n",
      "          \"n_ref\": 71076\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_074.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_074.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.6734422807808164,\n",
      "          \"FN\": 12750,\n",
      "          \"FP\": 14786,\n",
      "          \"IoU\": 0.5076614994010263,\n",
      "          \"TN\": 69936519,\n",
      "          \"TP\": 28393,\n",
      "          \"n_pred\": 43179,\n",
      "          \"n_ref\": 41143\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/3d_fullres_output/lung_086.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_086.nii.gz\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Mean Metrics:\n",
      "Error reading results file: unsupported format string passed to dict.__format__\n",
      "\n",
      "Running folder-based evaluation on postprocessed predictions...\n",
      "\n",
      "=== Running Folder-based Evaluation ===\n",
      "Using predictions from: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing\n",
      "Using ground truth from: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs\n",
      "Using dataset.json from: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/dataset.json\n",
      "Using plans.json from: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json\n",
      "Results will be saved to: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/evaluation_results_postprocessed.json\n",
      "Running command: nnUNetv2_evaluate_folder /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing -djfile /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/dataset.json -pfile /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json -o /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/evaluation_results_postprocessed.json\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "Evaluation Results:\n",
      "{\n",
      "  \"foreground_mean\": {\n",
      "    \"Dice\": 0.7807254323319224,\n",
      "    \"FN\": 8220.384615384615,\n",
      "    \"FP\": 4758.923076923077,\n",
      "    \"IoU\": 0.6486048387770675,\n",
      "    \"TN\": 79338346.61538461,\n",
      "    \"TP\": 17811.30769230769,\n",
      "    \"n_pred\": 22570.23076923077,\n",
      "    \"n_ref\": 26031.69230769231\n",
      "  },\n",
      "  \"mean\": {\n",
      "    \"1\": {\n",
      "      \"Dice\": 0.7807254323319224,\n",
      "      \"FN\": 8220.384615384615,\n",
      "      \"FP\": 4758.923076923077,\n",
      "      \"IoU\": 0.6486048387770675,\n",
      "      \"TN\": 79338346.61538461,\n",
      "      \"TP\": 17811.30769230769,\n",
      "      \"n_pred\": 22570.23076923077,\n",
      "      \"n_ref\": 26031.69230769231\n",
      "    }\n",
      "  },\n",
      "  \"metric_per_case\": [\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8073229291716687,\n",
      "          \"FN\": 29,\n",
      "          \"FP\": 1255,\n",
      "          \"IoU\": 0.676899849018621,\n",
      "          \"TN\": 79687802,\n",
      "          \"TP\": 2690,\n",
      "          \"n_pred\": 3945,\n",
      "          \"n_ref\": 2719\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_001.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_001.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8626617732277381,\n",
      "          \"FN\": 351,\n",
      "          \"FP\": 1782,\n",
      "          \"IoU\": 0.7584918478260869,\n",
      "          \"TN\": 148626816,\n",
      "          \"TP\": 6699,\n",
      "          \"n_pred\": 8481,\n",
      "          \"n_ref\": 7050\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_006.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_006.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8422535211267606,\n",
      "          \"FN\": 1302,\n",
      "          \"FP\": 490,\n",
      "          \"IoU\": 0.7274939172749392,\n",
      "          \"TN\": 82306640,\n",
      "          \"TP\": 4784,\n",
      "          \"n_pred\": 5274,\n",
      "          \"n_ref\": 6086\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_020.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_020.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8608374606033319,\n",
      "          \"FN\": 5379,\n",
      "          \"FP\": 2348,\n",
      "          \"IoU\": 0.7556757098589768,\n",
      "          \"TN\": 59475062,\n",
      "          \"TP\": 23899,\n",
      "          \"n_pred\": 26247,\n",
      "          \"n_ref\": 29278\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_026.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_026.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8876805921286741,\n",
      "          \"FN\": 650,\n",
      "          \"FP\": 7241,\n",
      "          \"IoU\": 0.7980446855885138,\n",
      "          \"TN\": 69166943,\n",
      "          \"TP\": 31182,\n",
      "          \"n_pred\": 38423,\n",
      "          \"n_ref\": 31832\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_027.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_027.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.8313568842841094,\n",
      "          \"FN\": 594,\n",
      "          \"FP\": 757,\n",
      "          \"IoU\": 0.7113864558854945,\n",
      "          \"TN\": 66842039,\n",
      "          \"TP\": 3330,\n",
      "          \"n_pred\": 4087,\n",
      "          \"n_ref\": 3924\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_029.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_029.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.598692259017085,\n",
      "          \"FN\": 65235,\n",
      "          \"FP\": 1356,\n",
      "          \"IoU\": 0.4272382443253658,\n",
      "          \"TN\": 79575513,\n",
      "          \"TP\": 49672,\n",
      "          \"n_pred\": 51028,\n",
      "          \"n_ref\": 114907\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_031.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_031.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.861640169998426,\n",
      "          \"FN\": 391,\n",
      "          \"FP\": 488,\n",
      "          \"IoU\": 0.7569137168141593,\n",
      "          \"TN\": 29356512,\n",
      "          \"TP\": 2737,\n",
      "          \"n_pred\": 3225,\n",
      "          \"n_ref\": 3128\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_057.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_057.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.7523056653491436,\n",
      "          \"FN\": 152,\n",
      "          \"FP\": 976,\n",
      "          \"IoU\": 0.6029567053854277,\n",
      "          \"TN\": 57144551,\n",
      "          \"TP\": 1713,\n",
      "          \"n_pred\": 2689,\n",
      "          \"n_ref\": 1865\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_059.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_059.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.7188235294117648,\n",
      "          \"FN\": 468,\n",
      "          \"FP\": 10,\n",
      "          \"IoU\": 0.5610651974288338,\n",
      "          \"TN\": 63437759,\n",
      "          \"TP\": 611,\n",
      "          \"n_pred\": 621,\n",
      "          \"n_ref\": 1079\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_069.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_069.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.6627689508180149,\n",
      "          \"FN\": 4657,\n",
      "          \"FP\": 15358,\n",
      "          \"IoU\": 0.49562785071693166,\n",
      "          \"TN\": 129197309,\n",
      "          \"TP\": 19668,\n",
      "          \"n_pred\": 35026,\n",
      "          \"n_ref\": 24325\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_073.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_073.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.7896446043974582,\n",
      "          \"FN\": 14907,\n",
      "          \"FP\": 15019,\n",
      "          \"IoU\": 0.6524072245775016,\n",
      "          \"TN\": 96645041,\n",
      "          \"TP\": 56169,\n",
      "          \"n_pred\": 71188,\n",
      "          \"n_ref\": 71076\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_074.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_074.nii.gz\"\n",
      "    },\n",
      "    {\n",
      "      \"metrics\": {\n",
      "        \"1\": {\n",
      "          \"Dice\": 0.6734422807808164,\n",
      "          \"FN\": 12750,\n",
      "          \"FP\": 14786,\n",
      "          \"IoU\": 0.5076614994010263,\n",
      "          \"TN\": 69936519,\n",
      "          \"TP\": 28393,\n",
      "          \"n_pred\": 43179,\n",
      "          \"n_ref\": 41143\n",
      "        }\n",
      "      },\n",
      "      \"prediction_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results/Dataset002_Lung_split/predictions/final_output_with_postprocessing/lung_086.nii.gz\",\n",
      "      \"reference_file\": \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset002_Lung_split/labelsTs/lung_086.nii.gz\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Mean Metrics:\n",
      "Error reading results file: unsupported format string passed to dict.__format__\n",
      "\n",
      "=== Evaluation Pipeline Completed Successfully ===\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class EvaluationPipeline:\n",
    "    def __init__(self, dataset_id: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize EvaluationPipeline\n",
    "        Args:\n",
    "            dataset_id: Dataset identifier (default: 2)\n",
    "        \"\"\"\n",
    "        self.dataset_id = dataset_id\n",
    "        self.dataset_name = f\"Dataset{dataset_id:03d}_Lung_split\"\n",
    "        \n",
    "        # Setup base directories\n",
    "        self.results_dir = os.environ[\"nnUNet_results\"]\n",
    "        self.raw_data_dir = os.environ[\"nnUNet_raw\"]\n",
    "        \n",
    "        # Setup paths\n",
    "        self.base_pred_folder = os.path.join(self.results_dir, f\"{self.dataset_name}/predictions\")\n",
    "        self.raw_preds = os.path.join(self.base_pred_folder, \"3d_fullres_output\")\n",
    "        self.postprocessed_preds = os.path.join(self.base_pred_folder, \"final_output_with_postprocessing\")\n",
    "        self.ground_truth = os.path.join(self.raw_data_dir, f\"{self.dataset_name}/labelsTs\")\n",
    "\n",
    "        # Dataset json and plans files\n",
    "        self.dataset_json = os.path.join(self.raw_data_dir, f\"{self.dataset_name}/dataset.json\")\n",
    "        self.plans_file = os.path.join(self.results_dir, f\"{self.dataset_name}/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json\")\n",
    "\n",
    "    def evaluate_folder(self, pred_folder: str = None):\n",
    "        \"\"\"\n",
    "        Evaluate predictions using folder-based evaluation\n",
    "        Args:\n",
    "            pred_folder: Path to prediction folder. If None, uses postprocessed predictions if available, else raw predictions\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Running Folder-based Evaluation ===\")\n",
    "        \n",
    "        # Determine which predictions to evaluate\n",
    "        if pred_folder is None:\n",
    "            pred_folder = self.postprocessed_preds if os.path.exists(self.postprocessed_preds) else self.raw_preds\n",
    "        \n",
    "        # Create output file for results\n",
    "        output_file = os.path.join(os.path.dirname(pred_folder), \n",
    "                                 \"evaluation_results_raw.json\" if pred_folder == self.raw_preds \n",
    "                                 else \"evaluation_results_postprocessed.json\")\n",
    "        \n",
    "        print(f\"Using predictions from: {pred_folder}\")\n",
    "        print(f\"Using ground truth from: {self.ground_truth}\")\n",
    "        print(f\"Using dataset.json from: {self.dataset_json}\")\n",
    "        print(f\"Using plans.json from: {self.plans_file}\")\n",
    "        print(f\"Results will be saved to: {output_file}\")\n",
    "        \n",
    "        command = (f'nnUNetv2_evaluate_folder '\n",
    "                  f'{self.ground_truth} {pred_folder} '\n",
    "                  f'-djfile {self.dataset_json} '\n",
    "                  f'-pfile {self.plans_file} '\n",
    "                  f'-o {output_file}')\n",
    "        \n",
    "        print(f\"Running command: {command}\")\n",
    "        return_code = os.system(command)\n",
    "        \n",
    "        # Display results if evaluation was successful\n",
    "        if return_code == 0 and os.path.exists(output_file):\n",
    "            try:\n",
    "                with open(output_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                print(\"\\nEvaluation Results:\")\n",
    "                print(json.dumps(results, indent=2))\n",
    "                \n",
    "                # Print mean metrics if available\n",
    "                if 'mean' in results:\n",
    "                    print(\"\\nMean Metrics:\")\n",
    "                    mean_metrics = results['mean']\n",
    "                    for metric, value in mean_metrics.items():\n",
    "                        print(f\"{metric}: {value:.4f}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading results file: {e}\")\n",
    "        \n",
    "        return return_code\n",
    "    \n",
    "    def run_evaluations(self):\n",
    "        \"\"\"Run evaluation pipeline\"\"\"\n",
    "        try:\n",
    "            # Run folder-based evaluation\n",
    "            print(\"\\nRunning folder-based evaluation on raw predictions...\")\n",
    "            self.evaluate_folder(self.raw_preds)\n",
    "            \n",
    "            if os.path.exists(self.postprocessed_preds):\n",
    "                print(\"\\nRunning folder-based evaluation on postprocessed predictions...\")\n",
    "                self.evaluate_folder(self.postprocessed_preds)\n",
    "                \n",
    "            print(\"\\n=== Evaluation Pipeline Completed Successfully ===\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize evaluation pipeline\n",
    "    evaluator = EvaluationPipeline(dataset_id=2)  # For Dataset002\n",
    "    \n",
    "    # Run evaluations\n",
    "    evaluator.run_evaluations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx6e79DDjAfq"
   },
   "source": [
    "# 5.  Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx6e79DDjAfq"
   },
   "source": [
    "### Note:\n",
    "In the Medical Segmentation Decathlon format, since you only have imagesTs without corresponding labelsTs, it means your **test set data is for blind testing/inference** - there are **no ground truth labels** available for evaluation.\n",
    "\n",
    "## Model Evaluation Results\n",
    "\n",
    "### Visualization Explanation\n",
    "The evaluation pipeline generates three-panel visualizations for each slice:\n",
    "\n",
    "1. **Original Image**: Raw CT scan slice from test set (`imagesTs`)\n",
    "2. **Segmentation Mask**: Model's tumor prediction \n",
    "3. **Overlay**: Segmentation (red) superimposed on original image for direct comparison\n",
    "\n",
    "### Sample Result Analysis\n",
    "![Sample Result](nnUNet_results/Dataset001_Lung/predictions/ensemble_3d_fullres_3d_low_res/final_ensemble_output_with_postprocessing/visualizations/lung_087_slice_184.png)\n",
    "The visualization shows:\n",
    "- Clear tumor identification in upper lung region\n",
    "- Good alignment between prediction and visible tumor mass\n",
    "- Clean segmentation boundaries\n",
    "\n",
    "### Key Takeaways\n",
    "- Model successfully identifies and segments lung tumors\n",
    "- Predictions align well with visible tumor structures\n",
    "- Visual inspection confirms reasonable segmentation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 3d_fullres\n",
      "Exporting fold_0\n",
      "Exporting fold_1\n",
      "Exporting fold_2\n",
      "Exporting fold_3\n",
      "Exporting fold_4\n",
      "No ensemble directory found for task 2\n"
     ]
    }
   ],
   "source": [
    "# Exporting the model\n",
    "!nnUNetv2_export_model_to_zip -d 2 -c 3d_fullres -f 0 1 2 3 4 -o lung_segmentation_model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 64\n",
      "drwxrwxrwx+ 8 doodledaron doodledaron  4096 Jan 19 01:17 .\n",
      "drwxrwxrwx+ 4 doodledaron doodledaron  4096 Jan 18 17:28 ..\n",
      "drwxrwxrwx+ 3 doodledaron doodledaron  4096 Jan 19 01:18 crossval_results_folds_0_1_2_3_4\n",
      "-rw-rw-rw-+ 1 doodledaron doodledaron   404 Jan 17 06:56 dataset.json\n",
      "-rw-rw-rw-+ 1 doodledaron doodledaron  9033 Jan 17 06:56 dataset_fingerprint.json\n",
      "drwxrwxrwx+ 3 doodledaron doodledaron  4096 Jan 15 10:27 fold_0\n",
      "drwxrwxrwx+ 3 doodledaron doodledaron  4096 Jan 15 10:42 fold_1\n",
      "drwxrwxrwx+ 3 doodledaron doodledaron  4096 Jan 15 23:28 fold_2\n",
      "drwxrwxrwx+ 3 doodledaron doodledaron  4096 Jan 17 06:38 fold_3\n",
      "drwxrwxrwx+ 3 doodledaron doodledaron  4096 Jan 18 13:51 fold_4\n",
      "-rw-rw-rw-+ 1 doodledaron doodledaron 16332 Jan 17 06:56 plans.json\n"
     ]
    }
   ],
   "source": [
    "!ls -la $nnUNet_results/Dataset002_Lung_split/nnUNetTrainer__nnUNetPlans__3d_fullres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung segmentation Training\n",
    "- TCIA Lung Dataset from https://www.cancerimagingarchive.net/collection/lctsc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Customize your paths\n",
    "nnUNet_raw = \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw\"\n",
    "nnUNet_preprocessed = \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed\"\n",
    "nnUNet_results = \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_results\"\n",
    "\n",
    "os.environ[\"nnUNet_raw\"] = nnUNet_raw\n",
    "os.environ[\"nnUNet_preprocessed\"] = nnUNet_preprocessed\n",
    "os.environ[\"nnUNet_results\"] = nnUNet_results\n",
    "\n",
    "# Ensure paths exist\n",
    "os.makedirs(nnUNet_raw, exist_ok=True)\n",
    "os.makedirs(nnUNet_preprocessed, exist_ok=True)\n",
    "os.makedirs(nnUNet_results, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found dataset: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset003_Lung_only\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"Dataset003_Lung_only\"\n",
    "dataset_path = os.path.join(nnUNet_raw, dataset_id)\n",
    "\n",
    "# Check if your dataset exists\n",
    "assert os.path.exists(dataset_path), f\"Dataset folder not found: {dataset_path}\"\n",
    "print(f\"✅ Found dataset: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprint extraction...\n",
      "Dataset003_Lung_only\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "####################\n",
      "verify_dataset_integrity Done. \n",
      "If you didn't see any error messages then your dataset is most likely OK!\n",
      "####################\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:09<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment planning...\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.5        1.00585938 1.00585938]. \n",
      "Current patch size: (np.int64(56), np.int64(192), np.int64(192)). \n",
      "Current median shape: [162.5        497.08737864 497.08737864]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.5        1.03603516 1.03603516]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(160)). \n",
      "Current median shape: [162.5        482.60910548 482.60910548]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.5        1.06711621 1.06711621]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(160)). \n",
      "Current median shape: [162.5        468.55252959 468.55252959]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.5       1.0991297 1.0991297]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(160)). \n",
      "Current median shape: [162.5        454.90536853 454.90536853]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.5        1.13210359 1.13210359]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(160)). \n",
      "Current median shape: [162.5       441.6556976 441.6556976]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.5       1.1660667 1.1660667]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(160)). \n",
      "Current median shape: [162.5        428.79193942 428.79193942]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.5       1.2010487 1.2010487]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(160)). \n",
      "Current median shape: [162.5        416.30285381 416.30285381]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.5        1.23708016 1.23708016]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(160)). \n",
      "Current median shape: [162.5        404.17752797 404.17752797]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.5        1.27419256 1.27419256]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [162.5        392.40536696 392.40536696]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.575      1.31241834 1.31241834]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [157.76699029 380.97608443 380.97608443]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.65225    1.35179089 1.35179089]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [153.17183523 369.87969362 369.87969362]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.7318175  1.39234462 1.39234462]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [148.71051964 359.10649866 359.10649866]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.81377203 1.43411495 1.43411495]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [144.37914529 348.64708608 348.64708608]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.89818519 1.4771384  1.4771384 ]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [140.17392746 338.49231658 338.49231658]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.98513074 1.52145256 1.52145256]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [136.09119171 328.63331707 328.63331707]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [3.07468466 1.56709613 1.56709613]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [132.12737059 319.06147288 319.06147288]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [3.1669252  1.61410902 1.61410902]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [128.27900058 309.76842027 309.76842027]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [3.26193296 1.66253229 1.66253229]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [124.54271901 300.7460391  300.7460391 ]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [3.35979095 1.71240825 1.71240825]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [120.91526117 291.98644573 291.98644573]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [3.46058468 1.7637805  1.7637805 ]. \n",
      "Current patch size: (np.int64(64), np.int64(192), np.int64(192)). \n",
      "Current median shape: [117.39345745 283.48198614 283.48198614]\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': (np.int64(512), np.int64(512)), 'median_image_size_in_voxels': array([512., 512.]), 'spacing': array([0.9765625, 0.9765625]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': (32, 64, 128, 256, 512, 512, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D lowres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (np.int64(64), np.int64(192), np.int64(192)), 'median_image_size_in_voxels': (117, 283, 283), 'spacing': array([3.46058468, 1.7637805 , 1.7637805 ]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (1, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'}\n",
      "\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (np.int64(56), np.int64(192), np.int64(192)), 'median_image_size_in_voxels': array([162.5, 512. , 512. ]), 'spacing': array([2.5      , 0.9765625, 0.9765625]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((1, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (1, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (1, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Plans were saved to /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset003_Lung_only\n",
      "Configuration: 2d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [02:14<00:00,  3.75s/it]\n",
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: 3d_fullres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [03:39<00:00,  6.09s/it]\n",
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: 3d_lowres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [01:44<00:00,  2.90s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "dataset_id = 3\n",
    "\n",
    "preprocess_command = f'nnUNetv2_plan_and_preprocess -d {dataset_id} -np 2 --verify_dataset_integrity'\n",
    "logger.info(f\"Running preprocessing command: {preprocess_command}\")\n",
    "os.system(preprocess_command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Train All 5 Folds (3D Fullres, Default Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Running Fold 0: nnUNetv2_train 3 3d_fullres 0 --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:1184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename_or_checkpoint, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-05-15 00:31:05.740764: Using torch.compile...\n",
      "2025-05-15 00:31:06.696052: do_dummy_2d_data_aug: True\n",
      "2025-05-15 00:31:06.696315: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-15 00:31:06.696740: The split file contains 5 splits.\n",
      "2025-05-15 00:31:06.696778: Desired fold for training: 0\n",
      "2025-05-15 00:31:06.696796: This split has 28 training and 8 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [56, 192, 192], 'median_image_size_in_voxels': [162.5, 512.0, 512.0], 'spacing': [2.5, 0.9765625, 0.9765625], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset003_Lung_only', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.9765625, 0.9765625], 'original_median_shape_after_transp': [156, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1910.0, 'mean': -742.67333984375, 'median': -794.0, 'min': -1023.0, 'percentile_00_5': -981.0, 'percentile_99_5': -51.0, 'std': 175.66612243652344}}} \n",
      "\n",
      "2025-05-15 00:31:08.798031: unpacking dataset...\n",
      "2025-05-15 00:31:13.079684: unpacking done...\n",
      "2025-05-15 00:31:13.081543: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-05-15 00:31:13.405293: Training done.\n",
      "2025-05-15 00:31:13.422453: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-15 00:31:13.422585: The split file contains 5 splits.\n",
      "2025-05-15 00:31:13.422606: Desired fold for training: 0\n",
      "2025-05-15 00:31:13.422619: This split has 28 training and 8 validation cases.\n",
      "2025-05-15 00:31:13.422775: predicting LCTSC-Train-S1-001\n",
      "2025-05-15 00:31:13.423334: LCTSC-Train-S1-001, shape torch.Size([1, 168, 512, 512]), rank 0\n",
      "2025-05-15 00:32:31.014997: predicting LCTSC-Train-S1-003\n",
      "2025-05-15 00:32:31.019411: LCTSC-Train-S1-003, shape torch.Size([1, 185, 512, 512]), rank 0\n",
      "2025-05-15 00:33:45.121912: predicting LCTSC-Train-S1-004\n",
      "2025-05-15 00:33:45.125884: LCTSC-Train-S1-004, shape torch.Size([1, 190, 512, 512]), rank 0\n",
      "2025-05-15 00:34:57.506667: predicting LCTSC-Train-S1-005\n",
      "2025-05-15 00:34:57.510699: LCTSC-Train-S1-005, shape torch.Size([1, 163, 512, 512]), rank 0\n",
      "2025-05-15 00:35:56.685773: predicting LCTSC-Train-S1-006\n",
      "2025-05-15 00:35:56.688625: LCTSC-Train-S1-006, shape torch.Size([1, 168, 512, 512]), rank 0\n",
      "2025-05-15 00:36:55.285079: predicting LCTSC-Train-S2-010\n",
      "2025-05-15 00:36:55.288262: LCTSC-Train-S2-010, shape torch.Size([1, 152, 512, 512]), rank 0\n",
      "2025-05-15 00:37:53.980631: predicting LCTSC-Train-S3-002\n",
      "2025-05-15 00:37:53.983516: LCTSC-Train-S3-002, shape torch.Size([1, 206, 512, 512]), rank 0\n",
      "2025-05-15 00:39:16.271345: predicting LCTSC-Train-S3-005\n",
      "2025-05-15 00:39:16.274783: LCTSC-Train-S3-005, shape torch.Size([1, 150, 614, 614]), rank 0\n",
      "2025-05-15 00:40:52.441353: Validation complete\n",
      "2025-05-15 00:40:52.441418: Mean Validation Dice:  0.9853151314340793\n",
      "🔁 Running Fold 1: nnUNetv2_train 3 3d_fullres 1 --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:1184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename_or_checkpoint, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-05-15 00:40:57.700351: Using torch.compile...\n",
      "2025-05-15 00:40:58.667857: do_dummy_2d_data_aug: True\n",
      "2025-05-15 00:40:58.668146: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-15 00:40:58.668242: The split file contains 5 splits.\n",
      "2025-05-15 00:40:58.668261: Desired fold for training: 1\n",
      "2025-05-15 00:40:58.668273: This split has 29 training and 7 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [56, 192, 192], 'median_image_size_in_voxels': [162.5, 512.0, 512.0], 'spacing': [2.5, 0.9765625, 0.9765625], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset003_Lung_only', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.9765625, 0.9765625], 'original_median_shape_after_transp': [156, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1910.0, 'mean': -742.67333984375, 'median': -794.0, 'min': -1023.0, 'percentile_00_5': -981.0, 'percentile_99_5': -51.0, 'std': 175.66612243652344}}} \n",
      "\n",
      "2025-05-15 00:41:00.143189: unpacking dataset...\n",
      "2025-05-15 00:41:04.964470: unpacking done...\n",
      "2025-05-15 00:41:04.965368: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-05-15 00:41:04.968766: \n",
      "2025-05-15 00:41:04.969079: Epoch 600\n",
      "2025-05-15 00:41:04.969199: Current learning rate: 0.00438\n",
      "2025-05-15 00:43:40.996947: train_loss -0.963\n",
      "2025-05-15 00:43:40.997173: val_loss -0.9544\n",
      "2025-05-15 00:43:40.997223: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 00:43:40.997260: Epoch time: 156.03 s\n",
      "2025-05-15 00:43:41.618747: \n",
      "2025-05-15 00:43:41.618952: Epoch 601\n",
      "2025-05-15 00:43:41.619090: Current learning rate: 0.00437\n",
      "2025-05-15 00:45:30.799754: train_loss -0.9617\n",
      "2025-05-15 00:45:30.799881: val_loss -0.9505\n",
      "2025-05-15 00:45:30.799915: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-15 00:45:30.799948: Epoch time: 109.18 s\n",
      "2025-05-15 00:45:31.358234: \n",
      "2025-05-15 00:45:31.358426: Epoch 602\n",
      "2025-05-15 00:45:31.358540: Current learning rate: 0.00436\n",
      "2025-05-15 00:47:20.490083: train_loss -0.9642\n",
      "2025-05-15 00:47:20.490206: val_loss -0.9612\n",
      "2025-05-15 00:47:20.490238: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 00:47:20.490269: Epoch time: 109.13 s\n",
      "2025-05-15 00:47:21.036765: \n",
      "2025-05-15 00:47:21.036838: Epoch 603\n",
      "2025-05-15 00:47:21.036897: Current learning rate: 0.00435\n",
      "2025-05-15 00:49:10.320127: train_loss -0.9665\n",
      "2025-05-15 00:49:10.320248: val_loss -0.9549\n",
      "2025-05-15 00:49:10.320281: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 00:49:10.320314: Epoch time: 109.28 s\n",
      "2025-05-15 00:49:10.874131: \n",
      "2025-05-15 00:49:10.874237: Epoch 604\n",
      "2025-05-15 00:49:10.874340: Current learning rate: 0.00434\n",
      "2025-05-15 00:51:00.025146: train_loss -0.9649\n",
      "2025-05-15 00:51:00.025330: val_loss -0.9567\n",
      "2025-05-15 00:51:00.025366: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 00:51:00.025399: Epoch time: 109.15 s\n",
      "2025-05-15 00:51:00.571891: \n",
      "2025-05-15 00:51:00.571987: Epoch 605\n",
      "2025-05-15 00:51:00.572061: Current learning rate: 0.00433\n",
      "2025-05-15 00:52:49.778622: train_loss -0.9653\n",
      "2025-05-15 00:52:49.778741: val_loss -0.9553\n",
      "2025-05-15 00:52:49.778776: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 00:52:49.778809: Epoch time: 109.21 s\n",
      "2025-05-15 00:52:50.326985: \n",
      "2025-05-15 00:52:50.327072: Epoch 606\n",
      "2025-05-15 00:52:50.327137: Current learning rate: 0.00432\n",
      "2025-05-15 00:54:39.441054: train_loss -0.964\n",
      "2025-05-15 00:54:39.441176: val_loss -0.9573\n",
      "2025-05-15 00:54:39.441209: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 00:54:39.441242: Epoch time: 109.11 s\n",
      "2025-05-15 00:54:39.997745: \n",
      "2025-05-15 00:54:39.997911: Epoch 607\n",
      "2025-05-15 00:54:39.997983: Current learning rate: 0.00431\n",
      "2025-05-15 00:56:29.245061: train_loss -0.9659\n",
      "2025-05-15 00:56:29.245232: val_loss -0.9544\n",
      "2025-05-15 00:56:29.245265: Pseudo dice [np.float32(0.9804)]\n",
      "2025-05-15 00:56:29.245299: Epoch time: 109.25 s\n",
      "2025-05-15 00:56:29.791247: \n",
      "2025-05-15 00:56:29.791327: Epoch 608\n",
      "2025-05-15 00:56:29.791392: Current learning rate: 0.0043\n",
      "2025-05-15 00:58:18.870455: train_loss -0.9658\n",
      "2025-05-15 00:58:18.870580: val_loss -0.9578\n",
      "2025-05-15 00:58:18.870612: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 00:58:18.870644: Epoch time: 109.08 s\n",
      "2025-05-15 00:58:19.422965: \n",
      "2025-05-15 00:58:19.423057: Epoch 609\n",
      "2025-05-15 00:58:19.423121: Current learning rate: 0.00429\n",
      "2025-05-15 01:00:08.702609: train_loss -0.9659\n",
      "2025-05-15 01:00:08.702834: val_loss -0.9557\n",
      "2025-05-15 01:00:08.702883: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 01:00:08.702920: Epoch time: 109.28 s\n",
      "2025-05-15 01:00:09.249156: \n",
      "2025-05-15 01:00:09.249291: Epoch 610\n",
      "2025-05-15 01:00:09.249365: Current learning rate: 0.00429\n",
      "2025-05-15 01:01:58.428742: train_loss -0.9672\n",
      "2025-05-15 01:01:58.428858: val_loss -0.9577\n",
      "2025-05-15 01:01:58.428890: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 01:01:58.428958: Epoch time: 109.18 s\n",
      "2025-05-15 01:01:58.978815: \n",
      "2025-05-15 01:01:58.978914: Epoch 611\n",
      "2025-05-15 01:01:58.979001: Current learning rate: 0.00428\n",
      "2025-05-15 01:03:48.058555: train_loss -0.9661\n",
      "2025-05-15 01:03:48.058750: val_loss -0.9581\n",
      "2025-05-15 01:03:48.058795: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 01:03:48.058830: Epoch time: 109.08 s\n",
      "2025-05-15 01:03:48.910498: \n",
      "2025-05-15 01:03:48.910722: Epoch 612\n",
      "2025-05-15 01:03:48.910795: Current learning rate: 0.00427\n",
      "2025-05-15 01:05:38.107507: train_loss -0.9646\n",
      "2025-05-15 01:05:38.107645: val_loss -0.9503\n",
      "2025-05-15 01:05:38.107678: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-15 01:05:38.107712: Epoch time: 109.2 s\n",
      "2025-05-15 01:05:38.659829: \n",
      "2025-05-15 01:05:38.659978: Epoch 613\n",
      "2025-05-15 01:05:38.660051: Current learning rate: 0.00426\n",
      "2025-05-15 01:07:28.074612: train_loss -0.9657\n",
      "2025-05-15 01:07:28.074748: val_loss -0.9567\n",
      "2025-05-15 01:07:28.074938: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 01:07:28.075036: Epoch time: 109.42 s\n",
      "2025-05-15 01:07:28.629466: \n",
      "2025-05-15 01:07:28.629777: Epoch 614\n",
      "2025-05-15 01:07:28.629855: Current learning rate: 0.00425\n",
      "2025-05-15 01:09:17.943953: train_loss -0.9645\n",
      "2025-05-15 01:09:17.944081: val_loss -0.9575\n",
      "2025-05-15 01:09:17.944112: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 01:09:17.944145: Epoch time: 109.32 s\n",
      "2025-05-15 01:09:18.495223: \n",
      "2025-05-15 01:09:18.495494: Epoch 615\n",
      "2025-05-15 01:09:18.495590: Current learning rate: 0.00424\n",
      "2025-05-15 01:11:07.614844: train_loss -0.9641\n",
      "2025-05-15 01:11:07.615123: val_loss -0.955\n",
      "2025-05-15 01:11:07.615183: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 01:11:07.615223: Epoch time: 109.12 s\n",
      "2025-05-15 01:11:08.170238: \n",
      "2025-05-15 01:11:08.170348: Epoch 616\n",
      "2025-05-15 01:11:08.170415: Current learning rate: 0.00423\n",
      "2025-05-15 01:12:57.464492: train_loss -0.9636\n",
      "2025-05-15 01:12:57.464612: val_loss -0.9577\n",
      "2025-05-15 01:12:57.464646: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 01:12:57.464679: Epoch time: 109.29 s\n",
      "2025-05-15 01:12:58.021835: \n",
      "2025-05-15 01:12:58.021970: Epoch 617\n",
      "2025-05-15 01:12:58.022040: Current learning rate: 0.00422\n",
      "2025-05-15 01:14:47.100110: train_loss -0.9625\n",
      "2025-05-15 01:14:47.100237: val_loss -0.9535\n",
      "2025-05-15 01:14:47.100496: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-15 01:14:47.100580: Epoch time: 109.08 s\n",
      "2025-05-15 01:14:47.657804: \n",
      "2025-05-15 01:14:47.657967: Epoch 618\n",
      "2025-05-15 01:14:47.658039: Current learning rate: 0.00421\n",
      "2025-05-15 01:16:36.952602: train_loss -0.9642\n",
      "2025-05-15 01:16:36.952831: val_loss -0.9547\n",
      "2025-05-15 01:16:36.952951: Pseudo dice [np.float32(0.9805)]\n",
      "2025-05-15 01:16:36.953022: Epoch time: 109.3 s\n",
      "2025-05-15 01:16:37.504878: \n",
      "2025-05-15 01:16:37.505276: Epoch 619\n",
      "2025-05-15 01:16:37.505348: Current learning rate: 0.0042\n",
      "2025-05-15 01:18:26.767624: train_loss -0.9647\n",
      "2025-05-15 01:18:26.767980: val_loss -0.9534\n",
      "2025-05-15 01:18:26.768170: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 01:18:26.768237: Epoch time: 109.26 s\n",
      "2025-05-15 01:18:27.316775: \n",
      "2025-05-15 01:18:27.316915: Epoch 620\n",
      "2025-05-15 01:18:27.316982: Current learning rate: 0.00419\n",
      "2025-05-15 01:20:16.422182: train_loss -0.9622\n",
      "2025-05-15 01:20:16.422309: val_loss -0.96\n",
      "2025-05-15 01:20:16.422341: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 01:20:16.422372: Epoch time: 109.11 s\n",
      "2025-05-15 01:20:16.978371: \n",
      "2025-05-15 01:20:16.978565: Epoch 621\n",
      "2025-05-15 01:20:16.978646: Current learning rate: 0.00418\n",
      "2025-05-15 01:22:06.311637: train_loss -0.9644\n",
      "2025-05-15 01:22:06.311904: val_loss -0.9604\n",
      "2025-05-15 01:22:06.311972: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 01:22:06.312021: Epoch time: 109.33 s\n",
      "2025-05-15 01:22:06.864982: \n",
      "2025-05-15 01:22:06.865242: Epoch 622\n",
      "2025-05-15 01:22:06.865369: Current learning rate: 0.00417\n",
      "2025-05-15 01:23:56.040070: train_loss -0.9663\n",
      "2025-05-15 01:23:56.040199: val_loss -0.956\n",
      "2025-05-15 01:23:56.040232: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 01:23:56.040267: Epoch time: 109.18 s\n",
      "2025-05-15 01:23:56.588808: \n",
      "2025-05-15 01:23:56.589056: Epoch 623\n",
      "2025-05-15 01:23:56.589168: Current learning rate: 0.00416\n",
      "2025-05-15 01:25:45.768360: train_loss -0.9661\n",
      "2025-05-15 01:25:45.768519: val_loss -0.9433\n",
      "2025-05-15 01:25:45.768550: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-15 01:25:45.768582: Epoch time: 109.18 s\n",
      "2025-05-15 01:25:46.328814: \n",
      "2025-05-15 01:25:46.328951: Epoch 624\n",
      "2025-05-15 01:25:46.329021: Current learning rate: 0.00415\n",
      "2025-05-15 01:27:35.617448: train_loss -0.9636\n",
      "2025-05-15 01:27:35.617642: val_loss -0.9517\n",
      "2025-05-15 01:27:35.617734: Pseudo dice [np.float32(0.9804)]\n",
      "2025-05-15 01:27:35.617788: Epoch time: 109.29 s\n",
      "2025-05-15 01:27:36.178675: \n",
      "2025-05-15 01:27:36.178839: Epoch 625\n",
      "2025-05-15 01:27:36.178941: Current learning rate: 0.00414\n",
      "2025-05-15 01:29:25.514241: train_loss -0.9616\n",
      "2025-05-15 01:29:25.514360: val_loss -0.9531\n",
      "2025-05-15 01:29:25.514418: Pseudo dice [np.float32(0.9803)]\n",
      "2025-05-15 01:29:25.514454: Epoch time: 109.34 s\n",
      "2025-05-15 01:29:26.070514: \n",
      "2025-05-15 01:29:26.070675: Epoch 626\n",
      "2025-05-15 01:29:26.070757: Current learning rate: 0.00413\n",
      "2025-05-15 01:31:15.169184: train_loss -0.9622\n",
      "2025-05-15 01:31:15.169396: val_loss -0.953\n",
      "2025-05-15 01:31:15.169577: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 01:31:15.169656: Epoch time: 109.1 s\n",
      "2025-05-15 01:31:15.722090: \n",
      "2025-05-15 01:31:15.722265: Epoch 627\n",
      "2025-05-15 01:31:15.722336: Current learning rate: 0.00412\n",
      "2025-05-15 01:33:04.871997: train_loss -0.9583\n",
      "2025-05-15 01:33:04.872182: val_loss -0.9487\n",
      "2025-05-15 01:33:04.872215: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-15 01:33:04.872248: Epoch time: 109.15 s\n",
      "2025-05-15 01:33:05.427820: \n",
      "2025-05-15 01:33:05.427969: Epoch 628\n",
      "2025-05-15 01:33:05.428056: Current learning rate: 0.00411\n",
      "2025-05-15 01:34:54.583944: train_loss -0.9393\n",
      "2025-05-15 01:34:54.584080: val_loss -0.9422\n",
      "2025-05-15 01:34:54.584110: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-15 01:34:54.584141: Epoch time: 109.16 s\n",
      "2025-05-15 01:34:55.143601: \n",
      "2025-05-15 01:34:55.143762: Epoch 629\n",
      "2025-05-15 01:34:55.143845: Current learning rate: 0.0041\n",
      "2025-05-15 01:36:44.271689: train_loss -0.947\n",
      "2025-05-15 01:36:44.271811: val_loss -0.9492\n",
      "2025-05-15 01:36:44.271842: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-15 01:36:44.271952: Epoch time: 109.13 s\n",
      "2025-05-15 01:36:45.133954: \n",
      "2025-05-15 01:36:45.134123: Epoch 630\n",
      "2025-05-15 01:36:45.134198: Current learning rate: 0.00409\n",
      "2025-05-15 01:38:34.377496: train_loss -0.9489\n",
      "2025-05-15 01:38:34.377709: val_loss -0.9454\n",
      "2025-05-15 01:38:34.377877: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-15 01:38:34.378014: Epoch time: 109.24 s\n",
      "2025-05-15 01:38:34.929452: \n",
      "2025-05-15 01:38:34.929714: Epoch 631\n",
      "2025-05-15 01:38:34.929801: Current learning rate: 0.00408\n",
      "2025-05-15 01:40:24.196893: train_loss -0.9516\n",
      "2025-05-15 01:40:24.197114: val_loss -0.9486\n",
      "2025-05-15 01:40:24.197169: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-15 01:40:24.197208: Epoch time: 109.27 s\n",
      "2025-05-15 01:40:24.754427: \n",
      "2025-05-15 01:40:24.754710: Epoch 632\n",
      "2025-05-15 01:40:24.754875: Current learning rate: 0.00407\n",
      "2025-05-15 01:42:14.111336: train_loss -0.9558\n",
      "2025-05-15 01:42:14.111474: val_loss -0.9511\n",
      "2025-05-15 01:42:14.111509: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-15 01:42:14.111542: Epoch time: 109.36 s\n",
      "2025-05-15 01:42:14.657798: \n",
      "2025-05-15 01:42:14.658086: Epoch 633\n",
      "2025-05-15 01:42:14.658157: Current learning rate: 0.00406\n",
      "2025-05-15 01:44:03.931666: train_loss -0.9607\n",
      "2025-05-15 01:44:03.931844: val_loss -0.9563\n",
      "2025-05-15 01:44:03.931888: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 01:44:03.931974: Epoch time: 109.27 s\n",
      "2025-05-15 01:44:04.490719: \n",
      "2025-05-15 01:44:04.491131: Epoch 634\n",
      "2025-05-15 01:44:04.491300: Current learning rate: 0.00405\n",
      "2025-05-15 01:45:53.568403: train_loss -0.9614\n",
      "2025-05-15 01:45:53.568632: val_loss -0.9521\n",
      "2025-05-15 01:45:53.568695: Pseudo dice [np.float32(0.98)]\n",
      "2025-05-15 01:45:53.568734: Epoch time: 109.08 s\n",
      "2025-05-15 01:45:54.124346: \n",
      "2025-05-15 01:45:54.124572: Epoch 635\n",
      "2025-05-15 01:45:54.124702: Current learning rate: 0.00404\n",
      "2025-05-15 01:47:43.428140: train_loss -0.9628\n",
      "2025-05-15 01:47:43.428265: val_loss -0.9604\n",
      "2025-05-15 01:47:43.428316: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 01:47:43.428455: Epoch time: 109.3 s\n",
      "2025-05-15 01:47:43.985579: \n",
      "2025-05-15 01:47:43.985720: Epoch 636\n",
      "2025-05-15 01:47:43.985792: Current learning rate: 0.00403\n",
      "2025-05-15 01:49:33.114559: train_loss -0.9602\n",
      "2025-05-15 01:49:33.114751: val_loss -0.9539\n",
      "2025-05-15 01:49:33.114784: Pseudo dice [np.float32(0.9805)]\n",
      "2025-05-15 01:49:33.114818: Epoch time: 109.13 s\n",
      "2025-05-15 01:49:33.671621: \n",
      "2025-05-15 01:49:33.671963: Epoch 637\n",
      "2025-05-15 01:49:33.672050: Current learning rate: 0.00402\n",
      "2025-05-15 01:51:22.895622: train_loss -0.9602\n",
      "2025-05-15 01:51:22.895747: val_loss -0.9575\n",
      "2025-05-15 01:51:22.895780: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-15 01:51:22.895813: Epoch time: 109.22 s\n",
      "2025-05-15 01:51:23.452138: \n",
      "2025-05-15 01:51:23.452268: Epoch 638\n",
      "2025-05-15 01:51:23.452333: Current learning rate: 0.00401\n",
      "2025-05-15 01:53:12.776746: train_loss -0.9604\n",
      "2025-05-15 01:53:12.776871: val_loss -0.953\n",
      "2025-05-15 01:53:12.776945: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-15 01:53:12.777050: Epoch time: 109.33 s\n",
      "2025-05-15 01:53:13.327804: \n",
      "2025-05-15 01:53:13.328165: Epoch 639\n",
      "2025-05-15 01:53:13.328249: Current learning rate: 0.004\n",
      "2025-05-15 01:55:02.538543: train_loss -0.9629\n",
      "2025-05-15 01:55:02.538753: val_loss -0.9565\n",
      "2025-05-15 01:55:02.538795: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 01:55:02.538829: Epoch time: 109.21 s\n",
      "2025-05-15 01:55:03.094475: \n",
      "2025-05-15 01:55:03.094563: Epoch 640\n",
      "2025-05-15 01:55:03.094627: Current learning rate: 0.00399\n",
      "2025-05-15 01:56:52.347281: train_loss -0.9631\n",
      "2025-05-15 01:56:52.347407: val_loss -0.9507\n",
      "2025-05-15 01:56:52.347441: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-15 01:56:52.347473: Epoch time: 109.25 s\n",
      "2025-05-15 01:56:52.901429: \n",
      "2025-05-15 01:56:52.901737: Epoch 641\n",
      "2025-05-15 01:56:52.901838: Current learning rate: 0.00398\n",
      "2025-05-15 01:58:42.178845: train_loss -0.9646\n",
      "2025-05-15 01:58:42.178966: val_loss -0.9576\n",
      "2025-05-15 01:58:42.179184: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 01:58:42.179260: Epoch time: 109.28 s\n",
      "2025-05-15 01:58:42.736149: \n",
      "2025-05-15 01:58:42.736231: Epoch 642\n",
      "2025-05-15 01:58:42.736293: Current learning rate: 0.00397\n",
      "2025-05-15 02:00:31.854124: train_loss -0.9654\n",
      "2025-05-15 02:00:31.854324: val_loss -0.9573\n",
      "2025-05-15 02:00:31.854503: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 02:00:31.854590: Epoch time: 109.12 s\n",
      "2025-05-15 02:00:32.404801: \n",
      "2025-05-15 02:00:32.405104: Epoch 643\n",
      "2025-05-15 02:00:32.405176: Current learning rate: 0.00396\n",
      "2025-05-15 02:02:21.668541: train_loss -0.9642\n",
      "2025-05-15 02:02:21.668666: val_loss -0.9556\n",
      "2025-05-15 02:02:21.668698: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 02:02:21.668730: Epoch time: 109.26 s\n",
      "2025-05-15 02:02:22.222906: \n",
      "2025-05-15 02:02:22.223150: Epoch 644\n",
      "2025-05-15 02:02:22.223230: Current learning rate: 0.00395\n",
      "2025-05-15 02:04:11.551256: train_loss -0.9645\n",
      "2025-05-15 02:04:11.551372: val_loss -0.9554\n",
      "2025-05-15 02:04:11.551404: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 02:04:11.551437: Epoch time: 109.33 s\n",
      "2025-05-15 02:04:12.105049: \n",
      "2025-05-15 02:04:12.105247: Epoch 645\n",
      "2025-05-15 02:04:12.105367: Current learning rate: 0.00394\n",
      "2025-05-15 02:06:01.422793: train_loss -0.9627\n",
      "2025-05-15 02:06:01.423082: val_loss -0.9586\n",
      "2025-05-15 02:06:01.423251: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 02:06:01.423328: Epoch time: 109.32 s\n",
      "2025-05-15 02:06:01.970192: \n",
      "2025-05-15 02:06:01.970631: Epoch 646\n",
      "2025-05-15 02:06:01.970736: Current learning rate: 0.00393\n",
      "2025-05-15 02:07:51.293567: train_loss -0.9588\n",
      "2025-05-15 02:07:51.293758: val_loss -0.9478\n",
      "2025-05-15 02:07:51.293830: Pseudo dice [np.float32(0.9798)]\n",
      "2025-05-15 02:07:51.293868: Epoch time: 109.32 s\n",
      "2025-05-15 02:07:52.162616: \n",
      "2025-05-15 02:07:52.162789: Epoch 647\n",
      "2025-05-15 02:07:52.162947: Current learning rate: 0.00392\n",
      "2025-05-15 02:09:41.509200: train_loss -0.9284\n",
      "2025-05-15 02:09:41.509323: val_loss -0.9035\n",
      "2025-05-15 02:09:41.509358: Pseudo dice [np.float32(0.9644)]\n",
      "2025-05-15 02:09:41.509391: Epoch time: 109.35 s\n",
      "2025-05-15 02:09:42.073024: \n",
      "2025-05-15 02:09:42.073328: Epoch 648\n",
      "2025-05-15 02:09:42.073401: Current learning rate: 0.00391\n",
      "2025-05-15 02:11:31.350337: train_loss -0.9217\n",
      "2025-05-15 02:11:31.350478: val_loss -0.9377\n",
      "2025-05-15 02:11:31.350516: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-15 02:11:31.350554: Epoch time: 109.28 s\n",
      "2025-05-15 02:11:31.907121: \n",
      "2025-05-15 02:11:31.907525: Epoch 649\n",
      "2025-05-15 02:11:31.907633: Current learning rate: 0.0039\n",
      "2025-05-15 02:13:21.201020: train_loss -0.9429\n",
      "2025-05-15 02:13:21.201149: val_loss -0.9485\n",
      "2025-05-15 02:13:21.201254: Pseudo dice [np.float32(0.98)]\n",
      "2025-05-15 02:13:21.201573: Epoch time: 109.29 s\n",
      "2025-05-15 02:13:21.981214: \n",
      "2025-05-15 02:13:21.981386: Epoch 650\n",
      "2025-05-15 02:13:21.981471: Current learning rate: 0.00389\n",
      "2025-05-15 02:15:11.297098: train_loss -0.9534\n",
      "2025-05-15 02:15:11.297312: val_loss -0.9496\n",
      "2025-05-15 02:15:11.297344: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-15 02:15:11.297377: Epoch time: 109.32 s\n",
      "2025-05-15 02:15:11.853052: \n",
      "2025-05-15 02:15:11.853158: Epoch 651\n",
      "2025-05-15 02:15:11.853232: Current learning rate: 0.00388\n",
      "2025-05-15 02:17:00.991583: train_loss -0.957\n",
      "2025-05-15 02:17:00.991813: val_loss -0.9563\n",
      "2025-05-15 02:17:00.991881: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 02:17:00.991920: Epoch time: 109.14 s\n",
      "2025-05-15 02:17:01.546673: \n",
      "2025-05-15 02:17:01.546890: Epoch 652\n",
      "2025-05-15 02:17:01.546965: Current learning rate: 0.00387\n",
      "2025-05-15 02:18:50.860419: train_loss -0.959\n",
      "2025-05-15 02:18:50.860598: val_loss -0.9583\n",
      "2025-05-15 02:18:50.860715: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 02:18:50.860795: Epoch time: 109.31 s\n",
      "2025-05-15 02:18:51.417332: \n",
      "2025-05-15 02:18:51.417459: Epoch 653\n",
      "2025-05-15 02:18:51.417633: Current learning rate: 0.00386\n",
      "2025-05-15 02:20:40.478711: train_loss -0.9545\n",
      "2025-05-15 02:20:40.478855: val_loss -0.9231\n",
      "2025-05-15 02:20:40.478889: Pseudo dice [np.float32(0.9705)]\n",
      "2025-05-15 02:20:40.478921: Epoch time: 109.06 s\n",
      "2025-05-15 02:20:41.028566: \n",
      "2025-05-15 02:20:41.028799: Epoch 654\n",
      "2025-05-15 02:20:41.028872: Current learning rate: 0.00385\n",
      "2025-05-15 02:22:30.115694: train_loss -0.9372\n",
      "2025-05-15 02:22:30.115870: val_loss -0.9254\n",
      "2025-05-15 02:22:30.115904: Pseudo dice [np.float32(0.9706)]\n",
      "2025-05-15 02:22:30.115946: Epoch time: 109.09 s\n",
      "2025-05-15 02:22:30.671354: \n",
      "2025-05-15 02:22:30.671449: Epoch 655\n",
      "2025-05-15 02:22:30.671514: Current learning rate: 0.00384\n",
      "2025-05-15 02:24:19.932269: train_loss -0.9346\n",
      "2025-05-15 02:24:19.932483: val_loss -0.9334\n",
      "2025-05-15 02:24:19.932517: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-15 02:24:19.932550: Epoch time: 109.26 s\n",
      "2025-05-15 02:24:20.485791: \n",
      "2025-05-15 02:24:20.485887: Epoch 656\n",
      "2025-05-15 02:24:20.485950: Current learning rate: 0.00383\n",
      "2025-05-15 02:26:09.583487: train_loss -0.9445\n",
      "2025-05-15 02:26:09.583614: val_loss -0.9486\n",
      "2025-05-15 02:26:09.583647: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-15 02:26:09.583678: Epoch time: 109.1 s\n",
      "2025-05-15 02:26:10.143603: \n",
      "2025-05-15 02:26:10.143742: Epoch 657\n",
      "2025-05-15 02:26:10.143815: Current learning rate: 0.00382\n",
      "2025-05-15 02:27:59.331898: train_loss -0.9565\n",
      "2025-05-15 02:27:59.332184: val_loss -0.9542\n",
      "2025-05-15 02:27:59.332228: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 02:27:59.332263: Epoch time: 109.19 s\n",
      "2025-05-15 02:27:59.883131: \n",
      "2025-05-15 02:27:59.883435: Epoch 658\n",
      "2025-05-15 02:27:59.883554: Current learning rate: 0.00381\n",
      "2025-05-15 02:29:49.161833: train_loss -0.9582\n",
      "2025-05-15 02:29:49.161982: val_loss -0.9518\n",
      "2025-05-15 02:29:49.162018: Pseudo dice [np.float32(0.9798)]\n",
      "2025-05-15 02:29:49.162050: Epoch time: 109.28 s\n",
      "2025-05-15 02:29:49.722170: \n",
      "2025-05-15 02:29:49.722300: Epoch 659\n",
      "2025-05-15 02:29:49.722367: Current learning rate: 0.0038\n",
      "2025-05-15 02:31:38.806376: train_loss -0.9607\n",
      "2025-05-15 02:31:38.806509: val_loss -0.9555\n",
      "2025-05-15 02:31:38.806541: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 02:31:38.806574: Epoch time: 109.08 s\n",
      "2025-05-15 02:31:39.364761: \n",
      "2025-05-15 02:31:39.364899: Epoch 660\n",
      "2025-05-15 02:31:39.365111: Current learning rate: 0.00379\n",
      "2025-05-15 02:33:28.624616: train_loss -0.9596\n",
      "2025-05-15 02:33:28.624792: val_loss -0.9551\n",
      "2025-05-15 02:33:28.624824: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 02:33:28.624857: Epoch time: 109.26 s\n",
      "2025-05-15 02:33:29.174468: \n",
      "2025-05-15 02:33:29.174607: Epoch 661\n",
      "2025-05-15 02:33:29.174679: Current learning rate: 0.00378\n",
      "2025-05-15 02:35:18.476616: train_loss -0.9584\n",
      "2025-05-15 02:35:18.476825: val_loss -0.9539\n",
      "2025-05-15 02:35:18.476942: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-15 02:35:18.477060: Epoch time: 109.3 s\n",
      "2025-05-15 02:35:19.030003: \n",
      "2025-05-15 02:35:19.030132: Epoch 662\n",
      "2025-05-15 02:35:19.030202: Current learning rate: 0.00377\n",
      "2025-05-15 02:37:08.113002: train_loss -0.9599\n",
      "2025-05-15 02:37:08.113130: val_loss -0.957\n",
      "2025-05-15 02:37:08.113163: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 02:37:08.113195: Epoch time: 109.08 s\n",
      "2025-05-15 02:37:08.661753: \n",
      "2025-05-15 02:37:08.661962: Epoch 663\n",
      "2025-05-15 02:37:08.662041: Current learning rate: 0.00376\n",
      "2025-05-15 02:38:57.900102: train_loss -0.9616\n",
      "2025-05-15 02:38:57.900235: val_loss -0.9547\n",
      "2025-05-15 02:38:57.900270: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 02:38:57.900302: Epoch time: 109.24 s\n",
      "2025-05-15 02:38:58.772470: \n",
      "2025-05-15 02:38:58.772575: Epoch 664\n",
      "2025-05-15 02:38:58.772665: Current learning rate: 0.00375\n",
      "2025-05-15 02:40:48.235935: train_loss -0.9618\n",
      "2025-05-15 02:40:48.236063: val_loss -0.9559\n",
      "2025-05-15 02:40:48.236095: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-15 02:40:48.236130: Epoch time: 109.46 s\n",
      "2025-05-15 02:40:48.814291: \n",
      "2025-05-15 02:40:48.814484: Epoch 665\n",
      "2025-05-15 02:40:48.814616: Current learning rate: 0.00374\n",
      "2025-05-15 02:42:39.829973: train_loss -0.9424\n",
      "2025-05-15 02:42:39.830123: val_loss -0.9413\n",
      "2025-05-15 02:42:39.830157: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-15 02:42:39.830189: Epoch time: 111.02 s\n",
      "2025-05-15 02:42:40.393782: \n",
      "2025-05-15 02:42:40.393916: Epoch 666\n",
      "2025-05-15 02:42:40.393995: Current learning rate: 0.00373\n",
      "2025-05-15 02:44:29.560054: train_loss -0.9367\n",
      "2025-05-15 02:44:29.560175: val_loss -0.9482\n",
      "2025-05-15 02:44:29.560207: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-15 02:44:29.560240: Epoch time: 109.17 s\n",
      "2025-05-15 02:44:30.125351: \n",
      "2025-05-15 02:44:30.125545: Epoch 667\n",
      "2025-05-15 02:44:30.125866: Current learning rate: 0.00372\n",
      "2025-05-15 02:46:19.379097: train_loss -0.9494\n",
      "2025-05-15 02:46:19.379284: val_loss -0.9488\n",
      "2025-05-15 02:46:19.379340: Pseudo dice [np.float32(0.9798)]\n",
      "2025-05-15 02:46:19.379381: Epoch time: 109.25 s\n",
      "2025-05-15 02:46:19.976556: \n",
      "2025-05-15 02:46:19.976765: Epoch 668\n",
      "2025-05-15 02:46:19.976842: Current learning rate: 0.00371\n",
      "2025-05-15 02:48:09.204353: train_loss -0.9573\n",
      "2025-05-15 02:48:09.204478: val_loss -0.9514\n",
      "2025-05-15 02:48:09.204509: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 02:48:09.204543: Epoch time: 109.23 s\n",
      "2025-05-15 02:48:09.774218: \n",
      "2025-05-15 02:48:09.774533: Epoch 669\n",
      "2025-05-15 02:48:09.774612: Current learning rate: 0.0037\n",
      "2025-05-15 02:49:58.889731: train_loss -0.9583\n",
      "2025-05-15 02:49:58.889862: val_loss -0.9533\n",
      "2025-05-15 02:49:58.889910: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 02:49:58.889959: Epoch time: 109.12 s\n",
      "2025-05-15 02:49:59.463685: \n",
      "2025-05-15 02:49:59.464017: Epoch 670\n",
      "2025-05-15 02:49:59.464120: Current learning rate: 0.00369\n",
      "2025-05-15 02:51:48.691528: train_loss -0.9569\n",
      "2025-05-15 02:51:48.691647: val_loss -0.9503\n",
      "2025-05-15 02:51:48.691683: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-15 02:51:48.691777: Epoch time: 109.23 s\n",
      "2025-05-15 02:51:49.266855: \n",
      "2025-05-15 02:51:49.267021: Epoch 671\n",
      "2025-05-15 02:51:49.267099: Current learning rate: 0.00368\n",
      "2025-05-15 02:53:38.334905: train_loss -0.955\n",
      "2025-05-15 02:53:38.335115: val_loss -0.9529\n",
      "2025-05-15 02:53:38.335150: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-15 02:53:38.335186: Epoch time: 109.07 s\n",
      "2025-05-15 02:53:38.903331: \n",
      "2025-05-15 02:53:38.903800: Epoch 672\n",
      "2025-05-15 02:53:38.903988: Current learning rate: 0.00367\n",
      "2025-05-15 02:55:27.930465: train_loss -0.9465\n",
      "2025-05-15 02:55:27.930586: val_loss -0.9407\n",
      "2025-05-15 02:55:27.930618: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-15 02:55:27.930651: Epoch time: 109.03 s\n",
      "2025-05-15 02:55:28.497068: \n",
      "2025-05-15 02:55:28.497430: Epoch 673\n",
      "2025-05-15 02:55:28.497512: Current learning rate: 0.00366\n",
      "2025-05-15 02:57:17.480058: train_loss -0.9436\n",
      "2025-05-15 02:57:17.480191: val_loss -0.9395\n",
      "2025-05-15 02:57:17.480223: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-15 02:57:17.480257: Epoch time: 108.98 s\n",
      "2025-05-15 02:57:18.047502: \n",
      "2025-05-15 02:57:18.047680: Epoch 674\n",
      "2025-05-15 02:57:18.047767: Current learning rate: 0.00365\n",
      "2025-05-15 02:59:06.998538: train_loss -0.9502\n",
      "2025-05-15 02:59:06.998684: val_loss -0.9521\n",
      "2025-05-15 02:59:06.998927: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 02:59:06.999014: Epoch time: 108.95 s\n",
      "2025-05-15 02:59:07.572086: \n",
      "2025-05-15 02:59:07.572481: Epoch 675\n",
      "2025-05-15 02:59:07.572678: Current learning rate: 0.00364\n",
      "2025-05-15 03:00:56.579723: train_loss -0.9408\n",
      "2025-05-15 03:00:56.580215: val_loss -0.9437\n",
      "2025-05-15 03:00:56.580252: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-15 03:00:56.580286: Epoch time: 109.01 s\n",
      "2025-05-15 03:00:57.144668: \n",
      "2025-05-15 03:00:57.145101: Epoch 676\n",
      "2025-05-15 03:00:57.145317: Current learning rate: 0.00363\n",
      "2025-05-15 03:02:45.996548: train_loss -0.9565\n",
      "2025-05-15 03:02:45.996699: val_loss -0.9494\n",
      "2025-05-15 03:02:45.996739: Pseudo dice [np.float32(0.98)]\n",
      "2025-05-15 03:02:45.996773: Epoch time: 108.85 s\n",
      "2025-05-15 03:02:46.566105: \n",
      "2025-05-15 03:02:46.566279: Epoch 677\n",
      "2025-05-15 03:02:46.566398: Current learning rate: 0.00362\n",
      "2025-05-15 03:04:35.402518: train_loss -0.9555\n",
      "2025-05-15 03:04:35.402733: val_loss -0.9569\n",
      "2025-05-15 03:04:35.402827: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 03:04:35.402942: Epoch time: 108.84 s\n",
      "2025-05-15 03:04:35.975768: \n",
      "2025-05-15 03:04:35.976260: Epoch 678\n",
      "2025-05-15 03:04:35.976356: Current learning rate: 0.00361\n",
      "2025-05-15 03:06:24.952419: train_loss -0.9585\n",
      "2025-05-15 03:06:24.952541: val_loss -0.9483\n",
      "2025-05-15 03:06:24.952573: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-15 03:06:24.952605: Epoch time: 108.98 s\n",
      "2025-05-15 03:06:25.524809: \n",
      "2025-05-15 03:06:25.525322: Epoch 679\n",
      "2025-05-15 03:06:25.525571: Current learning rate: 0.0036\n",
      "2025-05-15 03:08:14.407003: train_loss -0.9599\n",
      "2025-05-15 03:08:14.407336: val_loss -0.9525\n",
      "2025-05-15 03:08:14.407394: Pseudo dice [np.float32(0.9798)]\n",
      "2025-05-15 03:08:14.407432: Epoch time: 108.88 s\n",
      "2025-05-15 03:08:14.973626: \n",
      "2025-05-15 03:08:14.974051: Epoch 680\n",
      "2025-05-15 03:08:14.974172: Current learning rate: 0.00359\n",
      "2025-05-15 03:10:03.737648: train_loss -0.9615\n",
      "2025-05-15 03:10:03.737756: val_loss -0.9536\n",
      "2025-05-15 03:10:03.737788: Pseudo dice [np.float32(0.9813)]\n",
      "2025-05-15 03:10:03.737821: Epoch time: 108.77 s\n",
      "2025-05-15 03:10:04.308827: \n",
      "2025-05-15 03:10:04.308939: Epoch 681\n",
      "2025-05-15 03:10:04.309043: Current learning rate: 0.00358\n",
      "2025-05-15 03:11:53.251332: train_loss -0.9611\n",
      "2025-05-15 03:11:53.251494: val_loss -0.9538\n",
      "2025-05-15 03:11:53.251529: Pseudo dice [np.float32(0.9809)]\n",
      "2025-05-15 03:11:53.251562: Epoch time: 108.94 s\n",
      "2025-05-15 03:11:54.165116: \n",
      "2025-05-15 03:11:54.165345: Epoch 682\n",
      "2025-05-15 03:11:54.165465: Current learning rate: 0.00357\n",
      "2025-05-15 03:13:43.020193: train_loss -0.9642\n",
      "2025-05-15 03:13:43.020317: val_loss -0.9499\n",
      "2025-05-15 03:13:43.020349: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-15 03:13:43.020381: Epoch time: 108.86 s\n",
      "2025-05-15 03:13:43.595489: \n",
      "2025-05-15 03:13:43.596013: Epoch 683\n",
      "2025-05-15 03:13:43.596142: Current learning rate: 0.00356\n",
      "2025-05-15 03:15:32.571875: train_loss -0.9613\n",
      "2025-05-15 03:15:32.572066: val_loss -0.9603\n",
      "2025-05-15 03:15:32.572155: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 03:15:32.572205: Epoch time: 108.98 s\n",
      "2025-05-15 03:15:33.144729: \n",
      "2025-05-15 03:15:33.145137: Epoch 684\n",
      "2025-05-15 03:15:33.145224: Current learning rate: 0.00355\n",
      "2025-05-15 03:17:22.123186: train_loss -0.962\n",
      "2025-05-15 03:17:22.123305: val_loss -0.9586\n",
      "2025-05-15 03:17:22.123337: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 03:17:22.123369: Epoch time: 108.98 s\n",
      "2025-05-15 03:17:22.695832: \n",
      "2025-05-15 03:17:22.696195: Epoch 685\n",
      "2025-05-15 03:17:22.696418: Current learning rate: 0.00354\n",
      "2025-05-15 03:19:11.713672: train_loss -0.9629\n",
      "2025-05-15 03:19:11.714019: val_loss -0.96\n",
      "2025-05-15 03:19:11.714135: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 03:19:11.714186: Epoch time: 109.02 s\n",
      "2025-05-15 03:19:12.287914: \n",
      "2025-05-15 03:19:12.288294: Epoch 686\n",
      "2025-05-15 03:19:12.288468: Current learning rate: 0.00353\n",
      "2025-05-15 03:21:01.192326: train_loss -0.963\n",
      "2025-05-15 03:21:01.192501: val_loss -0.9561\n",
      "2025-05-15 03:21:01.192534: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 03:21:01.192567: Epoch time: 108.91 s\n",
      "2025-05-15 03:21:01.765718: \n",
      "2025-05-15 03:21:01.766024: Epoch 687\n",
      "2025-05-15 03:21:01.766339: Current learning rate: 0.00352\n",
      "2025-05-15 03:22:50.788390: train_loss -0.9635\n",
      "2025-05-15 03:22:50.788594: val_loss -0.959\n",
      "2025-05-15 03:22:50.788728: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 03:22:50.788842: Epoch time: 109.02 s\n",
      "2025-05-15 03:22:51.359739: \n",
      "2025-05-15 03:22:51.360226: Epoch 688\n",
      "2025-05-15 03:22:51.360405: Current learning rate: 0.00351\n",
      "2025-05-15 03:24:40.210630: train_loss -0.9635\n",
      "2025-05-15 03:24:40.210773: val_loss -0.9569\n",
      "2025-05-15 03:24:40.210810: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 03:24:40.210845: Epoch time: 108.85 s\n",
      "2025-05-15 03:24:40.781786: \n",
      "2025-05-15 03:24:40.782006: Epoch 689\n",
      "2025-05-15 03:24:40.782093: Current learning rate: 0.0035\n",
      "2025-05-15 03:26:29.742214: train_loss -0.9633\n",
      "2025-05-15 03:26:29.742340: val_loss -0.9561\n",
      "2025-05-15 03:26:29.742374: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 03:26:29.742409: Epoch time: 108.96 s\n",
      "2025-05-15 03:26:30.308875: \n",
      "2025-05-15 03:26:30.309391: Epoch 690\n",
      "2025-05-15 03:26:30.309736: Current learning rate: 0.00349\n",
      "2025-05-15 03:28:19.065831: train_loss -0.9647\n",
      "2025-05-15 03:28:19.066127: val_loss -0.9578\n",
      "2025-05-15 03:28:19.066193: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 03:28:19.066370: Epoch time: 108.76 s\n",
      "2025-05-15 03:28:19.640897: \n",
      "2025-05-15 03:28:19.641241: Epoch 691\n",
      "2025-05-15 03:28:19.641322: Current learning rate: 0.00348\n",
      "2025-05-15 03:30:08.405999: train_loss -0.9615\n",
      "2025-05-15 03:30:08.406150: val_loss -0.9546\n",
      "2025-05-15 03:30:08.406250: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 03:30:08.406393: Epoch time: 108.77 s\n",
      "2025-05-15 03:30:08.979434: \n",
      "2025-05-15 03:30:08.979794: Epoch 692\n",
      "2025-05-15 03:30:08.979925: Current learning rate: 0.00346\n",
      "2025-05-15 03:31:57.847623: train_loss -0.9646\n",
      "2025-05-15 03:31:57.847851: val_loss -0.9524\n",
      "2025-05-15 03:31:57.847884: Pseudo dice [np.float32(0.9813)]\n",
      "2025-05-15 03:31:57.847919: Epoch time: 108.87 s\n",
      "2025-05-15 03:31:58.416525: \n",
      "2025-05-15 03:31:58.416617: Epoch 693\n",
      "2025-05-15 03:31:58.416686: Current learning rate: 0.00345\n",
      "2025-05-15 03:33:47.408742: train_loss -0.9647\n",
      "2025-05-15 03:33:47.408868: val_loss -0.9576\n",
      "2025-05-15 03:33:47.408900: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 03:33:47.408934: Epoch time: 108.99 s\n",
      "2025-05-15 03:33:47.968512: \n",
      "2025-05-15 03:33:47.969033: Epoch 694\n",
      "2025-05-15 03:33:47.969206: Current learning rate: 0.00344\n",
      "2025-05-15 03:35:36.928829: train_loss -0.9642\n",
      "2025-05-15 03:35:36.928968: val_loss -0.9564\n",
      "2025-05-15 03:35:36.929010: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 03:35:36.929044: Epoch time: 108.96 s\n",
      "2025-05-15 03:35:37.495885: \n",
      "2025-05-15 03:35:37.496217: Epoch 695\n",
      "2025-05-15 03:35:37.496410: Current learning rate: 0.00343\n",
      "2025-05-15 03:37:26.496116: train_loss -0.9636\n",
      "2025-05-15 03:37:26.496257: val_loss -0.9554\n",
      "2025-05-15 03:37:26.496288: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 03:37:26.496321: Epoch time: 109.0 s\n",
      "2025-05-15 03:37:27.065058: \n",
      "2025-05-15 03:37:27.065449: Epoch 696\n",
      "2025-05-15 03:37:27.065552: Current learning rate: 0.00342\n",
      "2025-05-15 03:39:16.043383: train_loss -0.9635\n",
      "2025-05-15 03:39:16.043506: val_loss -0.9581\n",
      "2025-05-15 03:39:16.043537: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 03:39:16.043569: Epoch time: 108.98 s\n",
      "2025-05-15 03:39:16.612731: \n",
      "2025-05-15 03:39:16.613039: Epoch 697\n",
      "2025-05-15 03:39:16.613126: Current learning rate: 0.00341\n",
      "2025-05-15 03:41:05.480586: train_loss -0.9651\n",
      "2025-05-15 03:41:05.480714: val_loss -0.9557\n",
      "2025-05-15 03:41:05.480747: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 03:41:05.480781: Epoch time: 108.87 s\n",
      "2025-05-15 03:41:06.057771: \n",
      "2025-05-15 03:41:06.058202: Epoch 698\n",
      "2025-05-15 03:41:06.058504: Current learning rate: 0.0034\n",
      "2025-05-15 03:42:54.963591: train_loss -0.9633\n",
      "2025-05-15 03:42:54.963704: val_loss -0.9591\n",
      "2025-05-15 03:42:54.963735: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 03:42:54.963766: Epoch time: 108.91 s\n",
      "2025-05-15 03:42:55.860744: \n",
      "2025-05-15 03:42:55.860884: Epoch 699\n",
      "2025-05-15 03:42:55.860952: Current learning rate: 0.00339\n",
      "2025-05-15 03:44:44.864859: train_loss -0.9643\n",
      "2025-05-15 03:44:44.865090: val_loss -0.9595\n",
      "2025-05-15 03:44:44.865198: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 03:44:44.865252: Epoch time: 109.0 s\n",
      "2025-05-15 03:44:45.656201: \n",
      "2025-05-15 03:44:45.656496: Epoch 700\n",
      "2025-05-15 03:44:45.656583: Current learning rate: 0.00338\n",
      "2025-05-15 03:46:34.484608: train_loss -0.9646\n",
      "2025-05-15 03:46:34.484731: val_loss -0.9608\n",
      "2025-05-15 03:46:34.484762: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 03:46:34.484795: Epoch time: 108.83 s\n",
      "2025-05-15 03:46:35.061013: \n",
      "2025-05-15 03:46:35.061203: Epoch 701\n",
      "2025-05-15 03:46:35.061281: Current learning rate: 0.00337\n",
      "2025-05-15 03:48:24.092371: train_loss -0.9645\n",
      "2025-05-15 03:48:24.092514: val_loss -0.9594\n",
      "2025-05-15 03:48:24.092547: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 03:48:24.092580: Epoch time: 109.03 s\n",
      "2025-05-15 03:48:24.665834: \n",
      "2025-05-15 03:48:24.666050: Epoch 702\n",
      "2025-05-15 03:48:24.666144: Current learning rate: 0.00336\n",
      "2025-05-15 03:50:13.680249: train_loss -0.9649\n",
      "2025-05-15 03:50:13.680372: val_loss -0.9554\n",
      "2025-05-15 03:50:13.680403: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-15 03:50:13.680437: Epoch time: 109.02 s\n",
      "2025-05-15 03:50:14.252254: \n",
      "2025-05-15 03:50:14.252491: Epoch 703\n",
      "2025-05-15 03:50:14.252591: Current learning rate: 0.00335\n",
      "2025-05-15 03:52:03.054549: train_loss -0.9646\n",
      "2025-05-15 03:52:03.054742: val_loss -0.9512\n",
      "2025-05-15 03:52:03.054775: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 03:52:03.054807: Epoch time: 108.8 s\n",
      "2025-05-15 03:52:03.620249: \n",
      "2025-05-15 03:52:03.620523: Epoch 704\n",
      "2025-05-15 03:52:03.620656: Current learning rate: 0.00334\n",
      "2025-05-15 03:53:52.627626: train_loss -0.9656\n",
      "2025-05-15 03:53:52.627839: val_loss -0.9572\n",
      "2025-05-15 03:53:52.627880: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 03:53:52.627916: Epoch time: 109.01 s\n",
      "2025-05-15 03:53:53.202166: \n",
      "2025-05-15 03:53:53.202553: Epoch 705\n",
      "2025-05-15 03:53:53.202722: Current learning rate: 0.00333\n",
      "2025-05-15 03:55:42.235038: train_loss -0.9647\n",
      "2025-05-15 03:55:42.235160: val_loss -0.9554\n",
      "2025-05-15 03:55:42.235191: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 03:55:42.235224: Epoch time: 109.03 s\n",
      "2025-05-15 03:55:42.808970: \n",
      "2025-05-15 03:55:42.809140: Epoch 706\n",
      "2025-05-15 03:55:42.809216: Current learning rate: 0.00332\n",
      "2025-05-15 03:57:31.629869: train_loss -0.9656\n",
      "2025-05-15 03:57:31.630198: val_loss -0.9555\n",
      "2025-05-15 03:57:31.630515: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 03:57:31.630569: Epoch time: 108.82 s\n",
      "2025-05-15 03:57:32.205227: \n",
      "2025-05-15 03:57:32.205687: Epoch 707\n",
      "2025-05-15 03:57:32.205857: Current learning rate: 0.00331\n",
      "2025-05-15 03:59:21.189935: train_loss -0.9638\n",
      "2025-05-15 03:59:21.190059: val_loss -0.956\n",
      "2025-05-15 03:59:21.190091: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 03:59:21.190124: Epoch time: 108.99 s\n",
      "2025-05-15 03:59:21.772280: \n",
      "2025-05-15 03:59:21.772605: Epoch 708\n",
      "2025-05-15 03:59:21.772689: Current learning rate: 0.0033\n",
      "2025-05-15 04:01:10.786726: train_loss -0.9652\n",
      "2025-05-15 04:01:10.786846: val_loss -0.9556\n",
      "2025-05-15 04:01:10.786879: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-15 04:01:10.786940: Epoch time: 109.02 s\n",
      "2025-05-15 04:01:11.362214: \n",
      "2025-05-15 04:01:11.362678: Epoch 709\n",
      "2025-05-15 04:01:11.363128: Current learning rate: 0.00329\n",
      "2025-05-15 04:03:00.148766: train_loss -0.9676\n",
      "2025-05-15 04:03:00.148941: val_loss -0.9557\n",
      "2025-05-15 04:03:00.148986: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 04:03:00.149103: Epoch time: 108.79 s\n",
      "2025-05-15 04:03:00.720598: \n",
      "2025-05-15 04:03:00.720855: Epoch 710\n",
      "2025-05-15 04:03:00.720934: Current learning rate: 0.00328\n",
      "2025-05-15 04:04:49.524749: train_loss -0.9658\n",
      "2025-05-15 04:04:49.524895: val_loss -0.9599\n",
      "2025-05-15 04:04:49.524928: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 04:04:49.524961: Epoch time: 108.8 s\n",
      "2025-05-15 04:04:50.092101: \n",
      "2025-05-15 04:04:50.092283: Epoch 711\n",
      "2025-05-15 04:04:50.092373: Current learning rate: 0.00327\n",
      "2025-05-15 04:06:38.947683: train_loss -0.9663\n",
      "2025-05-15 04:06:38.947816: val_loss -0.9616\n",
      "2025-05-15 04:06:38.947849: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 04:06:38.947882: Epoch time: 108.86 s\n",
      "2025-05-15 04:06:39.519157: \n",
      "2025-05-15 04:06:39.520032: Epoch 712\n",
      "2025-05-15 04:06:39.520272: Current learning rate: 0.00326\n",
      "2025-05-15 04:08:28.508855: train_loss -0.9649\n",
      "2025-05-15 04:08:28.508977: val_loss -0.9611\n",
      "2025-05-15 04:08:28.509008: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 04:08:28.509043: Epoch time: 108.99 s\n",
      "2025-05-15 04:08:29.086436: \n",
      "2025-05-15 04:08:29.086946: Epoch 713\n",
      "2025-05-15 04:08:29.087058: Current learning rate: 0.00325\n",
      "2025-05-15 04:10:17.831692: train_loss -0.9659\n",
      "2025-05-15 04:10:17.831870: val_loss -0.9577\n",
      "2025-05-15 04:10:17.831904: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-15 04:10:17.831939: Epoch time: 108.75 s\n",
      "2025-05-15 04:10:18.411269: \n",
      "2025-05-15 04:10:18.411632: Epoch 714\n",
      "2025-05-15 04:10:18.411814: Current learning rate: 0.00324\n",
      "2025-05-15 04:12:07.415619: train_loss -0.9651\n",
      "2025-05-15 04:12:07.415749: val_loss -0.9567\n",
      "2025-05-15 04:12:07.415882: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 04:12:07.415936: Epoch time: 109.01 s\n",
      "2025-05-15 04:12:07.988924: \n",
      "2025-05-15 04:12:07.989279: Epoch 715\n",
      "2025-05-15 04:12:07.989446: Current learning rate: 0.00323\n",
      "2025-05-15 04:13:57.010031: train_loss -0.9656\n",
      "2025-05-15 04:13:57.010218: val_loss -0.9559\n",
      "2025-05-15 04:13:57.010254: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-15 04:13:57.010287: Epoch time: 109.02 s\n",
      "2025-05-15 04:13:57.917481: \n",
      "2025-05-15 04:13:57.917766: Epoch 716\n",
      "2025-05-15 04:13:57.917847: Current learning rate: 0.00322\n",
      "2025-05-15 04:15:46.913983: train_loss -0.9668\n",
      "2025-05-15 04:15:46.914161: val_loss -0.9547\n",
      "2025-05-15 04:15:46.914197: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 04:15:46.914230: Epoch time: 109.0 s\n",
      "2025-05-15 04:15:47.496783: \n",
      "2025-05-15 04:15:47.497280: Epoch 717\n",
      "2025-05-15 04:15:47.497369: Current learning rate: 0.00321\n",
      "2025-05-15 04:17:36.532088: train_loss -0.9659\n",
      "2025-05-15 04:17:36.532208: val_loss -0.9537\n",
      "2025-05-15 04:17:36.532240: Pseudo dice [np.float32(0.9809)]\n",
      "2025-05-15 04:17:36.532273: Epoch time: 109.04 s\n",
      "2025-05-15 04:17:37.108857: \n",
      "2025-05-15 04:17:37.109208: Epoch 718\n",
      "2025-05-15 04:17:37.109423: Current learning rate: 0.0032\n",
      "2025-05-15 04:19:25.914544: train_loss -0.9674\n",
      "2025-05-15 04:19:25.914670: val_loss -0.9602\n",
      "2025-05-15 04:19:25.914705: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 04:19:25.914737: Epoch time: 108.81 s\n",
      "2025-05-15 04:19:26.490911: \n",
      "2025-05-15 04:19:26.491257: Epoch 719\n",
      "2025-05-15 04:19:26.491477: Current learning rate: 0.00319\n",
      "2025-05-15 04:21:15.258200: train_loss -0.9675\n",
      "2025-05-15 04:21:15.258427: val_loss -0.9587\n",
      "2025-05-15 04:21:15.258461: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 04:21:15.258493: Epoch time: 108.77 s\n",
      "2025-05-15 04:21:15.830098: \n",
      "2025-05-15 04:21:15.830366: Epoch 720\n",
      "2025-05-15 04:21:15.830595: Current learning rate: 0.00318\n",
      "2025-05-15 04:23:04.774315: train_loss -0.9653\n",
      "2025-05-15 04:23:04.774485: val_loss -0.958\n",
      "2025-05-15 04:23:04.774684: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 04:23:04.774814: Epoch time: 108.94 s\n",
      "2025-05-15 04:23:05.356930: \n",
      "2025-05-15 04:23:05.357260: Epoch 721\n",
      "2025-05-15 04:23:05.357392: Current learning rate: 0.00317\n",
      "2025-05-15 04:24:54.390684: train_loss -0.9659\n",
      "2025-05-15 04:24:54.390858: val_loss -0.956\n",
      "2025-05-15 04:24:54.391008: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 04:24:54.391079: Epoch time: 109.03 s\n",
      "2025-05-15 04:24:54.972422: \n",
      "2025-05-15 04:24:54.972898: Epoch 722\n",
      "2025-05-15 04:24:54.973023: Current learning rate: 0.00316\n",
      "2025-05-15 04:26:43.891527: train_loss -0.9638\n",
      "2025-05-15 04:26:43.891656: val_loss -0.9581\n",
      "2025-05-15 04:26:43.891688: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 04:26:43.891720: Epoch time: 108.92 s\n",
      "2025-05-15 04:26:44.470346: \n",
      "2025-05-15 04:26:44.470542: Epoch 723\n",
      "2025-05-15 04:26:44.470616: Current learning rate: 0.00315\n",
      "2025-05-15 04:28:33.393786: train_loss -0.967\n",
      "2025-05-15 04:28:33.393916: val_loss -0.9526\n",
      "2025-05-15 04:28:33.393951: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-15 04:28:33.393999: Epoch time: 108.92 s\n",
      "2025-05-15 04:28:33.969401: \n",
      "2025-05-15 04:28:33.969643: Epoch 724\n",
      "2025-05-15 04:28:33.969930: Current learning rate: 0.00314\n",
      "2025-05-15 04:30:22.907797: train_loss -0.9661\n",
      "2025-05-15 04:30:22.907924: val_loss -0.9575\n",
      "2025-05-15 04:30:22.908117: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 04:30:22.908220: Epoch time: 108.94 s\n",
      "2025-05-15 04:30:23.492335: \n",
      "2025-05-15 04:30:23.492797: Epoch 725\n",
      "2025-05-15 04:30:23.492999: Current learning rate: 0.00313\n",
      "2025-05-15 04:32:12.336692: train_loss -0.9651\n",
      "2025-05-15 04:32:12.336818: val_loss -0.9636\n",
      "2025-05-15 04:32:12.336848: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-15 04:32:12.336881: Epoch time: 108.84 s\n",
      "2025-05-15 04:32:12.914404: \n",
      "2025-05-15 04:32:12.914737: Epoch 726\n",
      "2025-05-15 04:32:12.914933: Current learning rate: 0.00312\n",
      "2025-05-15 04:34:01.916840: train_loss -0.9677\n",
      "2025-05-15 04:34:01.917029: val_loss -0.9554\n",
      "2025-05-15 04:34:01.917079: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 04:34:01.917118: Epoch time: 109.0 s\n",
      "2025-05-15 04:34:01.917143: Yayy! New best EMA pseudo Dice: 0.9825000166893005\n",
      "2025-05-15 04:34:02.688221: \n",
      "2025-05-15 04:34:02.688724: Epoch 727\n",
      "2025-05-15 04:34:02.688844: Current learning rate: 0.00311\n",
      "2025-05-15 04:35:51.681469: train_loss -0.9662\n",
      "2025-05-15 04:35:51.681591: val_loss -0.9575\n",
      "2025-05-15 04:35:51.681639: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 04:35:51.681726: Epoch time: 108.99 s\n",
      "2025-05-15 04:35:52.254977: \n",
      "2025-05-15 04:35:52.255615: Epoch 728\n",
      "2025-05-15 04:35:52.255849: Current learning rate: 0.0031\n",
      "2025-05-15 04:37:41.094022: train_loss -0.9659\n",
      "2025-05-15 04:37:41.094145: val_loss -0.9586\n",
      "2025-05-15 04:37:41.094220: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 04:37:41.094260: Epoch time: 108.84 s\n",
      "2025-05-15 04:37:41.094317: Yayy! New best EMA pseudo Dice: 0.9825999736785889\n",
      "2025-05-15 04:37:41.883365: \n",
      "2025-05-15 04:37:41.883800: Epoch 729\n",
      "2025-05-15 04:37:41.884008: Current learning rate: 0.00309\n",
      "2025-05-15 04:39:30.841136: train_loss -0.9673\n",
      "2025-05-15 04:39:30.841277: val_loss -0.9605\n",
      "2025-05-15 04:39:30.841308: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 04:39:30.841341: Epoch time: 108.96 s\n",
      "2025-05-15 04:39:30.841361: Yayy! New best EMA pseudo Dice: 0.9825999736785889\n",
      "2025-05-15 04:39:31.619366: \n",
      "2025-05-15 04:39:31.619740: Epoch 730\n",
      "2025-05-15 04:39:31.619831: Current learning rate: 0.00308\n",
      "2025-05-15 04:41:20.659405: train_loss -0.9651\n",
      "2025-05-15 04:41:20.659603: val_loss -0.9565\n",
      "2025-05-15 04:41:20.659635: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 04:41:20.659666: Epoch time: 109.04 s\n",
      "2025-05-15 04:41:21.230166: \n",
      "2025-05-15 04:41:21.230484: Epoch 731\n",
      "2025-05-15 04:41:21.230601: Current learning rate: 0.00307\n",
      "2025-05-15 04:43:10.064520: train_loss -0.9671\n",
      "2025-05-15 04:43:10.064651: val_loss -0.9563\n",
      "2025-05-15 04:43:10.064685: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 04:43:10.064718: Epoch time: 108.83 s\n",
      "2025-05-15 04:43:10.633634: \n",
      "2025-05-15 04:43:10.633895: Epoch 732\n",
      "2025-05-15 04:43:10.634060: Current learning rate: 0.00306\n",
      "2025-05-15 04:44:59.625366: train_loss -0.9668\n",
      "2025-05-15 04:44:59.625489: val_loss -0.958\n",
      "2025-05-15 04:44:59.625522: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 04:44:59.625554: Epoch time: 108.99 s\n",
      "2025-05-15 04:45:00.527295: \n",
      "2025-05-15 04:45:00.527538: Epoch 733\n",
      "2025-05-15 04:45:00.527620: Current learning rate: 0.00305\n",
      "2025-05-15 04:46:49.364288: train_loss -0.9659\n",
      "2025-05-15 04:46:49.364455: val_loss -0.9584\n",
      "2025-05-15 04:46:49.364569: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 04:46:49.364617: Epoch time: 108.84 s\n",
      "2025-05-15 04:46:49.947092: \n",
      "2025-05-15 04:46:49.947434: Epoch 734\n",
      "2025-05-15 04:46:49.947510: Current learning rate: 0.00304\n",
      "2025-05-15 04:48:38.946379: train_loss -0.9667\n",
      "2025-05-15 04:48:38.946513: val_loss -0.96\n",
      "2025-05-15 04:48:38.946546: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 04:48:38.946579: Epoch time: 109.0 s\n",
      "2025-05-15 04:48:39.521041: \n",
      "2025-05-15 04:48:39.521524: Epoch 735\n",
      "2025-05-15 04:48:39.521617: Current learning rate: 0.00303\n",
      "2025-05-15 04:50:28.510194: train_loss -0.9669\n",
      "2025-05-15 04:50:28.510305: val_loss -0.9579\n",
      "2025-05-15 04:50:28.510336: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 04:50:28.510369: Epoch time: 108.99 s\n",
      "2025-05-15 04:50:29.087868: \n",
      "2025-05-15 04:50:29.088181: Epoch 736\n",
      "2025-05-15 04:50:29.088274: Current learning rate: 0.00302\n",
      "2025-05-15 04:52:18.115949: train_loss -0.9664\n",
      "2025-05-15 04:52:18.116073: val_loss -0.9629\n",
      "2025-05-15 04:52:18.116106: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-15 04:52:18.116140: Epoch time: 109.03 s\n",
      "2025-05-15 04:52:18.689667: \n",
      "2025-05-15 04:52:18.690222: Epoch 737\n",
      "2025-05-15 04:52:18.690344: Current learning rate: 0.00301\n",
      "2025-05-15 04:54:07.709039: train_loss -0.9654\n",
      "2025-05-15 04:54:07.709159: val_loss -0.956\n",
      "2025-05-15 04:54:07.709203: Pseudo dice [np.float32(0.9809)]\n",
      "2025-05-15 04:54:07.709242: Epoch time: 109.02 s\n",
      "2025-05-15 04:54:08.285129: \n",
      "2025-05-15 04:54:08.285794: Epoch 738\n",
      "2025-05-15 04:54:08.285882: Current learning rate: 0.003\n",
      "2025-05-15 04:55:57.293892: train_loss -0.9683\n",
      "2025-05-15 04:55:57.294030: val_loss -0.9556\n",
      "2025-05-15 04:55:57.294066: Pseudo dice [np.float32(0.9809)]\n",
      "2025-05-15 04:55:57.294100: Epoch time: 109.01 s\n",
      "2025-05-15 04:55:57.868366: \n",
      "2025-05-15 04:55:57.868584: Epoch 739\n",
      "2025-05-15 04:55:57.868724: Current learning rate: 0.00299\n",
      "2025-05-15 04:57:46.835548: train_loss -0.9674\n",
      "2025-05-15 04:57:46.835750: val_loss -0.9547\n",
      "2025-05-15 04:57:46.835792: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 04:57:46.835826: Epoch time: 108.97 s\n",
      "2025-05-15 04:57:47.408816: \n",
      "2025-05-15 04:57:47.409223: Epoch 740\n",
      "2025-05-15 04:57:47.409306: Current learning rate: 0.00297\n",
      "2025-05-15 04:59:36.433728: train_loss -0.9685\n",
      "2025-05-15 04:59:36.433935: val_loss -0.9557\n",
      "2025-05-15 04:59:36.433971: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 04:59:36.434005: Epoch time: 109.03 s\n",
      "2025-05-15 04:59:37.002393: \n",
      "2025-05-15 04:59:37.002870: Epoch 741\n",
      "2025-05-15 04:59:37.003031: Current learning rate: 0.00296\n",
      "2025-05-15 05:01:26.045087: train_loss -0.9686\n",
      "2025-05-15 05:01:26.045311: val_loss -0.9578\n",
      "2025-05-15 05:01:26.045398: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 05:01:26.045445: Epoch time: 109.04 s\n",
      "2025-05-15 05:01:26.616767: \n",
      "2025-05-15 05:01:26.617019: Epoch 742\n",
      "2025-05-15 05:01:26.617265: Current learning rate: 0.00295\n",
      "2025-05-15 05:03:15.677068: train_loss -0.9683\n",
      "2025-05-15 05:03:15.677306: val_loss -0.9612\n",
      "2025-05-15 05:03:15.677346: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 05:03:15.677381: Epoch time: 109.06 s\n",
      "2025-05-15 05:03:16.253448: \n",
      "2025-05-15 05:03:16.254121: Epoch 743\n",
      "2025-05-15 05:03:16.254316: Current learning rate: 0.00294\n",
      "2025-05-15 05:05:05.303671: train_loss -0.9648\n",
      "2025-05-15 05:05:05.303793: val_loss -0.9601\n",
      "2025-05-15 05:05:05.303826: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 05:05:05.303858: Epoch time: 109.05 s\n",
      "2025-05-15 05:05:05.877956: \n",
      "2025-05-15 05:05:05.878505: Epoch 744\n",
      "2025-05-15 05:05:05.878690: Current learning rate: 0.00293\n",
      "2025-05-15 05:06:54.898402: train_loss -0.9664\n",
      "2025-05-15 05:06:54.898570: val_loss -0.9604\n",
      "2025-05-15 05:06:54.898604: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 05:06:54.898637: Epoch time: 109.02 s\n",
      "2025-05-15 05:06:55.467662: \n",
      "2025-05-15 05:06:55.467925: Epoch 745\n",
      "2025-05-15 05:06:55.468026: Current learning rate: 0.00292\n",
      "2025-05-15 05:08:44.223858: train_loss -0.9656\n",
      "2025-05-15 05:08:44.223981: val_loss -0.9564\n",
      "2025-05-15 05:08:44.224013: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 05:08:44.224045: Epoch time: 108.76 s\n",
      "2025-05-15 05:08:44.799474: \n",
      "2025-05-15 05:08:44.799827: Epoch 746\n",
      "2025-05-15 05:08:44.799905: Current learning rate: 0.00291\n",
      "2025-05-15 05:10:33.812263: train_loss -0.9683\n",
      "2025-05-15 05:10:33.812472: val_loss -0.9572\n",
      "2025-05-15 05:10:33.812505: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 05:10:33.812540: Epoch time: 109.01 s\n",
      "2025-05-15 05:10:34.379543: \n",
      "2025-05-15 05:10:34.379866: Epoch 747\n",
      "2025-05-15 05:10:34.379953: Current learning rate: 0.0029\n",
      "2025-05-15 05:12:23.384370: train_loss -0.9669\n",
      "2025-05-15 05:12:23.384508: val_loss -0.9577\n",
      "2025-05-15 05:12:23.384543: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 05:12:23.384577: Epoch time: 109.01 s\n",
      "2025-05-15 05:12:23.948385: \n",
      "2025-05-15 05:12:23.948787: Epoch 748\n",
      "2025-05-15 05:12:23.948931: Current learning rate: 0.00289\n",
      "2025-05-15 05:14:12.962376: train_loss -0.97\n",
      "2025-05-15 05:14:12.962503: val_loss -0.9588\n",
      "[np.float32(0.9829)]962537: Pseudo dice \n",
      "2025-05-15 05:14:12.962633: Epoch time: 109.01 s\n",
      "2025-05-15 05:14:13.539922: \n",
      "2025-05-15 05:14:13.540408: Epoch 749\n",
      "2025-05-15 05:14:13.540619: Current learning rate: 0.00288\n",
      "2025-05-15 05:16:02.498883: train_loss -0.9669\n",
      "2025-05-15 05:16:02.499162: val_loss -0.9569\n",
      "2025-05-15 05:16:02.499198: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 05:16:02.499230: Epoch time: 108.96 s\n",
      "2025-05-15 05:16:03.624789: \n",
      "2025-05-15 05:16:03.625201: Epoch 750\n",
      "2025-05-15 05:16:03.625318: Current learning rate: 0.00287\n",
      "2025-05-15 05:17:52.548071: train_loss -0.9689\n",
      "2025-05-15 05:17:52.548194: val_loss -0.9607\n",
      "2025-05-15 05:17:52.548370: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 05:17:52.548417: Epoch time: 108.92 s\n",
      "2025-05-15 05:17:53.123695: \n",
      "2025-05-15 05:17:53.124404: Epoch 751\n",
      "2025-05-15 05:17:53.124677: Current learning rate: 0.00286\n",
      "2025-05-15 05:19:42.152683: train_loss -0.9669\n",
      "2025-05-15 05:19:42.152804: val_loss -0.9537\n",
      "2025-05-15 05:19:42.152836: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 05:19:42.152868: Epoch time: 109.03 s\n",
      "2025-05-15 05:19:42.735351: \n",
      "2025-05-15 05:19:42.735900: Epoch 752\n",
      "2025-05-15 05:19:42.736232: Current learning rate: 0.00285\n",
      "2025-05-15 05:21:31.714513: train_loss -0.9691\n",
      "2025-05-15 05:21:31.714639: val_loss -0.9594\n",
      "2025-05-15 05:21:31.714672: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 05:21:31.714707: Epoch time: 108.98 s\n",
      "2025-05-15 05:21:32.290103: \n",
      "2025-05-15 05:21:32.290477: Epoch 753\n",
      "2025-05-15 05:21:32.290584: Current learning rate: 0.00284\n",
      "2025-05-15 05:23:21.205683: train_loss -0.967\n",
      "2025-05-15 05:23:21.205802: val_loss -0.9608\n",
      "2025-05-15 05:23:21.205835: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-15 05:23:21.205868: Epoch time: 108.92 s\n",
      "2025-05-15 05:23:21.780133: \n",
      "2025-05-15 05:23:21.780310: Epoch 754\n",
      "2025-05-15 05:23:21.780405: Current learning rate: 0.00283\n",
      "2025-05-15 05:25:10.694880: train_loss -0.9673\n",
      "2025-05-15 05:25:10.695011: val_loss -0.9543\n",
      "2025-05-15 05:25:10.695043: Pseudo dice [np.float32(0.9804)]\n",
      "2025-05-15 05:25:10.695078: Epoch time: 108.92 s\n",
      "2025-05-15 05:25:11.265352: \n",
      "2025-05-15 05:25:11.265471: Epoch 755\n",
      "2025-05-15 05:25:11.265546: Current learning rate: 0.00282\n",
      "2025-05-15 05:27:00.223979: train_loss -0.9671\n",
      "2025-05-15 05:27:00.224116: val_loss -0.9589\n",
      "2025-05-15 05:27:00.224150: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 05:27:00.224184: Epoch time: 108.96 s\n",
      "2025-05-15 05:27:00.796497: \n",
      "2025-05-15 05:27:00.796871: Epoch 756\n",
      "2025-05-15 05:27:00.796954: Current learning rate: 0.00281\n",
      "2025-05-15 05:28:49.664105: train_loss -0.9676\n",
      "2025-05-15 05:28:49.664219: val_loss -0.9599\n",
      "2025-05-15 05:28:49.664260: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 05:28:49.664310: Epoch time: 108.87 s\n",
      "2025-05-15 05:28:50.239996: \n",
      "2025-05-15 05:28:50.240424: Epoch 757\n",
      "2025-05-15 05:28:50.240544: Current learning rate: 0.0028\n",
      "2025-05-15 05:30:39.226938: train_loss -0.9678\n",
      "2025-05-15 05:30:39.227123: val_loss -0.9602\n",
      "2025-05-15 05:30:39.227162: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 05:30:39.227197: Epoch time: 108.99 s\n",
      "2025-05-15 05:30:39.794683: \n",
      "2025-05-15 05:30:39.794974: Epoch 758\n",
      "2025-05-15 05:30:39.795120: Current learning rate: 0.00279\n",
      "2025-05-15 05:32:28.809210: train_loss -0.968\n",
      "2025-05-15 05:32:28.809442: val_loss -0.9576\n",
      "2025-05-15 05:32:28.809505: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 05:32:28.809548: Epoch time: 109.02 s\n",
      "2025-05-15 05:32:28.809577: Yayy! New best EMA pseudo Dice: 0.9825999736785889\n",
      "2025-05-15 05:32:29.589066: \n",
      "2025-05-15 05:32:29.589491: Epoch 759\n",
      "2025-05-15 05:32:29.589644: Current learning rate: 0.00278\n",
      "2025-05-15 05:34:18.637841: train_loss -0.9691\n",
      "2025-05-15 05:34:18.637962: val_loss -0.9523\n",
      "2025-05-15 05:34:18.638182: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-15 05:34:18.638250: Epoch time: 109.05 s\n",
      "2025-05-15 05:34:19.214436: \n",
      "2025-05-15 05:34:19.214953: Epoch 760\n",
      "2025-05-15 05:34:19.215087: Current learning rate: 0.00277\n",
      "2025-05-15 05:36:08.175253: train_loss -0.9678\n",
      "2025-05-15 05:36:08.175376: val_loss -0.9547\n",
      "2025-05-15 05:36:08.175411: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 05:36:08.175446: Epoch time: 108.96 s\n",
      "2025-05-15 05:36:08.747207: \n",
      "2025-05-15 05:36:08.747597: Epoch 761\n",
      "2025-05-15 05:36:08.747836: Current learning rate: 0.00276\n",
      "2025-05-15 05:37:57.682129: train_loss -0.9683\n",
      "2025-05-15 05:37:57.682263: val_loss -0.9571\n",
      "2025-05-15 05:37:57.682295: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 05:37:57.682327: Epoch time: 108.94 s\n",
      "2025-05-15 05:37:58.251327: \n",
      "2025-05-15 05:37:58.251455: Epoch 762\n",
      "2025-05-15 05:37:58.251538: Current learning rate: 0.00275\n",
      "2025-05-15 05:39:47.266744: train_loss -0.9687\n",
      "2025-05-15 05:39:47.266870: val_loss -0.9595\n",
      "2025-05-15 05:39:47.266903: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 05:39:47.266939: Epoch time: 109.02 s\n",
      "2025-05-15 05:39:47.847383: \n",
      "2025-05-15 05:39:47.847809: Epoch 763\n",
      "2025-05-15 05:39:47.847922: Current learning rate: 0.00274\n",
      "2025-05-15 05:41:36.689773: train_loss -0.9675\n",
      "2025-05-15 05:41:36.689912: val_loss -0.9561\n",
      "2025-05-15 05:41:36.689950: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-15 05:41:36.689984: Epoch time: 108.84 s\n",
      "2025-05-15 05:41:37.265811: \n",
      "2025-05-15 05:41:37.265939: Epoch 764\n",
      "2025-05-15 05:41:37.266068: Current learning rate: 0.00273\n",
      "2025-05-15 05:43:26.179456: train_loss -0.9691\n",
      "2025-05-15 05:43:26.179603: val_loss -0.9574\n",
      "2025-05-15 05:43:26.179661: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 05:43:26.179701: Epoch time: 108.91 s\n",
      "2025-05-15 05:43:26.764038: \n",
      "2025-05-15 05:43:26.764567: Epoch 765\n",
      "2025-05-15 05:43:26.764678: Current learning rate: 0.00272\n",
      "2025-05-15 05:45:15.731122: train_loss -0.9679\n",
      "2025-05-15 05:45:15.731245: val_loss -0.9581\n",
      "2025-05-15 05:45:15.731277: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 05:45:15.731309: Epoch time: 108.97 s\n",
      "2025-05-15 05:45:16.318385: \n",
      "2025-05-15 05:45:16.318750: Epoch 766\n",
      "2025-05-15 05:45:16.318901: Current learning rate: 0.00271\n",
      "2025-05-15 05:47:05.352187: train_loss -0.9667\n",
      "2025-05-15 05:47:05.352313: val_loss -0.9579\n",
      "2025-05-15 05:47:05.352346: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 05:47:05.352380: Epoch time: 109.03 s\n",
      "2025-05-15 05:47:06.272345: \n",
      "2025-05-15 05:47:06.272714: Epoch 767\n",
      "2025-05-15 05:47:06.273184: Current learning rate: 0.0027\n",
      "2025-05-15 05:48:55.367335: train_loss -0.967\n",
      "2025-05-15 05:48:55.367550: val_loss -0.9611\n",
      "2025-05-15 05:48:55.367656: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 05:48:55.367734: Epoch time: 109.1 s\n",
      "2025-05-15 05:48:55.951559: \n",
      "2025-05-15 05:48:55.951875: Epoch 768\n",
      "2025-05-15 05:48:55.951957: Current learning rate: 0.00268\n",
      "2025-05-15 05:50:44.874642: train_loss -0.9679\n",
      "2025-05-15 05:50:44.874763: val_loss -0.961\n",
      "2025-05-15 05:50:44.874798: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 05:50:44.874839: Epoch time: 108.92 s\n",
      "2025-05-15 05:50:45.459641: \n",
      "2025-05-15 05:50:45.460529: Epoch 769\n",
      "2025-05-15 05:50:45.460761: Current learning rate: 0.00267\n",
      "2025-05-15 05:52:34.493801: train_loss -0.9681\n",
      "2025-05-15 05:52:34.494049: val_loss -0.9578\n",
      "2025-05-15 05:52:34.494118: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 05:52:34.494175: Epoch time: 109.03 s\n",
      "2025-05-15 05:52:35.075833: \n",
      "2025-05-15 05:52:35.076331: Epoch 770\n",
      "2025-05-15 05:52:35.076430: Current learning rate: 0.00266\n",
      "2025-05-15 05:54:24.121104: train_loss -0.9671\n",
      "2025-05-15 05:54:24.121286: val_loss -0.9575\n",
      "2025-05-15 05:54:24.121318: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-15 05:54:24.121350: Epoch time: 109.05 s\n",
      "2025-05-15 05:54:24.709768: \n",
      "2025-05-15 05:54:24.709995: Epoch 771\n",
      "2025-05-15 05:54:24.710177: Current learning rate: 0.00265\n",
      "2025-05-15 05:56:13.554539: train_loss -0.9686\n",
      "2025-05-15 05:56:13.554678: val_loss -0.9554\n",
      "2025-05-15 05:56:13.554713: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 05:56:13.554804: Epoch time: 108.85 s\n",
      "2025-05-15 05:56:14.136257: \n",
      "2025-05-15 05:56:14.136449: Epoch 772\n",
      "2025-05-15 05:56:14.136606: Current learning rate: 0.00264\n",
      "2025-05-15 05:58:03.066220: train_loss -0.9679\n",
      "2025-05-15 05:58:03.066348: val_loss -0.9579\n",
      "2025-05-15 05:58:03.066381: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 05:58:03.066414: Epoch time: 108.93 s\n",
      "2025-05-15 05:58:03.645210: \n",
      "2025-05-15 05:58:03.645748: Epoch 773\n",
      "2025-05-15 05:58:03.645862: Current learning rate: 0.00263\n",
      "2025-05-15 05:59:52.628366: train_loss -0.9685\n",
      "2025-05-15 05:59:52.628550: val_loss -0.9594\n",
      "2025-05-15 05:59:52.628911: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 05:59:52.629339: Epoch time: 108.98 s\n",
      "2025-05-15 05:59:53.207201: \n",
      "2025-05-15 05:59:53.207443: Epoch 774\n",
      "2025-05-15 05:59:53.207531: Current learning rate: 0.00262\n",
      "2025-05-15 06:01:42.269894: train_loss -0.9696\n",
      "2025-05-15 06:01:42.270026: val_loss -0.9618\n",
      "2025-05-15 06:01:42.270070: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 06:01:42.270106: Epoch time: 109.06 s\n",
      "2025-05-15 06:01:42.844372: \n",
      "2025-05-15 06:01:42.844802: Epoch 775\n",
      "2025-05-15 06:01:42.844912: Current learning rate: 0.00261\n",
      "2025-05-15 06:03:31.844738: train_loss -0.9656\n",
      "2025-05-15 06:03:31.844878: val_loss -0.9546\n",
      "2025-05-15 06:03:31.844913: Pseudo dice [np.float32(0.9813)]\n",
      "2025-05-15 06:03:31.844945: Epoch time: 109.0 s\n",
      "2025-05-15 06:03:32.429215: \n",
      "2025-05-15 06:03:32.429435: Epoch 776\n",
      "2025-05-15 06:03:32.429608: Current learning rate: 0.0026\n",
      "2025-05-15 06:05:21.246819: train_loss -0.9659\n",
      "2025-05-15 06:05:21.246947: val_loss -0.9566\n",
      "2025-05-15 06:05:21.246979: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 06:05:21.247014: Epoch time: 108.82 s\n",
      "2025-05-15 06:05:21.824888: \n",
      "2025-05-15 06:05:21.825525: Epoch 777\n",
      "2025-05-15 06:05:21.825664: Current learning rate: 0.00259\n",
      "2025-05-15 06:07:10.649036: train_loss -0.9694\n",
      "2025-05-15 06:07:10.649179: val_loss -0.9535\n",
      "2025-05-15 06:07:10.649216: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 06:07:10.649249: Epoch time: 108.83 s\n",
      "2025-05-15 06:07:11.228854: \n",
      "2025-05-15 06:07:11.229437: Epoch 778\n",
      "2025-05-15 06:07:11.229560: Current learning rate: 0.00258\n",
      "2025-05-15 06:09:00.105271: train_loss -0.9675\n",
      "2025-05-15 06:09:00.105552: val_loss -0.9594\n",
      "2025-05-15 06:09:00.105591: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 06:09:00.105624: Epoch time: 108.88 s\n",
      "2025-05-15 06:09:00.679243: \n",
      "2025-05-15 06:09:00.679758: Epoch 779\n",
      "2025-05-15 06:09:00.679891: Current learning rate: 0.00257\n",
      "2025-05-15 06:10:49.645774: train_loss -0.9692\n",
      "2025-05-15 06:10:49.646112: val_loss -0.9559\n",
      "2025-05-15 06:10:49.646253: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 06:10:49.646342: Epoch time: 108.97 s\n",
      "2025-05-15 06:10:50.228813: \n",
      "2025-05-15 06:10:50.229314: Epoch 780\n",
      "2025-05-15 06:10:50.229494: Current learning rate: 0.00256\n",
      "2025-05-15 06:12:39.877117: train_loss -0.9687\n",
      "2025-05-15 06:12:39.877238: val_loss -0.9603\n",
      "2025-05-15 06:12:39.877269: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 06:12:39.877301: Epoch time: 109.65 s\n",
      "2025-05-15 06:12:40.453365: \n",
      "2025-05-15 06:12:40.453453: Epoch 781\n",
      "2025-05-15 06:12:40.453602: Current learning rate: 0.00255\n",
      "2025-05-15 06:14:29.717044: train_loss -0.9687\n",
      "2025-05-15 06:14:29.717182: val_loss -0.9566\n",
      "2025-05-15 06:14:29.717223: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 06:14:29.717258: Epoch time: 109.26 s\n",
      "2025-05-15 06:14:30.293060: \n",
      "2025-05-15 06:14:30.293328: Epoch 782\n",
      "2025-05-15 06:14:30.293533: Current learning rate: 0.00254\n",
      "2025-05-15 06:16:19.605729: train_loss -0.9685\n",
      "2025-05-15 06:16:19.605868: val_loss -0.9624\n",
      "2025-05-15 06:16:19.605905: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-15 06:16:19.605938: Epoch time: 109.31 s\n",
      "2025-05-15 06:16:20.187004: \n",
      "2025-05-15 06:16:20.187271: Epoch 783\n",
      "2025-05-15 06:16:20.187355: Current learning rate: 0.00253\n",
      "2025-05-15 06:18:09.312982: train_loss -0.968\n",
      "2025-05-15 06:18:09.313102: val_loss -0.9549\n",
      "2025-05-15 06:18:09.313134: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 06:18:09.313167: Epoch time: 109.13 s\n",
      "2025-05-15 06:18:10.243504: \n",
      "2025-05-15 06:18:10.243689: Epoch 784\n",
      "2025-05-15 06:18:10.243839: Current learning rate: 0.00252\n",
      "2025-05-15 06:19:59.622908: train_loss -0.9685\n",
      "2025-05-15 06:19:59.623086: val_loss -0.9579\n",
      "2025-05-15 06:19:59.623132: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 06:19:59.623178: Epoch time: 109.38 s\n",
      "2025-05-15 06:20:00.207932: \n",
      "2025-05-15 06:20:00.208145: Epoch 785\n",
      "2025-05-15 06:20:00.208298: Current learning rate: 0.00251\n",
      "2025-05-15 06:21:49.510533: train_loss -0.9685\n",
      "2025-05-15 06:21:49.510708: val_loss -0.9561\n",
      "2025-05-15 06:21:49.510741: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 06:21:49.510774: Epoch time: 109.3 s\n",
      "2025-05-15 06:21:50.097232: \n",
      "2025-05-15 06:21:50.097707: Epoch 786\n",
      "2025-05-15 06:21:50.097822: Current learning rate: 0.0025\n",
      "2025-05-15 06:23:39.104304: train_loss -0.9688\n",
      "2025-05-15 06:23:39.104425: val_loss -0.9583\n",
      "2025-05-15 06:23:39.104457: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 06:23:39.104490: Epoch time: 109.01 s\n",
      "2025-05-15 06:23:39.691003: \n",
      "2025-05-15 06:23:39.691504: Epoch 787\n",
      "2025-05-15 06:23:39.691755: Current learning rate: 0.00249\n",
      "2025-05-15 06:25:28.751441: train_loss -0.9678\n",
      "2025-05-15 06:25:28.751585: val_loss -0.9555\n",
      "2025-05-15 06:25:28.751634: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 06:25:28.751671: Epoch time: 109.06 s\n",
      "2025-05-15 06:25:29.330759: \n",
      "2025-05-15 06:25:29.331138: Epoch 788\n",
      "2025-05-15 06:25:29.331276: Current learning rate: 0.00248\n",
      "2025-05-15 06:27:18.208250: train_loss -0.9687\n",
      "2025-05-15 06:27:18.208401: val_loss -0.9552\n",
      "2025-05-15 06:27:18.208434: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 06:27:18.208468: Epoch time: 108.88 s\n",
      "2025-05-15 06:27:18.783928: \n",
      "2025-05-15 06:27:18.784186: Epoch 789\n",
      "2025-05-15 06:27:18.784343: Current learning rate: 0.00247\n",
      "2025-05-15 06:29:07.829796: train_loss -0.9684\n",
      "2025-05-15 06:29:07.830206: val_loss -0.9559\n",
      "2025-05-15 06:29:07.830304: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 06:29:07.830392: Epoch time: 109.05 s\n",
      "2025-05-15 06:29:08.409111: \n",
      "2025-05-15 06:29:08.409325: Epoch 790\n",
      "2025-05-15 06:29:08.409492: Current learning rate: 0.00245\n",
      "2025-05-15 06:30:57.395542: train_loss -0.9694\n",
      "2025-05-15 06:30:57.395662: val_loss -0.9481\n",
      "2025-05-15 06:30:57.395696: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-15 06:30:57.395741: Epoch time: 108.99 s\n",
      "2025-05-15 06:30:57.979639: \n",
      "2025-05-15 06:30:57.980250: Epoch 791\n",
      "2025-05-15 06:30:57.980438: Current learning rate: 0.00244\n",
      "2025-05-15 06:32:46.853564: train_loss -0.9711\n",
      "2025-05-15 06:32:46.853687: val_loss -0.9572\n",
      "2025-05-15 06:32:46.853719: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 06:32:46.853767: Epoch time: 108.87 s\n",
      "2025-05-15 06:32:47.432455: \n",
      "2025-05-15 06:32:47.432728: Epoch 792\n",
      "2025-05-15 06:32:47.432837: Current learning rate: 0.00243\n",
      "2025-05-15 06:34:36.375859: train_loss -0.9704\n",
      "2025-05-15 06:34:36.375985: val_loss -0.9599\n",
      "2025-05-15 06:34:36.376017: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 06:34:36.376051: Epoch time: 108.94 s\n",
      "2025-05-15 06:34:36.952747: \n",
      "2025-05-15 06:34:36.953049: Epoch 793\n",
      "2025-05-15 06:34:36.953139: Current learning rate: 0.00242\n",
      "2025-05-15 06:36:25.838730: train_loss -0.9696\n",
      "2025-05-15 06:36:25.838897: val_loss -0.9567\n",
      "2025-05-15 06:36:25.838929: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 06:36:25.838964: Epoch time: 108.89 s\n",
      "2025-05-15 06:36:26.412798: \n",
      "2025-05-15 06:36:26.413171: Epoch 794\n",
      "2025-05-15 06:36:26.413350: Current learning rate: 0.00241\n",
      "2025-05-15 06:38:15.224455: train_loss -0.97\n",
      "2025-05-15 06:38:15.224586: val_loss -0.9581\n",
      "2025-05-15 06:38:15.224632: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 06:38:15.224669: Epoch time: 108.81 s\n",
      "2025-05-15 06:38:15.813688: \n",
      "2025-05-15 06:38:15.814216: Epoch 795\n",
      "2025-05-15 06:38:15.814395: Current learning rate: 0.0024\n",
      "2025-05-15 06:40:04.798166: train_loss -0.9686\n",
      "2025-05-15 06:40:04.798290: val_loss -0.9558\n",
      "2025-05-15 06:40:04.798326: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-15 06:40:04.798360: Epoch time: 108.99 s\n",
      "2025-05-15 06:40:05.376984: \n",
      "2025-05-15 06:40:05.377340: Epoch 796\n",
      "2025-05-15 06:40:05.377432: Current learning rate: 0.00239\n",
      "2025-05-15 06:41:54.422827: train_loss -0.9691\n",
      "2025-05-15 06:41:54.422951: val_loss -0.9533\n",
      "2025-05-15 06:41:54.422984: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 06:41:54.423017: Epoch time: 109.05 s\n",
      "2025-05-15 06:41:55.004058: \n",
      "2025-05-15 06:41:55.004157: Epoch 797\n",
      "2025-05-15 06:41:55.004583: Current learning rate: 0.00238\n",
      "2025-05-15 06:43:44.021773: train_loss -0.9702\n",
      "2025-05-15 06:43:44.021935: val_loss -0.9535\n",
      "2025-05-15 06:43:44.021966: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 06:43:44.022000: Epoch time: 109.02 s\n",
      "2025-05-15 06:43:44.603536: \n",
      "2025-05-15 06:43:44.603632: Epoch 798\n",
      "2025-05-15 06:43:44.603772: Current learning rate: 0.00237\n",
      "2025-05-15 06:45:33.630056: train_loss -0.9691\n",
      "2025-05-15 06:45:33.630194: val_loss -0.9579\n",
      "2025-05-15 06:45:33.630283: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 06:45:33.630339: Epoch time: 109.03 s\n",
      "2025-05-15 06:45:34.219450: \n",
      "2025-05-15 06:45:34.219816: Epoch 799\n",
      "2025-05-15 06:45:34.219923: Current learning rate: 0.00236\n",
      "2025-05-15 06:47:23.226923: train_loss -0.9708\n",
      "2025-05-15 06:47:23.227059: val_loss -0.9513\n",
      "2025-05-15 06:47:23.227091: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-15 06:47:23.227124: Epoch time: 109.01 s\n",
      "2025-05-15 06:47:24.022015: \n",
      "2025-05-15 06:47:24.022298: Epoch 800\n",
      "2025-05-15 06:47:24.022570: Current learning rate: 0.00235\n",
      "2025-05-15 06:49:13.022480: train_loss -0.9705\n",
      "2025-05-15 06:49:13.022643: val_loss -0.9538\n",
      "2025-05-15 06:49:13.022677: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 06:49:13.022711: Epoch time: 109.0 s\n",
      "2025-05-15 06:49:13.978486: \n",
      "2025-05-15 06:49:13.979091: Epoch 801\n",
      "2025-05-15 06:49:13.979237: Current learning rate: 0.00234\n",
      "2025-05-15 06:51:02.995656: train_loss -0.9688\n",
      "2025-05-15 06:51:02.995871: val_loss -0.9588\n",
      "2025-05-15 06:51:02.995910: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 06:51:02.995950: Epoch time: 109.02 s\n",
      "2025-05-15 06:51:03.579668: \n",
      "2025-05-15 06:51:03.579917: Epoch 802\n",
      "2025-05-15 06:51:03.580078: Current learning rate: 0.00233\n",
      "2025-05-15 06:52:52.677034: train_loss -0.9693\n",
      "2025-05-15 06:52:52.677158: val_loss -0.9569\n",
      "2025-05-15 06:52:52.677190: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 06:52:52.677223: Epoch time: 109.1 s\n",
      "2025-05-15 06:52:53.255664: \n",
      "2025-05-15 06:52:53.256210: Epoch 803\n",
      "2025-05-15 06:52:53.256297: Current learning rate: 0.00232\n",
      "2025-05-15 06:54:42.105946: train_loss -0.9685\n",
      "2025-05-15 06:54:42.106201: val_loss -0.9589\n",
      "2025-05-15 06:54:42.106244: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 06:54:42.106278: Epoch time: 108.85 s\n",
      "2025-05-15 06:54:42.689861: \n",
      "2025-05-15 06:54:42.690550: Epoch 804\n",
      "2025-05-15 06:54:42.690730: Current learning rate: 0.00231\n",
      "2025-05-15 06:56:31.701991: train_loss -0.9694\n",
      "2025-05-15 06:56:31.702183: val_loss -0.9625\n",
      "2025-05-15 06:56:31.702219: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-15 06:56:31.702254: Epoch time: 109.01 s\n",
      "2025-05-15 06:56:32.300915: \n",
      "2025-05-15 06:56:32.301047: Epoch 805\n",
      "2025-05-15 06:56:32.301123: Current learning rate: 0.0023\n",
      "2025-05-15 06:58:21.239189: train_loss -0.9694\n",
      "2025-05-15 06:58:21.239452: val_loss -0.9607\n",
      "2025-05-15 06:58:21.239494: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 06:58:21.239527: Epoch time: 108.94 s\n",
      "2025-05-15 06:58:21.822630: \n",
      "2025-05-15 06:58:21.823299: Epoch 806\n",
      "2025-05-15 06:58:21.823562: Current learning rate: 0.00229\n",
      "2025-05-15 07:00:10.647322: train_loss -0.9705\n",
      "2025-05-15 07:00:10.647518: val_loss -0.9572\n",
      "2025-05-15 07:00:10.647554: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 07:00:10.647601: Epoch time: 108.83 s\n",
      "2025-05-15 07:00:11.234430: \n",
      "2025-05-15 07:00:11.234804: Epoch 807\n",
      "2025-05-15 07:00:11.235125: Current learning rate: 0.00228\n",
      "2025-05-15 07:02:00.275937: train_loss -0.9699\n",
      "2025-05-15 07:02:00.276068: val_loss -0.9542\n",
      "2025-05-15 07:02:00.276101: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 07:02:00.276135: Epoch time: 109.04 s\n",
      "2025-05-15 07:02:00.855740: \n",
      "2025-05-15 07:02:00.856410: Epoch 808\n",
      "2025-05-15 07:02:00.856618: Current learning rate: 0.00226\n",
      "2025-05-15 07:03:49.639110: train_loss -0.9687\n",
      "2025-05-15 07:03:49.639230: val_loss -0.9549\n",
      "2025-05-15 07:03:49.639263: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 07:03:49.639295: Epoch time: 108.78 s\n",
      "2025-05-15 07:03:50.224650: \n",
      "2025-05-15 07:03:50.225082: Epoch 809\n",
      "2025-05-15 07:03:50.225255: Current learning rate: 0.00225\n",
      "2025-05-15 07:05:39.071527: train_loss -0.9703\n",
      "2025-05-15 07:05:39.071710: val_loss -0.9563\n",
      "2025-05-15 07:05:39.071792: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 07:05:39.071859: Epoch time: 108.85 s\n",
      "2025-05-15 07:05:39.650363: \n",
      "2025-05-15 07:05:39.650518: Epoch 810\n",
      "2025-05-15 07:05:39.650586: Current learning rate: 0.00224\n",
      "2025-05-15 07:07:28.474902: train_loss -0.97\n",
      "2025-05-15 07:07:28.475035: val_loss -0.9601\n",
      "2025-05-15 07:07:28.475068: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 07:07:28.475101: Epoch time: 108.83 s\n",
      "2025-05-15 07:07:29.069413: \n",
      "2025-05-15 07:07:29.069788: Epoch 811\n",
      "2025-05-15 07:07:29.069901: Current learning rate: 0.00223\n",
      "2025-05-15 07:09:17.922416: train_loss -0.9687\n",
      "2025-05-15 07:09:17.922541: val_loss -0.9585\n",
      "2025-05-15 07:09:17.922572: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 07:09:17.922605: Epoch time: 108.85 s\n",
      "2025-05-15 07:09:18.508292: \n",
      "2025-05-15 07:09:18.508766: Epoch 812\n",
      "2025-05-15 07:09:18.508862: Current learning rate: 0.00222\n",
      "2025-05-15 07:11:07.534206: train_loss -0.9676\n",
      "2025-05-15 07:11:07.534335: val_loss -0.9563\n",
      "2025-05-15 07:11:07.534368: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 07:11:07.534478: Epoch time: 109.03 s\n",
      "2025-05-15 07:11:08.112342: \n",
      "2025-05-15 07:11:08.112802: Epoch 813\n",
      "2025-05-15 07:11:08.112900: Current learning rate: 0.00221\n",
      "2025-05-15 07:12:57.112500: train_loss -0.967\n",
      "2025-05-15 07:12:57.112669: val_loss -0.9599\n",
      "2025-05-15 07:12:57.112701: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 07:12:57.112736: Epoch time: 109.0 s\n",
      "2025-05-15 07:12:57.694111: \n",
      "2025-05-15 07:12:57.694367: Epoch 814\n",
      "2025-05-15 07:12:57.694453: Current learning rate: 0.0022\n",
      "2025-05-15 07:14:46.633601: train_loss -0.969\n",
      "2025-05-15 07:14:46.633733: val_loss -0.9571\n",
      "2025-05-15 07:14:46.633767: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 07:14:46.633801: Epoch time: 108.94 s\n",
      "2025-05-15 07:14:47.211395: \n",
      "2025-05-15 07:14:47.211761: Epoch 815\n",
      "2025-05-15 07:14:47.211839: Current learning rate: 0.00219\n",
      "2025-05-15 07:16:36.206180: train_loss -0.9695\n",
      "2025-05-15 07:16:36.206421: val_loss -0.956\n",
      "2025-05-15 07:16:36.206518: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 07:16:36.206642: Epoch time: 109.0 s\n",
      "2025-05-15 07:16:36.788419: \n",
      "2025-05-15 07:16:36.788636: Epoch 816\n",
      "2025-05-15 07:16:36.788711: Current learning rate: 0.00218\n",
      "2025-05-15 07:18:25.735101: train_loss -0.9704\n",
      "2025-05-15 07:18:25.735242: val_loss -0.9553\n",
      "2025-05-15 07:18:25.735279: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 07:18:25.735314: Epoch time: 108.95 s\n",
      "2025-05-15 07:18:26.313684: \n",
      "2025-05-15 07:18:26.313792: Epoch 817\n",
      "2025-05-15 07:18:26.313891: Current learning rate: 0.00217\n",
      "2025-05-15 07:20:15.173993: train_loss -0.969\n",
      "2025-05-15 07:20:15.174190: val_loss -0.9524\n",
      "2025-05-15 07:20:15.174225: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-15 07:20:15.174258: Epoch time: 108.86 s\n",
      "2025-05-15 07:20:16.101673: \n",
      "2025-05-15 07:20:16.101802: Epoch 818\n",
      "2025-05-15 07:20:16.101873: Current learning rate: 0.00216\n",
      "2025-05-15 07:22:05.127531: train_loss -0.9693\n",
      "2025-05-15 07:22:05.127711: val_loss -0.952\n",
      "2025-05-15 07:22:05.127744: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 07:22:05.127777: Epoch time: 109.03 s\n",
      "2025-05-15 07:22:05.708766: \n",
      "2025-05-15 07:22:05.709161: Epoch 819\n",
      "2025-05-15 07:22:05.709256: Current learning rate: 0.00215\n",
      "2025-05-15 07:23:54.495497: train_loss -0.9696\n",
      "2025-05-15 07:23:54.495718: val_loss -0.9549\n",
      "2025-05-15 07:23:54.495764: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 07:23:54.495799: Epoch time: 108.79 s\n",
      "2025-05-15 07:23:55.053383: \n",
      "2025-05-15 07:23:55.053605: Epoch 820\n",
      "2025-05-15 07:23:55.053711: Current learning rate: 0.00214\n",
      "2025-05-15 07:25:44.018836: train_loss -0.9699\n",
      "2025-05-15 07:25:44.018961: val_loss -0.9548\n",
      "2025-05-15 07:25:44.018993: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 07:25:44.019025: Epoch time: 108.97 s\n",
      "2025-05-15 07:25:44.579592: \n",
      "2025-05-15 07:25:44.580274: Epoch 821\n",
      "2025-05-15 07:25:44.580518: Current learning rate: 0.00213\n",
      "2025-05-15 07:27:33.404350: train_loss -0.9694\n",
      "2025-05-15 07:27:33.404520: val_loss -0.9554\n",
      "2025-05-15 07:27:33.404555: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 07:27:33.404589: Epoch time: 108.83 s\n",
      "2025-05-15 07:27:33.966715: \n",
      "2025-05-15 07:27:33.966939: Epoch 822\n",
      "2025-05-15 07:27:33.967151: Current learning rate: 0.00212\n",
      "2025-05-15 07:29:22.992435: train_loss -0.971\n",
      "2025-05-15 07:29:22.992562: val_loss -0.9598\n",
      "2025-05-15 07:29:22.992594: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 07:29:22.992626: Epoch time: 109.03 s\n",
      "2025-05-15 07:29:23.548005: \n",
      "2025-05-15 07:29:23.548362: Epoch 823\n",
      "2025-05-15 07:29:23.548460: Current learning rate: 0.0021\n",
      "2025-05-15 07:31:12.396101: train_loss -0.9685\n",
      "2025-05-15 07:31:12.396238: val_loss -0.9584\n",
      "2025-05-15 07:31:12.396328: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 07:31:12.396391: Epoch time: 108.85 s\n",
      "2025-05-15 07:31:12.965342: \n",
      "2025-05-15 07:31:12.965816: Epoch 824\n",
      "2025-05-15 07:31:12.965920: Current learning rate: 0.00209\n",
      "2025-05-15 07:33:01.778675: train_loss -0.97\n",
      "2025-05-15 07:33:01.778898: val_loss -0.9625\n",
      "2025-05-15 07:33:01.779048: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-15 07:33:01.779149: Epoch time: 108.81 s\n",
      "2025-05-15 07:33:02.350403: \n",
      "2025-05-15 07:33:02.351067: Epoch 825\n",
      "2025-05-15 07:33:02.351198: Current learning rate: 0.00208\n",
      "2025-05-15 07:34:51.200701: train_loss -0.9705\n",
      "2025-05-15 07:34:51.200904: val_loss -0.9582\n",
      "2025-05-15 07:34:51.200938: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 07:34:51.200971: Epoch time: 108.85 s\n",
      "2025-05-15 07:34:51.758837: \n",
      "2025-05-15 07:34:51.759041: Epoch 826\n",
      "2025-05-15 07:34:51.759128: Current learning rate: 0.00207\n",
      "2025-05-15 07:36:40.739718: train_loss -0.9701\n",
      "2025-05-15 07:36:40.739900: val_loss -0.9517\n",
      "2025-05-15 07:36:40.740088: Pseudo dice [np.float32(0.9795)]\n",
      "2025-05-15 07:36:40.740160: Epoch time: 108.98 s\n",
      "2025-05-15 07:36:41.298442: \n",
      "2025-05-15 07:36:41.299070: Epoch 827\n",
      "2025-05-15 07:36:41.299221: Current learning rate: 0.00206\n",
      "2025-05-15 07:38:30.286278: train_loss -0.9705\n",
      "2025-05-15 07:38:30.286409: val_loss -0.9571\n",
      "2025-05-15 07:38:30.286441: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 07:38:30.286474: Epoch time: 108.99 s\n",
      "2025-05-15 07:38:30.841786: \n",
      "2025-05-15 07:38:30.841924: Epoch 828\n",
      "2025-05-15 07:38:30.841996: Current learning rate: 0.00205\n",
      "2025-05-15 07:40:19.858593: train_loss -0.9699\n",
      "2025-05-15 07:40:19.858718: val_loss -0.956\n",
      "2025-05-15 07:40:19.858749: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 07:40:19.858783: Epoch time: 109.02 s\n",
      "2025-05-15 07:40:20.421789: \n",
      "2025-05-15 07:40:20.422116: Epoch 829\n",
      "2025-05-15 07:40:20.422264: Current learning rate: 0.00204\n",
      "2025-05-15 07:42:09.352639: train_loss -0.9704\n",
      "2025-05-15 07:42:09.352770: val_loss -0.9534\n",
      "2025-05-15 07:42:09.352803: Pseudo dice [np.float32(0.9809)]\n",
      "2025-05-15 07:42:09.352852: Epoch time: 108.93 s\n",
      "2025-05-15 07:42:09.906067: \n",
      "2025-05-15 07:42:09.906178: Epoch 830\n",
      "2025-05-15 07:42:09.906432: Current learning rate: 0.00203\n",
      "2025-05-15 07:43:58.989346: train_loss -0.9701\n",
      "2025-05-15 07:43:58.989474: val_loss -0.9591\n",
      "2025-05-15 07:43:58.989505: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 07:43:58.989537: Epoch time: 109.08 s\n",
      "2025-05-15 07:43:59.549499: \n",
      "2025-05-15 07:43:59.549668: Epoch 831\n",
      "2025-05-15 07:43:59.549759: Current learning rate: 0.00202\n",
      "2025-05-15 07:45:48.457300: train_loss -0.9696\n",
      "2025-05-15 07:45:48.457446: val_loss -0.9583\n",
      "2025-05-15 07:45:48.457478: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 07:45:48.457540: Epoch time: 108.91 s\n",
      "2025-05-15 07:45:49.026100: \n",
      "2025-05-15 07:45:49.026530: Epoch 832\n",
      "2025-05-15 07:45:49.026789: Current learning rate: 0.00201\n",
      "2025-05-15 07:47:37.960433: train_loss -0.9703\n",
      "2025-05-15 07:47:37.960562: val_loss -0.956\n",
      "2025-05-15 07:47:37.960597: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 07:47:37.960629: Epoch time: 108.94 s\n",
      "2025-05-15 07:47:38.518750: \n",
      "2025-05-15 07:47:38.518913: Epoch 833\n",
      "2025-05-15 07:47:38.518994: Current learning rate: 0.002\n",
      "2025-05-15 07:49:27.364649: train_loss -0.9702\n",
      "2025-05-15 07:49:27.364791: val_loss -0.9577\n",
      "2025-05-15 07:49:27.364823: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 07:49:27.364861: Epoch time: 108.85 s\n",
      "2025-05-15 07:49:27.922023: \n",
      "2025-05-15 07:49:27.922156: Epoch 834\n",
      "2025-05-15 07:49:27.922247: Current learning rate: 0.00199\n",
      "2025-05-15 07:51:16.886594: train_loss -0.9709\n",
      "2025-05-15 07:51:16.886811: val_loss -0.9611\n",
      "2025-05-15 07:51:16.886912: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-15 07:51:16.886972: Epoch time: 108.97 s\n",
      "2025-05-15 07:51:17.443750: \n",
      "2025-05-15 07:51:17.443902: Epoch 835\n",
      "2025-05-15 07:51:17.443993: Current learning rate: 0.00198\n",
      "2025-05-15 07:53:06.557007: train_loss -0.972\n",
      "2025-05-15 07:53:06.557188: val_loss -0.9574\n",
      "2025-05-15 07:53:06.557266: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 07:53:06.557568: Epoch time: 109.11 s\n",
      "2025-05-15 07:53:07.453018: \n",
      "2025-05-15 07:53:07.453178: Epoch 836\n",
      "2025-05-15 07:53:07.453331: Current learning rate: 0.00196\n",
      "2025-05-15 07:54:56.487966: train_loss -0.9705\n",
      "2025-05-15 07:54:56.488143: val_loss -0.959\n",
      "2025-05-15 07:54:56.488176: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 07:54:56.488208: Epoch time: 109.04 s\n",
      "2025-05-15 07:54:57.047192: \n",
      "2025-05-15 07:54:57.047319: Epoch 837\n",
      "2025-05-15 07:54:57.047565: Current learning rate: 0.00195\n",
      "2025-05-15 07:56:46.039441: train_loss -0.9702\n",
      "2025-05-15 07:56:46.039567: val_loss -0.9566\n",
      "2025-05-15 07:56:46.039599: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 07:56:46.039631: Epoch time: 108.99 s\n",
      "2025-05-15 07:56:46.599856: \n",
      "2025-05-15 07:56:46.600224: Epoch 838\n",
      "2025-05-15 07:56:46.600438: Current learning rate: 0.00194\n",
      "2025-05-15 07:58:35.657786: train_loss -0.9701\n",
      "2025-05-15 07:58:35.657920: val_loss -0.9558\n",
      "2025-05-15 07:58:35.657952: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 07:58:35.657985: Epoch time: 109.06 s\n",
      "2025-05-15 07:58:36.224514: \n",
      "2025-05-15 07:58:36.224688: Epoch 839\n",
      "2025-05-15 07:58:36.224771: Current learning rate: 0.00193\n",
      "2025-05-15 08:00:25.245700: train_loss -0.9693\n",
      "2025-05-15 08:00:25.246022: val_loss -0.959\n",
      "2025-05-15 08:00:25.246064: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-15 08:00:25.246097: Epoch time: 109.02 s\n",
      "2025-05-15 08:00:25.805902: \n",
      "2025-05-15 08:00:25.806238: Epoch 840\n",
      "2025-05-15 08:00:25.806384: Current learning rate: 0.00192\n",
      "2025-05-15 08:02:14.838735: train_loss -0.9711\n",
      "2025-05-15 08:02:14.838856: val_loss -0.9572\n",
      "2025-05-15 08:02:14.838886: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 08:02:14.838919: Epoch time: 109.03 s\n",
      "2025-05-15 08:02:15.399929: \n",
      "2025-05-15 08:02:15.400420: Epoch 841\n",
      "2025-05-15 08:02:15.400671: Current learning rate: 0.00191\n",
      "2025-05-15 08:04:04.274938: train_loss -0.9707\n",
      "2025-05-15 08:04:04.275157: val_loss -0.9567\n",
      "2025-05-15 08:04:04.275226: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 08:04:04.275271: Epoch time: 108.88 s\n",
      "2025-05-15 08:04:04.275298: Yayy! New best EMA pseudo Dice: 0.982699990272522\n",
      "2025-05-15 08:04:05.052640: \n",
      "2025-05-15 08:04:05.052967: Epoch 842\n",
      "2025-05-15 08:04:05.053052: Current learning rate: 0.0019\n",
      "2025-05-15 08:05:53.873452: train_loss -0.9714\n",
      "2025-05-15 08:05:53.873645: val_loss -0.9504\n",
      "2025-05-15 08:05:53.873836: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-15 08:05:53.873885: Epoch time: 108.82 s\n",
      "2025-05-15 08:05:54.430426: \n",
      "2025-05-15 08:05:54.430754: Epoch 843\n",
      "2025-05-15 08:05:54.430890: Current learning rate: 0.00189\n",
      "2025-05-15 08:07:43.446944: train_loss -0.9717\n",
      "2025-05-15 08:07:43.447067: val_loss -0.9578\n",
      "2025-05-15 08:07:43.447099: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 08:07:43.447131: Epoch time: 109.02 s\n",
      "2025-05-15 08:07:44.015619: \n",
      "2025-05-15 08:07:44.015943: Epoch 844\n",
      "2025-05-15 08:07:44.016155: Current learning rate: 0.00188\n",
      "2025-05-15 08:09:32.961478: train_loss -0.971\n",
      "2025-05-15 08:09:32.961681: val_loss -0.9599\n",
      "2025-05-15 08:09:32.961735: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 08:09:32.961772: Epoch time: 108.95 s\n",
      "2025-05-15 08:09:33.521148: \n",
      "2025-05-15 08:09:33.521333: Epoch 845\n",
      "2025-05-15 08:09:33.521419: Current learning rate: 0.00187\n",
      "2025-05-15 08:11:22.303329: train_loss -0.9706\n",
      "2025-05-15 08:11:22.303517: val_loss -0.9548\n",
      "2025-05-15 08:11:22.303549: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 08:11:22.303589: Epoch time: 108.78 s\n",
      "2025-05-15 08:11:22.857162: \n",
      "2025-05-15 08:11:22.857316: Epoch 846\n",
      "2025-05-15 08:11:22.857389: Current learning rate: 0.00186\n",
      "2025-05-15 08:13:11.838691: train_loss -0.9708\n",
      "2025-05-15 08:13:11.838875: val_loss -0.9578\n",
      "2025-05-15 08:13:11.839055: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 08:13:11.839114: Epoch time: 108.98 s\n",
      "2025-05-15 08:13:12.398341: \n",
      "2025-05-15 08:13:12.398654: Epoch 847\n",
      "2025-05-15 08:13:12.398743: Current learning rate: 0.00185\n",
      "2025-05-15 08:15:01.118573: train_loss -0.9707\n",
      "2025-05-15 08:15:01.118726: val_loss -0.9535\n",
      "2025-05-15 08:15:01.118760: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 08:15:01.118794: Epoch time: 108.72 s\n",
      "2025-05-15 08:15:01.673626: \n",
      "2025-05-15 08:15:01.673906: Epoch 848\n",
      "Current learning rate: 0.00184\n",
      "2025-05-15 08:16:50.706133: train_loss -0.9707\n",
      "2025-05-15 08:16:50.706280: val_loss -0.9576\n",
      "2025-05-15 08:16:50.706335: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 08:16:50.706370: Epoch time: 109.03 s\n",
      "2025-05-15 08:16:51.268959: \n",
      "2025-05-15 08:16:51.269159: Epoch 849\n",
      "2025-05-15 08:16:51.269252: Current learning rate: 0.00182\n",
      "2025-05-15 08:18:40.289855: train_loss -0.9683\n",
      "2025-05-15 08:18:40.289975: val_loss -0.9582\n",
      "2025-05-15 08:18:40.290010: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 08:18:40.290045: Epoch time: 109.02 s\n",
      "2025-05-15 08:18:41.057506: \n",
      "2025-05-15 08:18:41.057872: Epoch 850\n",
      "2025-05-15 08:18:41.057960: Current learning rate: 0.00181\n",
      "2025-05-15 08:20:29.859182: train_loss -0.9703\n",
      "2025-05-15 08:20:29.859315: val_loss -0.9584\n",
      "2025-05-15 08:20:29.859349: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 08:20:29.859381: Epoch time: 108.8 s\n",
      "2025-05-15 08:20:30.419277: \n",
      "2025-05-15 08:20:30.419537: Epoch 851\n",
      "2025-05-15 08:20:30.419621: Current learning rate: 0.0018\n",
      "2025-05-15 08:22:19.305535: train_loss -0.97\n",
      "2025-05-15 08:22:19.305661: val_loss -0.951\n",
      "2025-05-15 08:22:19.305692: Pseudo dice [np.float32(0.9803)]\n",
      "2025-05-15 08:22:19.305737: Epoch time: 108.89 s\n",
      "2025-05-15 08:22:19.859870: \n",
      "2025-05-15 08:22:19.860025: Epoch 852\n",
      "2025-05-15 08:22:19.860096: Current learning rate: 0.00179\n",
      "2025-05-15 08:24:08.674186: train_loss -0.9707\n",
      "2025-05-15 08:24:08.674393: val_loss -0.956\n",
      "2025-05-15 08:24:08.674463: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 08:24:08.674502: Epoch time: 108.81 s\n",
      "2025-05-15 08:24:09.234281: \n",
      "2025-05-15 08:24:09.234534: Epoch 853\n",
      "2025-05-15 08:24:09.234671: Current learning rate: 0.00178\n",
      "2025-05-15 08:25:58.048045: train_loss -0.9713\n",
      "2025-05-15 08:25:58.048170: val_loss -0.9576\n",
      "2025-05-15 08:25:58.048199: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 08:25:58.048231: Epoch time: 108.81 s\n",
      "2025-05-15 08:25:58.932187: \n",
      "2025-05-15 08:25:58.932343: Epoch 854\n",
      "2025-05-15 08:25:58.932565: Current learning rate: 0.00177\n",
      "2025-05-15 08:27:47.785248: train_loss -0.9707\n",
      "2025-05-15 08:27:47.785423: val_loss -0.9533\n",
      "2025-05-15 08:27:47.785457: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 08:27:47.785490: Epoch time: 108.85 s\n",
      "2025-05-15 08:27:48.333266: \n",
      "2025-05-15 08:27:48.333369: Epoch 855\n",
      "2025-05-15 08:27:48.333438: Current learning rate: 0.00176\n",
      "2025-05-15 08:29:37.195431: train_loss -0.9716\n",
      "2025-05-15 08:29:37.195702: val_loss -0.9537\n",
      "2025-05-15 08:29:37.195864: Pseudo dice [np.float32(0.9798)]\n",
      "2025-05-15 08:29:37.195908: Epoch time: 108.86 s\n",
      "2025-05-15 08:29:37.752053: \n",
      "2025-05-15 08:29:37.752378: Epoch 856\n",
      "2025-05-15 08:29:37.752501: Current learning rate: 0.00175\n",
      "2025-05-15 08:31:26.730395: train_loss -0.9703\n",
      "2025-05-15 08:31:26.730539: val_loss -0.9552\n",
      "2025-05-15 08:31:26.730576: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 08:31:26.730608: Epoch time: 108.98 s\n",
      "2025-05-15 08:31:27.286227: \n",
      "2025-05-15 08:31:27.286690: Epoch 857\n",
      "2025-05-15 08:31:27.286792: Current learning rate: 0.00174\n",
      "2025-05-15 08:33:16.270035: train_loss -0.9708\n",
      "2025-05-15 08:33:16.270173: val_loss -0.9585\n",
      "2025-05-15 08:33:16.270206: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 08:33:16.270239: Epoch time: 108.98 s\n",
      "2025-05-15 08:33:16.821569: \n",
      "2025-05-15 08:33:16.821779: Epoch 858\n",
      "2025-05-15 08:33:16.821859: Current learning rate: 0.00173\n",
      "2025-05-15 08:35:05.757579: train_loss -0.9709\n",
      "2025-05-15 08:35:05.757696: val_loss -0.9572\n",
      "2025-05-15 08:35:05.757727: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 08:35:05.757761: Epoch time: 108.94 s\n",
      "2025-05-15 08:35:06.312828: \n",
      "2025-05-15 08:35:06.313414: Epoch 859\n",
      "2025-05-15 08:35:06.313587: Current learning rate: 0.00172\n",
      "2025-05-15 08:36:55.150447: train_loss -0.9717\n",
      "2025-05-15 08:36:55.150569: val_loss -0.959\n",
      "2025-05-15 08:36:55.150601: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 08:36:55.150650: Epoch time: 108.84 s\n",
      "2025-05-15 08:36:55.706823: \n",
      "2025-05-15 08:36:55.706952: Epoch 860\n",
      "2025-05-15 08:36:55.707043: Current learning rate: 0.0017\n",
      "2025-05-15 08:38:44.696944: train_loss -0.9711\n",
      "2025-05-15 08:38:44.697049: val_loss -0.9564\n",
      "2025-05-15 08:38:44.697074: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 08:38:44.697105: Epoch time: 108.99 s\n",
      "2025-05-15 08:38:45.245091: \n",
      "2025-05-15 08:38:45.245529: Epoch 861\n",
      "2025-05-15 08:38:45.245729: Current learning rate: 0.00169\n",
      "2025-05-15 08:40:34.263718: train_loss -0.9717\n",
      "2025-05-15 08:40:34.263896: val_loss -0.9566\n",
      "2025-05-15 08:40:34.264011: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 08:40:34.264118: Epoch time: 109.02 s\n",
      "2025-05-15 08:40:34.819375: \n",
      "2025-05-15 08:40:34.819800: Epoch 862\n",
      "2025-05-15 08:40:34.819926: Current learning rate: 0.00168\n",
      "2025-05-15 08:42:23.876177: train_loss -0.9711\n",
      "2025-05-15 08:42:23.876374: val_loss -0.9595\n",
      "2025-05-15 08:42:23.876409: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 08:42:23.876443: Epoch time: 109.06 s\n",
      "2025-05-15 08:42:24.425719: \n",
      "2025-05-15 08:42:24.425905: Epoch 863\n",
      "2025-05-15 08:42:24.425983: Current learning rate: 0.00167\n",
      "2025-05-15 08:44:13.312130: train_loss -0.9717\n",
      "2025-05-15 08:44:13.312284: val_loss -0.9586\n",
      "2025-05-15 08:44:13.312321: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 08:44:13.312352: Epoch time: 108.89 s\n",
      "2025-05-15 08:44:13.867294: \n",
      "2025-05-15 08:44:13.867747: Epoch 864\n",
      "2025-05-15 08:44:13.867914: Current learning rate: 0.00166\n",
      "2025-05-15 08:46:02.887599: train_loss -0.9707\n",
      "2025-05-15 08:46:02.887820: val_loss -0.96\n",
      "2025-05-15 08:46:02.887891: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 08:46:02.887940: Epoch time: 109.02 s\n",
      "2025-05-15 08:46:03.433878: \n",
      "2025-05-15 08:46:03.434315: Epoch 865\n",
      "2025-05-15 08:46:03.434440: Current learning rate: 0.00165\n",
      "2025-05-15 08:47:52.269412: train_loss -0.9712\n",
      "2025-05-15 08:47:52.269528: val_loss -0.9578\n",
      "2025-05-15 08:47:52.269561: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 08:47:52.269628: Epoch time: 108.84 s\n",
      "2025-05-15 08:47:52.824279: \n",
      "2025-05-15 08:47:52.824740: Epoch 866\n",
      "2025-05-15 08:47:52.824830: Current learning rate: 0.00164\n",
      "2025-05-15 08:49:41.783732: train_loss -0.9712\n",
      "2025-05-15 08:49:41.783856: val_loss -0.956\n",
      "2025-05-15 08:49:41.783888: Pseudo dice [np.float32(0.9809)]\n",
      "2025-05-15 08:49:41.783920: Epoch time: 108.96 s\n",
      "2025-05-15 08:49:42.340611: \n",
      "2025-05-15 08:49:42.340867: Epoch 867\n",
      "2025-05-15 08:49:42.340942: Current learning rate: 0.00163\n",
      "2025-05-15 08:51:31.315475: train_loss -0.9711\n",
      "2025-05-15 08:51:31.315618: val_loss -0.9591\n",
      "2025-05-15 08:51:31.315724: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 08:51:31.315767: Epoch time: 108.98 s\n",
      "2025-05-15 08:51:31.869383: \n",
      "2025-05-15 08:51:31.869627: Epoch 868\n",
      "2025-05-15 08:51:31.869728: Current learning rate: 0.00162\n",
      "2025-05-15 08:53:20.800677: train_loss -0.9728\n",
      "2025-05-15 08:53:20.800797: val_loss -0.96\n",
      "2025-05-15 08:53:20.800830: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-15 08:53:20.800863: Epoch time: 108.93 s\n",
      "2025-05-15 08:53:21.356404: \n",
      "2025-05-15 08:53:21.356719: Epoch 869\n",
      "2025-05-15 08:53:21.356853: Current learning rate: 0.00161\n",
      "2025-05-15 08:55:10.420481: train_loss -0.9717\n",
      "2025-05-15 08:55:10.420657: val_loss -0.9579\n",
      "2025-05-15 08:55:10.420690: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 08:55:10.420724: Epoch time: 109.06 s\n",
      "2025-05-15 08:55:10.974572: \n",
      "2025-05-15 08:55:10.975050: Epoch 870\n",
      "2025-05-15 08:55:10.975150: Current learning rate: 0.00159\n",
      "2025-05-15 08:56:59.792932: train_loss -0.9716\n",
      "2025-05-15 08:56:59.793051: val_loss -0.9556\n",
      "2025-05-15 08:56:59.793085: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-15 08:56:59.793118: Epoch time: 108.82 s\n",
      "2025-05-15 08:57:00.339489: \n",
      "2025-05-15 08:57:00.339672: Epoch 871\n",
      "2025-05-15 08:57:00.339751: Current learning rate: 0.00158\n",
      "2025-05-15 08:58:49.391920: train_loss -0.971\n",
      "2025-05-15 08:58:49.392087: val_loss -0.9537\n",
      "2025-05-15 08:58:49.392120: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 08:58:49.392153: Epoch time: 109.05 s\n",
      "2025-05-15 08:58:49.944216: \n",
      "2025-05-15 08:58:49.944297: Epoch 872\n",
      "2025-05-15 08:58:49.944363: Current learning rate: 0.00157\n",
      "2025-05-15 09:00:38.888258: train_loss -0.9716\n",
      "2025-05-15 09:00:38.888415: val_loss -0.9598\n",
      "2025-05-15 09:00:38.888447: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-15 09:00:38.888479: Epoch time: 108.94 s\n",
      "2025-05-15 09:00:39.438953: \n",
      "2025-05-15 09:00:39.439131: Epoch 873\n",
      "Current learning rate: 0.00156\n",
      "2025-05-15 09:02:28.241453: train_loss -0.9721\n",
      "2025-05-15 09:02:28.241586: val_loss -0.9597\n",
      "2025-05-15 09:02:28.241620: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 09:02:28.241653: Epoch time: 108.8 s\n",
      "2025-05-15 09:02:28.241679: Yayy! New best EMA pseudo Dice: 0.9828000068664551\n",
      "2025-05-15 09:02:29.340921: \n",
      "2025-05-15 09:02:29.341039: Epoch 874\n",
      "2025-05-15 09:02:29.341106: Current learning rate: 0.00155\n",
      "2025-05-15 09:04:18.354757: train_loss -0.9719\n",
      "2025-05-15 09:04:18.354878: val_loss -0.9572\n",
      "2025-05-15 09:04:18.354908: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 09:04:18.354982: Epoch time: 109.01 s\n",
      "2025-05-15 09:04:18.914265: \n",
      "2025-05-15 09:04:18.914418: Epoch 875\n",
      "2025-05-15 09:04:18.914521: Current learning rate: 0.00154\n",
      "2025-05-15 09:06:07.932260: train_loss -0.9718\n",
      "2025-05-15 09:06:07.932389: val_loss -0.9509\n",
      "2025-05-15 09:06:07.932435: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-15 09:06:07.932473: Epoch time: 109.02 s\n",
      "2025-05-15 09:06:08.484136: \n",
      "2025-05-15 09:06:08.484325: Epoch 876\n",
      "2025-05-15 09:06:08.484402: Current learning rate: 0.00153\n",
      "2025-05-15 09:07:57.462451: train_loss -0.9719\n",
      "2025-05-15 09:07:57.462577: val_loss -0.9601\n",
      "2025-05-15 09:07:57.462616: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 09:07:57.462656: Epoch time: 108.98 s\n",
      "2025-05-15 09:07:58.012803: \n",
      "2025-05-15 09:07:58.013212: Epoch 877\n",
      "2025-05-15 09:07:58.013324: Current learning rate: 0.00152\n",
      "2025-05-15 09:09:46.987729: train_loss -0.9728\n",
      "2025-05-15 09:09:46.987856: val_loss -0.9558\n",
      "2025-05-15 09:09:46.987891: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 09:09:46.987926: Epoch time: 108.98 s\n",
      "2025-05-15 09:09:47.547865: \n",
      "2025-05-15 09:09:47.548183: Epoch 878\n",
      "2025-05-15 09:09:47.548370: Current learning rate: 0.00151\n",
      "2025-05-15 09:11:36.547738: train_loss -0.973\n",
      "2025-05-15 09:11:36.547862: val_loss -0.9579\n",
      "2025-05-15 09:11:36.547897: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 09:11:36.547955: Epoch time: 109.0 s\n",
      "2025-05-15 09:11:37.100280: \n",
      "2025-05-15 09:11:37.100636: Epoch 879\n",
      "2025-05-15 09:11:37.100804: Current learning rate: 0.00149\n",
      "2025-05-15 09:13:26.190225: train_loss -0.9725\n",
      "2025-05-15 09:13:26.190409: val_loss -0.9558\n",
      "2025-05-15 09:13:26.190442: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 09:13:26.190475: Epoch time: 109.09 s\n",
      "2025-05-15 09:13:26.747179: \n",
      "2025-05-15 09:13:26.747290: Epoch 880\n",
      "2025-05-15 09:13:26.747362: Current learning rate: 0.00148\n",
      "2025-05-15 09:15:15.692590: train_loss -0.9716\n",
      "2025-05-15 09:15:15.692830: val_loss -0.9521\n",
      "2025-05-15 09:15:15.692969: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 09:15:15.693065: Epoch time: 108.95 s\n",
      "2025-05-15 09:15:16.247080: \n",
      "2025-05-15 09:15:16.247409: Epoch 881\n",
      "2025-05-15 09:15:16.247601: Current learning rate: 0.00147\n",
      "2025-05-15 09:17:05.114529: train_loss -0.9709\n",
      "2025-05-15 09:17:05.114718: val_loss -0.9574\n",
      "2025-05-15 09:17:05.114786: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 09:17:05.114826: Epoch time: 108.87 s\n",
      "2025-05-15 09:17:05.668734: \n",
      "2025-05-15 09:17:05.669030: Epoch 882\n",
      "2025-05-15 09:17:05.669190: Current learning rate: 0.00146\n",
      "2025-05-15 09:18:54.525183: train_loss -0.9728\n",
      "2025-05-15 09:18:54.525311: val_loss -0.9594\n",
      "2025-05-15 09:18:54.525345: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 09:18:54.525378: Epoch time: 108.86 s\n",
      "2025-05-15 09:18:55.075749: \n",
      "2025-05-15 09:18:55.075997: Epoch 883\n",
      "2025-05-15 09:18:55.076082: Current learning rate: 0.00145\n",
      "2025-05-15 09:20:43.906342: train_loss -0.973\n",
      "2025-05-15 09:20:43.906469: val_loss -0.9623\n",
      "2025-05-15 09:20:43.906503: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-15 09:20:43.906548: Epoch time: 108.83 s\n",
      "2025-05-15 09:20:43.906647: Yayy! New best EMA pseudo Dice: 0.9829000234603882\n",
      "2025-05-15 09:20:44.679969: \n",
      "2025-05-15 09:20:44.680177: Epoch 884\n",
      "2025-05-15 09:20:44.680262: Current learning rate: 0.00144\n",
      "2025-05-15 09:22:33.496162: train_loss -0.9721\n",
      "2025-05-15 09:22:33.496435: val_loss -0.957\n",
      "2025-05-15 09:22:33.496580: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 09:22:33.496645: Epoch time: 108.82 s\n",
      "2025-05-15 09:22:33.496680: Yayy! New best EMA pseudo Dice: 0.9829000234603882\n",
      "2025-05-15 09:22:34.254423: \n",
      "2025-05-15 09:22:34.254660: Epoch 885\n",
      "2025-05-15 09:22:34.254776: Current learning rate: 0.00143\n",
      "2025-05-15 09:24:23.225580: train_loss -0.9722\n",
      "2025-05-15 09:24:23.225778: val_loss -0.9527\n",
      "2025-05-15 09:24:23.225817: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-15 09:24:23.225851: Epoch time: 108.97 s\n",
      "2025-05-15 09:24:23.780134: \n",
      "2025-05-15 09:24:23.780454: Epoch 886\n",
      "2025-05-15 09:24:23.780545: Current learning rate: 0.00142\n",
      "2025-05-15 09:26:12.845112: train_loss -0.9721\n",
      "2025-05-15 09:26:12.845275: val_loss -0.957\n",
      "2025-05-15 09:26:12.845315: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 09:26:12.845378: Epoch time: 109.07 s\n",
      "2025-05-15 09:26:13.400926: \n",
      "2025-05-15 09:26:13.401119: Epoch 887\n",
      "2025-05-15 09:26:13.401240: Current learning rate: 0.00141\n",
      "2025-05-15 09:28:02.245842: train_loss -0.9711\n",
      "2025-05-15 09:28:02.246018: val_loss -0.9593\n",
      "2025-05-15 09:28:02.246049: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 09:28:02.246080: Epoch time: 108.85 s\n",
      "2025-05-15 09:28:02.805367: \n",
      "2025-05-15 09:28:02.805789: Epoch 888\n",
      "2025-05-15 09:28:02.805890: Current learning rate: 0.00139\n",
      "2025-05-15 09:29:51.598033: train_loss -0.9711\n",
      "2025-05-15 09:29:51.598174: val_loss -0.958\n",
      "2025-05-15 09:29:51.598207: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 09:29:51.598240: Epoch time: 108.79 s\n",
      "2025-05-15 09:29:52.157106: \n",
      "2025-05-15 09:29:52.157204: Epoch 889\n",
      "2025-05-15 09:29:52.157350: Current learning rate: 0.00138\n",
      "2025-05-15 09:31:41.074526: train_loss -0.9732\n",
      "2025-05-15 09:31:41.074640: val_loss -0.9552\n",
      "2025-05-15 09:31:41.074750: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 09:31:41.074806: Epoch time: 108.92 s\n",
      "2025-05-15 09:31:41.635346: \n",
      "2025-05-15 09:31:41.635475: Epoch 890\n",
      "2025-05-15 09:31:41.635565: Current learning rate: 0.00137\n",
      "2025-05-15 09:33:30.385068: train_loss -0.9734\n",
      "2025-05-15 09:33:30.385247: val_loss -0.9569\n",
      "2025-05-15 09:33:30.385385: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 09:33:30.385451: Epoch time: 108.75 s\n",
      "2025-05-15 09:33:30.939094: \n",
      "2025-05-15 09:33:30.939265: Epoch 891\n",
      "2025-05-15 09:33:30.939346: Current learning rate: 0.00136\n",
      "2025-05-15 09:35:19.756170: train_loss -0.9736\n",
      "2025-05-15 09:35:19.756292: val_loss -0.9532\n",
      "2025-05-15 09:35:19.756326: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-15 09:35:19.756358: Epoch time: 108.82 s\n",
      "2025-05-15 09:35:20.640401: \n",
      "2025-05-15 09:35:20.640684: Epoch 892\n",
      "2025-05-15 09:35:20.640815: Current learning rate: 0.00135\n",
      "2025-05-15 09:37:09.486710: train_loss -0.9726\n",
      "2025-05-15 09:37:09.486838: val_loss -0.9593\n",
      "2025-05-15 09:37:09.486870: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 09:37:09.486904: Epoch time: 108.85 s\n",
      "2025-05-15 09:37:10.040796: \n",
      "2025-05-15 09:37:10.041043: Epoch 893\n",
      "2025-05-15 09:37:10.041122: Current learning rate: 0.00134\n",
      "2025-05-15 09:38:58.874743: train_loss -0.9719\n",
      "2025-05-15 09:38:58.874867: val_loss -0.9592\n",
      "2025-05-15 09:38:58.874900: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 09:38:58.874932: Epoch time: 108.83 s\n",
      "2025-05-15 09:38:59.437699: \n",
      "2025-05-15 09:38:59.437947: Epoch 894\n",
      "2025-05-15 09:38:59.438071: Current learning rate: 0.00133\n",
      "2025-05-15 09:40:48.356012: train_loss -0.9724\n",
      "2025-05-15 09:40:48.356130: val_loss -0.9596\n",
      "2025-05-15 09:40:48.356162: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-15 09:40:48.356196: Epoch time: 108.92 s\n",
      "2025-05-15 09:40:48.911198: \n",
      "2025-05-15 09:40:48.911514: Epoch 895\n",
      "2025-05-15 09:40:48.911657: Current learning rate: 0.00132\n",
      "2025-05-15 09:42:37.747519: train_loss -0.9713\n",
      "2025-05-15 09:42:37.747641: val_loss -0.9587\n",
      "2025-05-15 09:42:37.747673: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 09:42:37.747706: Epoch time: 108.84 s\n",
      "2025-05-15 09:42:38.309681: \n",
      "2025-05-15 09:42:38.309952: Epoch 896\n",
      "2025-05-15 09:42:38.310031: Current learning rate: 0.0013\n",
      "2025-05-15 09:44:27.294931: train_loss -0.9724\n",
      "2025-05-15 09:44:27.295055: val_loss -0.9578\n",
      "2025-05-15 09:44:27.295088: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 09:44:27.295153: Epoch time: 108.99 s\n",
      "2025-05-15 09:44:27.850163: \n",
      "2025-05-15 09:44:27.850322: Epoch 897\n",
      "2025-05-15 09:44:27.850400: Current learning rate: 0.00129\n",
      "2025-05-15 09:46:16.599863: train_loss -0.9732\n",
      "2025-05-15 09:46:16.600003: val_loss -0.9541\n",
      "2025-05-15 09:46:16.600098: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 09:46:16.600231: Epoch time: 108.75 s\n",
      "2025-05-15 09:46:17.160151: \n",
      "2025-05-15 09:46:17.160464: Epoch 898\n",
      "2025-05-15 09:46:17.160580: Current learning rate: 0.00128\n",
      "2025-05-15 09:48:06.001481: train_loss -0.9731\n",
      "2025-05-15 09:48:06.001607: val_loss -0.9539\n",
      "2025-05-15 09:48:06.001639: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 09:48:06.001672: Epoch time: 108.84 s\n",
      "2025-05-15 09:48:06.554749: \n",
      "2025-05-15 09:48:06.554898: Epoch 899\n",
      "2025-05-15 09:48:06.554970: Current learning rate: 0.00127\n",
      "2025-05-15 09:49:55.426200: train_loss -0.973\n",
      "2025-05-15 09:49:55.426317: val_loss -0.9589\n",
      "2025-05-15 09:49:55.426378: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 09:49:55.426464: Epoch time: 108.87 s\n",
      "2025-05-15 09:49:56.206482: \n",
      "2025-05-15 09:49:56.206649: Epoch 900\n",
      "2025-05-15 09:49:56.206734: Current learning rate: 0.00126\n",
      "2025-05-15 09:51:44.999638: train_loss -0.9717\n",
      "2025-05-15 09:51:44.999764: val_loss -0.9557\n",
      "2025-05-15 09:51:44.999799: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 09:51:44.999834: Epoch time: 108.79 s\n",
      "2025-05-15 09:51:45.550385: \n",
      "2025-05-15 09:51:45.550724: Epoch 901\n",
      "2025-05-15 09:51:45.550914: Current learning rate: 0.00125\n",
      "2025-05-15 09:53:34.712089: train_loss -0.9706\n",
      "2025-05-15 09:53:34.712241: val_loss -0.9589\n",
      "2025-05-15 09:53:34.712275: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 09:53:34.712311: Epoch time: 109.16 s\n",
      "2025-05-15 09:53:35.295138: \n",
      "2025-05-15 09:53:35.295474: Epoch 902\n",
      "2025-05-15 09:53:35.295652: Current learning rate: 0.00124\n",
      "2025-05-15 09:55:24.739228: train_loss -0.9725\n",
      "2025-05-15 09:55:24.739368: val_loss -0.9579\n",
      "2025-05-15 09:55:24.739401: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 09:55:24.739434: Epoch time: 109.44 s\n",
      "2025-05-15 09:55:25.300139: \n",
      "2025-05-15 09:55:25.300421: Epoch 903\n",
      "2025-05-15 09:55:25.300502: Current learning rate: 0.00122\n",
      "2025-05-15 09:57:14.400711: train_loss -0.9726\n",
      "2025-05-15 09:57:14.400887: val_loss -0.9541\n",
      "2025-05-15 09:57:14.400924: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 09:57:14.400959: Epoch time: 109.1 s\n",
      "2025-05-15 09:57:14.968945: \n",
      "2025-05-15 09:57:14.969239: Epoch 904\n",
      "2025-05-15 09:57:14.969539: Current learning rate: 0.00121\n",
      "2025-05-15 09:59:04.180467: train_loss -0.9723\n",
      "2025-05-15 09:59:04.180674: val_loss -0.9574\n",
      "2025-05-15 09:59:04.180709: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 09:59:04.180817: Epoch time: 109.21 s\n",
      "2025-05-15 09:59:04.754250: \n",
      "2025-05-15 09:59:04.754660: Epoch 905\n",
      "2025-05-15 09:59:04.754892: Current learning rate: 0.0012\n",
      "2025-05-15 10:00:53.945775: train_loss -0.9728\n",
      "2025-05-15 10:00:53.946035: val_loss -0.9566\n",
      "2025-05-15 10:00:53.946082: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 10:00:53.946118: Epoch time: 109.19 s\n",
      "2025-05-15 10:00:54.504675: \n",
      "2025-05-15 10:00:54.505044: Epoch 906\n",
      "2025-05-15 10:00:54.505233: Current learning rate: 0.00119\n",
      "2025-05-15 10:02:43.631652: train_loss -0.9729\n",
      "2025-05-15 10:02:43.631829: val_loss -0.9576\n",
      "2025-05-15 10:02:43.631866: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 10:02:43.631899: Epoch time: 109.13 s\n",
      "2025-05-15 10:02:44.191539: \n",
      "2025-05-15 10:02:44.191806: Epoch 907\n",
      "2025-05-15 10:02:44.191936: Current learning rate: 0.00118\n",
      "2025-05-15 10:04:33.400895: train_loss -0.9728\n",
      "2025-05-15 10:04:33.401064: val_loss -0.9567\n",
      "2025-05-15 10:04:33.401185: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 10:04:33.401371: Epoch time: 109.21 s\n",
      "2025-05-15 10:04:33.956039: \n",
      "2025-05-15 10:04:33.956581: Epoch 908\n",
      "2025-05-15 10:04:33.956783: Current learning rate: 0.00117\n",
      "2025-05-15 10:06:23.122134: train_loss -0.9734\n",
      "2025-05-15 10:06:23.122266: val_loss -0.9532\n",
      "2025-05-15 10:06:23.122300: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 10:06:23.122333: Epoch time: 109.17 s\n",
      "2025-05-15 10:06:23.676002: \n",
      "2025-05-15 10:06:23.676105: Epoch 909\n",
      "2025-05-15 10:06:23.676174: Current learning rate: 0.00116\n",
      "2025-05-15 10:08:12.830496: train_loss -0.973\n",
      "2025-05-15 10:08:12.830615: val_loss -0.9557\n",
      "2025-05-15 10:08:12.830646: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 10:08:12.830678: Epoch time: 109.16 s\n",
      "2025-05-15 10:08:13.750068: \n",
      "2025-05-15 10:08:13.750387: Epoch 910\n",
      "2025-05-15 10:08:13.750489: Current learning rate: 0.00115\n",
      "2025-05-15 10:10:02.970439: train_loss -0.9716\n",
      "2025-05-15 10:10:02.970562: val_loss -0.9509\n",
      "2025-05-15 10:10:02.970595: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 10:10:02.970626: Epoch time: 109.22 s\n",
      "2025-05-15 10:10:03.531841: \n",
      "2025-05-15 10:10:03.532026: Epoch 911\n",
      "2025-05-15 10:10:03.532114: Current learning rate: 0.00113\n",
      "2025-05-15 10:11:52.899495: train_loss -0.973\n",
      "2025-05-15 10:11:52.899615: val_loss -0.9581\n",
      "2025-05-15 10:11:52.899658: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-15 10:11:52.899695: Epoch time: 109.37 s\n",
      "2025-05-15 10:11:53.457165: \n",
      "2025-05-15 10:11:53.457531: Epoch 912\n",
      "2025-05-15 10:11:53.457635: Current learning rate: 0.00112\n",
      "2025-05-15 10:13:42.673606: train_loss -0.9725\n",
      "2025-05-15 10:13:42.673738: val_loss -0.9543\n",
      "2025-05-15 10:13:42.673771: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-15 10:13:42.673805: Epoch time: 109.22 s\n",
      "2025-05-15 10:13:43.235247: \n",
      "2025-05-15 10:13:43.235351: Epoch 913\n",
      "2025-05-15 10:13:43.235417: Current learning rate: 0.00111\n",
      "2025-05-15 10:15:32.391811: train_loss -0.9718\n",
      "2025-05-15 10:15:32.391997: val_loss -0.9609\n",
      "2025-05-15 10:15:32.392028: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 10:15:32.392061: Epoch time: 109.16 s\n",
      "2025-05-15 10:15:32.957834: \n",
      "2025-05-15 10:15:32.958006: Epoch 914\n",
      "2025-05-15 10:15:32.958076: Current learning rate: 0.0011\n",
      "2025-05-15 10:17:22.143401: train_loss -0.9729\n",
      "2025-05-15 10:17:22.143525: val_loss -0.9579\n",
      "2025-05-15 10:17:22.143558: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 10:17:22.143724: Epoch time: 109.19 s\n",
      "2025-05-15 10:17:22.701119: \n",
      "2025-05-15 10:17:22.701314: Epoch 915\n",
      "2025-05-15 10:17:22.701386: Current learning rate: 0.00109\n",
      "2025-05-15 10:19:12.018468: train_loss -0.9714\n",
      "2025-05-15 10:19:12.018681: val_loss -0.9566\n",
      "2025-05-15 10:19:12.018715: Pseudo dice [np.float32(0.9813)]\n",
      "2025-05-15 10:19:12.018748: Epoch time: 109.32 s\n",
      "2025-05-15 10:19:12.575397: \n",
      "2025-05-15 10:19:12.575496: Epoch 916\n",
      "2025-05-15 10:19:12.575558: Current learning rate: 0.00108\n",
      "2025-05-15 10:21:01.860298: train_loss -0.972\n",
      "2025-05-15 10:21:01.860481: val_loss -0.9555\n",
      "2025-05-15 10:21:01.860661: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 10:21:01.860710: Epoch time: 109.29 s\n",
      "2025-05-15 10:21:02.419793: \n",
      "2025-05-15 10:21:02.420087: Epoch 917\n",
      "2025-05-15 10:21:02.420169: Current learning rate: 0.00106\n",
      "2025-05-15 10:22:51.565924: train_loss -0.9718\n",
      "2025-05-15 10:22:51.566110: val_loss -0.9534\n",
      "2025-05-15 10:22:51.566144: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-15 10:22:51.566178: Epoch time: 109.15 s\n",
      "2025-05-15 10:22:52.116833: \n",
      "2025-05-15 10:22:52.116968: Epoch 918\n",
      "2025-05-15 10:22:52.117037: Current learning rate: 0.00105\n",
      "2025-05-15 10:24:41.398225: train_loss -0.9728\n",
      "2025-05-15 10:24:41.398359: val_loss -0.9636\n",
      "2025-05-15 10:24:41.398408: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-15 10:24:41.398446: Epoch time: 109.28 s\n",
      "2025-05-15 10:24:41.957673: \n",
      "2025-05-15 10:24:41.958154: Epoch 919\n",
      "2025-05-15 10:24:41.958236: Current learning rate: 0.00104\n",
      "2025-05-15 10:26:31.073850: train_loss -0.9728\n",
      "2025-05-15 10:26:31.074035: val_loss -0.9544\n",
      "2025-05-15 10:26:31.074066: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 10:26:31.074099: Epoch time: 109.12 s\n",
      "2025-05-15 10:26:31.628126: \n",
      "2025-05-15 10:26:31.628545: Epoch 920\n",
      "2025-05-15 10:26:31.628714: Current learning rate: 0.00103\n",
      "2025-05-15 10:28:20.774058: train_loss -0.9716\n",
      "2025-05-15 10:28:20.774195: val_loss -0.9569\n",
      "2025-05-15 10:28:20.774227: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 10:28:20.774260: Epoch time: 109.15 s\n",
      "2025-05-15 10:28:21.327280: \n",
      "2025-05-15 10:28:21.327719: Epoch 921\n",
      "2025-05-15 10:28:21.327877: Current learning rate: 0.00102\n",
      "2025-05-15 10:30:10.614013: train_loss -0.9731\n",
      "2025-05-15 10:30:10.614228: val_loss -0.9518\n",
      "2025-05-15 10:30:10.614295: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-15 10:30:10.614336: Epoch time: 109.29 s\n",
      "2025-05-15 10:30:11.170005: \n",
      "2025-05-15 10:30:11.170105: Epoch 922\n",
      "2025-05-15 10:30:11.170178: Current learning rate: 0.00101\n",
      "2025-05-15 10:31:59.977686: train_loss -0.9737\n",
      "2025-05-15 10:31:59.977846: val_loss -0.9571\n",
      "2025-05-15 10:31:59.978103: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 10:31:59.978432: Epoch time: 108.81 s\n",
      "2025-05-15 10:32:00.538913: \n",
      "2025-05-15 10:32:00.539028: Epoch 923\n",
      "2025-05-15 10:32:00.539112: Current learning rate: 0.001\n",
      "2025-05-15 10:33:49.456637: train_loss -0.9739\n",
      "2025-05-15 10:33:49.456900: val_loss -0.9517\n",
      "2025-05-15 10:33:49.456975: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 10:33:49.457019: Epoch time: 108.92 s\n",
      "2025-05-15 10:33:50.013287: \n",
      "2025-05-15 10:33:50.013470: Epoch 924\n",
      "2025-05-15 10:33:50.013556: Current learning rate: 0.00098\n",
      "2025-05-15 10:35:38.934612: train_loss -0.9725\n",
      "2025-05-15 10:35:38.934746: val_loss -0.9567\n",
      "2025-05-15 10:35:38.934782: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 10:35:38.934817: Epoch time: 108.92 s\n",
      "2025-05-15 10:35:39.489633: \n",
      "2025-05-15 10:35:39.489730: Epoch 925\n",
      "2025-05-15 10:35:39.489796: Current learning rate: 0.00097\n",
      "2025-05-15 10:37:28.436470: train_loss -0.9727\n",
      "2025-05-15 10:37:28.436613: val_loss -0.9582\n",
      "2025-05-15 10:37:28.436646: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 10:37:28.436683: Epoch time: 108.95 s\n",
      "2025-05-15 10:37:28.993443: \n",
      "2025-05-15 10:37:28.993533: Epoch 926\n",
      "2025-05-15 10:37:28.993601: Current learning rate: 0.00096\n",
      "2025-05-15 10:39:17.870568: train_loss -0.9726\n",
      "2025-05-15 10:39:17.870698: val_loss -0.9566\n",
      "2025-05-15 10:39:17.870729: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 10:39:17.870760: Epoch time: 108.88 s\n",
      "2025-05-15 10:39:18.420151: \n",
      "2025-05-15 10:39:18.420238: Epoch 927\n",
      "2025-05-15 10:39:18.420304: Current learning rate: 0.00095\n",
      "2025-05-15 10:41:07.210998: train_loss -0.9743\n",
      "2025-05-15 10:41:07.211119: val_loss -0.954\n",
      "2025-05-15 10:41:07.211151: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 10:41:07.211183: Epoch time: 108.79 s\n",
      "2025-05-15 10:41:07.765496: \n",
      "2025-05-15 10:41:07.765688: Epoch 928\n",
      "2025-05-15 10:41:07.765766: Current learning rate: 0.00094\n",
      "2025-05-15 10:42:56.711281: train_loss -0.9722\n",
      "2025-05-15 10:42:56.711564: val_loss -0.9591\n",
      "2025-05-15 10:42:56.711629: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 10:42:56.711673: Epoch time: 108.95 s\n",
      "2025-05-15 10:42:57.599690: \n",
      "2025-05-15 10:42:57.599813: Epoch 929\n",
      "2025-05-15 10:42:57.599919: Current learning rate: 0.00092\n",
      "2025-05-15 10:44:46.501918: train_loss -0.9744\n",
      "2025-05-15 10:44:46.502059: val_loss -0.9585\n",
      "2025-05-15 10:44:46.502093: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-15 10:44:46.502124: Epoch time: 108.9 s\n",
      "2025-05-15 10:44:47.064788: \n",
      "2025-05-15 10:44:47.064898: Epoch 930\n",
      "2025-05-15 10:44:47.064970: Current learning rate: 0.00091\n",
      "2025-05-15 10:46:35.863349: train_loss -0.9723\n",
      "2025-05-15 10:46:35.863554: val_loss -0.9503\n",
      "2025-05-15 10:46:35.863611: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-15 10:46:35.863670: Epoch time: 108.8 s\n",
      "2025-05-15 10:46:36.418067: \n",
      "2025-05-15 10:46:36.418473: Epoch 931\n",
      "2025-05-15 10:46:36.418623: Current learning rate: 0.0009\n",
      "2025-05-15 10:48:25.245643: train_loss -0.9726\n",
      "2025-05-15 10:48:25.245769: val_loss -0.9566\n",
      "2025-05-15 10:48:25.245801: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 10:48:25.245835: Epoch time: 108.83 s\n",
      "2025-05-15 10:48:25.800462: \n",
      "2025-05-15 10:48:25.800909: Epoch 932\n",
      "2025-05-15 10:48:25.801021: Current learning rate: 0.00089\n",
      "2025-05-15 10:50:14.770391: train_loss -0.9725\n",
      "2025-05-15 10:50:14.770528: val_loss -0.9589\n",
      "2025-05-15 10:50:14.770559: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 10:50:14.770591: Epoch time: 108.97 s\n",
      "2025-05-15 10:50:15.330113: \n",
      "2025-05-15 10:50:15.330216: Epoch 933\n",
      "2025-05-15 10:50:15.330284: Current learning rate: 0.00088\n",
      "2025-05-15 10:52:04.111357: train_loss -0.9737\n",
      "2025-05-15 10:52:04.111500: val_loss -0.9591\n",
      "2025-05-15 10:52:04.111533: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-15 10:52:04.111568: Epoch time: 108.78 s\n",
      "2025-05-15 10:52:04.669024: \n",
      "2025-05-15 10:52:04.669412: Epoch 934\n",
      "2025-05-15 10:52:04.669521: Current learning rate: 0.00087\n",
      "2025-05-15 10:53:53.497230: train_loss -0.9723\n",
      "2025-05-15 10:53:53.497476: val_loss -0.9557\n",
      "2025-05-15 10:53:53.497570: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 10:53:53.497787: Epoch time: 108.83 s\n",
      "2025-05-15 10:53:54.053488: \n",
      "2025-05-15 10:53:54.053686: Epoch 935\n",
      "2025-05-15 10:53:54.053785: Current learning rate: 0.00085\n",
      "2025-05-15 10:55:42.898426: train_loss -0.9738\n",
      "2025-05-15 10:55:42.898600: val_loss -0.9552\n",
      "2025-05-15 10:55:42.898636: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 10:55:42.898670: Epoch time: 108.85 s\n",
      "2025-05-15 10:55:43.460776: \n",
      "2025-05-15 10:55:43.460891: Epoch 936\n",
      "2025-05-15 10:55:43.460962: Current learning rate: 0.00084\n",
      "2025-05-15 10:57:32.244026: train_loss -0.973\n",
      "2025-05-15 10:57:32.244154: val_loss -0.9558\n",
      "2025-05-15 10:57:32.244187: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 10:57:32.244221: Epoch time: 108.78 s\n",
      "2025-05-15 10:57:32.807980: \n",
      "2025-05-15 10:57:32.808239: Epoch 937\n",
      "2025-05-15 10:57:32.808365: Current learning rate: 0.00083\n",
      "2025-05-15 10:59:21.626595: train_loss -0.9737\n",
      "2025-05-15 10:59:21.626788: val_loss -0.9577\n",
      "2025-05-15 10:59:21.626824: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 10:59:21.626872: Epoch time: 108.82 s\n",
      "2025-05-15 10:59:22.179127: \n",
      "2025-05-15 10:59:22.179213: Epoch 938\n",
      "2025-05-15 10:59:22.179280: Current learning rate: 0.00082\n",
      "2025-05-15 11:01:11.038935: train_loss -0.9731\n",
      "2025-05-15 11:01:11.039083: val_loss -0.96\n",
      "2025-05-15 11:01:11.039119: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 11:01:11.039150: Epoch time: 108.86 s\n",
      "2025-05-15 11:01:11.590942: \n",
      "2025-05-15 11:01:11.591153: Epoch 939\n",
      "2025-05-15 11:01:11.591283: Current learning rate: 0.00081\n",
      "2025-05-15 11:03:00.361862: train_loss -0.9734\n",
      "2025-05-15 11:03:00.362001: val_loss -0.9567\n",
      "2025-05-15 11:03:00.362039: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 11:03:00.362071: Epoch time: 108.77 s\n",
      "2025-05-15 11:03:00.917995: \n",
      "2025-05-15 11:03:00.918171: Epoch 940\n",
      "2025-05-15 11:03:00.918247: Current learning rate: 0.00079\n",
      "2025-05-15 11:04:49.717771: train_loss -0.9743\n",
      "2025-05-15 11:04:49.717900: val_loss -0.9559\n",
      "2025-05-15 11:04:49.717934: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 11:04:49.717969: Epoch time: 108.8 s\n",
      "2025-05-15 11:04:50.268979: \n",
      "2025-05-15 11:04:50.269215: Epoch 941\n",
      "2025-05-15 11:04:50.269353: Current learning rate: 0.00078\n",
      "2025-05-15 11:06:39.091937: train_loss -0.9731\n",
      "2025-05-15 11:06:39.092219: val_loss -0.9571\n",
      "2025-05-15 11:06:39.092275: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 11:06:39.092316: Epoch time: 108.82 s\n",
      "2025-05-15 11:06:39.638366: \n",
      "2025-05-15 11:06:39.638534: Epoch 942\n",
      "2025-05-15 11:06:39.638628: Current learning rate: 0.00077\n",
      "2025-05-15 11:08:28.420648: train_loss -0.9728\n",
      "2025-05-15 11:08:28.420779: val_loss -0.9595\n",
      "2025-05-15 11:08:28.420818: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 11:08:28.420858: Epoch time: 108.78 s\n",
      "2025-05-15 11:08:28.978410: \n",
      "2025-05-15 11:08:28.978655: Epoch 943\n",
      "2025-05-15 11:08:28.978751: Current learning rate: 0.00076\n",
      "2025-05-15 11:10:17.754377: train_loss -0.973\n",
      "2025-05-15 11:10:17.754548: val_loss -0.9587\n",
      "2025-05-15 11:10:17.754616: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 11:10:17.754772: Epoch time: 108.78 s\n",
      "2025-05-15 11:10:18.315134: \n",
      "2025-05-15 11:10:18.315364: Epoch 944\n",
      "2025-05-15 11:10:18.315593: Current learning rate: 0.00075\n",
      "2025-05-15 11:12:07.122665: train_loss -0.9743\n",
      "2025-05-15 11:12:07.122844: val_loss -0.9619\n",
      "2025-05-15 11:12:07.122875: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-15 11:12:07.122906: Epoch time: 108.81 s\n",
      "2025-05-15 11:12:07.680438: \n",
      "2025-05-15 11:12:07.680567: Epoch 945\n",
      "2025-05-15 11:12:07.680688: Current learning rate: 0.00074\n",
      "2025-05-15 11:13:56.539174: train_loss -0.9746\n",
      "2025-05-15 11:13:56.539327: val_loss -0.9583\n",
      "2025-05-15 11:13:56.539460: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-15 11:13:56.539526: Epoch time: 108.86 s\n",
      "2025-05-15 11:13:56.539564: Yayy! New best EMA pseudo Dice: 0.9829999804496765\n",
      "2025-05-15 11:13:57.335230: \n",
      "2025-05-15 11:13:57.335775: Epoch 946\n",
      "2025-05-15 11:13:57.336040: Current learning rate: 0.00072\n",
      "2025-05-15 11:15:46.106401: train_loss -0.9736\n",
      "2025-05-15 11:15:46.106554: val_loss -0.9559\n",
      "2025-05-15 11:15:46.106589: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 11:15:46.106622: Epoch time: 108.77 s\n",
      "2025-05-15 11:15:46.106642: Yayy! New best EMA pseudo Dice: 0.9829999804496765\n",
      "2025-05-15 11:15:46.881886: \n",
      "2025-05-15 11:15:46.882186: Epoch 947\n",
      "2025-05-15 11:15:46.882306: Current learning rate: 0.00071\n",
      "2025-05-15 11:17:35.801003: train_loss -0.9749\n",
      "2025-05-15 11:17:35.801162: val_loss -0.955\n",
      "2025-05-15 11:17:35.801315: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 11:17:35.801392: Epoch time: 108.92 s\n",
      "2025-05-15 11:17:36.682673: \n",
      "2025-05-15 11:17:36.683161: Epoch 948\n",
      "2025-05-15 11:17:36.683397: Current learning rate: 0.0007\n",
      "2025-05-15 11:19:25.598671: train_loss -0.9734\n",
      "2025-05-15 11:19:25.598818: val_loss -0.958\n",
      "2025-05-15 11:19:25.598860: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 11:19:25.598892: Epoch time: 108.92 s\n",
      "2025-05-15 11:19:26.150496: \n",
      "2025-05-15 11:19:26.150671: Epoch 949\n",
      "2025-05-15 11:19:26.150871: Current learning rate: 0.00069\n",
      "2025-05-15 11:21:15.044889: train_loss -0.9742\n",
      "2025-05-15 11:21:15.045019: val_loss -0.9583\n",
      "2025-05-15 11:21:15.045053: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 11:21:15.045089: Epoch time: 108.89 s\n",
      "2025-05-15 11:21:15.833133: \n",
      "2025-05-15 11:21:15.833235: Epoch 950\n",
      "2025-05-15 11:21:15.833305: Current learning rate: 0.00067\n",
      "2025-05-15 11:23:04.831873: train_loss -0.9729\n",
      "2025-05-15 11:23:04.832259: val_loss -0.9581\n",
      "2025-05-15 11:23:04.832403: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 11:23:04.832484: Epoch time: 109.0 s\n",
      "2025-05-15 11:23:05.387815: \n",
      "2025-05-15 11:23:05.387968: Epoch 951\n",
      "2025-05-15 11:23:05.388075: Current learning rate: 0.00066\n",
      "2025-05-15 11:24:54.215703: train_loss -0.9732\n",
      "2025-05-15 11:24:54.215844: val_loss -0.9571\n",
      "2025-05-15 11:24:54.215879: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 11:24:54.215912: Epoch time: 108.83 s\n",
      "2025-05-15 11:24:54.770195: \n",
      "2025-05-15 11:24:54.770399: Epoch 952\n",
      "2025-05-15 11:24:54.770535: Current learning rate: 0.00065\n",
      "2025-05-15 11:26:43.671252: train_loss -0.9727\n",
      "2025-05-15 11:26:43.671375: val_loss -0.9597\n",
      "2025-05-15 11:26:43.671407: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-15 11:26:43.671440: Epoch time: 108.9 s\n",
      "2025-05-15 11:26:44.230837: \n",
      "2025-05-15 11:26:44.231122: Epoch 953\n",
      "2025-05-15 11:26:44.231196: Current learning rate: 0.00064\n",
      "2025-05-15 11:28:33.177517: train_loss -0.9723\n",
      "2025-05-15 11:28:33.177708: val_loss -0.9558\n",
      "2025-05-15 11:28:33.177741: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-15 11:28:33.177777: Epoch time: 108.95 s\n",
      "2025-05-15 11:28:33.177799: Yayy! New best EMA pseudo Dice: 0.9829999804496765\n",
      "2025-05-15 11:28:33.959023: \n",
      "2025-05-15 11:28:33.959218: Epoch 954\n",
      "2025-05-15 11:28:33.959447: Current learning rate: 0.00063\n",
      "2025-05-15 11:30:22.855924: train_loss -0.9744\n",
      "2025-05-15 11:30:22.856058: val_loss -0.9565\n",
      "2025-05-15 11:30:22.856252: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 11:30:22.856330: Epoch time: 108.9 s\n",
      "2025-05-15 11:30:23.414683: \n",
      "2025-05-15 11:30:23.414846: Epoch 955\n",
      "2025-05-15 11:30:23.414949: Current learning rate: 0.00061\n",
      "2025-05-15 11:32:12.224422: train_loss -0.9751\n",
      "2025-05-15 11:32:12.224558: val_loss -0.958\n",
      "2025-05-15 11:32:12.224592: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 11:32:12.224626: Epoch time: 108.81 s\n",
      "2025-05-15 11:32:12.791016: \n",
      "2025-05-15 11:32:12.791131: Epoch 956\n",
      "2025-05-15 11:32:12.791230: Current learning rate: 0.0006\n",
      "2025-05-15 11:34:01.572541: train_loss -0.9745\n",
      "2025-05-15 11:34:01.572678: val_loss -0.954\n",
      "2025-05-15 11:34:01.572712: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 11:34:01.572745: Epoch time: 108.78 s\n",
      "2025-05-15 11:34:02.134713: \n",
      "2025-05-15 11:34:02.134815: Epoch 957\n",
      "2025-05-15 11:34:02.134887: Current learning rate: 0.00059\n",
      "2025-05-15 11:35:50.915609: train_loss -0.9738\n",
      "2025-05-15 11:35:50.915729: val_loss -0.9556\n",
      "2025-05-15 11:35:50.915761: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 11:35:50.915795: Epoch time: 108.78 s\n",
      "2025-05-15 11:35:51.482691: \n",
      "2025-05-15 11:35:51.482826: Epoch 958\n",
      "2025-05-15 11:35:51.482913: Current learning rate: 0.00058\n",
      "2025-05-15 11:37:40.264111: train_loss -0.974\n",
      "2025-05-15 11:37:40.264242: val_loss -0.9603\n",
      "2025-05-15 11:37:40.264275: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 11:37:40.264308: Epoch time: 108.78 s\n",
      "2025-05-15 11:37:40.815970: \n",
      "2025-05-15 11:37:40.816223: Epoch 959\n",
      "2025-05-15 11:37:40.816313: Current learning rate: 0.00056\n",
      "2025-05-15 11:39:29.582555: train_loss -0.9743\n",
      "2025-05-15 11:39:29.582725: val_loss -0.9523\n",
      "2025-05-15 11:39:29.582762: Pseudo dice [np.float32(0.9798)]\n",
      "2025-05-15 11:39:29.582796: Epoch time: 108.77 s\n",
      "2025-05-15 11:39:30.134649: \n",
      "2025-05-15 11:39:30.135048: Epoch 960\n",
      "2025-05-15 11:39:30.135167: Current learning rate: 0.00055\n",
      "2025-05-15 11:41:18.922976: train_loss -0.9735\n",
      "2025-05-15 11:41:18.923110: val_loss -0.9475\n",
      "2025-05-15 11:41:18.923142: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-15 11:41:18.923176: Epoch time: 108.79 s\n",
      "2025-05-15 11:41:19.481826: \n",
      "2025-05-15 11:41:19.481984: Epoch 961\n",
      "2025-05-15 11:41:19.482063: Current learning rate: 0.00054\n",
      "2025-05-15 11:43:08.312885: train_loss -0.9745\n",
      "2025-05-15 11:43:08.313065: val_loss -0.9545\n",
      "2025-05-15 11:43:08.313097: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 11:43:08.313129: Epoch time: 108.83 s\n",
      "2025-05-15 11:43:08.874499: \n",
      "2025-05-15 11:43:08.874684: Epoch 962\n",
      "2025-05-15 11:43:08.874801: Current learning rate: 0.00053\n",
      "2025-05-15 11:44:57.826780: train_loss -0.9739\n",
      "2025-05-15 11:44:57.826938: val_loss -0.9494\n",
      "2025-05-15 11:44:57.826970: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-15 11:44:57.827004: Epoch time: 108.95 s\n",
      "2025-05-15 11:44:58.379941: \n",
      "2025-05-15 11:44:58.380369: Epoch 963\n",
      "2025-05-15 11:44:58.380498: Current learning rate: 0.00051\n",
      "2025-05-15 11:46:47.189952: train_loss -0.9739\n",
      "2025-05-15 11:46:47.190134: val_loss -0.9536\n",
      "2025-05-15 11:46:47.190166: Pseudo dice [np.float32(0.9813)]\n",
      "2025-05-15 11:46:47.190200: Epoch time: 108.81 s\n",
      "2025-05-15 11:46:47.753962: \n",
      "2025-05-15 11:46:47.754277: Epoch 964\n",
      "2025-05-15 11:46:47.754431: Current learning rate: 0.0005\n",
      "2025-05-15 11:48:36.604155: train_loss -0.9749\n",
      "2025-05-15 11:48:36.604294: val_loss -0.96\n",
      "2025-05-15 11:48:36.604325: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-15 11:48:36.604359: Epoch time: 108.85 s\n",
      "2025-05-15 11:48:37.157565: \n",
      "2025-05-15 11:48:37.157854: Epoch 965\n",
      "2025-05-15 11:48:37.157945: Current learning rate: 0.00049\n",
      "2025-05-15 11:50:25.941810: train_loss -0.9745\n",
      "2025-05-15 11:50:25.941935: val_loss -0.9577\n",
      "2025-05-15 11:50:25.941966: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 11:50:25.941999: Epoch time: 108.78 s\n",
      "2025-05-15 11:50:26.820870: \n",
      "2025-05-15 11:50:26.821043: Epoch 966\n",
      "2025-05-15 11:50:26.821118: Current learning rate: 0.00048\n",
      "2025-05-15 11:52:15.744538: train_loss -0.9737\n",
      "2025-05-15 11:52:15.744739: val_loss -0.9525\n",
      "2025-05-15 11:52:15.744781: Pseudo dice [np.float32(0.9803)]\n",
      "2025-05-15 11:52:15.744823: Epoch time: 108.92 s\n",
      "2025-05-15 11:52:16.309994: \n",
      "2025-05-15 11:52:16.310219: Epoch 967\n",
      "2025-05-15 11:52:16.310396: Current learning rate: 0.00046\n",
      "2025-05-15 11:54:05.263608: train_loss -0.9738\n",
      "2025-05-15 11:54:05.263738: val_loss -0.9594\n",
      "2025-05-15 11:54:05.263769: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 11:54:05.263849: Epoch time: 108.95 s\n",
      "2025-05-15 11:54:05.824121: \n",
      "2025-05-15 11:54:05.824472: Epoch 968\n",
      "2025-05-15 11:54:05.824639: Current learning rate: 0.00045\n",
      "2025-05-15 11:55:54.785972: train_loss -0.9733\n",
      "2025-05-15 11:55:54.786138: val_loss -0.9595\n",
      "2025-05-15 11:55:54.786172: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 11:55:54.786204: Epoch time: 108.96 s\n",
      "2025-05-15 11:55:55.350074: \n",
      "2025-05-15 11:55:55.350249: Epoch 969\n",
      "2025-05-15 11:55:55.350319: Current learning rate: 0.00044\n",
      "2025-05-15 11:57:44.109384: train_loss -0.9744\n",
      "2025-05-15 11:57:44.109586: val_loss -0.9589\n",
      "2025-05-15 11:57:44.109621: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 11:57:44.109655: Epoch time: 108.76 s\n",
      "2025-05-15 11:57:44.670045: \n",
      "2025-05-15 11:57:44.670217: Epoch 970\n",
      "2025-05-15 11:57:44.670286: Current learning rate: 0.00043\n",
      "2025-05-15 11:59:33.513387: train_loss -0.9748\n",
      "2025-05-15 11:59:33.513567: val_loss -0.9578\n",
      "2025-05-15 11:59:33.513605: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 11:59:33.513638: Epoch time: 108.84 s\n",
      "2025-05-15 11:59:34.081113: \n",
      "2025-05-15 11:59:34.081339: Epoch 971\n",
      "2025-05-15 11:59:34.081426: Current learning rate: 0.00041\n",
      "2025-05-15 12:01:22.913445: train_loss -0.9736\n",
      "2025-05-15 12:01:22.913634: val_loss -0.9583\n",
      "[np.float32(0.983)].913670: Pseudo dice \n",
      "2025-05-15 12:01:22.913766: Epoch time: 108.83 s\n",
      "2025-05-15 12:01:23.477359: \n",
      "2025-05-15 12:01:23.477481: Epoch 972\n",
      "2025-05-15 12:01:23.477552: Current learning rate: 0.0004\n",
      "2025-05-15 12:03:12.241962: train_loss -0.9739\n",
      "2025-05-15 12:03:12.242085: val_loss -0.9621\n",
      "2025-05-15 12:03:12.242118: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 12:03:12.242174: Epoch time: 108.77 s\n",
      "2025-05-15 12:03:12.802124: \n",
      "2025-05-15 12:03:12.802281: Epoch 973\n",
      "2025-05-15 12:03:12.802353: Current learning rate: 0.00039\n",
      "2025-05-15 12:05:01.736331: train_loss -0.9737\n",
      "2025-05-15 12:05:01.736461: val_loss -0.9573\n",
      "2025-05-15 12:05:01.736496: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 12:05:01.736530: Epoch time: 108.93 s\n",
      "2025-05-15 12:05:02.298597: \n",
      "2025-05-15 12:05:02.298738: Epoch 974\n",
      "2025-05-15 12:05:02.298818: Current learning rate: 0.00037\n",
      "2025-05-15 12:06:51.288461: train_loss -0.974\n",
      "2025-05-15 12:06:51.288636: val_loss -0.9574\n",
      "2025-05-15 12:06:51.288671: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 12:06:51.288706: Epoch time: 108.99 s\n",
      "2025-05-15 12:06:51.847414: \n",
      "2025-05-15 12:06:51.847686: Epoch 975\n",
      "2025-05-15 12:06:51.847766: Current learning rate: 0.00036\n",
      "2025-05-15 12:08:40.563708: train_loss -0.9738\n",
      "2025-05-15 12:08:40.564002: val_loss -0.9595\n",
      "2025-05-15 12:08:40.564102: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 12:08:40.564147: Epoch time: 108.72 s\n",
      "2025-05-15 12:08:41.137237: \n",
      "2025-05-15 12:08:41.137634: Epoch 976\n",
      "2025-05-15 12:08:41.137824: Current learning rate: 0.00035\n",
      "2025-05-15 12:10:29.936043: train_loss -0.9735\n",
      "2025-05-15 12:10:29.936284: val_loss -0.9622\n",
      "2025-05-15 12:10:29.936445: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 12:10:29.936614: Epoch time: 108.8 s\n",
      "2025-05-15 12:10:30.497848: \n",
      "2025-05-15 12:10:30.498224: Epoch 977\n",
      "2025-05-15 12:10:30.498338: Current learning rate: 0.00034\n",
      "2025-05-15 12:12:19.272386: train_loss -0.9727\n",
      "2025-05-15 12:12:19.272585: val_loss -0.9531\n",
      "2025-05-15 12:12:19.272618: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 12:12:19.272652: Epoch time: 108.78 s\n",
      "2025-05-15 12:12:19.272675: Yayy! New best EMA pseudo Dice: 0.9829999804496765\n",
      "2025-05-15 12:12:20.058738: \n",
      "2025-05-15 12:12:20.059294: Epoch 978\n",
      "2025-05-15 12:12:20.059479: Current learning rate: 0.00032\n",
      "2025-05-15 12:14:08.874599: train_loss -0.9741\n",
      "2025-05-15 12:14:08.874810: val_loss -0.96\n",
      "2025-05-15 12:14:08.874851: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 12:14:08.874894: Epoch time: 108.82 s\n",
      "2025-05-15 12:14:08.874933: Yayy! New best EMA pseudo Dice: 0.9829999804496765\n",
      "2025-05-15 12:14:09.657338: \n",
      "2025-05-15 12:14:09.657515: Epoch 979\n",
      "2025-05-15 12:14:09.657594: Current learning rate: 0.00031\n",
      "2025-05-15 12:15:58.438007: train_loss -0.9736\n",
      "2025-05-15 12:15:58.438198: val_loss -0.9582\n",
      "2025-05-15 12:15:58.438232: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 12:15:58.438265: Epoch time: 108.78 s\n",
      "2025-05-15 12:15:58.997399: \n",
      "2025-05-15 12:15:58.997769: Epoch 980\n",
      "2025-05-15 12:15:58.997883: Current learning rate: 0.0003\n",
      "2025-05-15 12:17:47.757524: train_loss -0.9743\n",
      "2025-05-15 12:17:47.757654: val_loss -0.9586\n",
      "2025-05-15 12:17:47.757700: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 12:17:47.757982: Epoch time: 108.76 s\n",
      "2025-05-15 12:17:48.316395: \n",
      "2025-05-15 12:17:48.316574: Epoch 981\n",
      "2025-05-15 12:17:48.316655: Current learning rate: 0.00028\n",
      "2025-05-15 12:19:37.136853: train_loss -0.9737\n",
      "2025-05-15 12:19:37.137024: val_loss -0.9531\n",
      "2025-05-15 12:19:37.137062: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-15 12:19:37.137097: Epoch time: 108.82 s\n",
      "2025-05-15 12:19:37.715434: \n",
      "2025-05-15 12:19:37.715766: Epoch 982\n",
      "2025-05-15 12:19:37.715943: Current learning rate: 0.00027\n",
      "2025-05-15 12:21:26.647930: train_loss -0.9741\n",
      "2025-05-15 12:21:26.648055: val_loss -0.9559\n",
      "2025-05-15 12:21:26.648135: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 12:21:26.648317: Epoch time: 108.93 s\n",
      "2025-05-15 12:21:27.528980: \n",
      "2025-05-15 12:21:27.529501: Epoch 983\n",
      "2025-05-15 12:21:27.529606: Current learning rate: 0.00026\n",
      "2025-05-15 12:23:16.429473: train_loss -0.974\n",
      "2025-05-15 12:23:16.429600: val_loss -0.9547\n",
      "2025-05-15 12:23:16.429633: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 12:23:16.429667: Epoch time: 108.9 s\n",
      "2025-05-15 12:23:17.007606: \n",
      "2025-05-15 12:23:17.007810: Epoch 984\n",
      "2025-05-15 12:23:17.007894: Current learning rate: 0.00024\n",
      "2025-05-15 12:25:05.994852: train_loss -0.9746\n",
      "2025-05-15 12:25:05.995032: val_loss -0.9565\n",
      "2025-05-15 12:25:05.995075: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 12:25:05.995111: Epoch time: 108.99 s\n",
      "2025-05-15 12:25:06.560596: \n",
      "2025-05-15 12:25:06.560776: Epoch 985\n",
      "2025-05-15 12:25:06.560847: Current learning rate: 0.00023\n",
      "2025-05-15 12:26:55.306971: train_loss -0.9751\n",
      "2025-05-15 12:26:55.307098: val_loss -0.9526\n",
      "2025-05-15 12:26:55.307133: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 12:26:55.307167: Epoch time: 108.75 s\n",
      "2025-05-15 12:26:55.871297: \n",
      "2025-05-15 12:26:55.871451: Epoch 986\n",
      "2025-05-15 12:26:55.871527: Current learning rate: 0.00021\n",
      "2025-05-15 12:28:44.654415: train_loss -0.9749\n",
      "2025-05-15 12:28:44.654548: val_loss -0.9552\n",
      "2025-05-15 12:28:44.654580: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 12:28:44.654613: Epoch time: 108.78 s\n",
      "2025-05-15 12:28:45.226062: \n",
      "2025-05-15 12:28:45.226487: Epoch 987\n",
      "2025-05-15 12:28:45.226703: Current learning rate: 0.0002\n",
      "2025-05-15 12:30:35.602599: train_loss -0.974\n",
      "2025-05-15 12:30:35.602718: val_loss -0.9553\n",
      "2025-05-15 12:30:35.602751: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 12:30:35.602786: Epoch time: 110.38 s\n",
      "2025-05-15 12:30:36.161483: \n",
      "2025-05-15 12:30:36.161820: Epoch 988\n",
      "2025-05-15 12:30:36.162026: Current learning rate: 0.00019\n",
      "2025-05-15 12:32:25.394343: train_loss -0.9742\n",
      "2025-05-15 12:32:25.394470: val_loss -0.9531\n",
      "2025-05-15 12:32:25.394503: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 12:32:25.394537: Epoch time: 109.23 s\n",
      "2025-05-15 12:32:25.948780: \n",
      "2025-05-15 12:32:25.948882: Epoch 989\n",
      "2025-05-15 12:32:25.948952: Current learning rate: 0.00017\n",
      "2025-05-15 12:34:15.144756: train_loss -0.9742\n",
      "2025-05-15 12:34:15.144939: val_loss -0.9601\n",
      "2025-05-15 12:34:15.144972: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 12:34:15.145015: Epoch time: 109.2 s\n",
      "2025-05-15 12:34:15.703507: \n",
      "2025-05-15 12:34:15.703722: Epoch 990\n",
      "2025-05-15 12:34:15.703788: Current learning rate: 0.00016\n",
      "2025-05-15 12:36:05.098093: train_loss -0.974\n",
      "2025-05-15 12:36:05.098325: val_loss -0.9549\n",
      "2025-05-15 12:36:05.098434: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 12:36:05.098611: Epoch time: 109.4 s\n",
      "2025-05-15 12:36:05.649724: \n",
      "2025-05-15 12:36:05.650013: Epoch 991\n",
      "2025-05-15 12:36:05.650120: Current learning rate: 0.00014\n",
      "2025-05-15 12:37:54.945806: train_loss -0.9731\n",
      "2025-05-15 12:37:54.946112: val_loss -0.9569\n",
      "2025-05-15 12:37:54.946222: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 12:37:54.946327: Epoch time: 109.3 s\n",
      "2025-05-15 12:37:55.497929: \n",
      "2025-05-15 12:37:55.498269: Epoch 992\n",
      "2025-05-15 12:37:55.498363: Current learning rate: 0.00013\n",
      "2025-05-15 12:39:44.733573: train_loss -0.9741\n",
      "2025-05-15 12:39:44.733698: val_loss -0.9596\n",
      "2025-05-15 12:39:44.733730: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 12:39:44.733763: Epoch time: 109.24 s\n",
      "2025-05-15 12:39:45.288581: \n",
      "2025-05-15 12:39:45.288833: Epoch 993\n",
      "2025-05-15 12:39:45.288922: Current learning rate: 0.00011\n",
      "2025-05-15 12:41:34.080793: train_loss -0.9749\n",
      "2025-05-15 12:41:34.080977: val_loss -0.9573\n",
      "2025-05-15 12:41:34.081022: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-15 12:41:34.081069: Epoch time: 108.79 s\n",
      "2025-05-15 12:41:34.637700: \n",
      "2025-05-15 12:41:34.637837: Epoch 994\n",
      "2025-05-15 12:41:34.637932: Current learning rate: 0.0001\n",
      "2025-05-15 12:43:23.484571: train_loss -0.9752\n",
      "2025-05-15 12:43:23.484745: val_loss -0.9557\n",
      "2025-05-15 12:43:23.484779: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 12:43:23.484814: Epoch time: 108.85 s\n",
      "2025-05-15 12:43:24.026227: \n",
      "2025-05-15 12:43:24.026325: Epoch 995\n",
      "2025-05-15 12:43:24.026451: Current learning rate: 8e-05\n",
      "2025-05-15 12:45:12.956701: train_loss -0.9742\n",
      "2025-05-15 12:45:12.956836: val_loss -0.9479\n",
      "2025-05-15 12:45:12.956869: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-15 12:45:12.956903: Epoch time: 108.93 s\n",
      "2025-05-15 12:45:13.520166: \n",
      "2025-05-15 12:45:13.520462: Epoch 996\n",
      "Current learning rate: 7e-05\n",
      "2025-05-15 12:47:02.449412: train_loss -0.9744\n",
      "2025-05-15 12:47:02.449588: val_loss -0.9548\n",
      "2025-05-15 12:47:02.449620: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 12:47:02.449654: Epoch time: 108.93 s\n",
      "2025-05-15 12:47:03.001158: \n",
      "2025-05-15 12:47:03.001330: Epoch 997\n",
      "2025-05-15 12:47:03.001469: Current learning rate: 5e-05\n",
      "2025-05-15 12:48:51.998240: train_loss -0.9737\n",
      "2025-05-15 12:48:51.998521: val_loss -0.9541\n",
      "2025-05-15 12:48:51.998691: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 12:48:51.998889: Epoch time: 109.0 s\n",
      "2025-05-15 12:48:52.542661: \n",
      "2025-05-15 12:48:52.542767: Epoch 998\n",
      "2025-05-15 12:48:52.542842: Current learning rate: 4e-05\n",
      "2025-05-15 12:50:41.327973: train_loss -0.9738\n",
      "2025-05-15 12:50:41.328147: val_loss -0.9566\n",
      "2025-05-15 12:50:41.328182: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 12:50:41.328215: Epoch time: 108.79 s\n",
      "2025-05-15 12:50:41.883762: \n",
      "2025-05-15 12:50:41.884126: Epoch 999\n",
      "2025-05-15 12:50:41.884233: Current learning rate: 2e-05\n",
      "2025-05-15 12:52:30.703372: train_loss -0.9746\n",
      "2025-05-15 12:52:30.703551: val_loss -0.9515\n",
      "2025-05-15 12:52:30.703643: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 12:52:30.703688: Epoch time: 108.82 s\n",
      "2025-05-15 12:52:31.513332: Training done.\n",
      "2025-05-15 12:52:31.518757: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-15 12:52:31.518982: The split file contains 5 splits.\n",
      "2025-05-15 12:52:31.519063: Desired fold for training: 1\n",
      "2025-05-15 12:52:31.519107: This split has 29 training and 7 validation cases.\n",
      "2025-05-15 12:52:31.519227: predicting LCTSC-Train-S1-009\n",
      "2025-05-15 12:52:31.520120: LCTSC-Train-S1-009, shape torch.Size([1, 180, 512, 512]), rank 0\n",
      "2025-05-15 12:53:52.321636: predicting LCTSC-Train-S2-001\n",
      "2025-05-15 12:53:52.324120: LCTSC-Train-S2-001, shape torch.Size([1, 160, 512, 512]), rank 0\n",
      "2025-05-15 12:54:51.327291: predicting LCTSC-Train-S2-004\n",
      "2025-05-15 12:54:51.329488: LCTSC-Train-S2-004, shape torch.Size([1, 168, 512, 512]), rank 0\n",
      "2025-05-15 12:55:50.230367: predicting LCTSC-Train-S2-008\n",
      "2025-05-15 12:55:50.232611: LCTSC-Train-S2-008, shape torch.Size([1, 168, 512, 512]), rank 0\n",
      "2025-05-15 12:56:49.140469: predicting LCTSC-Train-S2-009\n",
      "2025-05-15 12:56:49.142553: LCTSC-Train-S2-009, shape torch.Size([1, 160, 512, 512]), rank 0\n",
      "2025-05-15 12:57:48.020452: predicting LCTSC-Train-S3-001\n",
      "2025-05-15 12:57:48.022423: LCTSC-Train-S3-001, shape torch.Size([1, 163, 614, 614]), rank 0\n",
      "2025-05-15 12:59:13.106762: predicting LCTSC-Train-S3-008\n",
      "2025-05-15 12:59:13.109546: LCTSC-Train-S3-008, shape torch.Size([1, 192, 512, 512]), rank 0\n",
      "2025-05-15 13:00:37.098781: Validation complete\n",
      "2025-05-15 13:00:37.098845: Mean Validation Dice:  0.9796793416914191\n",
      "🔁 Running Fold 2: nnUNetv2_train 3 3d_fullres 2 --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "WARNING: Cannot continue training because there seems to be no checkpoint available to continue from. Starting a new training...\n",
      "2025-05-15 13:00:42.245259: do_dummy_2d_data_aug: True\n",
      "2025-05-15 13:00:42.245511: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-15 13:00:42.245605: The split file contains 5 splits.\n",
      "2025-05-15 13:00:42.245632: Desired fold for training: 2\n",
      "2025-05-15 13:00:42.245648: This split has 29 training and 7 validation cases.\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pin_memory on device 0\n",
      "2025-05-15 13:00:44.320007: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [56, 192, 192], 'median_image_size_in_voxels': [162.5, 512.0, 512.0], 'spacing': [2.5, 0.9765625, 0.9765625], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset003_Lung_only', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.9765625, 0.9765625], 'original_median_shape_after_transp': [156, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1910.0, 'mean': -742.67333984375, 'median': -794.0, 'min': -1023.0, 'percentile_00_5': -981.0, 'percentile_99_5': -51.0, 'std': 175.66612243652344}}} \n",
      "\n",
      "2025-05-15 13:00:44.907603: unpacking dataset...\n",
      "2025-05-15 13:00:48.771054: unpacking done...\n",
      "2025-05-15 13:00:48.771909: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-05-15 13:00:48.774624: \n",
      "2025-05-15 13:00:48.774730: Epoch 0\n",
      "2025-05-15 13:00:48.774806: Current learning rate: 0.01\n",
      "2025-05-15 13:02:56.468825: train_loss -0.4242\n",
      "2025-05-15 13:02:56.469013: val_loss -0.6971\n",
      "2025-05-15 13:02:56.469062: Pseudo dice [np.float32(0.8679)]\n",
      "2025-05-15 13:02:56.469100: Epoch time: 127.69 s\n",
      "2025-05-15 13:02:56.469122: Yayy! New best EMA pseudo Dice: 0.867900013923645\n",
      "2025-05-15 13:02:57.126209: \n",
      "2025-05-15 13:02:57.126451: Epoch 1\n",
      "2025-05-15 13:02:57.126552: Current learning rate: 0.00999\n",
      "2025-05-15 13:04:45.957081: train_loss -0.6871\n",
      "2025-05-15 13:04:45.957407: val_loss -0.808\n",
      "2025-05-15 13:04:45.957446: Pseudo dice [np.float32(0.9266)]\n",
      "2025-05-15 13:04:45.957487: Epoch time: 108.83 s\n",
      "2025-05-15 13:04:45.957513: Yayy! New best EMA pseudo Dice: 0.8737000226974487\n",
      "2025-05-15 13:04:46.810439: \n",
      "2025-05-15 13:04:46.810526: Epoch 2\n",
      "2025-05-15 13:04:46.810591: Current learning rate: 0.00998\n",
      "2025-05-15 13:06:35.703650: train_loss -0.7307\n",
      "2025-05-15 13:06:35.703973: val_loss -0.8625\n",
      "2025-05-15 13:06:35.704083: Pseudo dice [np.float32(0.9504)]\n",
      "2025-05-15 13:06:35.704129: Epoch time: 108.89 s\n",
      "2025-05-15 13:06:35.704174: Yayy! New best EMA pseudo Dice: 0.8813999891281128\n",
      "2025-05-15 13:06:36.414788: \n",
      "2025-05-15 13:06:36.414930: Epoch 3\n",
      "2025-05-15 13:06:36.414998: Current learning rate: 0.00997\n",
      "2025-05-15 13:08:25.226481: train_loss -0.7839\n",
      "2025-05-15 13:08:25.226877: val_loss -0.8236\n",
      "2025-05-15 13:08:25.227030: Pseudo dice [np.float32(0.934)]\n",
      "2025-05-15 13:08:25.227094: Epoch time: 108.81 s\n",
      "2025-05-15 13:08:25.227132: Yayy! New best EMA pseudo Dice: 0.8866999745368958\n",
      "2025-05-15 13:08:25.915547: \n",
      "2025-05-15 13:08:25.915815: Epoch 4\n",
      "2025-05-15 13:08:25.915972: Current learning rate: 0.00996\n",
      "2025-05-15 13:10:14.714827: train_loss -0.7943\n",
      "2025-05-15 13:10:14.715015: val_loss -0.873\n",
      "2025-05-15 13:10:14.715050: Pseudo dice [np.float32(0.9528)]\n",
      "2025-05-15 13:10:14.715149: Epoch time: 108.8 s\n",
      "2025-05-15 13:10:14.715341: Yayy! New best EMA pseudo Dice: 0.8932999968528748\n",
      "2025-05-15 13:10:15.433187: \n",
      "2025-05-15 13:10:15.433272: Epoch 5\n",
      "2025-05-15 13:10:15.433336: Current learning rate: 0.00995\n",
      "2025-05-15 13:12:04.263808: train_loss -0.8371\n",
      "2025-05-15 13:12:04.264005: val_loss -0.8947\n",
      "2025-05-15 13:12:04.264039: Pseudo dice [np.float32(0.9631)]\n",
      "2025-05-15 13:12:04.264074: Epoch time: 108.83 s\n",
      "2025-05-15 13:12:04.264094: Yayy! New best EMA pseudo Dice: 0.9003000259399414\n",
      "2025-05-15 13:12:04.946826: \n",
      "2025-05-15 13:12:04.946909: Epoch 6\n",
      "2025-05-15 13:12:04.946974: Current learning rate: 0.00995\n",
      "2025-05-15 13:13:53.884890: train_loss -0.8431\n",
      "2025-05-15 13:13:53.885098: val_loss -0.9098\n",
      "2025-05-15 13:13:53.885166: Pseudo dice [np.float32(0.9674)]\n",
      "2025-05-15 13:13:53.885231: Epoch time: 108.94 s\n",
      "2025-05-15 13:13:53.885262: Yayy! New best EMA pseudo Dice: 0.9070000052452087\n",
      "2025-05-15 13:13:54.589035: \n",
      "2025-05-15 13:13:54.589190: Epoch 7\n",
      "2025-05-15 13:13:54.589286: Current learning rate: 0.00994\n",
      "2025-05-15 13:15:43.509804: train_loss -0.844\n",
      "2025-05-15 13:15:43.509928: val_loss -0.8811\n",
      "2025-05-15 13:15:43.509964: Pseudo dice [np.float32(0.9537)]\n",
      "2025-05-15 13:15:43.509997: Epoch time: 108.92 s\n",
      "2025-05-15 13:15:43.510018: Yayy! New best EMA pseudo Dice: 0.9115999937057495\n",
      "2025-05-15 13:15:44.219128: \n",
      "2025-05-15 13:15:44.219278: Epoch 8\n",
      "2025-05-15 13:15:44.219347: Current learning rate: 0.00993\n",
      "2025-05-15 13:17:33.234361: train_loss -0.8669\n",
      "2025-05-15 13:17:33.234559: val_loss -0.8507\n",
      "2025-05-15 13:17:33.234602: Pseudo dice [np.float32(0.9419)]\n",
      "2025-05-15 13:17:33.234636: Epoch time: 109.02 s\n",
      "2025-05-15 13:17:33.234655: Yayy! New best EMA pseudo Dice: 0.9146999716758728\n",
      "2025-05-15 13:17:33.947420: \n",
      "2025-05-15 13:17:33.947573: Epoch 9\n",
      "2025-05-15 13:17:33.947640: Current learning rate: 0.00992\n",
      "2025-05-15 13:19:22.768696: train_loss -0.8673\n",
      "2025-05-15 13:19:22.768890: val_loss -0.8747\n",
      "2025-05-15 13:19:22.768925: Pseudo dice [np.float32(0.9518)]\n",
      "2025-05-15 13:19:22.768959: Epoch time: 108.82 s\n",
      "2025-05-15 13:19:22.768979: Yayy! New best EMA pseudo Dice: 0.91839998960495\n",
      "2025-05-15 13:19:23.468161: \n",
      "2025-05-15 13:19:23.468357: Epoch 10\n",
      "2025-05-15 13:19:23.468465: Current learning rate: 0.00991\n",
      "2025-05-15 13:21:12.415069: train_loss -0.8754\n",
      "2025-05-15 13:21:12.415194: val_loss -0.9254\n",
      "2025-05-15 13:21:12.415241: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-15 13:21:12.415276: Epoch time: 108.95 s\n",
      "2025-05-15 13:21:12.415297: Yayy! New best EMA pseudo Dice: 0.923799991607666\n",
      "2025-05-15 13:21:13.116822: \n",
      "2025-05-15 13:21:13.116912: Epoch 11\n",
      "2025-05-15 13:21:13.116978: Current learning rate: 0.0099\n",
      "2025-05-15 13:23:02.009480: train_loss -0.8798\n",
      "2025-05-15 13:23:02.009692: val_loss -0.9299\n",
      "2025-05-15 13:23:02.009866: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-15 13:23:02.009948: Epoch time: 108.89 s\n",
      "2025-05-15 13:23:02.009982: Yayy! New best EMA pseudo Dice: 0.9290000200271606\n",
      "2025-05-15 13:23:02.701488: \n",
      "2025-05-15 13:23:02.701567: Epoch 12\n",
      "2025-05-15 13:23:02.701630: Current learning rate: 0.00989\n",
      "2025-05-15 13:24:51.673584: train_loss -0.8833\n",
      "2025-05-15 13:24:51.673746: val_loss -0.9144\n",
      "2025-05-15 13:24:51.673782: Pseudo dice [np.float32(0.971)]\n",
      "2025-05-15 13:24:51.673815: Epoch time: 108.97 s\n",
      "2025-05-15 13:24:51.673836: Yayy! New best EMA pseudo Dice: 0.9332000017166138\n",
      "2025-05-15 13:24:52.376345: \n",
      "2025-05-15 13:24:52.376479: Epoch 13\n",
      "2025-05-15 13:24:52.376549: Current learning rate: 0.00988\n",
      "2025-05-15 13:26:41.148763: train_loss -0.8791\n",
      "2025-05-15 13:26:41.149139: val_loss -0.8767\n",
      "2025-05-15 13:26:41.149180: Pseudo dice [np.float32(0.9535)]\n",
      "2025-05-15 13:26:41.149297: Epoch time: 108.77 s\n",
      "2025-05-15 13:26:41.149458: Yayy! New best EMA pseudo Dice: 0.9351999759674072\n",
      "2025-05-15 13:26:42.033699: \n",
      "2025-05-15 13:26:42.033800: Epoch 14\n",
      "2025-05-15 13:26:42.033864: Current learning rate: 0.00987\n",
      "2025-05-15 13:28:30.896002: train_loss -0.8847\n",
      "2025-05-15 13:28:30.896174: val_loss -0.936\n",
      "2025-05-15 13:28:30.896210: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-15 13:28:30.896244: Epoch time: 108.86 s\n",
      "2025-05-15 13:28:30.896265: Yayy! New best EMA pseudo Dice: 0.9394000172615051\n",
      "2025-05-15 13:28:31.613728: \n",
      "2025-05-15 13:28:31.613906: Epoch 15\n",
      "2025-05-15 13:28:31.614003: Current learning rate: 0.00986\n",
      "2025-05-15 13:30:20.498825: train_loss -0.8899\n",
      "2025-05-15 13:30:20.498953: val_loss -0.9004\n",
      "2025-05-15 13:30:20.498986: Pseudo dice [np.float32(0.9645)]\n",
      "2025-05-15 13:30:20.499020: Epoch time: 108.89 s\n",
      "2025-05-15 13:30:20.499040: Yayy! New best EMA pseudo Dice: 0.9419000148773193\n",
      "2025-05-15 13:30:21.213828: \n",
      "2025-05-15 13:30:21.214013: Epoch 16\n",
      "2025-05-15 13:30:21.214088: Current learning rate: 0.00986\n",
      "2025-05-15 13:32:10.061080: train_loss -0.8789\n",
      "2025-05-15 13:32:10.061201: val_loss -0.9144\n",
      "2025-05-15 13:32:10.061235: Pseudo dice [np.float32(0.968)]\n",
      "2025-05-15 13:32:10.061267: Epoch time: 108.85 s\n",
      "2025-05-15 13:32:10.061288: Yayy! New best EMA pseudo Dice: 0.9445000290870667\n",
      "2025-05-15 13:32:10.782008: \n",
      "2025-05-15 13:32:10.782102: Epoch 17\n",
      "2025-05-15 13:32:10.782167: Current learning rate: 0.00985\n",
      "2025-05-15 13:33:59.571562: train_loss -0.8771\n",
      "2025-05-15 13:33:59.571748: val_loss -0.9104\n",
      "2025-05-15 13:33:59.571782: Pseudo dice [np.float32(0.9692)]\n",
      "2025-05-15 13:33:59.571815: Epoch time: 108.79 s\n",
      "2025-05-15 13:33:59.571835: Yayy! New best EMA pseudo Dice: 0.9470000267028809\n",
      "2025-05-15 13:34:00.302469: \n",
      "2025-05-15 13:34:00.302568: Epoch 18\n",
      "2025-05-15 13:34:00.302637: Current learning rate: 0.00984\n",
      "2025-05-15 13:35:49.115649: train_loss -0.8927\n",
      "2025-05-15 13:35:49.115771: val_loss -0.9376\n",
      "2025-05-15 13:35:49.115803: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-15 13:35:49.115837: Epoch time: 108.81 s\n",
      "2025-05-15 13:35:49.115857: Yayy! New best EMA pseudo Dice: 0.9498000144958496\n",
      "2025-05-15 13:35:49.843256: \n",
      "2025-05-15 13:35:49.843432: Epoch 19\n",
      "2025-05-15 13:35:49.843504: Current learning rate: 0.00983\n",
      "2025-05-15 13:37:38.677571: train_loss -0.8955\n",
      "2025-05-15 13:37:38.677689: val_loss -0.9259\n",
      "2025-05-15 13:37:38.677724: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-15 13:37:38.677758: Epoch time: 108.83 s\n",
      "Yayy! New best EMA pseudo Dice: 0.9521999955177307\n",
      "2025-05-15 13:37:39.398988: \n",
      "2025-05-15 13:37:39.399195: Epoch 20\n",
      "2025-05-15 13:37:39.399297: Current learning rate: 0.00982\n",
      "2025-05-15 13:39:28.224369: train_loss -0.9118\n",
      "2025-05-15 13:39:28.224486: val_loss -0.9404\n",
      "2025-05-15 13:39:28.224520: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-15 13:39:28.224553: Epoch time: 108.83 s\n",
      "2025-05-15 13:39:28.224575: Yayy! New best EMA pseudo Dice: 0.9545000195503235\n",
      "2025-05-15 13:39:28.962980: \n",
      "2025-05-15 13:39:28.963058: Epoch 21\n",
      "2025-05-15 13:39:28.963122: Current learning rate: 0.00981\n",
      "2025-05-15 13:41:17.907339: train_loss -0.9031\n",
      "2025-05-15 13:41:17.907461: val_loss -0.9425\n",
      "2025-05-15 13:41:17.907502: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-15 13:41:17.907538: Epoch time: 108.94 s\n",
      "2025-05-15 13:41:17.907560: Yayy! New best EMA pseudo Dice: 0.9570000171661377\n",
      "2025-05-15 13:41:18.617242: \n",
      "2025-05-15 13:41:18.617327: Epoch 22\n",
      "2025-05-15 13:41:18.617392: Current learning rate: 0.0098\n",
      "2025-05-15 13:43:07.482090: train_loss -0.903\n",
      "2025-05-15 13:43:07.482293: val_loss -0.941\n",
      "2025-05-15 13:43:07.482327: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-15 13:43:07.482361: Epoch time: 108.87 s\n",
      "2025-05-15 13:43:07.482383: Yayy! New best EMA pseudo Dice: 0.9589999914169312\n",
      "2025-05-15 13:43:08.208055: \n",
      "2025-05-15 13:43:08.208425: Epoch 23\n",
      "2025-05-15 13:43:08.208506: Current learning rate: 0.00979\n",
      "2025-05-15 13:44:57.112386: train_loss -0.9031\n",
      "2025-05-15 13:44:57.112511: val_loss -0.9229\n",
      "2025-05-15 13:44:57.112544: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-15 13:44:57.112577: Epoch time: 108.9 s\n",
      "2025-05-15 13:44:57.112597: Yayy! New best EMA pseudo Dice: 0.9603000283241272\n",
      "2025-05-15 13:44:57.814615: \n",
      "2025-05-15 13:44:57.814696: Epoch 24\n",
      "2025-05-15 13:44:57.814760: Current learning rate: 0.00978\n",
      "2025-05-15 13:46:46.655243: train_loss -0.896\n",
      "2025-05-15 13:46:46.655483: val_loss -0.9112\n",
      "2025-05-15 13:46:46.655616: Pseudo dice [np.float32(0.9634)]\n",
      "2025-05-15 13:46:46.655685: Epoch time: 108.84 s\n",
      "2025-05-15 13:46:46.655727: Yayy! New best EMA pseudo Dice: 0.9606000185012817\n",
      "2025-05-15 13:46:47.362602: \n",
      "2025-05-15 13:46:47.362681: Epoch 25\n",
      "2025-05-15 13:46:47.362745: Current learning rate: 0.00977\n",
      "2025-05-15 13:48:36.313901: train_loss -0.8892\n",
      "2025-05-15 13:48:36.314070: val_loss -0.9215\n",
      "2025-05-15 13:48:36.314104: Pseudo dice [np.float32(0.9688)]\n",
      "2025-05-15 13:48:36.314355: Epoch time: 108.95 s\n",
      "2025-05-15 13:48:36.314432: Yayy! New best EMA pseudo Dice: 0.9613999724388123\n",
      "2025-05-15 13:48:37.018102: \n",
      "2025-05-15 13:48:37.018422: Epoch 26\n",
      "2025-05-15 13:48:37.018527: Current learning rate: 0.00977\n",
      "2025-05-15 13:50:25.807952: train_loss -0.9155\n",
      "2025-05-15 13:50:25.808075: val_loss -0.9408\n",
      "2025-05-15 13:50:25.808109: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-15 13:50:25.808144: Epoch time: 108.79 s\n",
      "2025-05-15 13:50:25.808164: Yayy! New best EMA pseudo Dice: 0.963100016117096\n",
      "2025-05-15 13:50:26.705454: \n",
      "2025-05-15 13:50:26.705556: Epoch 27\n",
      "2025-05-15 13:50:26.705621: Current learning rate: 0.00976\n",
      "2025-05-15 13:52:15.560740: train_loss -0.9025\n",
      "2025-05-15 13:52:15.560863: val_loss -0.9529\n",
      "2025-05-15 13:52:15.560896: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 13:52:15.560929: Epoch time: 108.86 s\n",
      "2025-05-15 13:52:15.560950: Yayy! New best EMA pseudo Dice: 0.964900016784668\n",
      "2025-05-15 13:52:16.270266: \n",
      "2025-05-15 13:52:16.270401: Epoch 28\n",
      "2025-05-15 13:52:16.270468: Current learning rate: 0.00975\n",
      "2025-05-15 13:54:05.147043: train_loss -0.8926\n",
      "2025-05-15 13:54:05.147167: val_loss -0.9431\n",
      "2025-05-15 13:54:05.147201: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-15 13:54:05.147233: Epoch time: 108.88 s\n",
      "2025-05-15 13:54:05.147254: Yayy! New best EMA pseudo Dice: 0.9663000106811523\n",
      "2025-05-15 13:54:05.859898: \n",
      "2025-05-15 13:54:05.860009: Epoch 29\n",
      "2025-05-15 13:54:05.860075: Current learning rate: 0.00974\n",
      "2025-05-15 13:55:54.777521: train_loss -0.9147\n",
      "2025-05-15 13:55:54.777647: val_loss -0.9441\n",
      "2025-05-15 13:55:54.777834: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-15 13:55:54.778042: Epoch time: 108.92 s\n",
      "2025-05-15 13:55:54.778115: Yayy! New best EMA pseudo Dice: 0.9675999879837036\n",
      "2025-05-15 13:55:55.500669: \n",
      "2025-05-15 13:55:55.500819: Epoch 30\n",
      "2025-05-15 13:55:55.500888: Current learning rate: 0.00973\n",
      "2025-05-15 13:57:44.311909: train_loss -0.915\n",
      "2025-05-15 13:57:44.312037: val_loss -0.951\n",
      "2025-05-15 13:57:44.312070: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 13:57:44.312104: Epoch time: 108.81 s\n",
      "2025-05-15 13:57:44.312123: Yayy! New best EMA pseudo Dice: 0.968999981880188\n",
      "2025-05-15 13:57:45.030686: \n",
      "2025-05-15 13:57:45.030825: Epoch 31\n",
      "2025-05-15 13:57:45.030992: Current learning rate: 0.00972\n",
      "2025-05-15 13:59:33.891594: train_loss -0.9162\n",
      "2025-05-15 13:59:33.891725: val_loss -0.9443\n",
      "2025-05-15 13:59:33.891760: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-15 13:59:33.891793: Epoch time: 108.86 s\n",
      "2025-05-15 13:59:33.891813: Yayy! New best EMA pseudo Dice: 0.9700000286102295\n",
      "2025-05-15 13:59:34.609255: \n",
      "2025-05-15 13:59:34.609518: Epoch 32\n",
      "2025-05-15 13:59:34.609590: Current learning rate: 0.00971\n",
      "2025-05-15 14:01:23.494729: train_loss -0.885\n",
      "2025-05-15 14:01:23.494869: val_loss -0.8858\n",
      "2025-05-15 14:01:23.494911: Pseudo dice [np.float32(0.9569)]\n",
      "2025-05-15 14:01:23.494946: Epoch time: 108.89 s\n",
      "2025-05-15 14:01:24.008692: \n",
      "2025-05-15 14:01:24.008874: Epoch 33\n",
      "2025-05-15 14:01:24.009071: Current learning rate: 0.0097\n",
      "2025-05-15 14:03:12.825251: train_loss -0.8768\n",
      "2025-05-15 14:03:12.825381: val_loss -0.9429\n",
      "2025-05-15 14:03:12.825415: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-15 14:03:12.825504: Epoch time: 108.82 s\n",
      "2025-05-15 14:03:13.332669: \n",
      "2025-05-15 14:03:13.332838: Epoch 34\n",
      "2025-05-15 14:03:13.332913: Current learning rate: 0.00969\n",
      "2025-05-15 14:05:02.349481: train_loss -0.9041\n",
      "2025-05-15 14:05:02.349600: val_loss -0.9425\n",
      "2025-05-15 14:05:02.349691: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-15 14:05:02.349732: Epoch time: 109.02 s\n",
      "2025-05-15 14:05:02.349809: Yayy! New best EMA pseudo Dice: 0.9706000089645386\n",
      "2025-05-15 14:05:03.077017: \n",
      "2025-05-15 14:05:03.077103: Epoch 35\n",
      "2025-05-15 14:05:03.077169: Current learning rate: 0.00968\n",
      "2025-05-15 14:06:51.921054: train_loss -0.9107\n",
      "2025-05-15 14:06:51.921251: val_loss -0.9386\n",
      "2025-05-15 14:06:51.921286: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-15 14:06:51.921319: Epoch time: 108.84 s\n",
      "2025-05-15 14:06:51.921440: Yayy! New best EMA pseudo Dice: 0.9707000255584717\n",
      "2025-05-15 14:06:52.633750: \n",
      "2025-05-15 14:06:52.633885: Epoch 36\n",
      "2025-05-15 14:06:52.633951: Current learning rate: 0.00968\n",
      "2025-05-15 14:08:41.448381: train_loss -0.9089\n",
      "2025-05-15 14:08:41.448509: val_loss -0.9284\n",
      "2025-05-15 14:08:41.448542: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-15 14:08:41.448576: Epoch time: 108.82 s\n",
      "2025-05-15 14:08:41.448596: Yayy! New best EMA pseudo Dice: 0.9710999727249146\n",
      "2025-05-15 14:08:42.181341: \n",
      "2025-05-15 14:08:42.181479: Epoch 37\n",
      "2025-05-15 14:08:42.181552: Current learning rate: 0.00967\n",
      "2025-05-15 14:10:30.992009: train_loss -0.9163\n",
      "2025-05-15 14:10:30.992149: val_loss -0.9373\n",
      "2025-05-15 14:10:30.992184: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-15 14:10:30.992218: Epoch time: 108.81 s\n",
      "2025-05-15 14:10:30.992239: Yayy! New best EMA pseudo Dice: 0.9713000059127808\n",
      "2025-05-15 14:10:31.711645: \n",
      "2025-05-15 14:10:31.711883: Epoch 38\n",
      "2025-05-15 14:10:31.711959: Current learning rate: 0.00966\n",
      "2025-05-15 14:12:20.676534: train_loss -0.9188\n",
      "2025-05-15 14:12:20.676699: val_loss -0.9378\n",
      "2025-05-15 14:12:20.676735: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-15 14:12:20.676769: Epoch time: 108.97 s\n",
      "2025-05-15 14:12:20.676790: Yayy! New best EMA pseudo Dice: 0.9717000126838684\n",
      "2025-05-15 14:12:21.573396: \n",
      "2025-05-15 14:12:21.573604: Epoch 39\n",
      "2025-05-15 14:12:21.573689: Current learning rate: 0.00965\n",
      "2025-05-15 14:14:10.444381: train_loss -0.891\n",
      "2025-05-15 14:14:10.444524: val_loss -0.9317\n",
      "2025-05-15 14:14:10.444559: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-15 14:14:10.444649: Epoch time: 108.87 s\n",
      "2025-05-15 14:14:10.444716: Yayy! New best EMA pseudo Dice: 0.9717000126838684\n",
      "2025-05-15 14:14:11.174570: \n",
      "2025-05-15 14:14:11.174700: Epoch 40\n",
      "2025-05-15 14:14:11.174816: Current learning rate: 0.00964\n",
      "2025-05-15 14:16:00.094003: train_loss -0.9129\n",
      "2025-05-15 14:16:00.094133: val_loss -0.9333\n",
      "2025-05-15 14:16:00.094222: Pseudo dice [np.float32(0.9706)]\n",
      "2025-05-15 14:16:00.094281: Epoch time: 108.92 s\n",
      "2025-05-15 14:16:00.625115: \n",
      "2025-05-15 14:16:00.625219: Epoch 41\n",
      "2025-05-15 14:16:00.625287: Current learning rate: 0.00963\n",
      "2025-05-15 14:17:49.677739: train_loss -0.9065\n",
      "2025-05-15 14:17:49.677929: val_loss -0.9496\n",
      "2025-05-15 14:17:49.677970: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 14:17:49.678032: Epoch time: 109.05 s\n",
      "2025-05-15 14:17:49.678064: Yayy! New best EMA pseudo Dice: 0.9726999998092651\n",
      "2025-05-15 14:17:50.392206: \n",
      "2025-05-15 14:17:50.392350: Epoch 42\n",
      "2025-05-15 14:17:50.392419: Current learning rate: 0.00962\n",
      "2025-05-15 14:19:39.224124: train_loss -0.9197\n",
      "2025-05-15 14:19:39.224252: val_loss -0.9358\n",
      "2025-05-15 14:19:39.224287: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-15 14:19:39.224321: Epoch time: 108.83 s\n",
      "2025-05-15 14:19:39.224350: Yayy! New best EMA pseudo Dice: 0.9726999998092651\n",
      "2025-05-15 14:19:39.939073: \n",
      "2025-05-15 14:19:39.939405: Epoch 43\n",
      "2025-05-15 14:19:39.939505: Current learning rate: 0.00961\n",
      "2025-05-15 14:21:28.851055: train_loss -0.9234\n",
      "2025-05-15 14:21:28.851180: val_loss -0.9554\n",
      "2025-05-15 14:21:28.851215: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 14:21:28.851249: Epoch time: 108.91 s\n",
      "2025-05-15 14:21:28.851271: Yayy! New best EMA pseudo Dice: 0.9735999703407288\n",
      "2025-05-15 14:21:29.564291: \n",
      "2025-05-15 14:21:29.564522: Epoch 44\n",
      "2025-05-15 14:21:29.564646: Current learning rate: 0.0096\n",
      "2025-05-15 14:23:18.525112: train_loss -0.9266\n",
      "2025-05-15 14:23:18.525212: val_loss -0.9498\n",
      "2025-05-15 14:23:18.525239: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-15 14:23:18.525270: Epoch time: 108.96 s\n",
      "2025-05-15 14:23:18.525291: Yayy! New best EMA pseudo Dice: 0.9743000268936157\n",
      "2025-05-15 14:23:19.257354: \n",
      "2025-05-15 14:23:19.257450: Epoch 45\n",
      "2025-05-15 14:23:19.257544: Current learning rate: 0.00959\n",
      "2025-05-15 14:25:08.136264: train_loss -0.928\n",
      "2025-05-15 14:25:08.136403: val_loss -0.944\n",
      "2025-05-15 14:25:08.136443: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-15 14:25:08.136482: Epoch time: 108.88 s\n",
      "2025-05-15 14:25:08.136506: Yayy! New best EMA pseudo Dice: 0.9747999906539917\n",
      "2025-05-15 14:25:08.847990: \n",
      "2025-05-15 14:25:08.848135: Epoch 46\n",
      "2025-05-15 14:25:08.848205: Current learning rate: 0.00959\n",
      "2025-05-15 14:26:57.712488: train_loss -0.9232\n",
      "2025-05-15 14:26:57.712603: val_loss -0.9404\n",
      "2025-05-15 14:26:57.712636: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-15 14:26:57.712669: Epoch time: 108.87 s\n",
      "2025-05-15 14:26:58.207487: \n",
      "2025-05-15 14:26:58.207706: Epoch 47\n",
      "2025-05-15 14:26:58.207825: Current learning rate: 0.00958\n",
      "2025-05-15 14:28:47.079547: train_loss -0.9287\n",
      "2025-05-15 14:28:47.079671: val_loss -0.9564\n",
      "2025-05-15 14:28:47.079706: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 14:28:47.079740: Epoch time: 108.87 s\n",
      "2025-05-15 14:28:47.079761: Yayy! New best EMA pseudo Dice: 0.975600004196167\n",
      "2025-05-15 14:28:47.795550: \n",
      "2025-05-15 14:28:47.795659: Epoch 48\n",
      "2025-05-15 14:28:47.795810: Current learning rate: 0.00957\n",
      "2025-05-15 14:30:36.665576: train_loss -0.9232\n",
      "2025-05-15 14:30:36.665711: val_loss -0.9514\n",
      "2025-05-15 14:30:36.665743: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-15 14:30:36.665859: Epoch time: 108.87 s\n",
      "2025-05-15 14:30:36.665983: Yayy! New best EMA pseudo Dice: 0.9761000275611877\n",
      "2025-05-15 14:30:37.376286: \n",
      "2025-05-15 14:30:37.376468: Epoch 49\n",
      "2025-05-15 14:30:37.376540: Current learning rate: 0.00956\n",
      "2025-05-15 14:32:26.199041: train_loss -0.9206\n",
      "2025-05-15 14:32:26.199231: val_loss -0.9496\n",
      "2025-05-15 14:32:26.199265: Pseudo dice [np.float32(0.9797)]\n",
      "2025-05-15 14:32:26.199297: Epoch time: 108.82 s\n",
      "2025-05-15 14:32:26.394856: Yayy! New best EMA pseudo Dice: 0.9764000177383423\n",
      "2025-05-15 14:32:27.101982: \n",
      "2025-05-15 14:32:27.102139: Epoch 50\n",
      "2025-05-15 14:32:27.102211: Current learning rate: 0.00955\n",
      "2025-05-15 14:34:15.977044: train_loss -0.9247\n",
      "2025-05-15 14:34:15.977170: val_loss -0.9444\n",
      "2025-05-15 14:34:15.977203: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-15 14:34:15.977237: Epoch time: 108.88 s\n",
      "2025-05-15 14:34:15.977257: Yayy! New best EMA pseudo Dice: 0.9764999747276306\n",
      "2025-05-15 14:34:16.859302: \n",
      "2025-05-15 14:34:16.859496: Epoch 51\n",
      "2025-05-15 14:34:16.859588: Current learning rate: 0.00954\n",
      "2025-05-15 14:36:05.821099: train_loss -0.9012\n",
      "2025-05-15 14:36:05.821215: val_loss -0.9304\n",
      "2025-05-15 14:36:05.821249: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-15 14:36:05.821283: Epoch time: 108.96 s\n",
      "2025-05-15 14:36:06.324725: \n",
      "2025-05-15 14:36:06.324991: Epoch 52\n",
      "2025-05-15 14:36:06.325095: Current learning rate: 0.00953\n",
      "2025-05-15 14:37:55.295202: train_loss -0.9101\n",
      "2025-05-15 14:37:55.295324: val_loss -0.9361\n",
      "2025-05-15 14:37:55.295356: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-15 14:37:55.295476: Epoch time: 108.97 s\n",
      "2025-05-15 14:37:55.806003: \n",
      "2025-05-15 14:37:55.806177: Epoch 53\n",
      "2025-05-15 14:37:55.806335: Current learning rate: 0.00952\n",
      "2025-05-15 14:39:44.720388: train_loss -0.9212\n",
      "2025-05-15 14:39:44.720513: val_loss -0.9312\n",
      "2025-05-15 14:39:44.720548: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-15 14:39:44.720583: Epoch time: 108.91 s\n",
      "2025-05-15 14:39:45.235612: \n",
      "2025-05-15 14:39:45.235773: Epoch 54\n",
      "2025-05-15 14:39:45.235842: Current learning rate: 0.00951\n",
      "2025-05-15 14:41:34.112111: train_loss -0.9309\n",
      "2025-05-15 14:41:34.112397: val_loss -0.9501\n",
      "2025-05-15 14:41:34.112635: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 14:41:34.112682: Epoch time: 108.88 s\n",
      "2025-05-15 14:41:34.618510: \n",
      "2025-05-15 14:41:34.618650: Epoch 55\n",
      "2025-05-15 14:41:34.618715: Current learning rate: 0.0095\n",
      "2025-05-15 14:43:23.496915: train_loss -0.9342\n",
      "2025-05-15 14:43:23.497070: val_loss -0.9544\n",
      "2025-05-15 14:43:23.497198: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 14:43:23.497252: Epoch time: 108.88 s\n",
      "2025-05-15 14:43:23.497276: Yayy! New best EMA pseudo Dice: 0.9765999913215637\n",
      "2025-05-15 14:43:24.215980: \n",
      "2025-05-15 14:43:24.216075: Epoch 56\n",
      "2025-05-15 14:43:24.216224: Current learning rate: 0.00949\n",
      "2025-05-15 14:45:13.036183: train_loss -0.937\n",
      "2025-05-15 14:45:13.036305: val_loss -0.9547\n",
      "2025-05-15 14:45:13.036340: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 14:45:13.036374: Epoch time: 108.82 s\n",
      "2025-05-15 14:45:13.036395: Yayy! New best EMA pseudo Dice: 0.9771000146865845\n",
      "2025-05-15 14:45:13.746364: \n",
      "2025-05-15 14:45:13.746465: Epoch 57\n",
      "2025-05-15 14:45:13.746538: Current learning rate: 0.00949\n",
      "2025-05-15 14:47:02.609353: train_loss -0.9371\n",
      "2025-05-15 14:47:02.609581: val_loss -0.9545\n",
      "2025-05-15 14:47:02.609627: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 14:47:02.609663: Epoch time: 108.86 s\n",
      "2025-05-15 14:47:02.609683: Yayy! New best EMA pseudo Dice: 0.9775999784469604\n",
      "2025-05-15 14:47:03.310187: \n",
      "2025-05-15 14:47:03.310456: Epoch 58\n",
      "2025-05-15 14:47:03.310549: Current learning rate: 0.00948\n",
      "2025-05-15 14:48:52.196233: train_loss -0.935\n",
      "2025-05-15 14:48:52.196364: val_loss -0.9547\n",
      "2025-05-15 14:48:52.196398: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 14:48:52.196431: Epoch time: 108.89 s\n",
      "2025-05-15 14:48:52.196453: Yayy! New best EMA pseudo Dice: 0.9781000018119812\n",
      "2025-05-15 14:48:52.920931: \n",
      "2025-05-15 14:48:52.921022: Epoch 59\n",
      "2025-05-15 14:48:52.921091: Current learning rate: 0.00947\n",
      "2025-05-15 14:50:41.753465: train_loss -0.9377\n",
      "2025-05-15 14:50:41.753717: val_loss -0.9566\n",
      "2025-05-15 14:50:41.753763: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 14:50:41.753798: Epoch time: 108.83 s\n",
      "2025-05-15 14:50:41.753821: Yayy! New best EMA pseudo Dice: 0.9785000085830688\n",
      "2025-05-15 14:50:42.471937: \n",
      "2025-05-15 14:50:42.472112: Epoch 60\n",
      "2025-05-15 14:50:42.472185: Current learning rate: 0.00946\n",
      "2025-05-15 14:52:31.313029: train_loss -0.9366\n",
      "2025-05-15 14:52:31.313224: val_loss -0.9538\n",
      "2025-05-15 14:52:31.313302: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 14:52:31.313346: Epoch time: 108.84 s\n",
      "2025-05-15 14:52:31.313370: Yayy! New best EMA pseudo Dice: 0.9789000153541565\n",
      "2025-05-15 14:52:32.040523: \n",
      "2025-05-15 14:52:32.040788: Epoch 61\n",
      "2025-05-15 14:52:32.040891: Current learning rate: 0.00945\n",
      "2025-05-15 14:54:20.958063: train_loss -0.9331\n",
      "2025-05-15 14:54:20.958234: val_loss -0.9533\n",
      "2025-05-15 14:54:20.958267: Pseudo dice [np.float32(0.9812)]\n",
      "Epoch time: 108.92 s958310: \n",
      "2025-05-15 14:54:20.958394: Yayy! New best EMA pseudo Dice: 0.9790999889373779\n",
      "2025-05-15 14:54:21.675113: \n",
      "2025-05-15 14:54:21.675277: Epoch 62\n",
      "2025-05-15 14:54:21.675357: Current learning rate: 0.00944\n",
      "2025-05-15 14:56:10.545502: train_loss -0.9352\n",
      "2025-05-15 14:56:10.545693: val_loss -0.9546\n",
      "2025-05-15 14:56:10.545728: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 14:56:10.545768: Epoch time: 108.87 s\n",
      "2025-05-15 14:56:10.545788: Yayy! New best EMA pseudo Dice: 0.9793999791145325\n",
      "2025-05-15 14:56:11.263804: \n",
      "2025-05-15 14:56:11.263964: Epoch 63\n",
      "2025-05-15 14:56:11.264035: Current learning rate: 0.00943\n",
      "2025-05-15 14:58:00.160954: train_loss -0.8865\n",
      "2025-05-15 14:58:00.161075: val_loss -0.9305\n",
      "2025-05-15 14:58:00.161110: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-15 14:58:00.161143: Epoch time: 108.9 s\n",
      "2025-05-15 14:58:00.852599: \n",
      "2025-05-15 14:58:00.852818: Epoch 64\n",
      "2025-05-15 14:58:00.853110: Current learning rate: 0.00942\n",
      "2025-05-15 14:59:49.807961: train_loss -0.9053\n",
      "2025-05-15 14:59:49.808167: val_loss -0.9525\n",
      "2025-05-15 14:59:49.808203: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 14:59:49.808268: Epoch time: 108.96 s\n",
      "2025-05-15 14:59:50.316125: \n",
      "2025-05-15 14:59:50.316219: Epoch 65\n",
      "2025-05-15 14:59:50.316284: Current learning rate: 0.00941\n",
      "2025-05-15 15:01:39.181546: train_loss -0.9044\n",
      "2025-05-15 15:01:39.181729: val_loss -0.9318\n",
      "2025-05-15 15:01:39.181875: Pseudo dice [np.float32(0.9716)]\n",
      "2025-05-15 15:01:39.182016: Epoch time: 108.87 s\n",
      "2025-05-15 15:01:39.697957: \n",
      "2025-05-15 15:01:39.698146: Epoch 66\n",
      "2025-05-15 15:01:39.698224: Current learning rate: 0.0094\n",
      "2025-05-15 15:03:28.565645: train_loss -0.9078\n",
      "2025-05-15 15:03:28.565854: val_loss -0.9515\n",
      "2025-05-15 15:03:28.565886: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 15:03:28.565919: Epoch time: 108.87 s\n",
      "2025-05-15 15:03:29.081540: \n",
      "2025-05-15 15:03:29.081644: Epoch 67\n",
      "2025-05-15 15:03:29.081709: Current learning rate: 0.00939\n",
      "2025-05-15 15:05:18.085607: train_loss -0.9187\n",
      "2025-05-15 15:05:18.085724: val_loss -0.9576\n",
      "2025-05-15 15:05:18.085758: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 15:05:18.085790: Epoch time: 109.0 s\n",
      "2025-05-15 15:05:18.605469: \n",
      "2025-05-15 15:05:18.605568: Epoch 68\n",
      "2025-05-15 15:05:18.605635: Current learning rate: 0.00939\n",
      "2025-05-15 15:07:07.515080: train_loss -0.9272\n",
      "2025-05-15 15:07:07.515255: val_loss -0.955\n",
      "2025-05-15 15:07:07.515346: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 15:07:07.515391: Epoch time: 108.91 s\n",
      "2025-05-15 15:07:07.515414: Yayy! New best EMA pseudo Dice: 0.9796000123023987\n",
      "2025-05-15 15:07:08.240854: \n",
      "2025-05-15 15:07:08.240955: Epoch 69\n",
      "2025-05-15 15:07:08.241025: Current learning rate: 0.00938\n",
      "2025-05-15 15:08:57.071079: train_loss -0.9295\n",
      "2025-05-15 15:08:57.071409: val_loss -0.9544\n",
      "2025-05-15 15:08:57.071487: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 15:08:57.071529: Epoch time: 108.83 s\n",
      "2025-05-15 15:08:57.071552: Yayy! New best EMA pseudo Dice: 0.9797999858856201\n",
      "2025-05-15 15:08:57.805481: \n",
      "2025-05-15 15:08:57.805592: Epoch 70\n",
      "2025-05-15 15:08:57.805728: Current learning rate: 0.00937\n",
      "2025-05-15 15:10:46.809152: train_loss -0.9365\n",
      "2025-05-15 15:10:46.809275: val_loss -0.9561\n",
      "2025-05-15 15:10:46.809309: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 15:10:46.809341: Epoch time: 109.0 s\n",
      "2025-05-15 15:10:46.809371: Yayy! New best EMA pseudo Dice: 0.9800999760627747\n",
      "2025-05-15 15:10:47.533733: \n",
      "2025-05-15 15:10:47.533929: Epoch 71\n",
      "2025-05-15 15:10:47.534024: Current learning rate: 0.00936\n",
      "2025-05-15 15:12:36.454890: train_loss -0.9347\n",
      "2025-05-15 15:12:36.455009: val_loss -0.9479\n",
      "2025-05-15 15:12:36.455042: Pseudo dice [np.float32(0.9797)]\n",
      "2025-05-15 15:12:36.455075: Epoch time: 108.92 s\n",
      "2025-05-15 15:12:36.969823: \n",
      "2025-05-15 15:12:36.969912: Epoch 72\n",
      "2025-05-15 15:12:36.969987: Current learning rate: 0.00935\n",
      "2025-05-15 15:14:25.820709: train_loss -0.9198\n",
      "2025-05-15 15:14:25.821028: val_loss -0.9486\n",
      "2025-05-15 15:14:25.821076: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-15 15:14:25.821151: Epoch time: 108.85 s\n",
      "2025-05-15 15:14:25.821221: Yayy! New best EMA pseudo Dice: 0.9800999760627747\n",
      "2025-05-15 15:14:26.542116: \n",
      "2025-05-15 15:14:26.542201: Epoch 73\n",
      "2025-05-15 15:14:26.542268: Current learning rate: 0.00934\n",
      "2025-05-15 15:16:15.470955: train_loss -0.9058\n",
      "2025-05-15 15:16:15.471081: val_loss -0.9516\n",
      "2025-05-15 15:16:15.471116: Pseudo dice [np.float32(0.9813)]\n",
      "2025-05-15 15:16:15.471151: Epoch time: 108.93 s\n",
      "2025-05-15 15:16:15.471177: Yayy! New best EMA pseudo Dice: 0.9801999926567078\n",
      "2025-05-15 15:16:16.199274: \n",
      "2025-05-15 15:16:16.199354: Epoch 74\n",
      "2025-05-15 15:16:16.199471: Current learning rate: 0.00933\n",
      "2025-05-15 15:18:05.082551: train_loss -0.9071\n",
      "2025-05-15 15:18:05.082673: val_loss -0.9371\n",
      "2025-05-15 15:18:05.082708: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-15 15:18:05.082741: Epoch time: 108.88 s\n",
      "2025-05-15 15:18:05.604936: \n",
      "2025-05-15 15:18:05.605167: Epoch 75\n",
      "2025-05-15 15:18:05.605243: Current learning rate: 0.00932\n",
      "2025-05-15 15:19:54.447669: train_loss -0.9236\n",
      "2025-05-15 15:19:54.447798: val_loss -0.9562\n",
      "2025-05-15 15:19:54.447844: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-15 15:19:54.447879: Epoch time: 108.84 s\n",
      "2025-05-15 15:19:55.141398: \n",
      "2025-05-15 15:19:55.141516: Epoch 76\n",
      "2025-05-15 15:19:55.141582: Current learning rate: 0.00931\n",
      "2025-05-15 15:21:43.977935: train_loss -0.91\n",
      "2025-05-15 15:21:43.978111: val_loss -0.914\n",
      "2025-05-15 15:21:43.978144: Pseudo dice [np.float32(0.9641)]\n",
      "2025-05-15 15:21:43.978178: Epoch time: 108.84 s\n",
      "2025-05-15 15:21:44.506709: \n",
      "2025-05-15 15:21:44.506803: Epoch 77\n",
      "2025-05-15 15:21:44.506962: Current learning rate: 0.0093\n",
      "2025-05-15 15:23:33.500410: train_loss -0.8784\n",
      "2025-05-15 15:23:33.500660: val_loss -0.9424\n",
      "2025-05-15 15:23:33.500721: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-15 15:23:33.500761: Epoch time: 108.99 s\n",
      "2025-05-15 15:23:34.023310: \n",
      "2025-05-15 15:23:34.023658: Epoch 78\n",
      "2025-05-15 15:23:34.023777: Current learning rate: 0.0093\n",
      "2025-05-15 15:25:22.936928: train_loss -0.9097\n",
      "2025-05-15 15:25:22.937056: val_loss -0.9361\n",
      "2025-05-15 15:25:22.937091: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-15 15:25:22.937124: Epoch time: 108.91 s\n",
      "2025-05-15 15:25:23.456453: \n",
      "2025-05-15 15:25:23.456590: Epoch 79\n",
      "2025-05-15 15:25:23.456681: Current learning rate: 0.00929\n",
      "2025-05-15 15:27:12.418523: train_loss -0.9161\n",
      "2025-05-15 15:27:12.418699: val_loss -0.956\n",
      "2025-05-15 15:27:12.418734: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 15:27:12.418767: Epoch time: 108.96 s\n",
      "2025-05-15 15:27:12.942472: \n",
      "2025-05-15 15:27:12.942620: Epoch 80\n",
      "2025-05-15 15:27:12.942739: Current learning rate: 0.00928\n",
      "2025-05-15 15:29:01.986443: train_loss -0.9326\n",
      "2025-05-15 15:29:01.986645: val_loss -0.9594\n",
      "2025-05-15 15:29:01.986681: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 15:29:01.986717: Epoch time: 109.04 s\n",
      "2025-05-15 15:29:02.514479: \n",
      "2025-05-15 15:29:02.514585: Epoch 81\n",
      "2025-05-15 15:29:02.514650: Current learning rate: 0.00927\n",
      "2025-05-15 15:30:51.519261: train_loss -0.9362\n",
      "2025-05-15 15:30:51.519392: val_loss -0.9531\n",
      "2025-05-15 15:30:51.519426: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 15:30:51.519459: Epoch time: 109.01 s\n",
      "2025-05-15 15:30:52.045346: \n",
      "2025-05-15 15:30:52.045434: Epoch 82\n",
      "2025-05-15 15:30:52.045512: Current learning rate: 0.00926\n",
      "2025-05-15 15:32:40.889349: train_loss -0.9367\n",
      "2025-05-15 15:32:40.889475: val_loss -0.9569\n",
      "2025-05-15 15:32:40.889511: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 15:32:40.889544: Epoch time: 108.84 s\n",
      "2025-05-15 15:32:41.397417: \n",
      "2025-05-15 15:32:41.397562: Epoch 83\n",
      "2025-05-15 15:32:41.397630: Current learning rate: 0.00925\n",
      "2025-05-15 15:34:30.270104: train_loss -0.9338\n",
      "2025-05-15 15:34:30.270238: val_loss -0.9516\n",
      "2025-05-15 15:34:30.270272: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-15 15:34:30.270304: Epoch time: 108.87 s\n",
      "2025-05-15 15:34:30.777155: \n",
      "2025-05-15 15:34:30.777241: Epoch 84\n",
      "2025-05-15 15:34:30.777306: Current learning rate: 0.00924\n",
      "2025-05-15 15:36:19.667250: train_loss -0.9259\n",
      "2025-05-15 15:36:19.667371: val_loss -0.9526\n",
      "2025-05-15 15:36:19.667402: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 15:36:19.667436: Epoch time: 108.89 s\n",
      "2025-05-15 15:36:20.166287: \n",
      "2025-05-15 15:36:20.166463: Epoch 85\n",
      "Current learning rate: 0.00923\n",
      "2025-05-15 15:38:09.130759: train_loss -0.9283\n",
      "2025-05-15 15:38:09.130890: val_loss -0.9494\n",
      "2025-05-15 15:38:09.130924: Pseudo dice [np.float32(0.9797)]\n",
      "2025-05-15 15:38:09.130956: Epoch time: 108.96 s\n",
      "2025-05-15 15:38:09.644988: \n",
      "2025-05-15 15:38:09.645069: Epoch 86\n",
      "2025-05-15 15:38:09.645136: Current learning rate: 0.00922\n",
      "2025-05-15 15:39:58.482606: train_loss -0.9169\n",
      "2025-05-15 15:39:58.482740: val_loss -0.9508\n",
      "2025-05-15 15:39:58.482778: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-15 15:39:58.482811: Epoch time: 108.84 s\n",
      "2025-05-15 15:39:58.985201: \n",
      "2025-05-15 15:39:58.985282: Epoch 87\n",
      "2025-05-15 15:39:58.985350: Current learning rate: 0.00921\n",
      "2025-05-15 15:41:47.876582: train_loss -0.9282\n",
      "2025-05-15 15:41:47.876706: val_loss -0.9502\n",
      "2025-05-15 15:41:47.876741: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-15 15:41:47.876774: Epoch time: 108.89 s\n",
      "2025-05-15 15:41:48.571369: \n",
      "2025-05-15 15:41:48.571504: Epoch 88\n",
      "2025-05-15 15:41:48.571605: Current learning rate: 0.0092\n",
      "2025-05-15 15:43:37.508759: train_loss -0.9372\n",
      "2025-05-15 15:43:37.508879: val_loss -0.9544\n",
      "2025-05-15 15:43:37.508914: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 15:43:37.508946: Epoch time: 108.94 s\n",
      "2025-05-15 15:43:38.013438: \n",
      "2025-05-15 15:43:38.013588: Epoch 89\n",
      "2025-05-15 15:43:38.013655: Current learning rate: 0.0092\n",
      "2025-05-15 15:45:26.871495: train_loss -0.9311\n",
      "2025-05-15 15:45:26.871619: val_loss -0.9511\n",
      "2025-05-15 15:45:26.871655: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-15 15:45:26.871689: Epoch time: 108.86 s\n",
      "2025-05-15 15:45:27.380471: \n",
      "2025-05-15 15:45:27.380616: Epoch 90\n",
      "2025-05-15 15:45:27.380683: Current learning rate: 0.00919\n",
      "2025-05-15 15:47:16.314829: train_loss -0.9348\n",
      "2025-05-15 15:47:16.315025: val_loss -0.9573\n",
      "2025-05-15 15:47:16.315060: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 15:47:16.315093: Epoch time: 108.93 s\n",
      "2025-05-15 15:47:16.821846: \n",
      "2025-05-15 15:47:16.822126: Epoch 91\n",
      "2025-05-15 15:47:16.822204: Current learning rate: 0.00918\n",
      "2025-05-15 15:49:05.906019: train_loss -0.9363\n",
      "2025-05-15 15:49:05.906218: val_loss -0.9564\n",
      "2025-05-15 15:49:05.906260: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 15:49:05.906293: Epoch time: 109.08 s\n",
      "2025-05-15 15:49:05.906315: Yayy! New best EMA pseudo Dice: 0.980400025844574\n",
      "2025-05-15 15:49:06.617536: \n",
      "2025-05-15 15:49:06.617621: Epoch 92\n",
      "2025-05-15 15:49:06.617688: Current learning rate: 0.00917\n",
      "2025-05-15 15:50:55.450372: train_loss -0.9364\n",
      "2025-05-15 15:50:55.450573: val_loss -0.9523\n",
      "2025-05-15 15:50:55.450608: Pseudo dice [np.float32(0.9803)]\n",
      "2025-05-15 15:50:55.450641: Epoch time: 108.83 s\n",
      "2025-05-15 15:50:55.950472: \n",
      "2025-05-15 15:50:55.950655: Epoch 93\n",
      "2025-05-15 15:50:55.950754: Current learning rate: 0.00916\n",
      "2025-05-15 15:52:44.965822: train_loss -0.9354\n",
      "2025-05-15 15:52:44.966037: val_loss -0.9384\n",
      "2025-05-15 15:52:44.966073: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-15 15:52:44.966108: Epoch time: 109.02 s\n",
      "2025-05-15 15:52:45.470337: \n",
      "2025-05-15 15:52:45.470512: Epoch 94\n",
      "2025-05-15 15:52:45.470582: Current learning rate: 0.00915\n",
      "2025-05-15 15:54:34.351247: train_loss -0.9322\n",
      "2025-05-15 15:54:34.351427: val_loss -0.9586\n",
      "2025-05-15 15:54:34.351461: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 15:54:34.351494: Epoch time: 108.88 s\n",
      "2025-05-15 15:54:34.850602: \n",
      "2025-05-15 15:54:34.850776: Epoch 95\n",
      "2025-05-15 15:54:34.850875: Current learning rate: 0.00914\n",
      "2025-05-15 15:56:23.683669: train_loss -0.9371\n",
      "2025-05-15 15:56:23.683812: val_loss -0.9587\n",
      "2025-05-15 15:56:23.683851: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 15:56:23.683887: Epoch time: 108.83 s\n",
      "2025-05-15 15:56:23.683910: Yayy! New best EMA pseudo Dice: 0.9804999828338623\n",
      "2025-05-15 15:56:24.393543: \n",
      "2025-05-15 15:56:24.393693: Epoch 96\n",
      "2025-05-15 15:56:24.393766: Current learning rate: 0.00913\n",
      "2025-05-15 15:58:13.353099: train_loss -0.9431\n",
      "2025-05-15 15:58:13.353269: val_loss -0.9615\n",
      "2025-05-15 15:58:13.353300: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 15:58:13.353332: Epoch time: 108.96 s\n",
      "2025-05-15 15:58:13.353416: Yayy! New best EMA pseudo Dice: 0.98089998960495\n",
      "2025-05-15 15:58:14.064219: \n",
      "2025-05-15 15:58:14.064471: Epoch 97\n",
      "2025-05-15 15:58:14.064581: Current learning rate: 0.00912\n",
      "2025-05-15 16:00:02.958074: train_loss -0.9301\n",
      "2025-05-15 16:00:02.958208: val_loss -0.9496\n",
      "2025-05-15 16:00:02.958246: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-15 16:00:02.958304: Epoch time: 108.89 s\n",
      "2025-05-15 16:00:03.457805: \n",
      "2025-05-15 16:00:03.457892: Epoch 98\n",
      "2025-05-15 16:00:03.457959: Current learning rate: 0.00911\n",
      "2025-05-15 16:01:52.386611: train_loss -0.928\n",
      "2025-05-15 16:01:52.386826: val_loss -0.9428\n",
      "2025-05-15 16:01:52.386862: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-15 16:01:52.386896: Epoch time: 108.93 s\n",
      "2025-05-15 16:01:52.890897: \n",
      "2025-05-15 16:01:52.891118: Epoch 99\n",
      "2025-05-15 16:01:52.891419: Current learning rate: 0.0091\n",
      "2025-05-15 16:03:41.863765: train_loss -0.9394\n",
      "2025-05-15 16:03:41.863916: val_loss -0.9609\n",
      "2025-05-15 16:03:41.863957: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 16:03:41.864063: Epoch time: 108.97 s\n",
      "2025-05-15 16:03:42.574913: \n",
      "2025-05-15 16:03:42.575007: Epoch 100\n",
      "2025-05-15 16:03:42.575082: Current learning rate: 0.0091\n",
      "2025-05-15 16:05:31.496840: train_loss -0.9431\n",
      "2025-05-15 16:05:31.496959: val_loss -0.9611\n",
      "2025-05-15 16:05:31.496998: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-15 16:05:31.497036: Epoch time: 108.92 s\n",
      "2025-05-15 16:05:31.497058: Yayy! New best EMA pseudo Dice: 0.9811999797821045\n",
      "2025-05-15 16:05:32.409386: \n",
      "2025-05-15 16:05:32.409709: Epoch 101\n",
      "2025-05-15 16:05:32.409795: Current learning rate: 0.00909\n",
      "2025-05-15 16:07:21.348558: train_loss -0.9392\n",
      "2025-05-15 16:07:21.348681: val_loss -0.955\n",
      "2025-05-15 16:07:21.348715: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 16:07:21.348746: Epoch time: 108.94 s\n",
      "2025-05-15 16:07:21.348770: Yayy! New best EMA pseudo Dice: 0.9812999963760376\n",
      "2025-05-15 16:07:22.075278: \n",
      "2025-05-15 16:07:22.075392: Epoch 102\n",
      "2025-05-15 16:07:22.075458: Current learning rate: 0.00908\n",
      "2025-05-15 16:09:10.941600: train_loss -0.9396\n",
      "2025-05-15 16:09:10.941722: val_loss -0.9458\n",
      "2025-05-15 16:09:10.941757: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-15 16:09:10.941788: Epoch time: 108.87 s\n",
      "2025-05-15 16:09:11.445464: \n",
      "2025-05-15 16:09:11.445688: Epoch 103\n",
      "2025-05-15 16:09:11.446117: Current learning rate: 0.00907\n",
      "2025-05-15 16:11:00.400901: train_loss -0.9033\n",
      "2025-05-15 16:11:00.401088: val_loss -0.9446\n",
      "2025-05-15 16:11:00.401132: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-15 16:11:00.401171: Epoch time: 108.96 s\n",
      "2025-05-15 16:11:00.901693: \n",
      "2025-05-15 16:11:00.901841: Epoch 104\n",
      "2025-05-15 16:11:00.901918: Current learning rate: 0.00906\n",
      "2025-05-15 16:12:49.806153: train_loss -0.9178\n",
      "2025-05-15 16:12:49.806274: val_loss -0.9519\n",
      "2025-05-15 16:12:49.806308: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 16:12:49.806341: Epoch time: 108.9 s\n",
      "2025-05-15 16:12:50.309890: \n",
      "2025-05-15 16:12:50.310086: Epoch 105\n",
      "2025-05-15 16:12:50.310163: Current learning rate: 0.00905\n",
      "2025-05-15 16:14:39.227103: train_loss -0.9113\n",
      "2025-05-15 16:14:39.227227: val_loss -0.9424\n",
      "2025-05-15 16:14:39.227262: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-15 16:14:39.227296: Epoch time: 108.92 s\n",
      "2025-05-15 16:14:39.734776: \n",
      "2025-05-15 16:14:39.734984: Epoch 106\n",
      "2025-05-15 16:14:39.735060: Current learning rate: 0.00904\n",
      "2025-05-15 16:16:28.605069: train_loss -0.9291\n",
      "2025-05-15 16:16:28.605192: val_loss -0.9601\n",
      "2025-05-15 16:16:28.605227: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 16:16:28.605263: Epoch time: 108.87 s\n",
      "2025-05-15 16:16:29.105491: \n",
      "2025-05-15 16:16:29.105876: Epoch 107\n",
      "2025-05-15 16:16:29.105956: Current learning rate: 0.00903\n",
      "2025-05-15 16:18:18.042286: train_loss -0.9369\n",
      "2025-05-15 16:18:18.042410: val_loss -0.9525\n",
      "2025-05-15 16:18:18.042444: Pseudo dice [np.float32(0.98)]\n",
      "2025-05-15 16:18:18.042515: Epoch time: 108.94 s\n",
      "2025-05-15 16:18:18.548151: \n",
      "2025-05-15 16:18:18.548234: Epoch 108\n",
      "2025-05-15 16:18:18.548316: Current learning rate: 0.00902\n",
      "2025-05-15 16:20:07.419424: train_loss -0.9297\n",
      "2025-05-15 16:20:07.419549: val_loss -0.9553\n",
      "2025-05-15 16:20:07.419679: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 16:20:07.419829: Epoch time: 108.87 s\n",
      "2025-05-15 16:20:07.929669: \n",
      "2025-05-15 16:20:07.929875: Epoch 109\n",
      "2025-05-15 16:20:07.930020: Current learning rate: 0.00901\n",
      "2025-05-15 16:21:56.883073: train_loss -0.938\n",
      "2025-05-15 16:21:56.883193: val_loss -0.9565\n",
      "2025-05-15 16:21:56.883227: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 16:21:56.883263: Epoch time: 108.95 s\n",
      "2025-05-15 16:21:57.389230: \n",
      "2025-05-15 16:21:57.389319: Epoch 110\n",
      "2025-05-15 16:21:57.389384: Current learning rate: 0.009\n",
      "2025-05-15 16:23:46.241305: train_loss -0.9405\n",
      "2025-05-15 16:23:46.241459: val_loss -0.9594\n",
      "2025-05-15 16:23:46.241501: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 16:23:46.241536: Epoch time: 108.85 s\n",
      "2025-05-15 16:23:46.241704: Yayy! New best EMA pseudo Dice: 0.9815000295639038\n",
      "2025-05-15 16:23:46.967288: \n",
      "2025-05-15 16:23:46.967448: Epoch 111\n",
      "2025-05-15 16:23:46.967549: Current learning rate: 0.009\n",
      "2025-05-15 16:25:35.976852: train_loss -0.9398\n",
      "2025-05-15 16:25:35.977090: val_loss -0.9573\n",
      "2025-05-15 16:25:35.977151: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 16:25:35.977194: Epoch time: 109.01 s\n",
      "2025-05-15 16:25:35.977217: Yayy! New best EMA pseudo Dice: 0.9815000295639038\n",
      "2025-05-15 16:25:36.690352: \n",
      "2025-05-15 16:25:36.690583: Epoch 112\n",
      "2025-05-15 16:25:36.690651: Current learning rate: 0.00899\n",
      "2025-05-15 16:27:25.540798: train_loss -0.94\n",
      "2025-05-15 16:27:25.540921: val_loss -0.9601\n",
      "2025-05-15 16:27:25.540956: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 16:27:25.540990: Epoch time: 108.85 s\n",
      "2025-05-15 16:27:25.541013: Yayy! New best EMA pseudo Dice: 0.9818999767303467\n",
      "2025-05-15 16:27:26.253007: \n",
      "2025-05-15 16:27:26.253322: Epoch 113\n",
      "2025-05-15 16:27:26.253408: Current learning rate: 0.00898\n",
      "2025-05-15 16:29:15.149257: train_loss -0.9401\n",
      "2025-05-15 16:29:15.149382: val_loss -0.9643\n",
      "2025-05-15 16:29:15.149418: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-15 16:29:15.149453: Epoch time: 108.9 s\n",
      "2025-05-15 16:29:15.149474: Yayy! New best EMA pseudo Dice: 0.9821000099182129\n",
      "2025-05-15 16:29:16.059458: \n",
      "2025-05-15 16:29:16.059549: Epoch 114\n",
      "2025-05-15 16:29:16.059614: Current learning rate: 0.00897\n",
      "2025-05-15 16:31:05.050322: train_loss -0.9451\n",
      "2025-05-15 16:31:05.050442: val_loss -0.9592\n",
      "2025-05-15 16:31:05.050477: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 16:31:05.050515: Epoch time: 108.99 s\n",
      "2025-05-15 16:31:05.050543: Yayy! New best EMA pseudo Dice: 0.982200026512146\n",
      "2025-05-15 16:31:05.774818: \n",
      "2025-05-15 16:31:05.774987: Epoch 115\n",
      "2025-05-15 16:31:05.775063: Current learning rate: 0.00896\n",
      "2025-05-15 16:32:54.616709: train_loss -0.9432\n",
      "2025-05-15 16:32:54.616871: val_loss -0.963\n",
      "2025-05-15 16:32:54.616905: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-15 16:32:54.616938: Epoch time: 108.84 s\n",
      "2025-05-15 16:32:54.616958: Yayy! New best EMA pseudo Dice: 0.9824000000953674\n",
      "2025-05-15 16:32:55.343693: \n",
      "2025-05-15 16:32:55.343786: Epoch 116\n",
      "2025-05-15 16:32:55.343849: Current learning rate: 0.00895\n",
      "2025-05-15 16:34:44.289180: train_loss -0.9414\n",
      "2025-05-15 16:34:44.289380: val_loss -0.9588\n",
      "2025-05-15 16:34:44.289422: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 16:34:44.289457: Epoch time: 108.95 s\n",
      "2025-05-15 16:34:44.289478: Yayy! New best EMA pseudo Dice: 0.9824000000953674\n",
      "2025-05-15 16:34:45.005885: \n",
      "2025-05-15 16:34:45.005995: Epoch 117\n",
      "2025-05-15 16:34:45.006061: Current learning rate: 0.00894\n",
      "2025-05-15 16:36:33.886921: train_loss -0.9459\n",
      "2025-05-15 16:36:33.887068: val_loss -0.9591\n",
      "2025-05-15 16:36:33.887114: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 16:36:33.887152: Epoch time: 108.88 s\n",
      "2025-05-15 16:36:33.887191: Yayy! New best EMA pseudo Dice: 0.9825000166893005\n",
      "2025-05-15 16:36:34.613277: \n",
      "2025-05-15 16:36:34.613395: Epoch 118\n",
      "2025-05-15 16:36:34.613465: Current learning rate: 0.00893\n",
      "2025-05-15 16:38:23.485104: train_loss -0.9417\n",
      "2025-05-15 16:38:23.485278: val_loss -0.9606\n",
      "2025-05-15 16:38:23.485313: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-15 16:38:23.485347: Epoch time: 108.87 s\n",
      "2025-05-15 16:38:23.485368: Yayy! New best EMA pseudo Dice: 0.9825999736785889\n",
      "2025-05-15 16:38:24.207976: \n",
      "2025-05-15 16:38:24.208317: Epoch 119\n",
      "2025-05-15 16:38:24.208408: Current learning rate: 0.00892\n",
      "2025-05-15 16:40:13.083609: train_loss -0.9446\n",
      "2025-05-15 16:40:13.083742: val_loss -0.9598\n",
      "2025-05-15 16:40:13.083779: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-15 16:40:13.083815: Epoch time: 108.88 s\n",
      "2025-05-15 16:40:13.083837: Yayy! New best EMA pseudo Dice: 0.982699990272522\n",
      "2025-05-15 16:40:13.814891: \n",
      "2025-05-15 16:40:13.815218: Epoch 120\n",
      "2025-05-15 16:40:13.815357: Current learning rate: 0.00891\n",
      "2025-05-15 16:42:02.704850: train_loss -0.9463\n",
      "2025-05-15 16:42:02.705179: val_loss -0.9612\n",
      "2025-05-15 16:42:02.705240: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-15 16:42:02.705280: Epoch time: 108.89 s\n",
      "2025-05-15 16:42:02.705302: Yayy! New best EMA pseudo Dice: 0.9828000068664551\n",
      "2025-05-15 16:42:03.432573: \n",
      "2025-05-15 16:42:03.432670: Epoch 121\n",
      "2025-05-15 16:42:03.432739: Current learning rate: 0.0089\n",
      "2025-05-15 16:43:52.502237: train_loss -0.9477\n",
      "2025-05-15 16:43:52.502369: val_loss -0.9584\n",
      "2025-05-15 16:43:52.502403: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-15 16:43:52.502437: Epoch time: 109.07 s\n",
      "2025-05-15 16:43:53.010379: \n",
      "2025-05-15 16:43:53.010532: Epoch 122\n",
      "2025-05-15 16:43:53.010616: Current learning rate: 0.00889\n",
      "2025-05-15 16:45:41.911374: train_loss -0.9469\n",
      "2025-05-15 16:45:41.911486: val_loss -0.9561\n",
      "2025-05-15 16:45:41.911572: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-15 16:45:41.911658: Epoch time: 108.9 s\n",
      "2025-05-15 16:45:42.427152: \n",
      "2025-05-15 16:45:42.427362: Epoch 123\n",
      "2025-05-15 16:45:42.427448: Current learning rate: 0.00889\n",
      "2025-05-15 16:47:31.282332: train_loss -0.9464\n",
      "2025-05-15 16:47:31.282456: val_loss -0.956\n",
      "2025-05-15 16:47:31.282489: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 16:47:31.282524: Epoch time: 108.86 s\n",
      "2025-05-15 16:47:31.795239: \n",
      "2025-05-15 16:47:31.795322: Epoch 124\n",
      "2025-05-15 16:47:31.795390: Current learning rate: 0.00888\n",
      "2025-05-15 16:49:20.704948: train_loss -0.9461\n",
      "2025-05-15 16:49:20.705220: val_loss -0.9593\n",
      "2025-05-15 16:49:20.705309: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 16:49:20.705379: Epoch time: 108.91 s\n",
      "2025-05-15 16:49:21.223215: \n",
      "2025-05-15 16:49:21.223408: Epoch 125\n",
      "2025-05-15 16:49:21.223532: Current learning rate: 0.00887\n",
      "2025-05-15 16:51:10.112873: train_loss -0.9497\n",
      "2025-05-15 16:51:10.113043: val_loss -0.9623\n",
      "2025-05-15 16:51:10.113077: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-15 16:51:10.113109: Epoch time: 108.89 s\n",
      "2025-05-15 16:51:10.802359: \n",
      "2025-05-15 16:51:10.802472: Epoch 126\n",
      "2025-05-15 16:51:10.802536: Current learning rate: 0.00886\n",
      "2025-05-15 16:52:59.679566: train_loss -0.9455\n",
      "2025-05-15 16:52:59.679747: val_loss -0.9509\n",
      "2025-05-15 16:52:59.679781: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-15 16:52:59.679900: Epoch time: 108.88 s\n",
      "2025-05-15 16:53:00.193534: \n",
      "2025-05-15 16:53:00.193751: Epoch 127\n",
      "2025-05-15 16:53:00.193887: Current learning rate: 0.00885\n",
      "2025-05-15 16:54:49.112387: train_loss -0.9467\n",
      "2025-05-15 16:54:49.112510: val_loss -0.9625\n",
      "2025-05-15 16:54:49.112546: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-15 16:54:49.112580: Epoch time: 108.92 s\n",
      "2025-05-15 16:54:49.622334: \n",
      "2025-05-15 16:54:49.622422: Epoch 128\n",
      "2025-05-15 16:54:49.622488: Current learning rate: 0.00884\n",
      "2025-05-15 16:56:38.458223: train_loss -0.9505\n",
      "2025-05-15 16:56:38.458354: val_loss -0.9649\n",
      "2025-05-15 16:56:38.458392: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-15 16:56:38.458425: Epoch time: 108.84 s\n",
      "2025-05-15 16:56:38.458446: Yayy! New best EMA pseudo Dice: 0.9829000234603882\n",
      "2025-05-15 16:56:39.180482: \n",
      "2025-05-15 16:56:39.180582: Epoch 129\n",
      "2025-05-15 16:56:39.180650: Current learning rate: 0.00883\n",
      "2025-05-15 16:58:28.038612: train_loss -0.9501\n",
      "2025-05-15 16:58:28.038834: val_loss -0.9605\n",
      "2025-05-15 16:58:28.038977: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 16:58:28.039050: Epoch time: 108.86 s\n",
      "2025-05-15 16:58:28.039079: Yayy! New best EMA pseudo Dice: 0.9829000234603882\n",
      "2025-05-15 16:58:28.755467: \n",
      "2025-05-15 16:58:28.755578: Epoch 130\n",
      "2025-05-15 16:58:28.755709: Current learning rate: 0.00882\n",
      "2025-05-15 17:00:17.699780: train_loss -0.9482\n",
      "2025-05-15 17:00:17.699915: val_loss -0.9563\n",
      "2025-05-15 17:00:17.699947: Pseudo dice [np.float32(0.9817)]\n",
      "Epoch time: 108.94 s699988: \n",
      "2025-05-15 17:00:18.219035: \n",
      "2025-05-15 17:00:18.219143: Epoch 131\n",
      "2025-05-15 17:00:18.219209: Current learning rate: 0.00881\n",
      "2025-05-15 17:02:07.167212: train_loss -0.9486\n",
      "2025-05-15 17:02:07.167370: val_loss -0.9595\n",
      "2025-05-15 17:02:07.167405: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 17:02:07.167450: Epoch time: 108.95 s\n",
      "2025-05-15 17:02:07.676670: \n",
      "2025-05-15 17:02:07.676883: Epoch 132\n",
      "2025-05-15 17:02:07.676957: Current learning rate: 0.0088\n",
      "2025-05-15 17:03:56.530895: train_loss -0.9486\n",
      "2025-05-15 17:03:56.531057: val_loss -0.9648\n",
      "2025-05-15 17:03:56.531160: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-15 17:03:56.531270: Epoch time: 108.85 s\n",
      "2025-05-15 17:03:56.531332: Yayy! New best EMA pseudo Dice: 0.9829999804496765\n",
      "2025-05-15 17:03:57.264819: \n",
      "2025-05-15 17:03:57.264957: Epoch 133\n",
      "2025-05-15 17:03:57.265025: Current learning rate: 0.00879\n",
      "2025-05-15 17:05:46.155213: train_loss -0.9508\n",
      "2025-05-15 17:05:46.155407: val_loss -0.9648\n",
      "2025-05-15 17:05:46.155498: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-15 17:05:46.155581: Epoch time: 108.89 s\n",
      "2025-05-15 17:05:46.155684: Yayy! New best EMA pseudo Dice: 0.9832000136375427\n",
      "2025-05-15 17:05:46.885166: \n",
      "2025-05-15 17:05:46.885343: Epoch 134\n",
      "2025-05-15 17:05:46.885412: Current learning rate: 0.00879\n",
      "2025-05-15 17:07:35.959090: train_loss -0.9484\n",
      "2025-05-15 17:07:35.959211: val_loss -0.9652\n",
      "2025-05-15 17:07:35.959244: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-15 17:07:35.959277: Epoch time: 109.07 s\n",
      "2025-05-15 17:07:35.959302: Yayy! New best EMA pseudo Dice: 0.9835000038146973\n",
      "2025-05-15 17:07:36.689296: \n",
      "2025-05-15 17:07:36.689429: Epoch 135\n",
      "2025-05-15 17:07:36.689500: Current learning rate: 0.00878\n",
      "2025-05-15 17:09:26.210115: train_loss -0.9489\n",
      "2025-05-15 17:09:26.210284: val_loss -0.9642\n",
      "2025-05-15 17:09:26.210319: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-15 17:09:26.210353: Epoch time: 109.52 s\n",
      "2025-05-15 17:09:26.210373: Yayy! New best EMA pseudo Dice: 0.9836000204086304\n",
      "2025-05-15 17:09:26.939811: \n",
      "2025-05-15 17:09:26.939900: Epoch 136\n",
      "2025-05-15 17:09:26.939966: Current learning rate: 0.00877\n",
      "2025-05-15 17:11:16.100550: train_loss -0.9428\n",
      "2025-05-15 17:11:16.100749: val_loss -0.9535\n",
      "2025-05-15 17:11:16.100822: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-15 17:11:16.100870: Epoch time: 109.16 s\n",
      "2025-05-15 17:11:16.622665: \n",
      "2025-05-15 17:11:16.623055: Epoch 137\n",
      "2025-05-15 17:11:16.623129: Current learning rate: 0.00876\n",
      "2025-05-15 17:13:05.821289: train_loss -0.9356\n",
      "2025-05-15 17:13:05.821442: val_loss -0.9589\n",
      "2025-05-15 17:13:05.821636: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 17:13:05.821718: Epoch time: 109.2 s\n",
      "2025-05-15 17:13:06.534859: \n",
      "2025-05-15 17:13:06.535017: Epoch 138\n",
      "2025-05-15 17:13:06.535128: Current learning rate: 0.00875\n",
      "2025-05-15 17:14:55.753356: train_loss -0.9378\n",
      "2025-05-15 17:14:55.753488: val_loss -0.9514\n",
      "2025-05-15 17:14:55.753521: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-15 17:14:55.753566: Epoch time: 109.22 s\n",
      "2025-05-15 17:14:56.273726: \n",
      "2025-05-15 17:14:56.274055: Epoch 139\n",
      "2025-05-15 17:14:56.274260: Current learning rate: 0.00874\n",
      "2025-05-15 17:16:45.469209: train_loss -0.9421\n",
      "2025-05-15 17:16:45.469331: val_loss -0.9592\n",
      "2025-05-15 17:16:45.469362: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 17:16:45.469396: Epoch time: 109.2 s\n",
      "2025-05-15 17:16:45.990158: \n",
      "2025-05-15 17:16:45.990318: Epoch 140\n",
      "2025-05-15 17:16:45.990383: Current learning rate: 0.00873\n",
      "2025-05-15 17:18:35.235590: train_loss -0.9435\n",
      "2025-05-15 17:18:35.235731: val_loss -0.9549\n",
      "2025-05-15 17:18:35.235897: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-15 17:18:35.235962: Epoch time: 109.25 s\n",
      "2025-05-15 17:18:35.756371: \n",
      "2025-05-15 17:18:35.756503: Epoch 141\n",
      "2025-05-15 17:18:35.756583: Current learning rate: 0.00872\n",
      "2025-05-15 17:20:24.541873: train_loss -0.9398\n",
      "2025-05-15 17:20:24.542041: val_loss -0.9547\n",
      "2025-05-15 17:20:24.542074: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-15 17:20:24.542148: Epoch time: 108.79 s\n",
      "2025-05-15 17:20:25.065251: \n",
      "2025-05-15 17:20:25.065415: Epoch 142\n",
      "2025-05-15 17:20:25.065491: Current learning rate: 0.00871\n",
      "2025-05-15 17:22:14.004911: train_loss -0.9426\n",
      "2025-05-15 17:22:14.005036: val_loss -0.9627\n",
      "2025-05-15 17:22:14.005069: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 17:22:14.005103: Epoch time: 108.94 s\n",
      "2025-05-15 17:22:14.524302: \n",
      "2025-05-15 17:22:14.524439: Epoch 143\n",
      "2025-05-15 17:22:14.524505: Current learning rate: 0.0087\n",
      "2025-05-15 17:24:03.349096: train_loss -0.944\n",
      "2025-05-15 17:24:03.349266: val_loss -0.9616\n",
      "2025-05-15 17:24:03.349299: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 17:24:03.349333: Epoch time: 108.83 s\n",
      "2025-05-15 17:24:03.885131: \n",
      "2025-05-15 17:24:03.885307: Epoch 144\n",
      "2025-05-15 17:24:03.885381: Current learning rate: 0.00869\n",
      "2025-05-15 17:25:52.919428: train_loss -0.9473\n",
      "2025-05-15 17:25:52.919596: val_loss -0.9654\n",
      "2025-05-15 17:25:52.919754: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-15 17:25:52.919873: Epoch time: 109.03 s\n",
      "2025-05-15 17:25:53.438705: \n",
      "2025-05-15 17:25:53.438861: Epoch 145\n",
      "2025-05-15 17:25:53.438968: Current learning rate: 0.00868\n",
      "2025-05-15 17:27:42.271958: train_loss -0.9476\n",
      "2025-05-15 17:27:42.272103: val_loss -0.9633\n",
      "2025-05-15 17:27:42.272244: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 17:27:42.272364: Epoch time: 108.83 s\n",
      "2025-05-15 17:27:42.793807: \n",
      "2025-05-15 17:27:42.794302: Epoch 146\n",
      "2025-05-15 17:27:42.794425: Current learning rate: 0.00868\n",
      "2025-05-15 17:29:31.775076: train_loss -0.9482\n",
      "2025-05-15 17:29:31.775511: val_loss -0.9597\n",
      "2025-05-15 17:29:31.775630: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-15 17:29:31.775731: Epoch time: 108.98 s\n",
      "2025-05-15 17:29:32.288964: \n",
      "2025-05-15 17:29:32.289042: Epoch 147\n",
      "2025-05-15 17:29:32.289104: Current learning rate: 0.00867\n",
      "2025-05-15 17:31:21.237789: train_loss -0.9488\n",
      "2025-05-15 17:31:21.237913: val_loss -0.9663\n",
      "2025-05-15 17:31:21.237947: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-15 17:31:21.237980: Epoch time: 108.95 s\n",
      "2025-05-15 17:31:21.762242: \n",
      "2025-05-15 17:31:21.762316: Epoch 148\n",
      "2025-05-15 17:31:21.762381: Current learning rate: 0.00866\n",
      "2025-05-15 17:33:10.622944: train_loss -0.9507\n",
      "2025-05-15 17:33:10.623056: val_loss -0.9646\n",
      "2025-05-15 17:33:10.623087: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 17:33:10.623119: Epoch time: 108.86 s\n",
      "2025-05-15 17:33:11.141571: \n",
      "2025-05-15 17:33:11.141776: Epoch 149\n",
      "2025-05-15 17:33:11.141887: Current learning rate: 0.00865\n",
      "2025-05-15 17:34:59.981365: train_loss -0.9482\n",
      "2025-05-15 17:34:59.981488: val_loss -0.9607\n",
      "2025-05-15 17:34:59.981522: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 17:34:59.981555: Epoch time: 108.84 s\n",
      "2025-05-15 17:35:00.919212: \n",
      "2025-05-15 17:35:00.919298: Epoch 150\n",
      "2025-05-15 17:35:00.919373: Current learning rate: 0.00864\n",
      "2025-05-15 17:36:49.816306: train_loss -0.9495\n",
      "2025-05-15 17:36:49.816544: val_loss -0.9428\n",
      "2025-05-15 17:36:49.816689: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-15 17:36:49.816742: Epoch time: 108.9 s\n",
      "2025-05-15 17:36:50.336167: \n",
      "2025-05-15 17:36:50.336295: Epoch 151\n",
      "2025-05-15 17:36:50.336478: Current learning rate: 0.00863\n",
      "2025-05-15 17:38:39.200759: train_loss -0.947\n",
      "2025-05-15 17:38:39.200942: val_loss -0.9595\n",
      "2025-05-15 17:38:39.200990: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 17:38:39.201026: Epoch time: 108.87 s\n",
      "2025-05-15 17:38:39.722749: \n",
      "2025-05-15 17:38:39.722911: Epoch 152\n",
      "2025-05-15 17:38:39.722988: Current learning rate: 0.00862\n",
      "2025-05-15 17:40:28.646827: train_loss -0.9485\n",
      "2025-05-15 17:40:28.646952: val_loss -0.9617\n",
      "2025-05-15 17:40:28.646984: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 17:40:28.647020: Epoch time: 108.92 s\n",
      "2025-05-15 17:40:29.172301: \n",
      "2025-05-15 17:40:29.172432: Epoch 153\n",
      "2025-05-15 17:40:29.172499: Current learning rate: 0.00861\n",
      "2025-05-15 17:42:17.986831: train_loss -0.9492\n",
      "2025-05-15 17:42:17.987034: val_loss -0.9642\n",
      "2025-05-15 17:42:17.987072: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-15 17:42:17.987108: Epoch time: 108.82 s\n",
      "2025-05-15 17:42:18.516392: \n",
      "2025-05-15 17:42:18.516563: Epoch 154\n",
      "2025-05-15 17:42:18.516650: Current learning rate: 0.0086\n",
      "2025-05-15 17:44:07.384472: train_loss -0.9481\n",
      "2025-05-15 17:44:07.384594: val_loss -0.964\n",
      "2025-05-15 17:44:07.384627: Pseudo dice [np.float32(0.985)]\n",
      "Epoch time: 108.87 s384663: \n",
      "2025-05-15 17:44:07.918439: \n",
      "2025-05-15 17:44:07.918664: Epoch 155\n",
      "2025-05-15 17:44:07.918776: Current learning rate: 0.00859\n",
      "2025-05-15 17:45:56.778365: train_loss -0.9509\n",
      "2025-05-15 17:45:56.778632: val_loss -0.9627\n",
      "2025-05-15 17:45:56.778673: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 17:45:56.778708: Epoch time: 108.86 s\n",
      "2025-05-15 17:45:57.316121: \n",
      "2025-05-15 17:45:57.316215: Epoch 156\n",
      "2025-05-15 17:45:57.316313: Current learning rate: 0.00858\n",
      "2025-05-15 17:47:46.203414: train_loss -0.9489\n",
      "2025-05-15 17:47:46.203530: val_loss -0.9637\n",
      "2025-05-15 17:47:46.203562: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-15 17:47:46.203698: Epoch time: 108.89 s\n",
      "2025-05-15 17:47:46.203753: Yayy! New best EMA pseudo Dice: 0.9836999773979187\n",
      "2025-05-15 17:47:46.944122: \n",
      "2025-05-15 17:47:46.944315: Epoch 157\n",
      "2025-05-15 17:47:46.944396: Current learning rate: 0.00858\n",
      "2025-05-15 17:49:35.917749: train_loss -0.9494\n",
      "2025-05-15 17:49:35.917872: val_loss -0.9632\n",
      "2025-05-15 17:49:35.917906: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 17:49:35.917938: Epoch time: 108.97 s\n",
      "2025-05-15 17:49:35.917960: Yayy! New best EMA pseudo Dice: 0.9837999939918518\n",
      "2025-05-15 17:49:36.654946: \n",
      "2025-05-15 17:49:36.655260: Epoch 158\n",
      "2025-05-15 17:49:36.655416: Current learning rate: 0.00857\n",
      "2025-05-15 17:51:25.638169: train_loss -0.9493\n",
      "2025-05-15 17:51:25.638366: val_loss -0.9609\n",
      "2025-05-15 17:51:25.638414: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 17:51:25.638450: Epoch time: 108.98 s\n",
      "2025-05-15 17:51:25.638474: Yayy! New best EMA pseudo Dice: 0.9837999939918518\n",
      "2025-05-15 17:51:26.380657: \n",
      "2025-05-15 17:51:26.380898: Epoch 159\n",
      "2025-05-15 17:51:26.381060: Current learning rate: 0.00856\n",
      "2025-05-15 17:53:15.396060: train_loss -0.9486\n",
      "2025-05-15 17:53:15.396290: val_loss -0.9622\n",
      "2025-05-15 17:53:15.396333: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 17:53:15.396369: Epoch time: 109.02 s\n",
      "2025-05-15 17:53:15.928994: \n",
      "2025-05-15 17:53:15.929086: Epoch 160\n",
      "2025-05-15 17:53:15.929151: Current learning rate: 0.00855\n",
      "2025-05-15 17:55:04.954624: train_loss -0.9492\n",
      "2025-05-15 17:55:04.954889: val_loss -0.9626\n",
      "2025-05-15 17:55:04.954982: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 17:55:04.955029: Epoch time: 109.03 s\n",
      "2025-05-15 17:55:05.477954: \n",
      "2025-05-15 17:55:05.478031: Epoch 161\n",
      "2025-05-15 17:55:05.478096: Current learning rate: 0.00854\n",
      "2025-05-15 17:56:54.373548: train_loss -0.9436\n",
      "2025-05-15 17:56:54.373669: val_loss -0.9592\n",
      "2025-05-15 17:56:54.373702: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 17:56:54.373735: Epoch time: 108.9 s\n",
      "2025-05-15 17:56:54.373756: Yayy! New best EMA pseudo Dice: 0.9837999939918518\n",
      "2025-05-15 17:56:55.300266: \n",
      "2025-05-15 17:56:55.300407: Epoch 162\n",
      "2025-05-15 17:56:55.300480: Current learning rate: 0.00853\n",
      "2025-05-15 17:58:44.263087: train_loss -0.9381\n",
      "2025-05-15 17:58:44.263255: val_loss -0.9529\n",
      "2025-05-15 17:58:44.263288: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-15 17:58:44.263320: Epoch time: 108.96 s\n",
      "2025-05-15 17:58:44.796232: \n",
      "2025-05-15 17:58:44.796329: Epoch 163\n",
      "2025-05-15 17:58:44.796395: Current learning rate: 0.00852\n",
      "2025-05-15 18:00:33.701706: train_loss -0.9424\n",
      "2025-05-15 18:00:33.701827: val_loss -0.9564\n",
      "2025-05-15 18:00:33.701859: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 18:00:33.701893: Epoch time: 108.91 s\n",
      "2025-05-15 18:00:34.232677: \n",
      "2025-05-15 18:00:34.232796: Epoch 164\n",
      "2025-05-15 18:00:34.232867: Current learning rate: 0.00851\n",
      "2025-05-15 18:02:23.184764: train_loss -0.9431\n",
      "2025-05-15 18:02:23.184903: val_loss -0.9593\n",
      "2025-05-15 18:02:23.184933: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-15 18:02:23.184966: Epoch time: 108.95 s\n",
      "2025-05-15 18:02:23.709009: \n",
      "2025-05-15 18:02:23.709141: Epoch 165\n",
      "2025-05-15 18:02:23.709208: Current learning rate: 0.0085\n",
      "2025-05-15 18:04:12.547787: train_loss -0.9401\n",
      "2025-05-15 18:04:12.547943: val_loss -0.9459\n",
      "2025-05-15 18:04:12.547976: Pseudo dice [np.float32(0.9795)]\n",
      "2025-05-15 18:04:12.548011: Epoch time: 108.84 s\n",
      "2025-05-15 18:04:13.071504: \n",
      "2025-05-15 18:04:13.071638: Epoch 166\n",
      "2025-05-15 18:04:13.071702: Current learning rate: 0.00849\n",
      "2025-05-15 18:06:02.062720: train_loss -0.9223\n",
      "2025-05-15 18:06:02.062953: val_loss -0.9575\n",
      "2025-05-15 18:06:02.063103: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 18:06:02.063183: Epoch time: 108.99 s\n",
      "2025-05-15 18:06:02.582086: \n",
      "2025-05-15 18:06:02.582253: Epoch 167\n",
      "2025-05-15 18:06:02.582353: Current learning rate: 0.00848\n",
      "2025-05-15 18:07:51.563040: train_loss -0.9241\n",
      "2025-05-15 18:07:51.563175: val_loss -0.9624\n",
      "2025-05-15 18:07:51.563208: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-15 18:07:51.563242: Epoch time: 108.98 s\n",
      "2025-05-15 18:07:52.094746: \n",
      "2025-05-15 18:07:52.094964: Epoch 168\n",
      "2025-05-15 18:07:52.095034: Current learning rate: 0.00847\n",
      "2025-05-15 18:09:40.892092: train_loss -0.8968\n",
      "2025-05-15 18:09:40.892211: val_loss -0.9132\n",
      "2025-05-15 18:09:40.892245: Pseudo dice [np.float32(0.9684)]\n",
      "2025-05-15 18:09:40.892278: Epoch time: 108.8 s\n",
      "2025-05-15 18:09:41.418777: \n",
      "2025-05-15 18:09:41.418978: Epoch 169\n",
      "2025-05-15 18:09:41.419134: Current learning rate: 0.00847\n",
      "2025-05-15 18:11:30.258471: train_loss -0.9283\n",
      "2025-05-15 18:11:30.258853: val_loss -0.9592\n",
      "2025-05-15 18:11:30.258990: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 18:11:30.259082: Epoch time: 108.84 s\n",
      "2025-05-15 18:11:30.785878: \n",
      "2025-05-15 18:11:30.785970: Epoch 170\n",
      "2025-05-15 18:11:30.786036: Current learning rate: 0.00846\n",
      "2025-05-15 18:13:19.789514: train_loss -0.9204\n",
      "2025-05-15 18:13:19.789631: val_loss -0.9428\n",
      "2025-05-15 18:13:19.789665: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-15 18:13:19.789699: Epoch time: 109.0 s\n",
      "2025-05-15 18:13:20.330719: \n",
      "2025-05-15 18:13:20.331084: Epoch 171\n",
      "2025-05-15 18:13:20.331304: Current learning rate: 0.00845\n",
      "2025-05-15 18:15:09.092992: train_loss -0.9299\n",
      "2025-05-15 18:15:09.093117: val_loss -0.9614\n",
      "2025-05-15 18:15:09.093149: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 18:15:09.093182: Epoch time: 108.76 s\n",
      "2025-05-15 18:15:09.622342: \n",
      "2025-05-15 18:15:09.622422: Epoch 172\n",
      "2025-05-15 18:15:09.622486: Current learning rate: 0.00844\n",
      "2025-05-15 18:16:58.541318: train_loss -0.9368\n",
      "2025-05-15 18:16:58.541477: val_loss -0.9586\n",
      "2025-05-15 18:16:58.541521: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 18:16:58.541558: Epoch time: 108.92 s\n",
      "2025-05-15 18:16:59.062453: \n",
      "2025-05-15 18:16:59.062593: Epoch 173\n",
      "2025-05-15 18:16:59.062659: Current learning rate: 0.00843\n",
      "2025-05-15 18:18:47.909931: train_loss -0.9378\n",
      "2025-05-15 18:18:47.910113: val_loss -0.9208\n",
      "2025-05-15 18:18:47.910152: Pseudo dice [np.float32(0.9699)]\n",
      "2025-05-15 18:18:47.910187: Epoch time: 108.85 s\n",
      "2025-05-15 18:18:48.620835: \n",
      "2025-05-15 18:18:48.620966: Epoch 174\n",
      "2025-05-15 18:18:48.621033: Current learning rate: 0.00842\n",
      "2025-05-15 18:20:37.387038: train_loss -0.933\n",
      "2025-05-15 18:20:37.387189: val_loss -0.96\n",
      "2025-05-15 18:20:37.387227: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-15 18:20:37.387268: Epoch time: 108.77 s\n",
      "2025-05-15 18:20:37.910255: \n",
      "2025-05-15 18:20:37.910356: Epoch 175\n",
      "2025-05-15 18:20:37.910430: Current learning rate: 0.00841\n",
      "2025-05-15 18:22:26.726180: train_loss -0.9374\n",
      "2025-05-15 18:22:26.726294: val_loss -0.9591\n",
      "2025-05-15 18:22:26.726388: Pseudo dice [np.float32(0.9845)]\n",
      "2025-05-15 18:22:26.726526: Epoch time: 108.82 s\n",
      "2025-05-15 18:22:27.258687: \n",
      "2025-05-15 18:22:27.258805: Epoch 176\n",
      "2025-05-15 18:22:27.258880: Current learning rate: 0.0084\n",
      "2025-05-15 18:24:16.077436: train_loss -0.9429\n",
      "2025-05-15 18:24:16.077564: val_loss -0.9588\n",
      "2025-05-15 18:24:16.077600: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 18:24:16.077635: Epoch time: 108.82 s\n",
      "2025-05-15 18:24:16.603711: \n",
      "2025-05-15 18:24:16.603804: Epoch 177\n",
      "2025-05-15 18:24:16.603870: Current learning rate: 0.00839\n",
      "2025-05-15 18:26:05.611960: train_loss -0.946\n",
      "2025-05-15 18:26:05.612252: val_loss -0.9602\n",
      "2025-05-15 18:26:05.612629: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-15 18:26:05.612739: Epoch time: 109.01 s\n",
      "2025-05-15 18:26:06.139260: \n",
      "2025-05-15 18:26:06.139366: Epoch 178\n",
      "2025-05-15 18:26:06.139432: Current learning rate: 0.00838\n",
      "2025-05-15 18:27:55.121868: train_loss -0.9432\n",
      "2025-05-15 18:27:55.121999: val_loss -0.9617\n",
      "2025-05-15 18:27:55.122074: Pseudo dice [np.float32(0.9837)]\n",
      "Epoch time: 108.98 s122116: \n",
      "2025-05-15 18:27:55.644899: \n",
      "2025-05-15 18:27:55.645056: Epoch 179\n",
      "2025-05-15 18:27:55.645128: Current learning rate: 0.00837\n",
      "2025-05-15 18:29:44.496330: train_loss -0.9449\n",
      "2025-05-15 18:29:44.496451: val_loss -0.9538\n",
      "2025-05-15 18:29:44.496484: Pseudo dice [np.float32(0.9814)]\n",
      "2025-05-15 18:29:44.496517: Epoch time: 108.85 s\n",
      "2025-05-15 18:29:45.019516: \n",
      "2025-05-15 18:29:45.019765: Epoch 180\n",
      "2025-05-15 18:29:45.019851: Current learning rate: 0.00836\n",
      "2025-05-15 18:31:34.029928: train_loss -0.9468\n",
      "2025-05-15 18:31:34.030105: val_loss -0.9623\n",
      "2025-05-15 18:31:34.030138: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 18:31:34.030172: Epoch time: 109.01 s\n",
      "2025-05-15 18:31:34.551221: \n",
      "2025-05-15 18:31:34.551316: Epoch 181\n",
      "2025-05-15 18:31:34.551427: Current learning rate: 0.00836\n",
      "2025-05-15 18:33:23.395962: train_loss -0.9507\n",
      "2025-05-15 18:33:23.396096: val_loss -0.9653\n",
      "2025-05-15 18:33:23.396130: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-15 18:33:23.396161: Epoch time: 108.85 s\n",
      "2025-05-15 18:33:23.911853: \n",
      "2025-05-15 18:33:23.911935: Epoch 182\n",
      "2025-05-15 18:33:23.911999: Current learning rate: 0.00835\n",
      "2025-05-15 18:35:12.716900: train_loss -0.949\n",
      "2025-05-15 18:35:12.717022: val_loss -0.9663\n",
      "2025-05-15 18:35:12.717057: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-15 18:35:12.717093: Epoch time: 108.81 s\n",
      "2025-05-15 18:35:13.232388: \n",
      "2025-05-15 18:35:13.232809: Epoch 183\n",
      "2025-05-15 18:35:13.232891: Current learning rate: 0.00834\n",
      "2025-05-15 18:37:02.297862: train_loss -0.9495\n",
      "2025-05-15 18:37:02.298018: val_loss -0.9632\n",
      "2025-05-15 18:37:02.298128: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 18:37:02.298184: Epoch time: 109.07 s\n",
      "2025-05-15 18:37:02.823565: \n",
      "2025-05-15 18:37:02.823706: Epoch 184\n",
      "2025-05-15 18:37:02.823778: Current learning rate: 0.00833\n",
      "2025-05-15 18:38:51.614528: train_loss -0.9512\n",
      "2025-05-15 18:38:51.614658: val_loss -0.9669\n",
      "2025-05-15 18:38:51.614693: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-15 18:38:51.614783: Epoch time: 108.79 s\n",
      "2025-05-15 18:38:52.136648: \n",
      "2025-05-15 18:38:52.136815: Epoch 185\n",
      "2025-05-15 18:38:52.136885: Current learning rate: 0.00832\n",
      "2025-05-15 18:40:41.119273: train_loss -0.9508\n",
      "2025-05-15 18:40:41.119397: val_loss -0.9633\n",
      "2025-05-15 18:40:41.119430: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 18:40:41.119465: Epoch time: 108.98 s\n",
      "2025-05-15 18:40:41.824764: \n",
      "2025-05-15 18:40:41.824943: Epoch 186\n",
      "2025-05-15 18:40:41.825139: Current learning rate: 0.00831\n",
      "2025-05-15 18:42:30.856644: train_loss -0.9516\n",
      "2025-05-15 18:42:30.856881: val_loss -0.9636\n",
      "2025-05-15 18:42:30.856926: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-15 18:42:30.856962: Epoch time: 109.03 s\n",
      "2025-05-15 18:42:31.376227: \n",
      "2025-05-15 18:42:31.376322: Epoch 187\n",
      "2025-05-15 18:42:31.376386: Current learning rate: 0.0083\n",
      "2025-05-15 18:44:20.177834: train_loss -0.9506\n",
      "2025-05-15 18:44:20.177985: val_loss -0.9643\n",
      "2025-05-15 18:44:20.178078: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-15 18:44:20.178127: Epoch time: 108.8 s\n",
      "2025-05-15 18:44:20.697740: \n",
      "2025-05-15 18:44:20.697877: Epoch 188\n",
      "2025-05-15 18:44:20.697944: Current learning rate: 0.00829\n",
      "2025-05-15 18:46:09.599985: train_loss -0.9533\n",
      "2025-05-15 18:46:09.600101: val_loss -0.9657\n",
      "2025-05-15 18:46:09.600136: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-15 18:46:09.600168: Epoch time: 108.9 s\n",
      "2025-05-15 18:46:10.128123: \n",
      "2025-05-15 18:46:10.128287: Epoch 189\n",
      "2025-05-15 18:46:10.128398: Current learning rate: 0.00828\n",
      "2025-05-15 18:47:59.158003: train_loss -0.9533\n",
      "2025-05-15 18:47:59.158132: val_loss -0.9658\n",
      "2025-05-15 18:47:59.158176: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-15 18:47:59.158224: Epoch time: 109.03 s\n",
      "2025-05-15 18:47:59.158253: Yayy! New best EMA pseudo Dice: 0.9837999939918518\n",
      "2025-05-15 18:47:59.900893: \n",
      "2025-05-15 18:47:59.901005: Epoch 190\n",
      "2025-05-15 18:47:59.901071: Current learning rate: 0.00827\n",
      "2025-05-15 18:49:48.746732: train_loss -0.9498\n",
      "2025-05-15 18:49:48.746853: val_loss -0.9618\n",
      "2025-05-15 18:49:48.746886: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 18:49:48.746919: Epoch time: 108.85 s\n",
      "2025-05-15 18:49:48.746940: Yayy! New best EMA pseudo Dice: 0.9839000105857849\n",
      "2025-05-15 18:49:49.484223: \n",
      "2025-05-15 18:49:49.484383: Epoch 191\n",
      "2025-05-15 18:49:49.484457: Current learning rate: 0.00826\n",
      "2025-05-15 18:51:38.543783: train_loss -0.945\n",
      "2025-05-15 18:51:38.543911: val_loss -0.9566\n",
      "2025-05-15 18:51:38.544099: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 18:51:38.544215: Epoch time: 109.06 s\n",
      "2025-05-15 18:51:39.087198: \n",
      "2025-05-15 18:51:39.087431: Epoch 192\n",
      "2025-05-15 18:51:39.087554: Current learning rate: 0.00825\n",
      "2025-05-15 18:53:27.949222: train_loss -0.9482\n",
      "2025-05-15 18:53:27.949416: val_loss -0.9644\n",
      "2025-05-15 18:53:27.949450: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-15 18:53:27.949494: Epoch time: 108.86 s\n",
      "2025-05-15 18:53:28.477977: \n",
      "2025-05-15 18:53:28.478064: Epoch 193\n",
      "2025-05-15 18:53:28.478130: Current learning rate: 0.00824\n",
      "2025-05-15 18:55:17.362643: train_loss -0.9428\n",
      "2025-05-15 18:55:17.362812: val_loss -0.964\n",
      "2025-05-15 18:55:17.362843: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-15 18:55:17.362875: Epoch time: 108.89 s\n",
      "2025-05-15 18:55:17.362896: Yayy! New best EMA pseudo Dice: 0.984000027179718\n",
      "2025-05-15 18:55:18.103280: \n",
      "2025-05-15 18:55:18.103366: Epoch 194\n",
      "2025-05-15 18:55:18.103429: Current learning rate: 0.00824\n",
      "2025-05-15 18:57:07.089023: train_loss -0.9475\n",
      "2025-05-15 18:57:07.089196: val_loss -0.9513\n",
      "2025-05-15 18:57:07.089229: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-15 18:57:07.089261: Epoch time: 108.99 s\n",
      "2025-05-15 18:57:07.615343: \n",
      "2025-05-15 18:57:07.615426: Epoch 195\n",
      "2025-05-15 18:57:07.615523: Current learning rate: 0.00823\n",
      "2025-05-15 18:58:56.641694: train_loss -0.9434\n",
      "2025-05-15 18:58:56.641868: val_loss -0.9587\n",
      "2025-05-15 18:58:56.641901: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 18:58:56.641934: Epoch time: 109.03 s\n",
      "2025-05-15 18:58:57.167882: \n",
      "2025-05-15 18:58:57.167962: Epoch 196\n",
      "2025-05-15 18:58:57.168024: Current learning rate: 0.00822\n",
      "2025-05-15 19:00:46.061071: train_loss -0.9458\n",
      "2025-05-15 19:00:46.061189: val_loss -0.9605\n",
      "2025-05-15 19:00:46.061224: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 19:00:46.061257: Epoch time: 108.89 s\n",
      "2025-05-15 19:00:46.583114: \n",
      "2025-05-15 19:00:46.583283: Epoch 197\n",
      "2025-05-15 19:00:46.583356: Current learning rate: 0.00821\n",
      "2025-05-15 19:02:35.386911: train_loss -0.938\n",
      "2025-05-15 19:02:35.387041: val_loss -0.956\n",
      "2025-05-15 19:02:35.387076: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 19:02:35.387108: Epoch time: 108.8 s\n",
      "2025-05-15 19:02:36.101477: \n",
      "2025-05-15 19:02:36.101803: Epoch 198\n",
      "2025-05-15 19:02:36.101904: Current learning rate: 0.0082\n",
      "2025-05-15 19:04:24.922365: train_loss -0.935\n",
      "2025-05-15 19:04:24.922564: val_loss -0.9628\n",
      "2025-05-15 19:04:24.922650: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-15 19:04:24.922780: Epoch time: 108.82 s\n",
      "2025-05-15 19:04:25.456910: \n",
      "2025-05-15 19:04:25.457078: Epoch 199\n",
      "2025-05-15 19:04:25.457203: Current learning rate: 0.00819\n",
      "2025-05-15 19:06:14.306411: train_loss -0.9274\n",
      "2025-05-15 19:06:14.306527: val_loss -0.9449\n",
      "2025-05-15 19:06:14.306561: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-15 19:06:14.306594: Epoch time: 108.85 s\n",
      "2025-05-15 19:06:15.038399: \n",
      "2025-05-15 19:06:15.038503: Epoch 200\n",
      "2025-05-15 19:06:15.038569: Current learning rate: 0.00818\n",
      "2025-05-15 19:08:04.034957: train_loss -0.9203\n",
      "2025-05-15 19:08:04.035080: val_loss -0.941\n",
      "2025-05-15 19:08:04.035114: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-15 19:08:04.035147: Epoch time: 109.0 s\n",
      "2025-05-15 19:08:04.576001: \n",
      "2025-05-15 19:08:04.576325: Epoch 201\n",
      "2025-05-15 19:08:04.576541: Current learning rate: 0.00817\n",
      "2025-05-15 19:09:53.349878: train_loss -0.9274\n",
      "2025-05-15 19:09:53.350003: val_loss -0.9501\n",
      "2025-05-15 19:09:53.350042: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-15 19:09:53.350186: Epoch time: 108.77 s\n",
      "2025-05-15 19:09:53.880943: \n",
      "2025-05-15 19:09:53.881058: Epoch 202\n",
      "2025-05-15 19:09:53.881132: Current learning rate: 0.00816\n",
      "2025-05-15 19:11:42.698733: train_loss -0.9392\n",
      "2025-05-15 19:11:42.698847: val_loss -0.9621\n",
      "2025-05-15 19:11:42.698879: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 19:11:42.698911: Epoch time: 108.82 s\n",
      "2025-05-15 19:11:43.229817: \n",
      "2025-05-15 19:11:43.230025: Epoch 203\n",
      "2025-05-15 19:11:43.230125: Current learning rate: 0.00815\n",
      "2025-05-15 19:13:32.336371: train_loss -0.9429\n",
      "2025-05-15 19:13:32.336508: val_loss -0.9403\n",
      "2025-05-15 19:13:32.336540: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-15 19:13:32.336573: Epoch time: 109.11 s\n",
      "2025-05-15 19:13:32.869572: \n",
      "2025-05-15 19:13:32.869958: Epoch 204\n",
      "2025-05-15 19:13:32.870034: Current learning rate: 0.00814\n",
      "2025-05-15 19:15:21.695295: train_loss -0.9415\n",
      "2025-05-15 19:15:21.695420: val_loss -0.9607\n",
      "2025-05-15 19:15:21.695456: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 19:15:21.695491: Epoch time: 108.83 s\n",
      "2025-05-15 19:15:22.231276: \n",
      "2025-05-15 19:15:22.231418: Epoch 205\n",
      "2025-05-15 19:15:22.231517: Current learning rate: 0.00813\n",
      "2025-05-15 19:17:11.202188: train_loss -0.948\n",
      "2025-05-15 19:17:11.202309: val_loss -0.9625\n",
      "2025-05-15 19:17:11.202341: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 19:17:11.202421: Epoch time: 108.97 s\n",
      "2025-05-15 19:17:11.706000: \n",
      "2025-05-15 19:17:11.706089: Epoch 206\n",
      "2025-05-15 19:17:11.706156: Current learning rate: 0.00813\n",
      "2025-05-15 19:19:00.705648: train_loss -0.9476\n",
      "2025-05-15 19:19:00.705772: val_loss -0.963\n",
      "2025-05-15 19:19:00.705805: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-15 19:19:00.705837: Epoch time: 109.0 s\n",
      "2025-05-15 19:19:01.212654: \n",
      "2025-05-15 19:19:01.212745: Epoch 207\n",
      "2025-05-15 19:19:01.212812: Current learning rate: 0.00812\n",
      "2025-05-15 19:20:50.036910: train_loss -0.9416\n",
      "2025-05-15 19:20:50.037097: val_loss -0.9645\n",
      "2025-05-15 19:20:50.037249: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-15 19:20:50.037319: Epoch time: 108.82 s\n",
      "2025-05-15 19:20:50.544079: \n",
      "2025-05-15 19:20:50.544221: Epoch 208\n",
      "2025-05-15 19:20:50.544287: Current learning rate: 0.00811\n",
      "2025-05-15 19:22:39.336617: train_loss -0.9471\n",
      "2025-05-15 19:22:39.336797: val_loss -0.9657\n",
      "2025-05-15 19:22:39.336831: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-15 19:22:39.336865: Epoch time: 108.79 s\n",
      "2025-05-15 19:22:39.854829: \n",
      "2025-05-15 19:22:39.854990: Epoch 209\n",
      "2025-05-15 19:22:39.855062: Current learning rate: 0.0081\n",
      "2025-05-15 19:24:28.861102: train_loss -0.9483\n",
      "2025-05-15 19:24:28.861230: val_loss -0.964\n",
      "2025-05-15 19:24:28.861270: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 19:24:28.861311: Epoch time: 109.01 s\n",
      "2025-05-15 19:24:29.540537: \n",
      "2025-05-15 19:24:29.540666: Epoch 210\n",
      "2025-05-15 19:24:29.540917: Current learning rate: 0.00809\n",
      "2025-05-15 19:26:18.347206: train_loss -0.9491\n",
      "2025-05-15 19:26:18.347353: val_loss -0.9603\n",
      "2025-05-15 19:26:18.347568: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-15 19:26:18.347636: Epoch time: 108.81 s\n",
      "2025-05-15 19:26:18.858645: \n",
      "2025-05-15 19:26:18.858748: Epoch 211\n",
      "2025-05-15 19:26:18.858813: Current learning rate: 0.00808\n",
      "2025-05-15 19:28:07.762473: train_loss -0.9497\n",
      "2025-05-15 19:28:07.762593: val_loss -0.9673\n",
      "2025-05-15 19:28:07.762624: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-15 19:28:07.762656: Epoch time: 108.9 s\n",
      "2025-05-15 19:28:08.273119: \n",
      "2025-05-15 19:28:08.273254: Epoch 212\n",
      "2025-05-15 19:28:08.273322: Current learning rate: 0.00807\n",
      "2025-05-15 19:29:57.095496: train_loss -0.9496\n",
      "2025-05-15 19:29:57.095616: val_loss -0.9625\n",
      "2025-05-15 19:29:57.095648: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-15 19:29:57.095681: Epoch time: 108.82 s\n",
      "2025-05-15 19:29:57.602470: \n",
      "2025-05-15 19:29:57.602654: Epoch 213\n",
      "2025-05-15 19:29:57.602770: Current learning rate: 0.00806\n",
      "2025-05-15 19:31:46.540384: train_loss -0.9486\n",
      "2025-05-15 19:31:46.540563: val_loss -0.9557\n",
      "2025-05-15 19:31:46.540599: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 19:31:46.540640: Epoch time: 108.94 s\n",
      "2025-05-15 19:31:47.051767: \n",
      "2025-05-15 19:31:47.051921: Epoch 214\n",
      "2025-05-15 19:31:47.051986: Current learning rate: 0.00805\n",
      "2025-05-15 19:33:35.919666: train_loss -0.9461\n",
      "2025-05-15 19:33:35.919793: val_loss -0.9659\n",
      "2025-05-15 19:33:35.919958: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-15 19:33:35.920074: Epoch time: 108.87 s\n",
      "2025-05-15 19:33:36.432469: \n",
      "2025-05-15 19:33:36.432571: Epoch 215\n",
      "2025-05-15 19:33:36.432638: Current learning rate: 0.00804\n",
      "2025-05-15 19:35:25.242769: train_loss -0.9517\n",
      "2025-05-15 19:35:25.242901: val_loss -0.9666\n",
      "2025-05-15 19:35:25.242934: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-15 19:35:25.242967: Epoch time: 108.81 s\n",
      "2025-05-15 19:35:25.754853: \n",
      "2025-05-15 19:35:25.755052: Epoch 216\n",
      "2025-05-15 19:35:25.755333: Current learning rate: 0.00803\n",
      "2025-05-15 19:37:14.695815: train_loss -0.9514\n",
      "2025-05-15 19:37:14.695926: val_loss -0.9622\n",
      "2025-05-15 19:37:14.695986: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-15 19:37:14.696135: Epoch time: 108.94 s\n",
      "2025-05-15 19:37:15.211181: \n",
      "2025-05-15 19:37:15.211336: Epoch 217\n",
      "2025-05-15 19:37:15.211412: Current learning rate: 0.00802\n",
      "2025-05-15 19:39:04.094683: train_loss -0.9501\n",
      "2025-05-15 19:39:04.094893: val_loss -0.9627\n",
      "2025-05-15 19:39:04.095010: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-15 19:39:04.095088: Epoch time: 108.88 s\n",
      "2025-05-15 19:39:04.602592: \n",
      "2025-05-15 19:39:04.602895: Epoch 218\n",
      "2025-05-15 19:39:04.602966: Current learning rate: 0.00801\n",
      "2025-05-15 19:40:53.673442: train_loss -0.9499\n",
      "2025-05-15 19:40:53.673572: val_loss -0.963\n",
      "2025-05-15 19:40:53.673605: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 19:40:53.673640: Epoch time: 109.07 s\n",
      "2025-05-15 19:40:54.188196: \n",
      "2025-05-15 19:40:54.188491: Epoch 219\n",
      "2025-05-15 19:40:54.188571: Current learning rate: 0.00801\n",
      "2025-05-15 19:42:43.238714: train_loss -0.952\n",
      "2025-05-15 19:42:43.238828: val_loss -0.9592\n",
      "2025-05-15 19:42:43.238858: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 19:42:43.238892: Epoch time: 109.05 s\n",
      "2025-05-15 19:42:43.743288: \n",
      "2025-05-15 19:42:43.743514: Epoch 220\n",
      "2025-05-15 19:42:43.743595: Current learning rate: 0.008\n",
      "2025-05-15 19:44:32.541709: train_loss -0.9501\n",
      "2025-05-15 19:44:32.541832: val_loss -0.9606\n",
      "2025-05-15 19:44:32.541866: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 19:44:32.541897: Epoch time: 108.8 s\n",
      "2025-05-15 19:44:33.051237: \n",
      "2025-05-15 19:44:33.051568: Epoch 221\n",
      "2025-05-15 19:44:33.051664: Current learning rate: 0.00799\n",
      "2025-05-15 19:46:21.840075: train_loss -0.952\n",
      "2025-05-15 19:46:21.840197: val_loss -0.9658\n",
      "2025-05-15 19:46:21.840229: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-15 19:46:21.840262: Epoch time: 108.79 s\n",
      "2025-05-15 19:46:22.537144: \n",
      "2025-05-15 19:46:22.537350: Epoch 222\n",
      "2025-05-15 19:46:22.537454: Current learning rate: 0.00798\n",
      "2025-05-15 19:48:11.448379: train_loss -0.9521\n",
      "2025-05-15 19:48:11.448500: val_loss -0.9676\n",
      "2025-05-15 19:48:11.448675: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-15 19:48:11.448744: Epoch time: 108.91 s\n",
      "2025-05-15 19:48:11.958669: \n",
      "2025-05-15 19:48:11.958876: Epoch 223\n",
      "2025-05-15 19:48:11.958982: Current learning rate: 0.00797\n",
      "2025-05-15 19:50:01.062443: train_loss -0.9519\n",
      "2025-05-15 19:50:01.062557: val_loss -0.9626\n",
      "2025-05-15 19:50:01.062592: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 19:50:01.062696: Epoch time: 109.1 s\n",
      "2025-05-15 19:50:01.573271: \n",
      "2025-05-15 19:50:01.573365: Epoch 224\n",
      "2025-05-15 19:50:01.573429: Current learning rate: 0.00796\n",
      "2025-05-15 19:51:50.491175: train_loss -0.9524\n",
      "2025-05-15 19:51:50.491295: val_loss -0.9675\n",
      "2025-05-15 19:51:50.491328: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-15 19:51:50.491467: Epoch time: 108.92 s\n",
      "2025-05-15 19:51:50.491535: Yayy! New best EMA pseudo Dice: 0.9840999841690063\n",
      "2025-05-15 19:51:51.215296: \n",
      "2025-05-15 19:51:51.215435: Epoch 225\n",
      "2025-05-15 19:51:51.215529: Current learning rate: 0.00795\n",
      "2025-05-15 19:53:40.054454: train_loss -0.953\n",
      "2025-05-15 19:53:40.054620: val_loss -0.9665\n",
      "2025-05-15 19:53:40.054654: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-15 19:53:40.054693: Epoch time: 108.84 s\n",
      "2025-05-15 19:53:40.054713: Yayy! New best EMA pseudo Dice: 0.9843000173568726\n",
      "2025-05-15 19:53:40.769049: \n",
      "2025-05-15 19:53:40.769270: Epoch 226\n",
      "2025-05-15 19:53:40.769343: Current learning rate: 0.00794\n",
      "2025-05-15 19:55:29.616987: train_loss -0.9486\n",
      "2025-05-15 19:55:29.617115: val_loss -0.9614\n",
      "2025-05-15 19:55:29.617147: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 19:55:29.617180: Epoch time: 108.85 s\n",
      "2025-05-15 19:55:30.124809: \n",
      "2025-05-15 19:55:30.124984: Epoch 227\n",
      "2025-05-15 19:55:30.125104: Current learning rate: 0.00793\n",
      "2025-05-15 19:57:19.010821: train_loss -0.9501\n",
      "2025-05-15 19:57:19.011022: val_loss -0.9686\n",
      "Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-15 19:57:19.011336: Epoch time: 108.89 s\n",
      "2025-05-15 19:57:19.011528: Yayy! New best EMA pseudo Dice: 0.984499990940094\n",
      "2025-05-15 19:57:19.732999: \n",
      "2025-05-15 19:57:19.733092: Epoch 228\n",
      "2025-05-15 19:57:19.733159: Current learning rate: 0.00792\n",
      "2025-05-15 19:59:08.830848: train_loss -0.9508\n",
      "2025-05-15 19:59:08.830966: val_loss -0.9608\n",
      "2025-05-15 19:59:08.830999: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-15 19:59:08.831032: Epoch time: 109.1 s\n",
      "2025-05-15 19:59:09.329503: \n",
      "2025-05-15 19:59:09.329675: Epoch 229\n",
      "2025-05-15 19:59:09.329753: Current learning rate: 0.00791\n",
      "2025-05-15 20:00:58.199401: train_loss -0.9493\n",
      "2025-05-15 20:00:58.199608: val_loss -0.9635\n",
      "2025-05-15 20:00:58.199831: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 20:00:58.199899: Epoch time: 108.87 s\n",
      "2025-05-15 20:00:58.694258: \n",
      "2025-05-15 20:00:58.694341: Epoch 230\n",
      "2025-05-15 20:00:58.694405: Current learning rate: 0.0079\n",
      "2025-05-15 20:02:47.454370: train_loss -0.9386\n",
      "2025-05-15 20:02:47.454576: val_loss -0.9573\n",
      "2025-05-15 20:02:47.454611: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-15 20:02:47.454711: Epoch time: 108.76 s\n",
      "2025-05-15 20:02:47.957278: \n",
      "2025-05-15 20:02:47.957366: Epoch 231\n",
      "2025-05-15 20:02:47.957428: Current learning rate: 0.00789\n",
      "2025-05-15 20:04:36.794203: train_loss -0.9438\n",
      "2025-05-15 20:04:36.794336: val_loss -0.9583\n",
      "2025-05-15 20:04:36.794369: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 20:04:36.794403: Epoch time: 108.84 s\n",
      "2025-05-15 20:04:37.290144: \n",
      "2025-05-15 20:04:37.290364: Epoch 232\n",
      "2025-05-15 20:04:37.290507: Current learning rate: 0.00789\n",
      "2025-05-15 20:06:26.142054: train_loss -0.9454\n",
      "2025-05-15 20:06:26.142169: val_loss -0.9643\n",
      "2025-05-15 20:06:26.142203: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 20:06:26.142239: Epoch time: 108.85 s\n",
      "2025-05-15 20:06:26.645895: \n",
      "2025-05-15 20:06:26.646070: Epoch 233\n",
      "2025-05-15 20:06:26.646187: Current learning rate: 0.00788\n",
      "2025-05-15 20:08:15.658924: train_loss -0.9318\n",
      "2025-05-15 20:08:15.659051: val_loss -0.9599\n",
      "2025-05-15 20:08:15.659087: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 20:08:15.659120: Epoch time: 109.01 s\n",
      "2025-05-15 20:08:16.160090: \n",
      "2025-05-15 20:08:16.160223: Epoch 234\n",
      "2025-05-15 20:08:16.160307: Current learning rate: 0.00787\n",
      "2025-05-15 20:10:05.165158: train_loss -0.9483\n",
      "2025-05-15 20:10:05.165411: val_loss -0.9629\n",
      "2025-05-15 20:10:05.165454: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 20:10:05.165557: Epoch time: 109.01 s\n",
      "2025-05-15 20:10:05.859384: \n",
      "2025-05-15 20:10:05.859609: Epoch 235\n",
      "2025-05-15 20:10:05.859713: Current learning rate: 0.00786\n",
      "2025-05-15 20:11:54.884817: train_loss -0.9448\n",
      "2025-05-15 20:11:54.885130: val_loss -0.9685\n",
      "2025-05-15 20:11:54.885245: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-15 20:11:54.885285: Epoch time: 109.03 s\n",
      "2025-05-15 20:11:55.380515: \n",
      "2025-05-15 20:11:55.380613: Epoch 236\n",
      "2025-05-15 20:11:55.380679: Current learning rate: 0.00785\n",
      "2025-05-15 20:13:44.449097: train_loss -0.9467\n",
      "2025-05-15 20:13:44.449248: val_loss -0.9593\n",
      "2025-05-15 20:13:44.449398: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 20:13:44.449470: Epoch time: 109.07 s\n",
      "2025-05-15 20:13:44.946786: \n",
      "2025-05-15 20:13:44.947032: Epoch 237\n",
      "2025-05-15 20:13:44.947165: Current learning rate: 0.00784\n",
      "2025-05-15 20:15:33.939980: train_loss -0.9437\n",
      "2025-05-15 20:15:33.940123: val_loss -0.9638\n",
      "2025-05-15 20:15:33.940217: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-15 20:15:33.940261: Epoch time: 108.99 s\n",
      "2025-05-15 20:15:34.440605: \n",
      "2025-05-15 20:15:34.440786: Epoch 238\n",
      "2025-05-15 20:15:34.440878: Current learning rate: 0.00783\n",
      "2025-05-15 20:17:23.313026: train_loss -0.9412\n",
      "2025-05-15 20:17:23.313209: val_loss -0.9506\n",
      "2025-05-15 20:17:23.313334: Pseudo dice [np.float32(0.9797)]\n",
      "2025-05-15 20:17:23.313423: Epoch time: 108.87 s\n",
      "2025-05-15 20:17:23.811361: \n",
      "2025-05-15 20:17:23.811450: Epoch 239\n",
      "2025-05-15 20:17:23.811513: Current learning rate: 0.00782\n",
      "2025-05-15 20:19:12.880041: train_loss -0.9082\n",
      "2025-05-15 20:19:12.880177: val_loss -0.8848\n",
      "2025-05-15 20:19:12.880216: Pseudo dice [np.float32(0.9522)]\n",
      "2025-05-15 20:19:12.880255: Epoch time: 109.07 s\n",
      "2025-05-15 20:19:13.389644: \n",
      "2025-05-15 20:19:13.390008: Epoch 240\n",
      "2025-05-15 20:19:13.390196: Current learning rate: 0.00781\n",
      "2025-05-15 20:21:02.306851: train_loss -0.9141\n",
      "2025-05-15 20:21:02.306980: val_loss -0.9386\n",
      "2025-05-15 20:21:02.307015: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-15 20:21:02.307048: Epoch time: 108.92 s\n",
      "2025-05-15 20:21:02.814771: \n",
      "2025-05-15 20:21:02.814879: Epoch 241\n",
      "2025-05-15 20:21:02.814944: Current learning rate: 0.0078\n",
      "2025-05-15 20:22:51.696585: train_loss -0.9368\n",
      "2025-05-15 20:22:51.696704: val_loss -0.963\n",
      "2025-05-15 20:22:51.696739: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-15 20:22:51.696772: Epoch time: 108.88 s\n",
      "2025-05-15 20:22:52.201323: \n",
      "2025-05-15 20:22:52.201461: Epoch 242\n",
      "2025-05-15 20:22:52.201530: Current learning rate: 0.00779\n",
      "2025-05-15 20:24:41.321127: train_loss -0.9422\n",
      "2025-05-15 20:24:41.321239: val_loss -0.9628\n",
      "2025-05-15 20:24:41.321271: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 20:24:41.321304: Epoch time: 109.12 s\n",
      "2025-05-15 20:24:41.832144: \n",
      "2025-05-15 20:24:41.832238: Epoch 243\n",
      "2025-05-15 20:24:41.832302: Current learning rate: 0.00778\n",
      "2025-05-15 20:26:30.738647: train_loss -0.9465\n",
      "2025-05-15 20:26:30.738817: val_loss -0.9614\n",
      "2025-05-15 20:26:30.738850: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 20:26:30.738883: Epoch time: 108.91 s\n",
      "2025-05-15 20:26:31.248392: \n",
      "2025-05-15 20:26:31.248536: Epoch 244\n",
      "2025-05-15 20:26:31.248612: Current learning rate: 0.00777\n",
      "2025-05-15 20:28:20.297975: train_loss -0.9449\n",
      "2025-05-15 20:28:20.298181: val_loss -0.9625\n",
      "2025-05-15 20:28:20.298215: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-15 20:28:20.298246: Epoch time: 109.05 s\n",
      "2025-05-15 20:28:20.802574: \n",
      "2025-05-15 20:28:20.802923: Epoch 245\n",
      "2025-05-15 20:28:20.803569: Current learning rate: 0.00777\n",
      "2025-05-15 20:30:09.696639: train_loss -0.9401\n",
      "2025-05-15 20:30:09.696753: val_loss -0.9411\n",
      "2025-05-15 20:30:09.696784: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-15 20:30:09.696820: Epoch time: 108.89 s\n",
      "2025-05-15 20:30:10.205051: \n",
      "2025-05-15 20:30:10.205146: Epoch 246\n",
      "2025-05-15 20:30:10.205214: Current learning rate: 0.00776\n",
      "2025-05-15 20:31:59.046111: train_loss -0.866\n",
      "2025-05-15 20:31:59.046288: val_loss -0.8836\n",
      "2025-05-15 20:31:59.046322: Pseudo dice [np.float32(0.9541)]\n",
      "2025-05-15 20:31:59.046354: Epoch time: 108.84 s\n",
      "2025-05-15 20:31:59.554118: \n",
      "2025-05-15 20:31:59.554320: Epoch 247\n",
      "2025-05-15 20:31:59.554391: Current learning rate: 0.00775\n",
      "2025-05-15 20:33:48.352526: train_loss -0.8862\n",
      "2025-05-15 20:33:48.352647: val_loss -0.9389\n",
      "2025-05-15 20:33:48.352691: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-15 20:33:48.352727: Epoch time: 108.8 s\n",
      "2025-05-15 20:33:49.062512: \n",
      "2025-05-15 20:33:49.062627: Epoch 248\n",
      "2025-05-15 20:33:49.062692: Current learning rate: 0.00774\n",
      "2025-05-15 20:35:38.042112: train_loss -0.9162\n",
      "2025-05-15 20:35:38.042294: val_loss -0.9524\n",
      "2025-05-15 20:35:38.042416: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-15 20:35:38.042491: Epoch time: 108.98 s\n",
      "2025-05-15 20:35:38.552308: \n",
      "2025-05-15 20:35:38.552449: Epoch 249\n",
      "2025-05-15 20:35:38.552561: Current learning rate: 0.00773\n",
      "2025-05-15 20:37:27.641418: train_loss -0.9409\n",
      "2025-05-15 20:37:27.641586: val_loss -0.9651\n",
      "2025-05-15 20:37:27.641620: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-15 20:37:27.641659: Epoch time: 109.09 s\n",
      "2025-05-15 20:37:28.363354: \n",
      "2025-05-15 20:37:28.363532: Epoch 250\n",
      "2025-05-15 20:37:28.363617: Current learning rate: 0.00772\n",
      "2025-05-15 20:39:17.181666: train_loss -0.9336\n",
      "2025-05-15 20:39:17.181802: val_loss -0.9511\n",
      "2025-05-15 20:39:17.181840: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-15 20:39:17.181973: Epoch time: 108.82 s\n",
      "2025-05-15 20:39:17.689700: \n",
      "2025-05-15 20:39:17.689868: Epoch 251\n",
      "2025-05-15 20:39:17.689935: Current learning rate: 0.00771\n",
      "2025-05-15 20:41:06.683181: train_loss -0.9378\n",
      "2025-05-15 20:41:06.683463: val_loss -0.9623\n",
      "2025-05-15 20:41:06.683511: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-15 20:41:06.683717: Epoch time: 108.99 s\n",
      "2025-05-15 20:41:07.198682: \n",
      "2025-05-15 20:41:07.198823: Epoch 252\n",
      "2025-05-15 20:41:07.198951: Current learning rate: 0.0077\n",
      "-0.9379-15 20:42:56.036457: train_loss \n",
      "2025-05-15 20:42:56.036852: val_loss -0.9374\n",
      "2025-05-15 20:42:56.036951: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-15 20:42:56.036995: Epoch time: 108.84 s\n",
      "2025-05-15 20:42:56.561335: \n",
      "2025-05-15 20:42:56.561494: Epoch 253\n",
      "2025-05-15 20:42:56.561596: Current learning rate: 0.00769\n",
      "2025-05-15 20:44:45.477025: train_loss -0.9395\n",
      "2025-05-15 20:44:45.477228: val_loss -0.9622\n",
      "2025-05-15 20:44:45.477264: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 20:44:45.477297: Epoch time: 108.92 s\n",
      "2025-05-15 20:44:46.000161: \n",
      "2025-05-15 20:44:46.000345: Epoch 254\n",
      "2025-05-15 20:44:46.000421: Current learning rate: 0.00768\n",
      "2025-05-15 20:46:34.825630: train_loss -0.9449\n",
      "2025-05-15 20:46:34.825751: val_loss -0.9611\n",
      "2025-05-15 20:46:34.825782: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 20:46:34.825817: Epoch time: 108.83 s\n",
      "2025-05-15 20:46:35.339402: \n",
      "2025-05-15 20:46:35.339510: Epoch 255\n",
      "2025-05-15 20:46:35.339578: Current learning rate: 0.00767\n",
      "2025-05-15 20:48:24.133879: train_loss -0.9492\n",
      "2025-05-15 20:48:24.134133: val_loss -0.9646\n",
      "2025-05-15 20:48:24.134275: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-15 20:48:24.134392: Epoch time: 108.8 s\n",
      "2025-05-15 20:48:24.644496: \n",
      "2025-05-15 20:48:24.644597: Epoch 256\n",
      "2025-05-15 20:48:24.644662: Current learning rate: 0.00766\n",
      "2025-05-15 20:50:13.665523: train_loss -0.9475\n",
      "2025-05-15 20:50:13.665704: val_loss -0.956\n",
      "2025-05-15 20:50:13.665738: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 20:50:13.665773: Epoch time: 109.02 s\n",
      "2025-05-15 20:50:14.204065: \n",
      "2025-05-15 20:50:14.204218: Epoch 257\n",
      "2025-05-15 20:50:14.204288: Current learning rate: 0.00765\n",
      "2025-05-15 20:52:03.123795: train_loss -0.9448\n",
      "2025-05-15 20:52:03.123931: val_loss -0.9623\n",
      "2025-05-15 20:52:03.123966: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 20:52:03.123999: Epoch time: 108.92 s\n",
      "2025-05-15 20:52:03.632410: \n",
      "2025-05-15 20:52:03.632498: Epoch 258\n",
      "2025-05-15 20:52:03.632563: Current learning rate: 0.00764\n",
      "2025-05-15 20:53:52.415601: train_loss -0.945\n",
      "2025-05-15 20:53:52.415789: val_loss -0.9615\n",
      "2025-05-15 20:53:52.415890: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-15 20:53:52.415953: Epoch time: 108.78 s\n",
      "2025-05-15 20:53:52.922261: \n",
      "2025-05-15 20:53:52.922344: Epoch 259\n",
      "2025-05-15 20:53:52.922407: Current learning rate: 0.00764\n",
      "2025-05-15 20:55:41.759671: train_loss -0.9473\n",
      "2025-05-15 20:55:41.759833: val_loss -0.9584\n",
      "2025-05-15 20:55:41.759864: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 20:55:41.759898: Epoch time: 108.84 s\n",
      "2025-05-15 20:55:42.273858: \n",
      "2025-05-15 20:55:42.273939: Epoch 260\n",
      "2025-05-15 20:55:42.274004: Current learning rate: 0.00763\n",
      "2025-05-15 20:57:31.071515: train_loss -0.943\n",
      "2025-05-15 20:57:31.071637: val_loss -0.9614\n",
      "2025-05-15 20:57:31.071670: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 20:57:31.071704: Epoch time: 108.8 s\n",
      "2025-05-15 20:57:31.578005: \n",
      "2025-05-15 20:57:31.578169: Epoch 261\n",
      "2025-05-15 20:57:31.578243: Current learning rate: 0.00762\n",
      "2025-05-15 20:59:20.613377: train_loss -0.9244\n",
      "2025-05-15 20:59:20.613499: val_loss -0.9579\n",
      "2025-05-15 20:59:20.613534: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-15 20:59:20.613568: Epoch time: 109.04 s\n",
      "2025-05-15 20:59:21.313047: \n",
      "2025-05-15 20:59:21.313200: Epoch 262\n",
      "2025-05-15 20:59:21.313304: Current learning rate: 0.00761\n",
      "2025-05-15 21:01:10.319736: train_loss -0.9417\n",
      "2025-05-15 21:01:10.319924: val_loss -0.9635\n",
      "2025-05-15 21:01:10.319957: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-15 21:01:10.319992: Epoch time: 109.01 s\n",
      "2025-05-15 21:01:10.833917: \n",
      "2025-05-15 21:01:10.834010: Epoch 263\n",
      "2025-05-15 21:01:10.834078: Current learning rate: 0.0076\n",
      "2025-05-15 21:02:59.826602: train_loss -0.949\n",
      "2025-05-15 21:02:59.826894: val_loss -0.9647\n",
      "2025-05-15 21:02:59.826939: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-15 21:02:59.826975: Epoch time: 108.99 s\n",
      "2025-05-15 21:03:00.346274: \n",
      "2025-05-15 21:03:00.346449: Epoch 264\n",
      "2025-05-15 21:03:00.346524: Current learning rate: 0.00759\n",
      "2025-05-15 21:04:49.310164: train_loss -0.9325\n",
      "2025-05-15 21:04:49.310326: val_loss -0.9485\n",
      "2025-05-15 21:04:49.310358: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-15 21:04:49.310394: Epoch time: 108.96 s\n",
      "2025-05-15 21:04:49.819908: \n",
      "2025-05-15 21:04:49.820218: Epoch 265\n",
      "2025-05-15 21:04:49.820297: Current learning rate: 0.00758\n",
      "2025-05-15 21:06:38.715969: train_loss -0.9309\n",
      "2025-05-15 21:06:38.716149: val_loss -0.956\n",
      "2025-05-15 21:06:38.716195: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-15 21:06:38.716230: Epoch time: 108.9 s\n",
      "2025-05-15 21:06:39.234389: \n",
      "2025-05-15 21:06:39.234495: Epoch 266\n",
      "2025-05-15 21:06:39.234586: Current learning rate: 0.00757\n",
      "2025-05-15 21:08:28.250645: train_loss -0.9434\n",
      "2025-05-15 21:08:28.250831: val_loss -0.9622\n",
      "2025-05-15 21:08:28.250864: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-15 21:08:28.250897: Epoch time: 109.02 s\n",
      "2025-05-15 21:08:28.761609: \n",
      "2025-05-15 21:08:28.761716: Epoch 267\n",
      "2025-05-15 21:08:28.761784: Current learning rate: 0.00756\n",
      "2025-05-15 21:10:17.792415: train_loss -0.9426\n",
      "2025-05-15 21:10:17.792550: val_loss -0.961\n",
      "2025-05-15 21:10:17.792606: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 21:10:17.792643: Epoch time: 109.03 s\n",
      "2025-05-15 21:10:18.302950: \n",
      "2025-05-15 21:10:18.303100: Epoch 268\n",
      "2025-05-15 21:10:18.303173: Current learning rate: 0.00755\n",
      "2025-05-15 21:12:07.269443: train_loss -0.9436\n",
      "2025-05-15 21:12:07.269638: val_loss -0.9618\n",
      "2025-05-15 21:12:07.269703: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-15 21:12:07.269742: Epoch time: 108.97 s\n",
      "2025-05-15 21:12:07.780584: \n",
      "2025-05-15 21:12:07.780712: Epoch 269\n",
      "2025-05-15 21:12:07.780779: Current learning rate: 0.00754\n",
      "2025-05-15 21:13:56.860732: train_loss -0.9478\n",
      "2025-05-15 21:13:56.860853: val_loss -0.9621\n",
      "2025-05-15 21:13:56.860887: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-15 21:13:56.860920: Epoch time: 109.08 s\n",
      "2025-05-15 21:13:57.381017: \n",
      "2025-05-15 21:13:57.381103: Epoch 270\n",
      "2025-05-15 21:13:57.381167: Current learning rate: 0.00753\n",
      "2025-05-15 21:15:46.217441: train_loss -0.9444\n",
      "2025-05-15 21:15:46.217632: val_loss -0.9465\n",
      "2025-05-15 21:15:46.217667: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-15 21:15:46.217701: Epoch time: 108.84 s\n",
      "2025-05-15 21:15:46.728152: \n",
      "2025-05-15 21:15:46.728312: Epoch 271\n",
      "2025-05-15 21:15:46.728388: Current learning rate: 0.00752\n",
      "2025-05-15 21:17:35.774633: train_loss -0.9283\n",
      "2025-05-15 21:17:35.774755: val_loss -0.9601\n",
      "2025-05-15 21:17:35.774791: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-15 21:17:35.774823: Epoch time: 109.05 s\n",
      "2025-05-15 21:17:36.282025: \n",
      "2025-05-15 21:17:36.282109: Epoch 272\n",
      "2025-05-15 21:17:36.282174: Current learning rate: 0.00751\n",
      "2025-05-15 21:19:25.176657: train_loss -0.9417\n",
      "2025-05-15 21:19:25.176780: val_loss -0.966\n",
      "2025-05-15 21:19:25.176815: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-15 21:19:25.176848: Epoch time: 108.9 s\n",
      "2025-05-15 21:19:25.691154: \n",
      "2025-05-15 21:19:25.691457: Epoch 273\n",
      "2025-05-15 21:19:25.691576: Current learning rate: 0.00751\n",
      "2025-05-15 21:21:14.661613: train_loss -0.9354\n",
      "2025-05-15 21:21:14.661807: val_loss -0.9601\n",
      "2025-05-15 21:21:14.661847: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 21:21:14.661881: Epoch time: 108.97 s\n",
      "2025-05-15 21:21:15.171650: \n",
      "2025-05-15 21:21:15.171727: Epoch 274\n",
      "2025-05-15 21:21:15.171789: Current learning rate: 0.0075\n",
      "2025-05-15 21:23:04.114126: train_loss -0.9415\n",
      "2025-05-15 21:23:04.114252: val_loss -0.9597\n",
      "2025-05-15 21:23:04.114284: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 21:23:04.114326: Epoch time: 108.94 s\n",
      "2025-05-15 21:23:04.632194: \n",
      "2025-05-15 21:23:04.632285: Epoch 275\n",
      "2025-05-15 21:23:04.632347: Current learning rate: 0.00749\n",
      "2025-05-15 21:24:53.537585: train_loss -0.9138\n",
      "2025-05-15 21:24:53.537879: val_loss -0.9476\n",
      "2025-05-15 21:24:53.537921: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-15 21:24:53.537956: Epoch time: 108.91 s\n",
      "2025-05-15 21:24:54.056692: \n",
      "2025-05-15 21:24:54.056795: Epoch 276\n",
      "2025-05-15 21:24:54.056859: Current learning rate: 0.00748\n",
      "2025-05-15 21:26:43.053043: train_loss -0.9382\n",
      "2025-05-15 21:26:43.053163: val_loss -0.9562\n",
      "2025-05-15 21:26:43.053199: Pseudo dice [np.float32(0.9824)]\n",
      "2025-05-15 21:26:43.053232: Epoch time: 109.0 s\n",
      "2025-05-15 21:26:43.563212: \n",
      "2025-05-15 21:26:43.563442: Epoch 277\n",
      "2025-05-15 21:26:43.563643: Current learning rate: 0.00747\n",
      "2025-05-15 21:28:32.474545: train_loss -0.9328\n",
      "2025-05-15 21:28:32.474781: val_loss -0.9488\n",
      "2025-05-15 21:28:32.474842: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-15 21:28:32.475173: Epoch time: 108.91 s\n",
      "2025-05-15 21:28:32.993662: \n",
      "2025-05-15 21:28:32.993813: Epoch 278\n",
      "2025-05-15 21:28:32.993881: Current learning rate: 0.00746\n",
      "2025-05-15 21:30:22.047623: train_loss -0.9392\n",
      "2025-05-15 21:30:22.047751: val_loss -0.954\n",
      "2025-05-15 21:30:22.048010: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-15 21:30:22.048136: Epoch time: 109.05 s\n",
      "2025-05-15 21:30:22.565543: \n",
      "2025-05-15 21:30:22.565722: Epoch 279\n",
      "2025-05-15 21:30:22.565824: Current learning rate: 0.00745\n",
      "2025-05-15 21:32:11.673222: train_loss -0.9406\n",
      "2025-05-15 21:32:11.673690: val_loss -0.9616\n",
      "2025-05-15 21:32:11.673760: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-15 21:32:11.673803: Epoch time: 109.11 s\n",
      "2025-05-15 21:32:12.184578: \n",
      "2025-05-15 21:32:12.184670: Epoch 280\n",
      "2025-05-15 21:32:12.184735: Current learning rate: 0.00744\n",
      "2025-05-15 21:34:01.017189: train_loss -0.9438\n",
      "2025-05-15 21:34:01.017322: val_loss -0.9591\n",
      "2025-05-15 21:34:01.017432: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 21:34:01.017477: Epoch time: 108.83 s\n",
      "2025-05-15 21:34:01.533783: \n",
      "2025-05-15 21:34:01.534094: Epoch 281\n",
      "2025-05-15 21:34:01.534164: Current learning rate: 0.00743\n",
      "2025-05-15 21:35:50.469256: train_loss -0.9465\n",
      "2025-05-15 21:35:50.469382: val_loss -0.9652\n",
      "2025-05-15 21:35:50.469416: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-15 21:35:50.469451: Epoch time: 108.94 s\n",
      "2025-05-15 21:35:50.979894: \n",
      "2025-05-15 21:35:50.979997: Epoch 282\n",
      "2025-05-15 21:35:50.980064: Current learning rate: 0.00742\n",
      "2025-05-15 21:37:39.891598: train_loss -0.9496\n",
      "2025-05-15 21:37:39.891808: val_loss -0.9671\n",
      "2025-05-15 21:37:39.891850: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-15 21:37:39.891899: Epoch time: 108.91 s\n",
      "2025-05-15 21:37:40.407625: \n",
      "2025-05-15 21:37:40.407890: Epoch 283\n",
      "2025-05-15 21:37:40.408011: Current learning rate: 0.00741\n",
      "2025-05-15 21:39:29.421772: train_loss -0.9495\n",
      "2025-05-15 21:39:29.422074: val_loss -0.9598\n",
      "2025-05-15 21:39:29.422224: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 21:39:29.422304: Epoch time: 109.01 s\n",
      "2025-05-15 21:39:29.935247: \n",
      "2025-05-15 21:39:29.935591: Epoch 284\n",
      "2025-05-15 21:39:29.935669: Current learning rate: 0.0074\n",
      "2025-05-15 21:41:18.817511: train_loss -0.9451\n",
      "2025-05-15 21:41:18.817626: val_loss -0.9653\n",
      "2025-05-15 21:41:18.817657: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-15 21:41:18.817688: Epoch time: 108.88 s\n",
      "2025-05-15 21:41:19.335685: \n",
      "2025-05-15 21:41:19.335867: Epoch 285\n",
      "2025-05-15 21:41:19.335966: Current learning rate: 0.00739\n",
      "2025-05-15 21:43:08.357058: train_loss -0.9454\n",
      "2025-05-15 21:43:08.357242: val_loss -0.9606\n",
      "2025-05-15 21:43:08.357275: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-15 21:43:08.357310: Epoch time: 109.02 s\n",
      "2025-05-15 21:43:08.862401: \n",
      "2025-05-15 21:43:08.862480: Epoch 286\n",
      "2025-05-15 21:43:08.862542: Current learning rate: 0.00738\n",
      "2025-05-15 21:44:57.808571: train_loss -0.9488\n",
      "2025-05-15 21:44:57.808758: val_loss -0.9612\n",
      "2025-05-15 21:44:57.808800: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 21:44:57.808834: Epoch time: 108.95 s\n",
      "2025-05-15 21:44:58.518326: \n",
      "2025-05-15 21:44:58.518681: Epoch 287\n",
      "2025-05-15 21:44:58.518828: Current learning rate: 0.00738\n",
      "2025-05-15 21:46:47.400484: train_loss -0.9501\n",
      "2025-05-15 21:46:47.400752: val_loss -0.9647\n",
      "2025-05-15 21:46:47.400828: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-15 21:46:47.400869: Epoch time: 108.88 s\n",
      "2025-05-15 21:46:47.917542: \n",
      "2025-05-15 21:46:47.917634: Epoch 288\n",
      "2025-05-15 21:46:47.917698: Current learning rate: 0.00737\n",
      "2025-05-15 21:48:36.949106: train_loss -0.9535\n",
      "2025-05-15 21:48:36.949229: val_loss -0.9635\n",
      "2025-05-15 21:48:36.949263: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-15 21:48:36.949296: Epoch time: 109.03 s\n",
      "2025-05-15 21:48:37.465018: \n",
      "2025-05-15 21:48:37.465395: Epoch 289\n",
      "2025-05-15 21:48:37.465482: Current learning rate: 0.00736\n",
      "2025-05-15 21:50:26.316053: train_loss -0.951\n",
      "2025-05-15 21:50:26.316214: val_loss -0.9634\n",
      "2025-05-15 21:50:26.316256: Pseudo dice [np.float32(0.9845)]\n",
      "2025-05-15 21:50:26.316294: Epoch time: 108.85 s\n",
      "2025-05-15 21:50:26.839200: \n",
      "2025-05-15 21:50:26.839288: Epoch 290\n",
      "2025-05-15 21:50:26.839352: Current learning rate: 0.00735\n",
      "2025-05-15 21:52:15.893058: train_loss -0.9485\n",
      "2025-05-15 21:52:15.893195: val_loss -0.9612\n",
      "2025-05-15 21:52:15.893226: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 21:52:15.893259: Epoch time: 109.05 s\n",
      "2025-05-15 21:52:16.413927: \n",
      "2025-05-15 21:52:16.414070: Epoch 291\n",
      "2025-05-15 21:52:16.414135: Current learning rate: 0.00734\n",
      "2025-05-15 21:54:05.301308: train_loss -0.9532\n",
      "2025-05-15 21:54:05.301522: val_loss -0.9658\n",
      "2025-05-15 21:54:05.301663: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-15 21:54:05.301730: Epoch time: 108.89 s\n",
      "2025-05-15 21:54:05.820029: \n",
      "2025-05-15 21:54:05.820215: Epoch 292\n",
      "2025-05-15 21:54:05.820312: Current learning rate: 0.00733\n",
      "2025-05-15 21:55:54.915190: train_loss -0.9528\n",
      "2025-05-15 21:55:54.915376: val_loss -0.9661\n",
      "2025-05-15 21:55:54.915411: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-15 21:55:54.915447: Epoch time: 109.1 s\n",
      "2025-05-15 21:55:55.434135: \n",
      "2025-05-15 21:55:55.434281: Epoch 293\n",
      "2025-05-15 21:55:55.434347: Current learning rate: 0.00732\n",
      "2025-05-15 21:57:44.388628: train_loss -0.9516\n",
      "2025-05-15 21:57:44.388795: val_loss -0.9661\n",
      "2025-05-15 21:57:44.388828: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-15 21:57:44.388860: Epoch time: 108.96 s\n",
      "2025-05-15 21:57:44.906239: \n",
      "2025-05-15 21:57:44.906489: Epoch 294\n",
      "2025-05-15 21:57:44.906580: Current learning rate: 0.00731\n",
      "2025-05-15 21:59:33.951441: train_loss -0.9514\n",
      "2025-05-15 21:59:33.951575: val_loss -0.9663\n",
      "2025-05-15 21:59:33.951611: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-15 21:59:33.951648: Epoch time: 109.05 s\n",
      "2025-05-15 21:59:34.465683: \n",
      "2025-05-15 21:59:34.465772: Epoch 295\n",
      "2025-05-15 21:59:34.465849: Current learning rate: 0.0073\n",
      "2025-05-15 22:01:23.505911: train_loss -0.9535\n",
      "2025-05-15 22:01:23.506033: val_loss -0.9655\n",
      "2025-05-15 22:01:23.506069: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-15 22:01:23.506103: Epoch time: 109.04 s\n",
      "2025-05-15 22:01:24.018499: \n",
      "2025-05-15 22:01:24.018627: Epoch 296\n",
      "2025-05-15 22:01:24.018701: Current learning rate: 0.00729\n",
      "2025-05-15 22:03:13.117248: train_loss -0.9516\n",
      "2025-05-15 22:03:13.117362: val_loss -0.9651\n",
      "2025-05-15 22:03:13.117393: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-15 22:03:13.117426: Epoch time: 109.1 s\n",
      "2025-05-15 22:03:13.117447: Yayy! New best EMA pseudo Dice: 0.984499990940094\n",
      "2025-05-15 22:03:13.851132: \n",
      "2025-05-15 22:03:13.851365: Epoch 297\n",
      "2025-05-15 22:03:13.851656: Current learning rate: 0.00728\n",
      "2025-05-15 22:05:02.685246: train_loss -0.9488\n",
      "2025-05-15 22:05:02.685369: val_loss -0.9617\n",
      "2025-05-15 22:05:02.685403: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-15 22:05:02.685436: Epoch time: 108.83 s\n",
      "2025-05-15 22:05:03.198812: \n",
      "2025-05-15 22:05:03.198974: Epoch 298\n",
      "2025-05-15 22:05:03.199050: Current learning rate: 0.00727\n",
      "2025-05-15 22:06:52.288835: train_loss -0.9375\n",
      "2025-05-15 22:06:52.288957: val_loss -0.9574\n",
      "2025-05-15 22:06:52.288998: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-15 22:06:52.289032: Epoch time: 109.09 s\n",
      "2025-05-15 22:06:52.805932: \n",
      "2025-05-15 22:06:52.806179: Epoch 299\n",
      "2025-05-15 22:06:52.806262: Current learning rate: 0.00726\n",
      "2025-05-15 22:08:41.807731: train_loss -0.9251\n",
      "2025-05-15 22:08:41.807963: val_loss -0.9397\n",
      "2025-05-15 22:08:41.808002: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-15 22:08:41.808035: Epoch time: 109.0 s\n",
      "2025-05-15 22:08:42.717612: \n",
      "2025-05-15 22:08:42.717894: Epoch 300\n",
      "2025-05-15 22:08:42.717970: Current learning rate: 0.00725\n",
      "2025-05-15 22:10:31.602671: train_loss -0.9296\n",
      "2025-05-15 22:10:31.602857: val_loss -0.9594\n",
      "2025-05-15 22:10:31.602888: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-15 22:10:31.602921: Epoch time: 108.89 s\n",
      "2025-05-15 22:10:32.122398: \n",
      "2025-05-15 22:10:32.122571: Epoch 301\n",
      "2025-05-15 22:10:32.122648: Current learning rate: 0.00724\n",
      "2025-05-15 22:12:21.160703: train_loss -0.9259\n",
      "2025-05-15 22:12:21.160914: val_loss -0.9589\n",
      "2025-05-15 22:12:21.160960: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-15 22:12:21.161040: Epoch time: 109.04 s\n",
      "2025-05-15 22:12:21.681585: \n",
      "2025-05-15 22:12:21.681916: Epoch 302\n",
      "Current learning rate: 0.00724\n",
      "2025-05-15 22:14:10.634294: train_loss -0.9371\n",
      "2025-05-15 22:14:10.634453: val_loss -0.9652\n",
      "2025-05-15 22:14:10.634486: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-15 22:14:10.634520: Epoch time: 108.95 s\n",
      "2025-05-15 22:14:11.157237: \n",
      "2025-05-15 22:14:11.157389: Epoch 303\n",
      "2025-05-15 22:14:11.157560: Current learning rate: 0.00723\n",
      "2025-05-15 22:16:00.139578: train_loss -0.9444\n",
      "2025-05-15 22:16:00.139747: val_loss -0.9634\n",
      "2025-05-15 22:16:00.140011: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-15 22:16:00.140106: Epoch time: 108.98 s\n",
      "2025-05-15 22:16:00.657187: \n",
      "2025-05-15 22:16:00.657332: Epoch 304\n",
      "2025-05-15 22:16:00.657515: Current learning rate: 0.00722\n",
      "2025-05-15 22:17:49.591345: train_loss -0.9494\n",
      "2025-05-15 22:17:49.591513: val_loss -0.9675\n",
      "2025-05-15 22:17:49.591592: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-15 22:17:49.591675: Epoch time: 108.93 s\n",
      "2025-05-15 22:17:50.111280: \n",
      "2025-05-15 22:17:50.111383: Epoch 305\n",
      "2025-05-15 22:17:50.111449: Current learning rate: 0.00721\n",
      "2025-05-15 22:19:39.038580: train_loss -0.9447\n",
      "2025-05-15 22:19:39.038707: val_loss -0.9645\n",
      "2025-05-15 22:19:39.038742: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-15 22:19:39.038841: Epoch time: 108.93 s\n",
      "2025-05-15 22:19:39.556437: \n",
      "2025-05-15 22:19:39.556652: Epoch 306\n",
      "2025-05-15 22:19:39.556777: Current learning rate: 0.0072\n",
      "2025-05-15 22:21:28.340243: train_loss -0.947\n",
      "2025-05-15 22:21:28.340368: val_loss -0.9625\n",
      "2025-05-15 22:21:28.340403: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-15 22:21:28.340436: Epoch time: 108.78 s\n",
      "2025-05-15 22:21:28.856514: \n",
      "2025-05-15 22:21:28.856782: Epoch 307\n",
      "2025-05-15 22:21:28.856862: Current learning rate: 0.00719\n",
      "2025-05-15 22:23:17.688499: train_loss -0.9504\n",
      "2025-05-15 22:23:17.688669: val_loss -0.9659\n",
      "2025-05-15 22:23:17.688722: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-15 22:23:17.688758: Epoch time: 108.83 s\n",
      "2025-05-15 22:23:18.210491: \n",
      "2025-05-15 22:23:18.210571: Epoch 308\n",
      "2025-05-15 22:23:18.210764: Current learning rate: 0.00718\n",
      "2025-05-15 22:25:07.277587: train_loss -0.9506\n",
      "2025-05-15 22:25:07.277721: val_loss -0.9617\n",
      "2025-05-15 22:25:07.277758: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 22:25:07.277791: Epoch time: 109.07 s\n",
      "2025-05-15 22:25:07.796080: \n",
      "2025-05-15 22:25:07.796299: Epoch 309\n",
      "2025-05-15 22:25:07.796371: Current learning rate: 0.00717\n",
      "2025-05-15 22:26:56.749242: train_loss -0.9512\n",
      "2025-05-15 22:26:56.749365: val_loss -0.9655\n",
      "2025-05-15 22:26:56.749399: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-15 22:26:56.749432: Epoch time: 108.95 s\n",
      "2025-05-15 22:26:57.281913: \n",
      "2025-05-15 22:26:57.281996: Epoch 310\n",
      "2025-05-15 22:26:57.282066: Current learning rate: 0.00716\n",
      "2025-05-15 22:28:46.323362: train_loss -0.9521\n",
      "2025-05-15 22:28:46.323516: val_loss -0.9628\n",
      "2025-05-15 22:28:46.323692: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-15 22:28:46.323773: Epoch time: 109.04 s\n",
      "2025-05-15 22:28:46.842450: \n",
      "2025-05-15 22:28:46.842785: Epoch 311\n",
      "2025-05-15 22:28:46.842860: Current learning rate: 0.00715\n",
      "2025-05-15 22:30:35.884184: train_loss -0.9507\n",
      "2025-05-15 22:30:35.884303: val_loss -0.9648\n",
      "2025-05-15 22:30:35.884335: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-15 22:30:35.884369: Epoch time: 109.04 s\n",
      "2025-05-15 22:30:35.884390: Yayy! New best EMA pseudo Dice: 0.984499990940094\n",
      "2025-05-15 22:30:36.613741: \n",
      "2025-05-15 22:30:36.613891: Epoch 312\n",
      "2025-05-15 22:30:36.613958: Current learning rate: 0.00714\n",
      "2025-05-15 22:32:25.397414: train_loss -0.9536\n",
      "2025-05-15 22:32:25.397532: val_loss -0.9651\n",
      "2025-05-15 22:32:25.397566: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-15 22:32:25.397599: Epoch time: 108.78 s\n",
      "2025-05-15 22:32:25.397620: Yayy! New best EMA pseudo Dice: 0.9846000075340271\n",
      "2025-05-15 22:32:26.312266: \n",
      "2025-05-15 22:32:26.312625: Epoch 313\n",
      "2025-05-15 22:32:26.312701: Current learning rate: 0.00713\n",
      "2025-05-15 22:34:15.304537: train_loss -0.9527\n",
      "2025-05-15 22:34:15.304663: val_loss -0.969\n",
      "2025-05-15 22:34:15.304697: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-15 22:34:15.304731: Epoch time: 108.99 s\n",
      "2025-05-15 22:34:15.304753: Yayy! New best EMA pseudo Dice: 0.9847000241279602\n",
      "2025-05-15 22:34:16.043703: \n",
      "2025-05-15 22:34:16.043880: Epoch 314\n",
      "2025-05-15 22:34:16.044079: Current learning rate: 0.00712\n",
      "2025-05-15 22:36:05.030614: train_loss -0.9532\n",
      "2025-05-15 22:36:05.031070: val_loss -0.967\n",
      "2025-05-15 22:36:05.031112: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-15 22:36:05.031146: Epoch time: 108.99 s\n",
      "2025-05-15 22:36:05.031167: Yayy! New best EMA pseudo Dice: 0.9847999811172485\n",
      "2025-05-15 22:36:05.768698: \n",
      "2025-05-15 22:36:05.768805: Epoch 315\n",
      "2025-05-15 22:36:05.768909: Current learning rate: 0.00711\n",
      "2025-05-15 22:37:54.775056: train_loss -0.9437\n",
      "2025-05-15 22:37:54.775308: val_loss -0.9624\n",
      "2025-05-15 22:37:54.775443: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 22:37:54.775542: Epoch time: 109.01 s\n",
      "2025-05-15 22:37:55.301895: \n",
      "2025-05-15 22:37:55.302223: Epoch 316\n",
      "2025-05-15 22:37:55.302296: Current learning rate: 0.0071\n",
      "2025-05-15 22:39:44.215993: train_loss -0.9493\n",
      "2025-05-15 22:39:44.216115: val_loss -0.9644\n",
      "2025-05-15 22:39:44.216149: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-15 22:39:44.216267: Epoch time: 108.91 s\n",
      "2025-05-15 22:39:44.737071: \n",
      "2025-05-15 22:39:44.737239: Epoch 317\n",
      "2025-05-15 22:39:44.737339: Current learning rate: 0.0071\n",
      "2025-05-15 22:41:33.620007: train_loss -0.9522\n",
      "2025-05-15 22:41:33.620131: val_loss -0.9666\n",
      "2025-05-15 22:41:33.620165: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-15 22:41:33.620197: Epoch time: 108.88 s\n",
      "2025-05-15 22:41:33.620219: Yayy! New best EMA pseudo Dice: 0.9847999811172485\n",
      "2025-05-15 22:41:34.358210: \n",
      "2025-05-15 22:41:34.358320: Epoch 318\n",
      "2025-05-15 22:41:34.358385: Current learning rate: 0.00709\n",
      "2025-05-15 22:43:23.446507: train_loss -0.9528\n",
      "2025-05-15 22:43:23.446683: val_loss -0.9644\n",
      "2025-05-15 22:43:23.446715: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-15 22:43:23.446749: Epoch time: 109.09 s\n",
      "2025-05-15 22:43:23.446771: Yayy! New best EMA pseudo Dice: 0.9848999977111816\n",
      "2025-05-15 22:43:24.174180: \n",
      "2025-05-15 22:43:24.174321: Epoch 319\n",
      "2025-05-15 22:43:24.174389: Current learning rate: 0.00708\n",
      "2025-05-15 22:45:12.995869: train_loss -0.9516\n",
      "2025-05-15 22:45:12.996091: val_loss -0.962\n",
      "2025-05-15 22:45:12.996140: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-15 22:45:12.996222: Epoch time: 108.82 s\n",
      "2025-05-15 22:45:13.517246: \n",
      "2025-05-15 22:45:13.517379: Epoch 320\n",
      "2025-05-15 22:45:13.517454: Current learning rate: 0.00707\n",
      "2025-05-15 22:47:02.335696: train_loss -0.952\n",
      "2025-05-15 22:47:02.335819: val_loss -0.9677\n",
      "2025-05-15 22:47:02.335852: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-15 22:47:02.335886: Epoch time: 108.82 s\n",
      "2025-05-15 22:47:02.854138: \n",
      "2025-05-15 22:47:02.854219: Epoch 321\n",
      "2025-05-15 22:47:02.854283: Current learning rate: 0.00706\n",
      "2025-05-15 22:48:51.958117: train_loss -0.9532\n",
      "2025-05-15 22:48:51.958382: val_loss -0.9641\n",
      "2025-05-15 22:48:51.958466: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-15 22:48:51.958505: Epoch time: 109.1 s\n",
      "2025-05-15 22:48:52.481801: \n",
      "2025-05-15 22:48:52.481958: Epoch 322\n",
      "2025-05-15 22:48:52.482025: Current learning rate: 0.00705\n",
      "2025-05-15 22:50:41.503461: train_loss -0.9542\n",
      "2025-05-15 22:50:41.503648: val_loss -0.963\n",
      "2025-05-15 22:50:41.503681: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-15 22:50:41.503716: Epoch time: 109.02 s\n",
      "2025-05-15 22:50:42.029647: \n",
      "2025-05-15 22:50:42.029953: Epoch 323\n",
      "2025-05-15 22:50:42.030026: Current learning rate: 0.00704\n",
      "2025-05-15 22:52:31.000737: train_loss -0.9519\n",
      "2025-05-15 22:52:31.000855: val_loss -0.9668\n",
      "2025-05-15 22:52:31.000890: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-15 22:52:31.000923: Epoch time: 108.97 s\n",
      "2025-05-15 22:52:31.711528: \n",
      "2025-05-15 22:52:31.711688: Epoch 324\n",
      "2025-05-15 22:52:31.711799: Current learning rate: 0.00703\n",
      "2025-05-15 22:54:20.754771: train_loss -0.9512\n",
      "2025-05-15 22:54:20.754896: val_loss -0.964\n",
      "2025-05-15 22:54:20.754933: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 22:54:20.754967: Epoch time: 109.04 s\n",
      "2025-05-15 22:54:21.271308: \n",
      "2025-05-15 22:54:21.271523: Epoch 325\n",
      "2025-05-15 22:54:21.271634: Current learning rate: 0.00702\n",
      "2025-05-15 22:56:10.114284: train_loss -0.9516\n",
      "2025-05-15 22:56:10.114483: val_loss -0.9655\n",
      "2025-05-15 22:56:10.114555: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-15 22:56:10.114596: Epoch time: 108.84 s\n",
      "2025-05-15 22:56:10.643259: \n",
      "2025-05-15 22:56:10.643464: Epoch 326\n",
      "2025-05-15 22:56:10.643537: Current learning rate: 0.00701\n",
      "2025-05-15 22:57:59.691991: train_loss -0.9526\n",
      "2025-05-15 22:57:59.692120: val_loss -0.9636\n",
      "2025-05-15 22:57:59.692154: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 22:57:59.692188: Epoch time: 109.05 s\n",
      "2025-05-15 22:58:00.216034: \n",
      "2025-05-15 22:58:00.216313: Epoch 327\n",
      "2025-05-15 22:58:00.216413: Current learning rate: 0.007\n",
      "2025-05-15 22:59:49.221872: train_loss -0.9399\n",
      "2025-05-15 22:59:49.222049: val_loss -0.9516\n",
      "2025-05-15 22:59:49.222081: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-15 22:59:49.222114: Epoch time: 109.01 s\n",
      "2025-05-15 22:59:49.745412: \n",
      "2025-05-15 22:59:49.745500: Epoch 328\n",
      "2025-05-15 22:59:49.745564: Current learning rate: 0.00699\n",
      "2025-05-15 23:01:38.775475: train_loss -0.9027\n",
      "2025-05-15 23:01:38.775600: val_loss -0.9425\n",
      "2025-05-15 23:01:38.775640: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-15 23:01:38.775679: Epoch time: 109.03 s\n",
      "2025-05-15 23:01:39.295998: \n",
      "2025-05-15 23:01:39.296179: Epoch 329\n",
      "2025-05-15 23:01:39.296252: Current learning rate: 0.00698\n",
      "2025-05-15 23:03:28.289363: train_loss -0.921\n",
      "2025-05-15 23:03:28.289533: val_loss -0.9444\n",
      "2025-05-15 23:03:28.289566: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-15 23:03:28.289601: Epoch time: 108.99 s\n",
      "2025-05-15 23:03:28.812944: \n",
      "2025-05-15 23:03:28.813177: Epoch 330\n",
      "2025-05-15 23:03:28.813250: Current learning rate: 0.00697\n",
      "2025-05-15 23:05:17.899044: train_loss -0.9353\n",
      "2025-05-15 23:05:17.899328: val_loss -0.9615\n",
      "2025-05-15 23:05:17.899368: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 23:05:17.899402: Epoch time: 109.09 s\n",
      "2025-05-15 23:05:18.426357: \n",
      "2025-05-15 23:05:18.426541: Epoch 331\n",
      "2025-05-15 23:05:18.426615: Current learning rate: 0.00696\n",
      "2025-05-15 23:07:07.502829: train_loss -0.9481\n",
      "2025-05-15 23:07:07.502952: val_loss -0.9682\n",
      "2025-05-15 23:07:07.502986: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-15 23:07:07.503020: Epoch time: 109.08 s\n",
      "2025-05-15 23:07:08.017628: \n",
      "2025-05-15 23:07:08.017854: Epoch 332\n",
      "2025-05-15 23:07:08.017931: Current learning rate: 0.00696\n",
      "2025-05-15 23:08:56.851654: train_loss -0.9447\n",
      "2025-05-15 23:08:56.851772: val_loss -0.9601\n",
      "2025-05-15 23:08:56.851806: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-15 23:08:56.851842: Epoch time: 108.83 s\n",
      "2025-05-15 23:08:57.371427: \n",
      "2025-05-15 23:08:57.371572: Epoch 333\n",
      "2025-05-15 23:08:57.371652: Current learning rate: 0.00695\n",
      "2025-05-15 23:10:46.425643: train_loss -0.9456\n",
      "2025-05-15 23:10:46.425821: val_loss -0.9647\n",
      "2025-05-15 23:10:46.425972: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-15 23:10:46.426156: Epoch time: 109.05 s\n",
      "2025-05-15 23:10:46.953271: \n",
      "2025-05-15 23:10:46.953453: Epoch 334\n",
      "2025-05-15 23:10:46.953595: Current learning rate: 0.00694\n",
      "2025-05-15 23:12:35.922886: train_loss -0.9511\n",
      "2025-05-15 23:12:35.923018: val_loss -0.9634\n",
      "2025-05-15 23:12:35.923056: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-15 23:12:35.923091: Epoch time: 108.97 s\n",
      "2025-05-15 23:12:36.443345: \n",
      "2025-05-15 23:12:36.443475: Epoch 335\n",
      "2025-05-15 23:12:36.443542: Current learning rate: 0.00693\n",
      "2025-05-15 23:14:25.437164: train_loss -0.9509\n",
      "2025-05-15 23:14:25.437282: val_loss -0.9664\n",
      "2025-05-15 23:14:25.437321: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-15 23:14:25.437359: Epoch time: 108.99 s\n",
      "2025-05-15 23:14:25.958633: \n",
      "2025-05-15 23:14:25.958899: Epoch 336\n",
      "2025-05-15 23:14:25.958969: Current learning rate: 0.00692\n",
      "2025-05-15 23:16:14.966494: train_loss -0.9518\n",
      "2025-05-15 23:16:14.966619: val_loss -0.9632\n",
      "2025-05-15 23:16:14.966651: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-15 23:16:14.966685: Epoch time: 109.01 s\n",
      "2025-05-15 23:16:15.683584: \n",
      "2025-05-15 23:16:15.683675: Epoch 337\n",
      "2025-05-15 23:16:15.683830: Current learning rate: 0.00691\n",
      "2025-05-15 23:18:04.757178: train_loss -0.9477\n",
      "2025-05-15 23:18:04.757441: val_loss -0.9637\n",
      "2025-05-15 23:18:04.757483: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-15 23:18:04.757522: Epoch time: 109.07 s\n",
      "2025-05-15 23:18:05.286286: \n",
      "2025-05-15 23:18:05.286383: Epoch 338\n",
      "2025-05-15 23:18:05.286449: Current learning rate: 0.0069\n",
      "2025-05-15 23:19:54.369397: train_loss -0.9539\n",
      "2025-05-15 23:19:54.369520: val_loss -0.9696\n",
      "2025-05-15 23:19:54.369552: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-15 23:19:54.369586: Epoch time: 109.08 s\n",
      "2025-05-15 23:19:54.893637: \n",
      "2025-05-15 23:19:54.893728: Epoch 339\n",
      "2025-05-15 23:19:54.893793: Current learning rate: 0.00689\n",
      "2025-05-15 23:21:43.927197: train_loss -0.9527\n",
      "2025-05-15 23:21:43.927344: val_loss -0.962\n",
      "2025-05-15 23:21:43.927378: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-15 23:21:43.927462: Epoch time: 109.03 s\n",
      "2025-05-15 23:21:44.459587: \n",
      "2025-05-15 23:21:44.459747: Epoch 340\n",
      "2025-05-15 23:21:44.459820: Current learning rate: 0.00688\n",
      "2025-05-15 23:23:33.577990: train_loss -0.9515\n",
      "2025-05-15 23:23:33.578170: val_loss -0.9636\n",
      "2025-05-15 23:23:33.578205: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-15 23:23:33.578242: Epoch time: 109.12 s\n",
      "2025-05-15 23:23:34.096756: \n",
      "2025-05-15 23:23:34.097038: Epoch 341\n",
      "2025-05-15 23:23:34.097111: Current learning rate: 0.00687\n",
      "2025-05-15 23:25:23.173617: train_loss -0.9517\n",
      "2025-05-15 23:25:23.173761: val_loss -0.9663\n",
      "2025-05-15 23:25:23.173812: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-15 23:25:23.173859: Epoch time: 109.08 s\n",
      "2025-05-15 23:25:23.700757: \n",
      "2025-05-15 23:25:23.701008: Epoch 342\n",
      "2025-05-15 23:25:23.701080: Current learning rate: 0.00686\n",
      "2025-05-15 23:27:12.584190: train_loss -0.9549\n",
      "2025-05-15 23:27:12.584308: val_loss -0.9669\n",
      "2025-05-15 23:27:12.584340: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-15 23:27:12.584373: Epoch time: 108.88 s\n",
      "2025-05-15 23:27:13.103132: \n",
      "2025-05-15 23:27:13.103225: Epoch 343\n",
      "2025-05-15 23:27:13.103290: Current learning rate: 0.00685\n",
      "2025-05-15 23:29:02.174818: train_loss -0.956\n",
      "2025-05-15 23:29:02.174986: val_loss -0.9679\n",
      "2025-05-15 23:29:02.175093: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-15 23:29:02.175168: Epoch time: 109.07 s\n",
      "2025-05-15 23:29:02.701757: \n",
      "2025-05-15 23:29:02.701888: Epoch 344\n",
      "2025-05-15 23:29:02.701955: Current learning rate: 0.00684\n",
      "2025-05-15 23:30:51.773288: train_loss -0.9532\n",
      "2025-05-15 23:30:51.773401: val_loss -0.968\n",
      "2025-05-15 23:30:51.773433: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-15 23:30:51.773467: Epoch time: 109.07 s\n",
      "2025-05-15 23:30:51.773487: Yayy! New best EMA pseudo Dice: 0.9850000143051147\n",
      "2025-05-15 23:30:52.518415: \n",
      "2025-05-15 23:30:52.518705: Epoch 345\n",
      "2025-05-15 23:30:52.518806: Current learning rate: 0.00683\n",
      "2025-05-15 23:32:41.522955: train_loss -0.9558\n",
      "2025-05-15 23:32:41.523113: val_loss -0.9602\n",
      "2025-05-15 23:32:41.523146: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-15 23:32:41.523180: Epoch time: 109.01 s\n",
      "2025-05-15 23:32:42.049121: \n",
      "2025-05-15 23:32:42.049206: Epoch 346\n",
      "2025-05-15 23:32:42.049269: Current learning rate: 0.00682\n",
      "2025-05-15 23:34:30.946007: train_loss -0.9522\n",
      "2025-05-15 23:34:30.946138: val_loss -0.965\n",
      "2025-05-15 23:34:30.946299: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-15 23:34:30.946376: Epoch time: 108.9 s\n",
      "2025-05-15 23:34:31.475480: \n",
      "2025-05-15 23:34:31.475771: Epoch 347\n",
      "2025-05-15 23:34:31.475847: Current learning rate: 0.00681\n",
      "2025-05-15 23:36:20.285947: train_loss -0.9547\n",
      "2025-05-15 23:36:20.286130: val_loss -0.9684\n",
      "2025-05-15 23:36:20.286164: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-15 23:36:20.286196: Epoch time: 108.81 s\n",
      "2025-05-15 23:36:20.811577: \n",
      "2025-05-15 23:36:20.811754: Epoch 348\n",
      "2025-05-15 23:36:20.811829: Current learning rate: 0.0068\n",
      "2025-05-15 23:38:09.794583: train_loss -0.9533\n",
      "2025-05-15 23:38:09.794801: val_loss -0.9688\n",
      "2025-05-15 23:38:09.794839: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-15 23:38:09.794904: Epoch time: 108.98 s\n",
      "2025-05-15 23:38:09.794943: Yayy! New best EMA pseudo Dice: 0.9850999712944031\n",
      "2025-05-15 23:38:10.746950: \n",
      "2025-05-15 23:38:10.747048: Epoch 349\n",
      "2025-05-15 23:38:10.747116: Current learning rate: 0.0068\n",
      "2025-05-15 23:39:59.609384: train_loss -0.9525\n",
      "2025-05-15 23:39:59.609566: val_loss -0.9695\n",
      "2025-05-15 23:39:59.609707: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-15 23:39:59.609819: Epoch time: 108.86 s\n",
      "2025-05-15 23:39:59.814006: Yayy! New best EMA pseudo Dice: 0.9853000044822693\n",
      "2025-05-15 23:40:00.551741: \n",
      "2025-05-15 23:40:00.552105: Epoch 350\n",
      "2025-05-15 23:40:00.552372: Current learning rate: 0.00679\n",
      "2025-05-15 23:41:49.616894: train_loss -0.9552\n",
      "2025-05-15 23:41:49.617073: val_loss -0.9692\n",
      "2025-05-15 23:41:49.617114: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-15 23:41:49.617154: Epoch time: 109.07 s\n",
      "2025-05-15 23:41:49.617180: Yayy! New best EMA pseudo Dice: 0.9854000210762024\n",
      "2025-05-15 23:41:50.356555: \n",
      "2025-05-15 23:41:50.356647: Epoch 351\n",
      "2025-05-15 23:41:50.356722: Current learning rate: 0.00678\n",
      "2025-05-15 23:43:39.454819: train_loss -0.9551\n",
      "2025-05-15 23:43:39.454942: val_loss -0.9656\n",
      "2025-05-15 23:43:39.454976: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-15 23:43:39.455009: Epoch time: 109.1 s\n",
      "2025-05-15 23:43:39.455030: Yayy! New best EMA pseudo Dice: 0.9854000210762024\n",
      "2025-05-15 23:43:40.196635: \n",
      "2025-05-15 23:43:40.196805: Epoch 352\n",
      "2025-05-15 23:43:40.196876: Current learning rate: 0.00677\n",
      "2025-05-15 23:45:29.170273: train_loss -0.9546\n",
      "2025-05-15 23:45:29.170491: val_loss -0.9655\n",
      "2025-05-15 23:45:29.170568: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-15 23:45:29.170613: Epoch time: 108.97 s\n",
      "2025-05-15 23:45:29.701821: \n",
      "2025-05-15 23:45:29.701910: Epoch 353\n",
      "2025-05-15 23:45:29.701975: Current learning rate: 0.00676\n",
      "2025-05-15 23:47:18.574672: train_loss -0.9566\n",
      "2025-05-15 23:47:18.575085: val_loss -0.9694\n",
      "2025-05-15 23:47:18.575133: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-15 23:47:18.575174: Epoch time: 108.87 s\n",
      "2025-05-15 23:47:18.575202: Yayy! New best EMA pseudo Dice: 0.9854999780654907\n",
      "2025-05-15 23:47:19.321065: \n",
      "2025-05-15 23:47:19.321276: Epoch 354\n",
      "2025-05-15 23:47:19.321367: Current learning rate: 0.00675\n",
      "2025-05-15 23:49:08.388898: train_loss -0.9544\n",
      "2025-05-15 23:49:08.389067: val_loss -0.97\n",
      "2025-05-15 23:49:08.389102: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-15 23:49:08.389137: Epoch time: 109.07 s\n",
      "2025-05-15 23:49:08.389158: Yayy! New best EMA pseudo Dice: 0.9857000112533569\n",
      "2025-05-15 23:49:09.133472: \n",
      "2025-05-15 23:49:09.133716: Epoch 355\n",
      "2025-05-15 23:49:09.133857: Current learning rate: 0.00674\n",
      "2025-05-15 23:50:58.056674: train_loss -0.9536\n",
      "2025-05-15 23:50:58.056827: val_loss -0.9662\n",
      "2025-05-15 23:50:58.056861: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-15 23:50:58.056894: Epoch time: 108.92 s\n",
      "2025-05-15 23:50:58.056915: Yayy! New best EMA pseudo Dice: 0.9857000112533569\n",
      "2025-05-15 23:50:58.799080: \n",
      "2025-05-15 23:50:58.799235: Epoch 356\n",
      "2025-05-15 23:50:58.799330: Current learning rate: 0.00673\n",
      "2025-05-15 23:52:47.667651: train_loss -0.9527\n",
      "2025-05-15 23:52:47.667860: val_loss -0.9657\n",
      "2025-05-15 23:52:47.667923: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-15 23:52:47.667961: Epoch time: 108.87 s\n",
      "2025-05-15 23:52:48.345015: \n",
      "2025-05-15 23:52:48.345172: Epoch 357\n",
      "2025-05-15 23:52:48.345240: Current learning rate: 0.00672\n",
      "2025-05-15 23:54:37.225818: train_loss -0.9547\n",
      "2025-05-15 23:54:37.225937: val_loss -0.9687\n",
      "2025-05-15 23:54:37.225972: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-15 23:54:37.226005: Epoch time: 108.88 s\n",
      "2025-05-15 23:54:37.226027: Yayy! New best EMA pseudo Dice: 0.98580002784729\n",
      "2025-05-15 23:54:37.955976: \n",
      "2025-05-15 23:54:37.956071: Epoch 358\n",
      "2025-05-15 23:54:37.956137: Current learning rate: 0.00671\n",
      "2025-05-15 23:56:26.915895: train_loss -0.9565\n",
      "2025-05-15 23:56:26.916067: val_loss -0.9684\n",
      "2025-05-15 23:56:26.916215: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-15 23:56:26.916324: Epoch time: 108.96 s\n",
      "2025-05-15 23:56:27.442541: \n",
      "2025-05-15 23:56:27.442621: Epoch 359\n",
      "2025-05-15 23:56:27.442684: Current learning rate: 0.0067\n",
      "2025-05-15 23:58:16.460925: train_loss -0.9552\n",
      "2025-05-15 23:58:16.461051: val_loss -0.9619\n",
      "2025-05-15 23:58:16.461084: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-15 23:58:16.461119: Epoch time: 109.02 s\n",
      "2025-05-15 23:58:16.981428: \n",
      "2025-05-15 23:58:16.981555: Epoch 360\n",
      "2025-05-15 23:58:16.981630: Current learning rate: 0.00669\n",
      "2025-05-16 00:00:05.958832: train_loss -0.9545\n",
      "2025-05-16 00:00:05.959028: val_loss -0.9654\n",
      "2025-05-16 00:00:05.959087: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 00:00:05.959135: Epoch time: 108.98 s\n",
      "2025-05-16 00:00:06.660348: \n",
      "2025-05-16 00:00:06.660572: Epoch 361\n",
      "2025-05-16 00:00:06.660647: Current learning rate: 0.00668\n",
      "2025-05-16 00:01:55.718726: train_loss -0.9547\n",
      "2025-05-16 00:01:55.718853: val_loss -0.9644\n",
      "2025-05-16 00:01:55.718889: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-16 00:01:55.718923: Epoch time: 109.06 s\n",
      "2025-05-16 00:01:56.247652: \n",
      "2025-05-16 00:01:56.247923: Epoch 362\n",
      "2025-05-16 00:01:56.247993: Current learning rate: 0.00667\n",
      "2025-05-16 00:03:45.270372: train_loss -0.9539\n",
      "2025-05-16 00:03:45.270537: val_loss -0.9649\n",
      "2025-05-16 00:03:45.270684: Pseudo dice [np.float32(0.9845)]\n",
      "2025-05-16 00:03:45.270739: Epoch time: 109.02 s\n",
      "2025-05-16 00:03:45.791533: \n",
      "2025-05-16 00:03:45.791643: Epoch 363\n",
      "2025-05-16 00:03:45.791709: Current learning rate: 0.00666\n",
      "2025-05-16 00:05:34.900858: train_loss -0.9537\n",
      "2025-05-16 00:05:34.900995: val_loss -0.9694\n",
      "2025-05-16 00:05:34.901028: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 00:05:34.901060: Epoch time: 109.11 s\n",
      "2025-05-16 00:05:35.431064: \n",
      "2025-05-16 00:05:35.431381: Epoch 364\n",
      "2025-05-16 00:05:35.431457: Current learning rate: 0.00665\n",
      "2025-05-16 00:07:24.403796: train_loss -0.9555\n",
      "2025-05-16 00:07:24.403929: val_loss -0.9711\n",
      "2025-05-16 00:07:24.403963: Pseudo dice [np.float32(0.9884)]\n",
      "2025-05-16 00:07:24.404009: Epoch time: 108.97 s\n",
      "2025-05-16 00:07:24.404037: Yayy! New best EMA pseudo Dice: 0.98580002784729\n",
      "2025-05-16 00:07:25.145798: \n",
      "2025-05-16 00:07:25.145899: Epoch 365\n",
      "2025-05-16 00:07:25.145966: Current learning rate: 0.00665\n",
      "2025-05-16 00:09:14.060605: train_loss -0.9555\n",
      "2025-05-16 00:09:14.060729: val_loss -0.9675\n",
      "2025-05-16 00:09:14.060774: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 00:09:14.060820: Epoch time: 108.92 s\n",
      "2025-05-16 00:09:14.582271: \n",
      "2025-05-16 00:09:14.582421: Epoch 366\n",
      "2025-05-16 00:09:14.582494: Current learning rate: 0.00664\n",
      "2025-05-16 00:11:03.685885: train_loss -0.9567\n",
      "2025-05-16 00:11:03.686116: val_loss -0.9645\n",
      "2025-05-16 00:11:03.686152: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 00:11:03.686239: Epoch time: 109.1 s\n",
      "2025-05-16 00:11:04.211471: \n",
      "2025-05-16 00:11:04.211555: Epoch 367\n",
      "2025-05-16 00:11:04.211620: Current learning rate: 0.00663\n",
      "2025-05-16 00:12:53.286062: train_loss -0.9535\n",
      "2025-05-16 00:12:53.286186: val_loss -0.9643\n",
      "2025-05-16 00:12:53.286218: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-16 00:12:53.286361: Epoch time: 109.08 s\n",
      "2025-05-16 00:12:53.806937: \n",
      "2025-05-16 00:12:53.807091: Epoch 368\n",
      "2025-05-16 00:12:53.807164: Current learning rate: 0.00662\n",
      "2025-05-16 00:14:42.832741: train_loss -0.9552\n",
      "2025-05-16 00:14:42.832945: val_loss -0.9681\n",
      "2025-05-16 00:14:42.832980: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 00:14:42.833018: Epoch time: 109.03 s\n",
      "2025-05-16 00:14:43.361467: \n",
      "2025-05-16 00:14:43.361560: Epoch 369\n",
      "2025-05-16 00:14:43.361628: Current learning rate: 0.00661\n",
      "2025-05-16 00:16:32.177123: train_loss -0.9535\n",
      "2025-05-16 00:16:32.177246: val_loss -0.968\n",
      "2025-05-16 00:16:32.177277: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 00:16:32.177311: Epoch time: 108.82 s\n",
      "2025-05-16 00:16:32.718484: \n",
      "2025-05-16 00:16:32.718616: Epoch 370\n",
      "2025-05-16 00:16:32.718689: Current learning rate: 0.0066\n",
      "2025-05-16 00:18:21.749436: train_loss -0.9548\n",
      "2025-05-16 00:18:21.749615: val_loss -0.9669\n",
      "2025-05-16 00:18:21.749656: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 00:18:21.749691: Epoch time: 109.03 s\n",
      "2025-05-16 00:18:22.273042: \n",
      "2025-05-16 00:18:22.273127: Epoch 371\n",
      "2025-05-16 00:18:22.273193: Current learning rate: 0.00659\n",
      "2025-05-16 00:20:11.147787: train_loss -0.9557\n",
      "2025-05-16 00:20:11.147907: val_loss -0.967\n",
      "2025-05-16 00:20:11.147943: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 00:20:11.147976: Epoch time: 108.88 s\n",
      "2025-05-16 00:20:11.670601: \n",
      "2025-05-16 00:20:11.670755: Epoch 372\n",
      "2025-05-16 00:20:11.670846: Current learning rate: 0.00658\n",
      "2025-05-16 00:22:00.535795: train_loss -0.9577\n",
      "2025-05-16 00:22:00.535957: val_loss -0.9665\n",
      "2025-05-16 00:22:00.535989: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 00:22:00.536022: Epoch time: 108.87 s\n",
      "2025-05-16 00:22:01.245535: \n",
      "2025-05-16 00:22:01.245743: Epoch 373\n",
      "2025-05-16 00:22:01.245830: Current learning rate: 0.00657\n",
      "2025-05-16 00:23:50.079923: train_loss -0.9536\n",
      "2025-05-16 00:23:50.080108: val_loss -0.9666\n",
      "2025-05-16 00:23:50.080163: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 00:23:50.080203: Epoch time: 108.83 s\n",
      "2025-05-16 00:23:50.609123: \n",
      "2025-05-16 00:23:50.609268: Epoch 374\n",
      "2025-05-16 00:23:50.609346: Current learning rate: 0.00656\n",
      "2025-05-16 00:25:39.743655: train_loss -0.9552\n",
      "2025-05-16 00:25:39.743849: val_loss -0.9666\n",
      "2025-05-16 00:25:39.743899: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 00:25:39.743938: Epoch time: 109.14 s\n",
      "2025-05-16 00:25:40.279404: \n",
      "2025-05-16 00:25:40.279589: Epoch 375\n",
      "2025-05-16 00:25:40.279823: Current learning rate: 0.00655\n",
      "2025-05-16 00:27:29.209990: train_loss -0.9558\n",
      "2025-05-16 00:27:29.210106: val_loss -0.967\n",
      "2025-05-16 00:27:29.210142: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 00:27:29.210175: Epoch time: 108.93 s\n",
      "2025-05-16 00:27:29.750800: \n",
      "2025-05-16 00:27:29.750978: Epoch 376\n",
      "2025-05-16 00:27:29.751047: Current learning rate: 0.00654\n",
      "2025-05-16 00:29:18.870863: train_loss -0.9569\n",
      "2025-05-16 00:29:18.871036: val_loss -0.9641\n",
      "2025-05-16 00:29:18.871073: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-16 00:29:18.871107: Epoch time: 109.12 s\n",
      "2025-05-16 00:29:19.400317: \n",
      "2025-05-16 00:29:19.400537: Epoch 377\n",
      "2025-05-16 00:29:19.400632: Current learning rate: 0.00653\n",
      "2025-05-16 00:31:08.418820: train_loss -0.9562\n",
      "2025-05-16 00:31:08.418939: val_loss -0.9659\n",
      "2025-05-16 00:31:08.418972: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 00:31:08.419005: Epoch time: 109.02 s\n",
      "2025-05-16 00:31:08.945802: \n",
      "2025-05-16 00:31:08.945889: Epoch 378\n",
      "2025-05-16 00:31:08.945954: Current learning rate: 0.00652\n",
      "2025-05-16 00:32:57.922007: train_loss -0.9557\n",
      "2025-05-16 00:32:57.922141: val_loss -0.9661\n",
      "2025-05-16 00:32:57.922256: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-16 00:32:57.922352: Epoch time: 108.98 s\n",
      "2025-05-16 00:32:58.446923: \n",
      "2025-05-16 00:32:58.447067: Epoch 379\n",
      "2025-05-16 00:32:58.447135: Current learning rate: 0.00651\n",
      "2025-05-16 00:34:47.320077: train_loss -0.9567\n",
      "2025-05-16 00:34:47.320198: val_loss -0.9674\n",
      "2025-05-16 00:34:47.320231: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 00:34:47.320263: Epoch time: 108.87 s\n",
      "2025-05-16 00:34:47.843531: \n",
      "2025-05-16 00:34:47.843618: Epoch 380\n",
      "2025-05-16 00:34:47.843682: Current learning rate: 0.0065\n",
      "2025-05-16 00:36:36.882248: train_loss -0.9563\n",
      "2025-05-16 00:36:36.882422: val_loss -0.9693\n",
      "2025-05-16 00:36:36.882537: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 00:36:36.882616: Epoch time: 109.04 s\n",
      "2025-05-16 00:36:37.408730: \n",
      "2025-05-16 00:36:37.408808: Epoch 381\n",
      "2025-05-16 00:36:37.408872: Current learning rate: 0.00649\n",
      "2025-05-16 00:38:26.187554: train_loss -0.9554\n",
      "2025-05-16 00:38:26.187666: val_loss -0.9681\n",
      "2025-05-16 00:38:26.187697: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 00:38:26.187729: Epoch time: 108.78 s\n",
      "2025-05-16 00:38:26.718420: \n",
      "2025-05-16 00:38:26.718502: Epoch 382\n",
      "2025-05-16 00:38:26.718564: Current learning rate: 0.00648\n",
      "2025-05-16 00:40:15.664993: train_loss -0.9552\n",
      "2025-05-16 00:40:15.665120: val_loss -0.9663\n",
      "2025-05-16 00:40:15.665153: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 00:40:15.665201: Epoch time: 108.95 s\n",
      "2025-05-16 00:40:16.196648: \n",
      "2025-05-16 00:40:16.196820: Epoch 383\n",
      "2025-05-16 00:40:16.196920: Current learning rate: 0.00648\n",
      "2025-05-16 00:42:05.155130: train_loss -0.955\n",
      "2025-05-16 00:42:05.155256: val_loss -0.9689\n",
      "2025-05-16 00:42:05.155288: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 00:42:05.155322: Epoch time: 108.96 s\n",
      "2025-05-16 00:42:05.684752: \n",
      "2025-05-16 00:42:05.684931: Epoch 384\n",
      "2025-05-16 00:42:05.685008: Current learning rate: 0.00647\n",
      "2025-05-16 00:43:54.602812: train_loss -0.9545\n",
      "2025-05-16 00:43:54.602929: val_loss -0.9601\n",
      "2025-05-16 00:43:54.602963: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-16 00:43:54.603005: Epoch time: 108.92 s\n",
      "2025-05-16 00:43:55.325735: \n",
      "2025-05-16 00:43:55.325828: Epoch 385\n",
      "2025-05-16 00:43:55.325894: Current learning rate: 0.00646\n",
      "2025-05-16 00:45:44.156730: train_loss -0.9541\n",
      "2025-05-16 00:45:44.156864: val_loss -0.9627\n",
      "2025-05-16 00:45:44.156970: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-16 00:45:44.157023: Epoch time: 108.83 s\n",
      "2025-05-16 00:45:44.695709: \n",
      "2025-05-16 00:45:44.695863: Epoch 386\n",
      "2025-05-16 00:45:44.695984: Current learning rate: 0.00645\n",
      "2025-05-16 00:47:33.672445: train_loss -0.9516\n",
      "2025-05-16 00:47:33.672563: val_loss -0.9678\n",
      "2025-05-16 00:47:33.672595: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 00:47:33.672627: Epoch time: 108.98 s\n",
      "2025-05-16 00:47:34.197372: \n",
      "2025-05-16 00:47:34.197463: Epoch 387\n",
      "2025-05-16 00:47:34.197526: Current learning rate: 0.00644\n",
      "2025-05-16 00:49:23.195762: train_loss -0.9516\n",
      "2025-05-16 00:49:23.195895: val_loss -0.9666\n",
      "2025-05-16 00:49:23.195930: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 00:49:23.195977: Epoch time: 109.0 s\n",
      "2025-05-16 00:49:23.730113: \n",
      "2025-05-16 00:49:23.730272: Epoch 388\n",
      "2025-05-16 00:49:23.730352: Current learning rate: 0.00643\n",
      "2025-05-16 00:51:12.655852: train_loss -0.954\n",
      "2025-05-16 00:51:12.656118: val_loss -0.9677\n",
      "2025-05-16 00:51:12.656159: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 00:51:12.656192: Epoch time: 108.93 s\n",
      "2025-05-16 00:51:13.197265: \n",
      "2025-05-16 00:51:13.197481: Epoch 389\n",
      "2025-05-16 00:51:13.197577: Current learning rate: 0.00642\n",
      "2025-05-16 00:53:02.253303: train_loss -0.9581\n",
      "2025-05-16 00:53:02.253451: val_loss -0.9701\n",
      "2025-05-16 00:53:02.253491: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 00:53:02.253525: Epoch time: 109.06 s\n",
      "2025-05-16 00:53:02.785170: \n",
      "2025-05-16 00:53:02.785263: Epoch 390\n",
      "2025-05-16 00:53:02.785331: Current learning rate: 0.00641\n",
      "2025-05-16 00:54:51.908698: train_loss -0.9537\n",
      "2025-05-16 00:54:51.908861: val_loss -0.9633\n",
      "2025-05-16 00:54:51.908941: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-16 00:54:51.909075: Epoch time: 109.12 s\n",
      "2025-05-16 00:54:52.437221: \n",
      "2025-05-16 00:54:52.437305: Epoch 391\n",
      "2025-05-16 00:54:52.437369: Current learning rate: 0.0064\n",
      "2025-05-16 00:56:41.492043: train_loss -0.9508\n",
      "2025-05-16 00:56:41.492213: val_loss -0.9621\n",
      "2025-05-16 00:56:41.492247: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-16 00:56:41.492280: Epoch time: 109.06 s\n",
      "2025-05-16 00:56:42.025227: \n",
      "2025-05-16 00:56:42.025362: Epoch 392\n",
      "2025-05-16 00:56:42.025457: Current learning rate: 0.00639\n",
      "2025-05-16 00:58:30.983282: train_loss -0.9525\n",
      "2025-05-16 00:58:30.983455: val_loss -0.9429\n",
      "2025-05-16 00:58:30.983488: Pseudo dice [np.float32(0.98)]\n",
      "2025-05-16 00:58:30.983526: Epoch time: 108.96 s\n",
      "2025-05-16 00:58:31.520275: \n",
      "2025-05-16 00:58:31.520356: Epoch 393\n",
      "2025-05-16 00:58:31.520421: Current learning rate: 0.00638\n",
      "2025-05-16 01:00:20.325479: train_loss -0.9535\n",
      "2025-05-16 01:00:20.325598: val_loss -0.9652\n",
      "2025-05-16 01:00:20.325631: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 01:00:20.325664: Epoch time: 108.81 s\n",
      "2025-05-16 01:00:20.878043: \n",
      "2025-05-16 01:00:20.878131: Epoch 394\n",
      "2025-05-16 01:00:20.878196: Current learning rate: 0.00637\n",
      "2025-05-16 01:02:09.901683: train_loss -0.9536\n",
      "2025-05-16 01:02:09.901813: val_loss -0.9667\n",
      "2025-05-16 01:02:09.901855: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-16 01:02:09.901894: Epoch time: 109.02 s\n",
      "2025-05-16 01:02:10.435000: \n",
      "2025-05-16 01:02:10.435080: Epoch 395\n",
      "2025-05-16 01:02:10.435140: Current learning rate: 0.00636\n",
      "2025-05-16 01:03:59.294607: train_loss -0.9527\n",
      "2025-05-16 01:03:59.294776: val_loss -0.9693\n",
      "2025-05-16 01:03:59.294809: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 01:03:59.294843: Epoch time: 108.86 s\n",
      "2025-05-16 01:03:59.828877: \n",
      "2025-05-16 01:03:59.828965: Epoch 396\n",
      "2025-05-16 01:03:59.829056: Current learning rate: 0.00635\n",
      "2025-05-16 01:05:48.653071: train_loss -0.9536\n",
      "2025-05-16 01:05:48.653189: val_loss -0.9691\n",
      "2025-05-16 01:05:48.653222: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 01:05:48.653256: Epoch time: 108.82 s\n",
      "2025-05-16 01:05:49.372970: \n",
      "2025-05-16 01:05:49.373058: Epoch 397\n",
      "2025-05-16 01:05:49.373131: Current learning rate: 0.00634\n",
      "2025-05-16 01:07:38.460498: train_loss -0.9516\n",
      "2025-05-16 01:07:38.460676: val_loss -0.963\n",
      "2025-05-16 01:07:38.460715: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-16 01:07:38.460751: Epoch time: 109.09 s\n",
      "2025-05-16 01:07:38.995378: \n",
      "2025-05-16 01:07:38.995530: Epoch 398\n",
      "2025-05-16 01:07:38.995596: Current learning rate: 0.00633\n",
      "2025-05-16 01:09:27.976986: train_loss -0.9548\n",
      "2025-05-16 01:09:27.977168: val_loss -0.9684\n",
      "2025-05-16 01:09:27.977223: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 01:09:27.977262: Epoch time: 108.98 s\n",
      "2025-05-16 01:09:28.515211: \n",
      "2025-05-16 01:09:28.515531: Epoch 399\n",
      "2025-05-16 01:09:28.515604: Current learning rate: 0.00632\n",
      "2025-05-16 01:11:17.517198: train_loss -0.957\n",
      "2025-05-16 01:11:17.517383: val_loss -0.9691\n",
      "2025-05-16 01:11:17.517425: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 01:11:17.517460: Epoch time: 109.0 s\n",
      "2025-05-16 01:11:18.258157: \n",
      "2025-05-16 01:11:18.258236: Epoch 400\n",
      "2025-05-16 01:11:18.258321: Current learning rate: 0.00631\n",
      "2025-05-16 01:13:07.399326: train_loss -0.9567\n",
      "2025-05-16 01:13:07.399690: val_loss -0.9688\n",
      "2025-05-16 01:13:07.399818: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 01:13:07.399884: Epoch time: 109.14 s\n",
      "2025-05-16 01:13:07.933034: \n",
      "2025-05-16 01:13:07.933203: Epoch 401\n",
      "2025-05-16 01:13:07.933274: Current learning rate: 0.0063\n",
      "2025-05-16 01:14:56.902343: train_loss -0.9566\n",
      "2025-05-16 01:14:56.902463: val_loss -0.9675\n",
      "2025-05-16 01:14:56.902502: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 01:14:56.902535: Epoch time: 108.97 s\n",
      "2025-05-16 01:14:57.443206: \n",
      "2025-05-16 01:14:57.443363: Epoch 402\n",
      "2025-05-16 01:14:57.443479: Current learning rate: 0.0063\n",
      "2025-05-16 01:16:46.412579: train_loss -0.9587\n",
      "2025-05-16 01:16:46.412706: val_loss -0.9726\n",
      "2025-05-16 01:16:46.412742: Pseudo dice [np.float32(0.9881)]\n",
      "2025-05-16 01:16:46.412776: Epoch time: 108.97 s\n",
      "2025-05-16 01:16:46.948769: \n",
      "2025-05-16 01:16:46.949044: Epoch 403\n",
      "2025-05-16 01:16:46.949157: Current learning rate: 0.00629\n",
      "2025-05-16 01:18:36.041602: train_loss -0.9575\n",
      "2025-05-16 01:18:36.041816: val_loss -0.967\n",
      "2025-05-16 01:18:36.041857: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 01:18:36.041890: Epoch time: 109.09 s\n",
      "2025-05-16 01:18:36.575623: \n",
      "2025-05-16 01:18:36.575709: Epoch 404\n",
      "2025-05-16 01:18:36.575774: Current learning rate: 0.00628\n",
      "2025-05-16 01:20:25.616026: train_loss -0.9573\n",
      "2025-05-16 01:20:25.616227: val_loss -0.9709\n",
      "2025-05-16 01:20:25.616262: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 01:20:25.616296: Epoch time: 109.04 s\n",
      "2025-05-16 01:20:25.616324: Yayy! New best EMA pseudo Dice: 0.98580002784729\n",
      "2025-05-16 01:20:26.359487: \n",
      "2025-05-16 01:20:26.359644: Epoch 405\n",
      "2025-05-16 01:20:26.359719: Current learning rate: 0.00627\n",
      "2025-05-16 01:22:15.305541: train_loss -0.955\n",
      "2025-05-16 01:22:15.305691: val_loss -0.97\n",
      "2025-05-16 01:22:15.305773: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 01:22:15.305934: Epoch time: 108.95 s\n",
      "2025-05-16 01:22:15.306002: Yayy! New best EMA pseudo Dice: 0.9860000014305115\n",
      "2025-05-16 01:22:16.054518: \n",
      "2025-05-16 01:22:16.054658: Epoch 406\n",
      "2025-05-16 01:22:16.054727: Current learning rate: 0.00626\n",
      "2025-05-16 01:24:04.900108: train_loss -0.958\n",
      "2025-05-16 01:24:04.900393: val_loss -0.9665\n",
      "2025-05-16 01:24:04.900437: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 01:24:04.900510: Epoch time: 108.85 s\n",
      "2025-05-16 01:24:05.431859: \n",
      "2025-05-16 01:24:05.431941: Epoch 407\n",
      "2025-05-16 01:24:05.432007: Current learning rate: 0.00625\n",
      "2025-05-16 01:25:54.283970: train_loss -0.9593\n",
      "2025-05-16 01:25:54.284307: val_loss -0.9691\n",
      "2025-05-16 01:25:54.284474: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 01:25:54.284637: Epoch time: 108.85 s\n",
      "2025-05-16 01:25:54.819945: \n",
      "2025-05-16 01:25:54.820080: Epoch 408\n",
      "2025-05-16 01:25:54.820148: Current learning rate: 0.00624\n",
      "2025-05-16 01:27:44.915796: train_loss -0.9527\n",
      "2025-05-16 01:27:44.915926: val_loss -0.9649\n",
      "2025-05-16 01:27:44.915962: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 01:27:44.915995: Epoch time: 110.1 s\n",
      "2025-05-16 01:27:45.655259: \n",
      "2025-05-16 01:27:45.655415: Epoch 409\n",
      "2025-05-16 01:27:45.655482: Current learning rate: 0.00623\n",
      "2025-05-16 01:29:35.620054: train_loss -0.9531\n",
      "2025-05-16 01:29:35.620268: val_loss -0.9664\n",
      "2025-05-16 01:29:35.620334: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 01:29:35.620422: Epoch time: 109.97 s\n",
      "2025-05-16 01:29:36.153981: \n",
      "2025-05-16 01:29:36.154126: Epoch 410\n",
      "2025-05-16 01:29:36.154250: Current learning rate: 0.00622\n",
      "2025-05-16 01:31:25.379476: train_loss -0.9499\n",
      "2025-05-16 01:31:25.379596: val_loss -0.9643\n",
      "2025-05-16 01:31:25.379631: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 01:31:25.379665: Epoch time: 109.23 s\n",
      "2025-05-16 01:31:25.888132: \n",
      "2025-05-16 01:31:25.888464: Epoch 411\n",
      "2025-05-16 01:31:25.888565: Current learning rate: 0.00621\n",
      "2025-05-16 01:33:15.271509: train_loss -0.941\n",
      "2025-05-16 01:33:15.271678: val_loss -0.962\n",
      "2025-05-16 01:33:15.271712: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-16 01:33:15.271748: Epoch time: 109.38 s\n",
      "2025-05-16 01:33:15.796387: \n",
      "2025-05-16 01:33:15.796552: Epoch 412\n",
      "2025-05-16 01:33:15.796640: Current learning rate: 0.0062\n",
      "2025-05-16 01:35:05.234852: train_loss -0.9482\n",
      "2025-05-16 01:35:05.235026: val_loss -0.9645\n",
      "2025-05-16 01:35:05.235061: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 01:35:05.235095: Epoch time: 109.44 s\n",
      "2025-05-16 01:35:05.748914: \n",
      "2025-05-16 01:35:05.749015: Epoch 413\n",
      "2025-05-16 01:35:05.749081: Current learning rate: 0.00619\n",
      "2025-05-16 01:36:55.071455: train_loss -0.9497\n",
      "2025-05-16 01:36:55.071586: val_loss -0.9635\n",
      "2025-05-16 01:36:55.071620: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 01:36:55.071655: Epoch time: 109.32 s\n",
      "2025-05-16 01:36:55.592774: \n",
      "2025-05-16 01:36:55.592911: Epoch 414\n",
      "2025-05-16 01:36:55.592979: Current learning rate: 0.00618\n",
      "2025-05-16 01:38:44.930645: train_loss -0.9526\n",
      "2025-05-16 01:38:44.930864: val_loss -0.9672\n",
      "2025-05-16 01:38:44.930974: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 01:38:44.931053: Epoch time: 109.34 s\n",
      "2025-05-16 01:38:45.444771: \n",
      "2025-05-16 01:38:45.444920: Epoch 415\n",
      "2025-05-16 01:38:45.444994: Current learning rate: 0.00617\n",
      "2025-05-16 01:40:34.846392: train_loss -0.9525\n",
      "2025-05-16 01:40:34.846524: val_loss -0.9687\n",
      "2025-05-16 01:40:34.846560: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 01:40:34.846593: Epoch time: 109.4 s\n",
      "2025-05-16 01:40:35.366795: \n",
      "2025-05-16 01:40:35.366886: Epoch 416\n",
      "2025-05-16 01:40:35.366979: Current learning rate: 0.00616\n",
      "2025-05-16 01:42:24.603549: train_loss -0.9549\n",
      "2025-05-16 01:42:24.603715: val_loss -0.9647\n",
      "2025-05-16 01:42:24.603827: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 01:42:24.603913: Epoch time: 109.24 s\n",
      "2025-05-16 01:42:25.116029: \n",
      "2025-05-16 01:42:25.116203: Epoch 417\n",
      "2025-05-16 01:42:25.116279: Current learning rate: 0.00615\n",
      "2025-05-16 01:44:14.386512: train_loss -0.9571\n",
      "2025-05-16 01:44:14.386826: val_loss -0.9699\n",
      "2025-05-16 01:44:14.386930: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 01:44:14.387010: Epoch time: 109.27 s\n",
      "2025-05-16 01:44:14.899298: \n",
      "2025-05-16 01:44:14.899435: Epoch 418\n",
      "2025-05-16 01:44:14.899508: Current learning rate: 0.00614\n",
      "2025-05-16 01:46:04.234206: train_loss -0.9564\n",
      "2025-05-16 01:46:04.234323: val_loss -0.9669\n",
      "2025-05-16 01:46:04.234356: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 01:46:04.234390: Epoch time: 109.34 s\n",
      "2025-05-16 01:46:04.749313: \n",
      "2025-05-16 01:46:04.749392: Epoch 419\n",
      "2025-05-16 01:46:04.749457: Current learning rate: 0.00613\n",
      "2025-05-16 01:47:54.153768: train_loss -0.955\n",
      "2025-05-16 01:47:54.153942: val_loss -0.9668\n",
      "2025-05-16 01:47:54.153974: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 01:47:54.154006: Epoch time: 109.4 s\n",
      "2025-05-16 01:47:54.669278: \n",
      "2025-05-16 01:47:54.669370: Epoch 420\n",
      "2025-05-16 01:47:54.669506: Current learning rate: 0.00612\n",
      "2025-05-16 01:49:43.866762: train_loss -0.9574\n",
      "2025-05-16 01:49:43.866885: val_loss -0.9675\n",
      "2025-05-16 01:49:43.867067: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 01:49:43.867142: Epoch time: 109.2 s\n",
      "2025-05-16 01:49:44.589538: \n",
      "2025-05-16 01:49:44.589764: Epoch 421\n",
      "2025-05-16 01:49:44.589862: Current learning rate: 0.00612\n",
      "2025-05-16 01:51:33.765161: train_loss -0.9559\n",
      "2025-05-16 01:51:33.765347: val_loss -0.9703\n",
      "2025-05-16 01:51:33.765384: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 01:51:33.765418: Epoch time: 109.18 s\n",
      "2025-05-16 01:51:34.282927: \n",
      "2025-05-16 01:51:34.283187: Epoch 422\n",
      "2025-05-16 01:51:34.283263: Current learning rate: 0.00611\n",
      "2025-05-16 01:53:23.785969: train_loss -0.9517\n",
      "2025-05-16 01:53:23.786177: val_loss -0.9666\n",
      "2025-05-16 01:53:23.786256: Pseudo dice [np.float32(0.9862)]\n",
      "Epoch time: 109.5 s.786327: \n",
      "2025-05-16 01:53:24.308356: \n",
      "2025-05-16 01:53:24.308491: Epoch 423\n",
      "2025-05-16 01:53:24.308559: Current learning rate: 0.0061\n",
      "2025-05-16 01:55:13.755229: train_loss -0.9526\n",
      "2025-05-16 01:55:13.755414: val_loss -0.9671\n",
      "2025-05-16 01:55:13.755450: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 01:55:13.755484: Epoch time: 109.45 s\n",
      "2025-05-16 01:55:14.272186: \n",
      "2025-05-16 01:55:14.272344: Epoch 424\n",
      "2025-05-16 01:55:14.272413: Current learning rate: 0.00609\n",
      "2025-05-16 01:57:03.408532: train_loss -0.951\n",
      "2025-05-16 01:57:03.408649: val_loss -0.9571\n",
      "2025-05-16 01:57:03.408682: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-16 01:57:03.408716: Epoch time: 109.14 s\n",
      "2025-05-16 01:57:03.927170: \n",
      "2025-05-16 01:57:03.927286: Epoch 425\n",
      "2025-05-16 01:57:03.927362: Current learning rate: 0.00608\n",
      "2025-05-16 01:58:53.214355: train_loss -0.9418\n",
      "2025-05-16 01:58:53.214476: val_loss -0.9593\n",
      "2025-05-16 01:58:53.214514: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-16 01:58:53.214548: Epoch time: 109.29 s\n",
      "2025-05-16 01:58:53.734830: \n",
      "2025-05-16 01:58:53.735141: Epoch 426\n",
      "2025-05-16 01:58:53.735237: Current learning rate: 0.00607\n",
      "2025-05-16 02:00:43.135164: train_loss -0.9499\n",
      "2025-05-16 02:00:43.135450: val_loss -0.9669\n",
      "2025-05-16 02:00:43.135488: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 02:00:43.135534: Epoch time: 109.4 s\n",
      "2025-05-16 02:00:43.647306: \n",
      "2025-05-16 02:00:43.647407: Epoch 427\n",
      "2025-05-16 02:00:43.647471: Current learning rate: 0.00606\n",
      "2025-05-16 02:02:33.021714: train_loss -0.9515\n",
      "2025-05-16 02:02:33.021860: val_loss -0.9632\n",
      "2025-05-16 02:02:33.021905: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-16 02:02:33.021941: Epoch time: 109.37 s\n",
      "2025-05-16 02:02:33.546313: \n",
      "2025-05-16 02:02:33.546399: Epoch 428\n",
      "2025-05-16 02:02:33.546463: Current learning rate: 0.00605\n",
      "2025-05-16 02:04:22.698754: train_loss -0.9527\n",
      "2025-05-16 02:04:22.698870: val_loss -0.9675\n",
      "2025-05-16 02:04:22.698902: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 02:04:22.698934: Epoch time: 109.15 s\n",
      "2025-05-16 02:04:23.211131: \n",
      "2025-05-16 02:04:23.211313: Epoch 429\n",
      "2025-05-16 02:04:23.211396: Current learning rate: 0.00604\n",
      "2025-05-16 02:06:12.336601: train_loss -0.9526\n",
      "2025-05-16 02:06:12.336802: val_loss -0.9611\n",
      "2025-05-16 02:06:12.336961: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 02:06:12.337010: Epoch time: 109.13 s\n",
      "2025-05-16 02:06:12.846235: \n",
      "2025-05-16 02:06:12.846390: Epoch 430\n",
      "2025-05-16 02:06:12.846460: Current learning rate: 0.00603\n",
      "2025-05-16 02:08:02.226024: train_loss -0.9513\n",
      "2025-05-16 02:08:02.226143: val_loss -0.967\n",
      "2025-05-16 02:08:02.226176: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 02:08:02.226209: Epoch time: 109.38 s\n",
      "2025-05-16 02:08:02.747810: \n",
      "2025-05-16 02:08:02.747941: Epoch 431\n",
      "2025-05-16 02:08:02.748007: Current learning rate: 0.00602\n",
      "2025-05-16 02:09:52.112256: train_loss -0.9556\n",
      "2025-05-16 02:09:52.112457: val_loss -0.9683\n",
      "2025-05-16 02:09:52.112493: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 02:09:52.112526: Epoch time: 109.36 s\n",
      "2025-05-16 02:09:52.628448: \n",
      "2025-05-16 02:09:52.628681: Epoch 432\n",
      "2025-05-16 02:09:52.628763: Current learning rate: 0.00601\n",
      "2025-05-16 02:11:42.079148: train_loss -0.9539\n",
      "2025-05-16 02:11:42.079274: val_loss -0.9674\n",
      "2025-05-16 02:11:42.079309: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 02:11:42.079343: Epoch time: 109.45 s\n",
      "2025-05-16 02:11:42.603204: \n",
      "2025-05-16 02:11:42.603284: Epoch 433\n",
      "2025-05-16 02:11:42.603349: Current learning rate: 0.006\n",
      "2025-05-16 02:13:32.029264: train_loss -0.9543\n",
      "2025-05-16 02:13:32.029378: val_loss -0.9675\n",
      "2025-05-16 02:13:32.029411: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 02:13:32.029444: Epoch time: 109.43 s\n",
      "2025-05-16 02:13:32.742251: \n",
      "2025-05-16 02:13:32.742355: Epoch 434\n",
      "2025-05-16 02:13:32.742419: Current learning rate: 0.00599\n",
      "2025-05-16 02:15:22.131630: train_loss -0.9549\n",
      "2025-05-16 02:15:22.131752: val_loss -0.9658\n",
      "2025-05-16 02:15:22.131784: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 02:15:22.131820: Epoch time: 109.39 s\n",
      "2025-05-16 02:15:22.651232: \n",
      "2025-05-16 02:15:22.651378: Epoch 435\n",
      "2025-05-16 02:15:22.651443: Current learning rate: 0.00598\n",
      "2025-05-16 02:17:12.052533: train_loss -0.9556\n",
      "2025-05-16 02:17:12.052649: val_loss -0.9659\n",
      "2025-05-16 02:17:12.052680: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 02:17:12.052713: Epoch time: 109.4 s\n",
      "2025-05-16 02:17:12.569737: \n",
      "2025-05-16 02:17:12.570004: Epoch 436\n",
      "2025-05-16 02:17:12.570074: Current learning rate: 0.00597\n",
      "2025-05-16 02:19:01.900953: train_loss -0.9439\n",
      "2025-05-16 02:19:01.901088: val_loss -0.9536\n",
      "2025-05-16 02:19:01.901276: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-16 02:19:01.901365: Epoch time: 109.33 s\n",
      "2025-05-16 02:19:02.417468: \n",
      "2025-05-16 02:19:02.417634: Epoch 437\n",
      "2025-05-16 02:19:02.417709: Current learning rate: 0.00596\n",
      "2025-05-16 02:20:51.779727: train_loss -0.9239\n",
      "2025-05-16 02:20:51.779950: val_loss -0.9598\n",
      "2025-05-16 02:20:51.780069: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-16 02:20:51.780142: Epoch time: 109.36 s\n",
      "2025-05-16 02:20:52.304444: \n",
      "2025-05-16 02:20:52.304630: Epoch 438\n",
      "2025-05-16 02:20:52.304850: Current learning rate: 0.00595\n",
      "2025-05-16 02:22:41.665575: train_loss -0.9416\n",
      "2025-05-16 02:22:41.665810: val_loss -0.9603\n",
      "2025-05-16 02:22:41.665909: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-16 02:22:41.665950: Epoch time: 109.36 s\n",
      "2025-05-16 02:22:42.185695: \n",
      "2025-05-16 02:22:42.185922: Epoch 439\n",
      "2025-05-16 02:22:42.186005: Current learning rate: 0.00594\n",
      "2025-05-16 02:24:31.578573: train_loss -0.9409\n",
      "2025-05-16 02:24:31.578694: val_loss -0.9578\n",
      "2025-05-16 02:24:31.578726: Pseudo dice [np.float32(0.9813)]\n",
      "2025-05-16 02:24:31.578761: Epoch time: 109.39 s\n",
      "2025-05-16 02:24:32.095468: \n",
      "2025-05-16 02:24:32.095594: Epoch 440\n",
      "2025-05-16 02:24:32.095671: Current learning rate: 0.00593\n",
      "2025-05-16 02:26:21.249700: train_loss -0.9494\n",
      "2025-05-16 02:26:21.249825: val_loss -0.9667\n",
      "2025-05-16 02:26:21.249860: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 02:26:21.249894: Epoch time: 109.15 s\n",
      "2025-05-16 02:26:21.784286: \n",
      "2025-05-16 02:26:21.784434: Epoch 441\n",
      "2025-05-16 02:26:21.784501: Current learning rate: 0.00592\n",
      "2025-05-16 02:28:10.913775: train_loss -0.9508\n",
      "2025-05-16 02:28:10.913941: val_loss -0.9662\n",
      "2025-05-16 02:28:10.913972: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 02:28:10.914006: Epoch time: 109.13 s\n",
      "2025-05-16 02:28:11.427087: \n",
      "2025-05-16 02:28:11.427173: Epoch 442\n",
      "2025-05-16 02:28:11.427237: Current learning rate: 0.00592\n",
      "2025-05-16 02:30:00.702817: train_loss -0.9533\n",
      "2025-05-16 02:30:00.702946: val_loss -0.9644\n",
      "2025-05-16 02:30:00.703025: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-16 02:30:00.703065: Epoch time: 109.28 s\n",
      "2025-05-16 02:30:01.221452: \n",
      "2025-05-16 02:30:01.221609: Epoch 443\n",
      "2025-05-16 02:30:01.221686: Current learning rate: 0.00591\n",
      "2025-05-16 02:31:50.470003: train_loss -0.9531\n",
      "2025-05-16 02:31:50.470135: val_loss -0.9662\n",
      "2025-05-16 02:31:50.470169: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 02:31:50.470204: Epoch time: 109.25 s\n",
      "2025-05-16 02:31:50.984738: \n",
      "2025-05-16 02:31:50.985009: Epoch 444\n",
      "2025-05-16 02:31:50.985171: Current learning rate: 0.0059\n",
      "2025-05-16 02:33:40.138067: train_loss -0.9501\n",
      "2025-05-16 02:33:40.138246: val_loss -0.9658\n",
      "2025-05-16 02:33:40.138366: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-16 02:33:40.138417: Epoch time: 109.15 s\n",
      "2025-05-16 02:33:40.650933: \n",
      "2025-05-16 02:33:40.651029: Epoch 445\n",
      "2025-05-16 02:33:40.651139: Current learning rate: 0.00589\n",
      "2025-05-16 02:35:30.041723: train_loss -0.9545\n",
      "2025-05-16 02:35:30.041842: val_loss -0.9626\n",
      "2025-05-16 02:35:30.041877: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-16 02:35:30.041911: Epoch time: 109.39 s\n",
      "2025-05-16 02:35:30.555049: \n",
      "2025-05-16 02:35:30.555261: Epoch 446\n",
      "2025-05-16 02:35:30.555386: Current learning rate: 0.00588\n",
      "2025-05-16 02:37:19.739802: train_loss -0.9551\n",
      "2025-05-16 02:37:19.740148: val_loss -0.9674\n",
      "2025-05-16 02:37:19.740255: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 02:37:19.740322: Epoch time: 109.19 s\n",
      "2025-05-16 02:37:20.452449: \n",
      "2025-05-16 02:37:20.452605: Epoch 447\n",
      "2025-05-16 02:37:20.452673: Current learning rate: 0.00587\n",
      "2025-05-16 02:39:09.612345: train_loss -0.9525\n",
      "2025-05-16 02:39:09.612478: val_loss -0.9649\n",
      "2025-05-16 02:39:09.612513: Pseudo dice [np.float32(0.9855)]\n",
      "Epoch time: 109.16 s612545: \n",
      "2025-05-16 02:39:10.137479: \n",
      "2025-05-16 02:39:10.137695: Epoch 448\n",
      "2025-05-16 02:39:10.137835: Current learning rate: 0.00586\n",
      "2025-05-16 02:40:59.623755: train_loss -0.9518\n",
      "2025-05-16 02:40:59.623918: val_loss -0.9652\n",
      "2025-05-16 02:40:59.623992: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 02:40:59.624033: Epoch time: 109.49 s\n",
      "2025-05-16 02:41:00.135044: \n",
      "2025-05-16 02:41:00.135163: Epoch 449\n",
      "2025-05-16 02:41:00.135295: Current learning rate: 0.00585\n",
      "2025-05-16 02:42:49.329665: train_loss -0.9553\n",
      "2025-05-16 02:42:49.329836: val_loss -0.9658\n",
      "2025-05-16 02:42:49.329870: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 02:42:49.329903: Epoch time: 109.2 s\n",
      "2025-05-16 02:42:50.065759: \n",
      "2025-05-16 02:42:50.065850: Epoch 450\n",
      "2025-05-16 02:42:50.065917: Current learning rate: 0.00584\n",
      "2025-05-16 02:44:39.339165: train_loss -0.9494\n",
      "2025-05-16 02:44:39.339371: val_loss -0.9658\n",
      "2025-05-16 02:44:39.339423: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 02:44:39.339493: Epoch time: 109.27 s\n",
      "2025-05-16 02:44:39.856006: \n",
      "2025-05-16 02:44:39.856156: Epoch 451\n",
      "2025-05-16 02:44:39.856224: Current learning rate: 0.00583\n",
      "2025-05-16 02:46:29.263068: train_loss -0.9346\n",
      "2025-05-16 02:46:29.263360: val_loss -0.9389\n",
      "2025-05-16 02:46:29.263716: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-16 02:46:29.263810: Epoch time: 109.41 s\n",
      "2025-05-16 02:46:29.775632: \n",
      "2025-05-16 02:46:29.775889: Epoch 452\n",
      "2025-05-16 02:46:29.775998: Current learning rate: 0.00582\n",
      "2025-05-16 02:48:19.184488: train_loss -0.9206\n",
      "2025-05-16 02:48:19.184668: val_loss -0.9501\n",
      "2025-05-16 02:48:19.184783: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-16 02:48:19.184824: Epoch time: 109.41 s\n",
      "2025-05-16 02:48:19.695386: \n",
      "2025-05-16 02:48:19.695561: Epoch 453\n",
      "2025-05-16 02:48:19.695642: Current learning rate: 0.00581\n",
      "2025-05-16 02:50:10.043631: train_loss -0.9362\n",
      "2025-05-16 02:50:10.044103: val_loss -0.9459\n",
      "2025-05-16 02:50:10.044153: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-16 02:50:10.044195: Epoch time: 110.35 s\n",
      "2025-05-16 02:50:10.571647: \n",
      "2025-05-16 02:50:10.571978: Epoch 454\n",
      "2025-05-16 02:50:10.572059: Current learning rate: 0.0058\n",
      "2025-05-16 02:51:59.910218: train_loss -0.9386\n",
      "2025-05-16 02:51:59.910558: val_loss -0.964\n",
      "2025-05-16 02:51:59.910714: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 02:51:59.910782: Epoch time: 109.34 s\n",
      "2025-05-16 02:52:00.432339: \n",
      "2025-05-16 02:52:00.432547: Epoch 455\n",
      "2025-05-16 02:52:00.432652: Current learning rate: 0.00579\n",
      "2025-05-16 02:53:49.782263: train_loss -0.9403\n",
      "2025-05-16 02:53:49.782385: val_loss -0.9631\n",
      "2025-05-16 02:53:49.782429: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 02:53:49.782530: Epoch time: 109.35 s\n",
      "2025-05-16 02:53:50.310747: \n",
      "2025-05-16 02:53:50.310915: Epoch 456\n",
      "2025-05-16 02:53:50.311002: Current learning rate: 0.00578\n",
      "2025-05-16 02:55:39.658686: train_loss -0.944\n",
      "2025-05-16 02:55:39.658807: val_loss -0.966\n",
      "2025-05-16 02:55:39.658839: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 02:55:39.658874: Epoch time: 109.35 s\n",
      "2025-05-16 02:55:40.182573: \n",
      "2025-05-16 02:55:40.182721: Epoch 457\n",
      "2025-05-16 02:55:40.182789: Current learning rate: 0.00577\n",
      "2025-05-16 02:57:29.439550: train_loss -0.9523\n",
      "2025-05-16 02:57:29.439684: val_loss -0.966\n",
      "2025-05-16 02:57:29.439718: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 02:57:29.439754: Epoch time: 109.26 s\n",
      "2025-05-16 02:57:29.959496: \n",
      "2025-05-16 02:57:29.959644: Epoch 458\n",
      "2025-05-16 02:57:29.959713: Current learning rate: 0.00576\n",
      "2025-05-16 02:59:19.094315: train_loss -0.9541\n",
      "2025-05-16 02:59:19.094463: val_loss -0.967\n",
      "2025-05-16 02:59:19.094497: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 02:59:19.094532: Epoch time: 109.14 s\n",
      "2025-05-16 02:59:19.611990: \n",
      "2025-05-16 02:59:19.612075: Epoch 459\n",
      "2025-05-16 02:59:19.612142: Current learning rate: 0.00575\n",
      "2025-05-16 03:01:08.730234: train_loss -0.9548\n",
      "2025-05-16 03:01:08.730353: val_loss -0.9677\n",
      "2025-05-16 03:01:08.730384: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 03:01:08.730419: Epoch time: 109.12 s\n",
      "2025-05-16 03:01:09.446211: \n",
      "2025-05-16 03:01:09.446404: Epoch 460\n",
      "2025-05-16 03:01:09.446483: Current learning rate: 0.00574\n",
      "2025-05-16 03:02:58.493721: train_loss -0.9557\n",
      "2025-05-16 03:02:58.493846: val_loss -0.9678\n",
      "2025-05-16 03:02:58.493879: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 03:02:58.493912: Epoch time: 109.05 s\n",
      "2025-05-16 03:02:59.017012: \n",
      "2025-05-16 03:02:59.017313: Epoch 461\n",
      "2025-05-16 03:02:59.017551: Current learning rate: 0.00573\n",
      "2025-05-16 03:04:47.884521: train_loss -0.953\n",
      "2025-05-16 03:04:47.884686: val_loss -0.9699\n",
      "2025-05-16 03:04:47.884719: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 03:04:47.884752: Epoch time: 108.87 s\n",
      "2025-05-16 03:04:48.400707: \n",
      "2025-05-16 03:04:48.400854: Epoch 462\n",
      "2025-05-16 03:04:48.400926: Current learning rate: 0.00572\n",
      "2025-05-16 03:06:37.462226: train_loss -0.9439\n",
      "2025-05-16 03:06:37.462401: val_loss -0.9594\n",
      "2025-05-16 03:06:37.462486: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-16 03:06:37.462585: Epoch time: 109.06 s\n",
      "2025-05-16 03:06:37.981205: \n",
      "2025-05-16 03:06:37.981576: Epoch 463\n",
      "2025-05-16 03:06:37.981745: Current learning rate: 0.00571\n",
      "2025-05-16 03:08:26.979434: train_loss -0.9129\n",
      "2025-05-16 03:08:26.979554: val_loss -0.943\n",
      "2025-05-16 03:08:26.979585: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-16 03:08:26.979617: Epoch time: 109.0 s\n",
      "2025-05-16 03:08:27.505780: \n",
      "2025-05-16 03:08:27.505885: Epoch 464\n",
      "2025-05-16 03:08:27.505957: Current learning rate: 0.0057\n",
      "2025-05-16 03:10:16.331119: train_loss -0.9085\n",
      "2025-05-16 03:10:16.331391: val_loss -0.948\n",
      "2025-05-16 03:10:16.331427: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-16 03:10:16.331459: Epoch time: 108.83 s\n",
      "2025-05-16 03:10:16.869986: \n",
      "2025-05-16 03:10:16.870267: Epoch 465\n",
      "2025-05-16 03:10:16.870443: Current learning rate: 0.0057\n",
      "2025-05-16 03:12:05.914497: train_loss -0.9349\n",
      "2025-05-16 03:12:05.914655: val_loss -0.9623\n",
      "2025-05-16 03:12:05.914688: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-16 03:12:05.914728: Epoch time: 109.05 s\n",
      "2025-05-16 03:12:06.437140: \n",
      "2025-05-16 03:12:06.437356: Epoch 466\n",
      "2025-05-16 03:12:06.437438: Current learning rate: 0.00569\n",
      "2025-05-16 03:13:55.482717: train_loss -0.9411\n",
      "2025-05-16 03:13:55.482837: val_loss -0.9591\n",
      "2025-05-16 03:13:55.482872: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-16 03:13:55.482905: Epoch time: 109.05 s\n",
      "2025-05-16 03:13:56.006344: \n",
      "2025-05-16 03:13:56.006627: Epoch 467\n",
      "2025-05-16 03:13:56.006868: Current learning rate: 0.00568\n",
      "2025-05-16 03:15:44.894919: train_loss -0.9442\n",
      "2025-05-16 03:15:44.895039: val_loss -0.9557\n",
      "2025-05-16 03:15:44.895073: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-16 03:15:44.895107: Epoch time: 108.89 s\n",
      "2025-05-16 03:15:45.407521: \n",
      "2025-05-16 03:15:45.407672: Epoch 468\n",
      "2025-05-16 03:15:45.407741: Current learning rate: 0.00567\n",
      "2025-05-16 03:17:34.443783: train_loss -0.921\n",
      "2025-05-16 03:17:34.443972: val_loss -0.9345\n",
      "2025-05-16 03:17:34.444010: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-16 03:17:34.444044: Epoch time: 109.04 s\n",
      "2025-05-16 03:17:34.966272: \n",
      "2025-05-16 03:17:34.966370: Epoch 469\n",
      "2025-05-16 03:17:34.966437: Current learning rate: 0.00566\n",
      "2025-05-16 03:19:23.819629: train_loss -0.9329\n",
      "2025-05-16 03:19:23.819741: val_loss -0.9567\n",
      "2025-05-16 03:19:23.819774: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-16 03:19:23.819808: Epoch time: 108.85 s\n",
      "2025-05-16 03:19:24.342120: \n",
      "2025-05-16 03:19:24.342300: Epoch 470\n",
      "2025-05-16 03:19:24.342372: Current learning rate: 0.00565\n",
      "2025-05-16 03:21:13.178091: train_loss -0.9397\n",
      "2025-05-16 03:21:13.178260: val_loss -0.9599\n",
      "2025-05-16 03:21:13.178311: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-16 03:21:13.178348: Epoch time: 108.84 s\n",
      "2025-05-16 03:21:13.692908: \n",
      "2025-05-16 03:21:13.693189: Epoch 471\n",
      "2025-05-16 03:21:13.693315: Current learning rate: 0.00564\n",
      "2025-05-16 03:23:02.700093: train_loss -0.9418\n",
      "2025-05-16 03:23:02.700213: val_loss -0.9639\n",
      "2025-05-16 03:23:02.700245: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-16 03:23:02.700276: Epoch time: 109.01 s\n",
      "2025-05-16 03:23:03.222735: \n",
      "2025-05-16 03:23:03.222860: Epoch 472\n",
      "2025-05-16 03:23:03.222965: Current learning rate: 0.00563\n",
      "2025-05-16 03:24:52.135866: train_loss -0.9518\n",
      "2025-05-16 03:24:52.135985: val_loss -0.9668\n",
      "2025-05-16 03:24:52.136019: Pseudo dice [np.float32(0.9851)]\n",
      "Epoch time: 108.91 s136052: \n",
      "2025-05-16 03:24:52.870870: \n",
      "2025-05-16 03:24:52.871171: Epoch 473\n",
      "2025-05-16 03:24:52.871281: Current learning rate: 0.00562\n",
      "2025-05-16 03:26:41.760190: train_loss -0.9491\n",
      "2025-05-16 03:26:41.760347: val_loss -0.9655\n",
      "2025-05-16 03:26:41.760594: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 03:26:41.760747: Epoch time: 108.89 s\n",
      "2025-05-16 03:26:42.285593: \n",
      "2025-05-16 03:26:42.285749: Epoch 474\n",
      "2025-05-16 03:26:42.285911: Current learning rate: 0.00561\n",
      "2025-05-16 03:28:31.284833: train_loss -0.9498\n",
      "2025-05-16 03:28:31.284953: val_loss -0.9677\n",
      "2025-05-16 03:28:31.284985: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 03:28:31.285016: Epoch time: 109.0 s\n",
      "2025-05-16 03:28:31.807304: \n",
      "2025-05-16 03:28:31.807420: Epoch 475\n",
      "2025-05-16 03:28:31.807509: Current learning rate: 0.0056\n",
      "2025-05-16 03:30:20.797178: train_loss -0.9524\n",
      "2025-05-16 03:30:20.797308: val_loss -0.9645\n",
      "2025-05-16 03:30:20.797398: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-16 03:30:20.797472: Epoch time: 108.99 s\n",
      "2025-05-16 03:30:21.315681: \n",
      "2025-05-16 03:30:21.315784: Epoch 476\n",
      "2025-05-16 03:30:21.315854: Current learning rate: 0.00559\n",
      "2025-05-16 03:32:10.382454: train_loss -0.9544\n",
      "2025-05-16 03:32:10.382576: val_loss -0.9611\n",
      "2025-05-16 03:32:10.382609: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-16 03:32:10.382643: Epoch time: 109.07 s\n",
      "2025-05-16 03:32:10.908379: \n",
      "2025-05-16 03:32:10.908478: Epoch 477\n",
      "2025-05-16 03:32:10.908545: Current learning rate: 0.00558\n",
      "2025-05-16 03:33:59.905900: train_loss -0.923\n",
      "2025-05-16 03:33:59.906017: val_loss -0.9412\n",
      "2025-05-16 03:33:59.906048: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-16 03:33:59.906080: Epoch time: 109.0 s\n",
      "2025-05-16 03:34:00.432054: \n",
      "2025-05-16 03:34:00.432306: Epoch 478\n",
      "2025-05-16 03:34:00.432408: Current learning rate: 0.00557\n",
      "2025-05-16 03:35:49.306252: train_loss -0.9277\n",
      "2025-05-16 03:35:49.306434: val_loss -0.9527\n",
      "2025-05-16 03:35:49.306469: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-16 03:35:49.306504: Epoch time: 108.87 s\n",
      "2025-05-16 03:35:49.826188: \n",
      "2025-05-16 03:35:49.826296: Epoch 479\n",
      "2025-05-16 03:35:49.826367: Current learning rate: 0.00556\n",
      "2025-05-16 03:37:38.839995: train_loss -0.927\n",
      "2025-05-16 03:37:38.840120: val_loss -0.9441\n",
      "2025-05-16 03:37:38.840149: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-16 03:37:38.840182: Epoch time: 109.01 s\n",
      "2025-05-16 03:37:39.363399: \n",
      "2025-05-16 03:37:39.363496: Epoch 480\n",
      "2025-05-16 03:37:39.363562: Current learning rate: 0.00555\n",
      "2025-05-16 03:39:28.397159: train_loss -0.9242\n",
      "2025-05-16 03:39:28.397287: val_loss -0.9386\n",
      "2025-05-16 03:39:28.397322: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-16 03:39:28.397357: Epoch time: 109.03 s\n",
      "2025-05-16 03:39:28.920183: \n",
      "2025-05-16 03:39:28.920379: Epoch 481\n",
      "2025-05-16 03:39:28.920462: Current learning rate: 0.00554\n",
      "2025-05-16 03:41:17.940334: train_loss -0.9315\n",
      "2025-05-16 03:41:17.940453: val_loss -0.9624\n",
      "2025-05-16 03:41:17.940487: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-16 03:41:17.940520: Epoch time: 109.02 s\n",
      "2025-05-16 03:41:18.463897: \n",
      "2025-05-16 03:41:18.464040: Epoch 482\n",
      "2025-05-16 03:41:18.464114: Current learning rate: 0.00553\n",
      "2025-05-16 03:43:07.397257: train_loss -0.9492\n",
      "2025-05-16 03:43:07.397425: val_loss -0.9636\n",
      "2025-05-16 03:43:07.397467: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-16 03:43:07.397509: Epoch time: 108.93 s\n",
      "2025-05-16 03:43:07.919143: \n",
      "2025-05-16 03:43:07.919239: Epoch 483\n",
      "2025-05-16 03:43:07.919307: Current learning rate: 0.00552\n",
      "2025-05-16 03:44:56.847361: train_loss -0.951\n",
      "2025-05-16 03:44:56.847479: val_loss -0.966\n",
      "2025-05-16 03:44:56.847524: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 03:44:56.847563: Epoch time: 108.93 s\n",
      "2025-05-16 03:44:57.394996: \n",
      "2025-05-16 03:44:57.395090: Epoch 484\n",
      "2025-05-16 03:44:57.395164: Current learning rate: 0.00551\n",
      "2025-05-16 03:46:46.330557: train_loss -0.9515\n",
      "2025-05-16 03:46:46.330718: val_loss -0.9668\n",
      "2025-05-16 03:46:46.330754: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 03:46:46.330956: Epoch time: 108.94 s\n",
      "2025-05-16 03:46:46.859017: \n",
      "2025-05-16 03:46:46.859103: Epoch 485\n",
      "2025-05-16 03:46:46.859172: Current learning rate: 0.0055\n",
      "2025-05-16 03:48:35.684442: train_loss -0.9527\n",
      "2025-05-16 03:48:35.684623: val_loss -0.967\n",
      "2025-05-16 03:48:35.684664: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 03:48:35.684699: Epoch time: 108.83 s\n",
      "2025-05-16 03:48:36.406955: \n",
      "2025-05-16 03:48:36.407071: Epoch 486\n",
      "2025-05-16 03:48:36.407147: Current learning rate: 0.00549\n",
      "2025-05-16 03:50:25.202810: train_loss -0.9536\n",
      "2025-05-16 03:50:25.202925: val_loss -0.9679\n",
      "2025-05-16 03:50:25.202959: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 03:50:25.202993: Epoch time: 108.8 s\n",
      "2025-05-16 03:50:25.725895: \n",
      "2025-05-16 03:50:25.726298: Epoch 487\n",
      "2025-05-16 03:50:25.726383: Current learning rate: 0.00548\n",
      "2025-05-16 03:52:14.585804: train_loss -0.9527\n",
      "2025-05-16 03:52:14.585924: val_loss -0.9656\n",
      "2025-05-16 03:52:14.585955: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 03:52:14.585988: Epoch time: 108.86 s\n",
      "2025-05-16 03:52:15.116365: \n",
      "2025-05-16 03:52:15.116471: Epoch 488\n",
      "2025-05-16 03:52:15.116569: Current learning rate: 0.00547\n",
      "2025-05-16 03:54:03.931175: train_loss -0.9531\n",
      "2025-05-16 03:54:03.931301: val_loss -0.9669\n",
      "2025-05-16 03:54:03.931333: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 03:54:03.931366: Epoch time: 108.82 s\n",
      "2025-05-16 03:54:04.464426: \n",
      "2025-05-16 03:54:04.464770: Epoch 489\n",
      "2025-05-16 03:54:04.464993: Current learning rate: 0.00546\n",
      "2025-05-16 03:55:53.401917: train_loss -0.9556\n",
      "2025-05-16 03:55:53.402143: val_loss -0.9631\n",
      "2025-05-16 03:55:53.402179: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-16 03:55:53.402212: Epoch time: 108.94 s\n",
      "2025-05-16 03:55:53.927397: \n",
      "2025-05-16 03:55:53.927597: Epoch 490\n",
      "2025-05-16 03:55:53.927674: Current learning rate: 0.00546\n",
      "2025-05-16 03:57:42.756303: train_loss -0.9529\n",
      "2025-05-16 03:57:42.756474: val_loss -0.9672\n",
      "2025-05-16 03:57:42.756507: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 03:57:42.756541: Epoch time: 108.83 s\n",
      "2025-05-16 03:57:43.290724: \n",
      "2025-05-16 03:57:43.290876: Epoch 491\n",
      "2025-05-16 03:57:43.290956: Current learning rate: 0.00545\n",
      "2025-05-16 03:59:32.243187: train_loss -0.9545\n",
      "2025-05-16 03:59:32.243307: val_loss -0.9615\n",
      "2025-05-16 03:59:32.243338: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 03:59:32.243371: Epoch time: 108.95 s\n",
      "2025-05-16 03:59:32.793194: \n",
      "2025-05-16 03:59:32.793355: Epoch 492\n",
      "2025-05-16 03:59:32.793448: Current learning rate: 0.00544\n",
      "2025-05-16 04:01:21.866861: train_loss -0.955\n",
      "2025-05-16 04:01:21.867007: val_loss -0.9654\n",
      "2025-05-16 04:01:21.867102: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 04:01:21.867154: Epoch time: 109.07 s\n",
      "2025-05-16 04:01:22.398489: \n",
      "2025-05-16 04:01:22.398667: Epoch 493\n",
      "2025-05-16 04:01:22.398791: Current learning rate: 0.00543\n",
      "2025-05-16 04:03:11.342084: train_loss -0.9531\n",
      "2025-05-16 04:03:11.342203: val_loss -0.9681\n",
      "2025-05-16 04:03:11.342235: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 04:03:11.342267: Epoch time: 108.94 s\n",
      "2025-05-16 04:03:11.881143: \n",
      "2025-05-16 04:03:11.881317: Epoch 494\n",
      "2025-05-16 04:03:11.881457: Current learning rate: 0.00542\n",
      "2025-05-16 04:05:00.855977: train_loss -0.9563\n",
      "2025-05-16 04:05:00.856086: val_loss -0.9691\n",
      "2025-05-16 04:05:00.856207: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 04:05:00.856319: Epoch time: 108.98 s\n",
      "2025-05-16 04:05:01.384678: \n",
      "2025-05-16 04:05:01.384895: Epoch 495\n",
      "2025-05-16 04:05:01.385056: Current learning rate: 0.00541\n",
      "2025-05-16 04:06:50.482931: train_loss -0.9566\n",
      "2025-05-16 04:06:50.483121: val_loss -0.9659\n",
      "2025-05-16 04:06:50.483195: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 04:06:50.483255: Epoch time: 109.1 s\n",
      "2025-05-16 04:06:51.012537: \n",
      "2025-05-16 04:06:51.012715: Epoch 496\n",
      "2025-05-16 04:06:51.012834: Current learning rate: 0.0054\n",
      "2025-05-16 04:08:40.055414: train_loss -0.9545\n",
      "2025-05-16 04:08:40.055594: val_loss -0.9684\n",
      "2025-05-16 04:08:40.055630: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 04:08:40.055662: Epoch time: 109.04 s\n",
      "2025-05-16 04:08:40.581171: \n",
      "2025-05-16 04:08:40.581613: Epoch 497\n",
      "2025-05-16 04:08:40.581714: Current learning rate: 0.00539\n",
      "2025-05-16 04:10:29.593548: train_loss -0.9579\n",
      "2025-05-16 04:10:29.593761: val_loss -0.9642\n",
      "2025-05-16 04:10:29.593815: Pseudo dice [np.float32(0.9845)]\n",
      "Epoch time: 109.01 s593862: \n",
      "2025-05-16 04:10:30.127172: \n",
      "2025-05-16 04:10:30.127374: Epoch 498\n",
      "2025-05-16 04:10:30.127452: Current learning rate: 0.00538\n",
      "2025-05-16 04:12:19.126118: train_loss -0.9584\n",
      "2025-05-16 04:12:19.126431: val_loss -0.9673\n",
      "2025-05-16 04:12:19.126483: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 04:12:19.126530: Epoch time: 109.0 s\n",
      "2025-05-16 04:12:19.855433: \n",
      "2025-05-16 04:12:19.855716: Epoch 499\n",
      "2025-05-16 04:12:19.855836: Current learning rate: 0.00537\n",
      "2025-05-16 04:14:08.930705: train_loss -0.9558\n",
      "2025-05-16 04:14:08.930827: val_loss -0.9664\n",
      "2025-05-16 04:14:08.930861: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 04:14:08.930894: Epoch time: 109.08 s\n",
      "2025-05-16 04:14:09.685086: \n",
      "2025-05-16 04:14:09.685360: Epoch 500\n",
      "2025-05-16 04:14:09.685442: Current learning rate: 0.00536\n",
      "2025-05-16 04:15:58.699960: train_loss -0.9576\n",
      "2025-05-16 04:15:58.700076: val_loss -0.9677\n",
      "2025-05-16 04:15:58.700109: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 04:15:58.700143: Epoch time: 109.02 s\n",
      "2025-05-16 04:15:59.228585: \n",
      "2025-05-16 04:15:59.228755: Epoch 501\n",
      "2025-05-16 04:15:59.228835: Current learning rate: 0.00535\n",
      "2025-05-16 04:17:48.308012: train_loss -0.9542\n",
      "2025-05-16 04:17:48.308131: val_loss -0.9711\n",
      "2025-05-16 04:17:48.308162: Pseudo dice [np.float32(0.9879)]\n",
      "2025-05-16 04:17:48.308197: Epoch time: 109.08 s\n",
      "2025-05-16 04:17:48.836609: \n",
      "2025-05-16 04:17:48.836791: Epoch 502\n",
      "2025-05-16 04:17:48.836862: Current learning rate: 0.00534\n",
      "2025-05-16 04:19:37.910920: train_loss -0.9554\n",
      "2025-05-16 04:19:37.911055: val_loss -0.9636\n",
      "2025-05-16 04:19:37.911088: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-16 04:19:37.911122: Epoch time: 109.07 s\n",
      "2025-05-16 04:19:38.437944: \n",
      "2025-05-16 04:19:38.438154: Epoch 503\n",
      "2025-05-16 04:19:38.438330: Current learning rate: 0.00533\n",
      "2025-05-16 04:21:27.434147: train_loss -0.9572\n",
      "2025-05-16 04:21:27.434467: val_loss -0.9656\n",
      "2025-05-16 04:21:27.434573: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 04:21:27.434641: Epoch time: 109.0 s\n",
      "2025-05-16 04:21:27.966124: \n",
      "2025-05-16 04:21:27.966298: Epoch 504\n",
      "2025-05-16 04:21:27.966371: Current learning rate: 0.00532\n",
      "2025-05-16 04:23:16.896818: train_loss -0.9576\n",
      "2025-05-16 04:23:16.896939: val_loss -0.9647\n",
      "2025-05-16 04:23:16.896995: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-16 04:23:16.897081: Epoch time: 108.93 s\n",
      "2025-05-16 04:23:17.432123: \n",
      "2025-05-16 04:23:17.432349: Epoch 505\n",
      "2025-05-16 04:23:17.432438: Current learning rate: 0.00531\n",
      "2025-05-16 04:25:06.489361: train_loss -0.9322\n",
      "2025-05-16 04:25:06.489515: val_loss -0.9503\n",
      "2025-05-16 04:25:06.489547: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-16 04:25:06.489581: Epoch time: 109.06 s\n",
      "2025-05-16 04:25:07.016290: \n",
      "2025-05-16 04:25:07.016417: Epoch 506\n",
      "2025-05-16 04:25:07.016562: Current learning rate: 0.0053\n",
      "2025-05-16 04:26:56.084391: train_loss -0.9415\n",
      "2025-05-16 04:26:56.084552: val_loss -0.956\n",
      "2025-05-16 04:26:56.084596: Pseudo dice [np.float32(0.9813)]\n",
      "2025-05-16 04:26:56.084642: Epoch time: 109.07 s\n",
      "2025-05-16 04:26:56.616210: \n",
      "2025-05-16 04:26:56.616308: Epoch 507\n",
      "2025-05-16 04:26:56.616380: Current learning rate: 0.00529\n",
      "2025-05-16 04:28:45.658362: train_loss -0.952\n",
      "2025-05-16 04:28:45.658486: val_loss -0.9639\n",
      "2025-05-16 04:28:45.658520: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 04:28:45.658553: Epoch time: 109.04 s\n",
      "2025-05-16 04:28:46.184439: \n",
      "2025-05-16 04:28:46.184550: Epoch 508\n",
      "2025-05-16 04:28:46.184661: Current learning rate: 0.00528\n",
      "2025-05-16 04:30:35.221893: train_loss -0.9489\n",
      "2025-05-16 04:30:35.222012: val_loss -0.9654\n",
      "2025-05-16 04:30:35.222046: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 04:30:35.222080: Epoch time: 109.04 s\n",
      "2025-05-16 04:30:35.743792: \n",
      "2025-05-16 04:30:35.743937: Epoch 509\n",
      "2025-05-16 04:30:35.744045: Current learning rate: 0.00527\n",
      "2025-05-16 04:32:24.556557: train_loss -0.9538\n",
      "2025-05-16 04:32:24.556676: val_loss -0.9626\n",
      "2025-05-16 04:32:24.556709: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-16 04:32:24.556744: Epoch time: 108.81 s\n",
      "2025-05-16 04:32:25.092582: \n",
      "2025-05-16 04:32:25.092776: Epoch 510\n",
      "2025-05-16 04:32:25.092849: Current learning rate: 0.00526\n",
      "2025-05-16 04:34:14.080706: train_loss -0.9543\n",
      "2025-05-16 04:34:14.080857: val_loss -0.9706\n",
      "2025-05-16 04:34:14.081006: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 04:34:14.081075: Epoch time: 108.99 s\n",
      "2025-05-16 04:34:14.611016: \n",
      "2025-05-16 04:34:14.611107: Epoch 511\n",
      "2025-05-16 04:34:14.611173: Current learning rate: 0.00525\n",
      "2025-05-16 04:36:03.499220: train_loss -0.9547\n",
      "2025-05-16 04:36:03.499385: val_loss -0.9663\n",
      "2025-05-16 04:36:03.499419: Pseudo dice [np.float32(0.9845)]\n",
      "2025-05-16 04:36:03.499450: Epoch time: 108.89 s\n",
      "2025-05-16 04:36:04.223160: \n",
      "2025-05-16 04:36:04.223364: Epoch 512\n",
      "2025-05-16 04:36:04.223465: Current learning rate: 0.00524\n",
      "2025-05-16 04:37:53.293586: train_loss -0.9555\n",
      "2025-05-16 04:37:53.293745: val_loss -0.9646\n",
      "2025-05-16 04:37:53.293788: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-16 04:37:53.293825: Epoch time: 109.07 s\n",
      "2025-05-16 04:37:53.820206: \n",
      "2025-05-16 04:37:53.820311: Epoch 513\n",
      "2025-05-16 04:37:53.820380: Current learning rate: 0.00523\n",
      "2025-05-16 04:39:42.824567: train_loss -0.9545\n",
      "2025-05-16 04:39:42.824680: val_loss -0.9661\n",
      "2025-05-16 04:39:42.824746: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 04:39:42.824782: Epoch time: 109.0 s\n",
      "2025-05-16 04:39:43.354342: \n",
      "2025-05-16 04:39:43.354558: Epoch 514\n",
      "2025-05-16 04:39:43.354662: Current learning rate: 0.00522\n",
      "2025-05-16 04:41:32.417917: train_loss -0.9566\n",
      "2025-05-16 04:41:32.418036: val_loss -0.9672\n",
      "2025-05-16 04:41:32.418072: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 04:41:32.418110: Epoch time: 109.06 s\n",
      "2025-05-16 04:41:32.963714: \n",
      "2025-05-16 04:41:32.963866: Epoch 515\n",
      "2025-05-16 04:41:32.963937: Current learning rate: 0.00521\n",
      "2025-05-16 04:43:21.870682: train_loss -0.9553\n",
      "2025-05-16 04:43:21.870799: val_loss -0.9693\n",
      "2025-05-16 04:43:21.870834: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 04:43:21.870869: Epoch time: 108.91 s\n",
      "2025-05-16 04:43:22.395148: \n",
      "2025-05-16 04:43:22.395281: Epoch 516\n",
      "2025-05-16 04:43:22.395356: Current learning rate: 0.0052\n",
      "2025-05-16 04:45:11.293435: train_loss -0.9557\n",
      "2025-05-16 04:45:11.293558: val_loss -0.969\n",
      "2025-05-16 04:45:11.293592: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 04:45:11.293625: Epoch time: 108.9 s\n",
      "2025-05-16 04:45:11.820374: \n",
      "2025-05-16 04:45:11.820485: Epoch 517\n",
      "2025-05-16 04:45:11.820584: Current learning rate: 0.00519\n",
      "2025-05-16 04:47:00.683338: train_loss -0.9585\n",
      "2025-05-16 04:47:00.683479: val_loss -0.9691\n",
      "2025-05-16 04:47:00.683514: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 04:47:00.683546: Epoch time: 108.86 s\n",
      "2025-05-16 04:47:01.210645: \n",
      "2025-05-16 04:47:01.210901: Epoch 518\n",
      "2025-05-16 04:47:01.211012: Current learning rate: 0.00518\n",
      "2025-05-16 04:48:50.208910: train_loss -0.9589\n",
      "2025-05-16 04:48:50.209099: val_loss -0.9705\n",
      "2025-05-16 04:48:50.209237: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 04:48:50.209324: Epoch time: 109.0 s\n",
      "2025-05-16 04:48:50.733842: \n",
      "2025-05-16 04:48:50.733936: Epoch 519\n",
      "2025-05-16 04:48:50.734006: Current learning rate: 0.00518\n",
      "2025-05-16 04:50:39.526116: train_loss -0.9568\n",
      "2025-05-16 04:50:39.526239: val_loss -0.9634\n",
      "2025-05-16 04:50:39.526335: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-16 04:50:39.526446: Epoch time: 108.79 s\n",
      "2025-05-16 04:50:40.075012: \n",
      "2025-05-16 04:50:40.075515: Epoch 520\n",
      "2025-05-16 04:50:40.075800: Current learning rate: 0.00517\n",
      "2025-05-16 04:52:28.981643: train_loss -0.9573\n",
      "2025-05-16 04:52:28.981777: val_loss -0.9673\n",
      "2025-05-16 04:52:28.981826: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 04:52:28.981861: Epoch time: 108.91 s\n",
      "2025-05-16 04:52:29.514523: \n",
      "2025-05-16 04:52:29.514864: Epoch 521\n",
      "2025-05-16 04:52:29.514943: Current learning rate: 0.00516\n",
      "2025-05-16 04:54:18.528615: train_loss -0.9554\n",
      "2025-05-16 04:54:18.528734: val_loss -0.966\n",
      "2025-05-16 04:54:18.528767: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 04:54:18.528800: Epoch time: 109.01 s\n",
      "2025-05-16 04:54:19.067337: \n",
      "2025-05-16 04:54:19.067512: Epoch 522\n",
      "2025-05-16 04:54:19.067595: Current learning rate: 0.00515\n",
      "2025-05-16 04:56:08.084610: train_loss -0.957\n",
      "2025-05-16 04:56:08.084772: val_loss -0.9625\n",
      "2025-05-16 04:56:08.084805: Pseudo dice [np.float32(0.9838)]\n",
      "Epoch time: 109.02 s084838: \n",
      "2025-05-16 04:56:08.623527: \n",
      "2025-05-16 04:56:08.623619: Epoch 523\n",
      "2025-05-16 04:56:08.623693: Current learning rate: 0.00514\n",
      "2025-05-16 04:57:57.632162: train_loss -0.9574\n",
      "2025-05-16 04:57:57.632284: val_loss -0.9704\n",
      "2025-05-16 04:57:57.632319: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-16 04:57:57.632350: Epoch time: 109.01 s\n",
      "2025-05-16 04:57:58.164076: \n",
      "2025-05-16 04:57:58.164466: Epoch 524\n",
      "2025-05-16 04:57:58.164588: Current learning rate: 0.00513\n",
      "2025-05-16 04:59:47.048370: train_loss -0.9591\n",
      "2025-05-16 04:59:47.048558: val_loss -0.9712\n",
      "2025-05-16 04:59:47.048600: Pseudo dice [np.float32(0.9877)]\n",
      "2025-05-16 04:59:47.048635: Epoch time: 108.88 s\n",
      "2025-05-16 04:59:47.785365: \n",
      "2025-05-16 04:59:47.785715: Epoch 525\n",
      "2025-05-16 04:59:47.785945: Current learning rate: 0.00512\n",
      "2025-05-16 05:01:36.718579: train_loss -0.9564\n",
      "2025-05-16 05:01:36.718701: val_loss -0.9669\n",
      "2025-05-16 05:01:36.718737: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 05:01:36.718771: Epoch time: 108.93 s\n",
      "2025-05-16 05:01:37.250617: \n",
      "2025-05-16 05:01:37.250823: Epoch 526\n",
      "2025-05-16 05:01:37.250918: Current learning rate: 0.00511\n",
      "2025-05-16 05:03:26.164626: train_loss -0.9586\n",
      "2025-05-16 05:03:26.164745: val_loss -0.9679\n",
      "2025-05-16 05:03:26.164778: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 05:03:26.164812: Epoch time: 108.91 s\n",
      "2025-05-16 05:03:26.703914: \n",
      "2025-05-16 05:03:26.704122: Epoch 527\n",
      "2025-05-16 05:03:26.704208: Current learning rate: 0.0051\n",
      "2025-05-16 05:05:15.681791: train_loss -0.9584\n",
      "2025-05-16 05:05:15.681910: val_loss -0.9724\n",
      "2025-05-16 05:05:15.681944: Pseudo dice [np.float32(0.9879)]\n",
      "2025-05-16 05:05:15.681976: Epoch time: 108.98 s\n",
      "2025-05-16 05:05:16.207728: \n",
      "2025-05-16 05:05:16.207865: Epoch 528\n",
      "2025-05-16 05:05:16.207941: Current learning rate: 0.00509\n",
      "2025-05-16 05:07:05.244407: train_loss -0.9585\n",
      "2025-05-16 05:07:05.244538: val_loss -0.9701\n",
      "2025-05-16 05:07:05.244573: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 05:07:05.244608: Epoch time: 109.04 s\n",
      "2025-05-16 05:07:05.772723: \n",
      "2025-05-16 05:07:05.772825: Epoch 529\n",
      "2025-05-16 05:07:05.772892: Current learning rate: 0.00508\n",
      "2025-05-16 05:08:54.763148: train_loss -0.9596\n",
      "2025-05-16 05:08:54.763273: val_loss -0.9693\n",
      "2025-05-16 05:08:54.763311: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 05:08:54.763345: Epoch time: 108.99 s\n",
      "2025-05-16 05:08:55.298820: \n",
      "2025-05-16 05:08:55.298933: Epoch 530\n",
      "2025-05-16 05:08:55.299002: Current learning rate: 0.00507\n",
      "2025-05-16 05:10:44.234544: train_loss -0.9597\n",
      "2025-05-16 05:10:44.234783: val_loss -0.9669\n",
      "2025-05-16 05:10:44.234825: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 05:10:44.234861: Epoch time: 108.94 s\n",
      "2025-05-16 05:10:44.771227: \n",
      "2025-05-16 05:10:44.771330: Epoch 531\n",
      "2025-05-16 05:10:44.771425: Current learning rate: 0.00506\n",
      "2025-05-16 05:12:33.682250: train_loss -0.9584\n",
      "2025-05-16 05:12:33.682369: val_loss -0.9672\n",
      "2025-05-16 05:12:33.682403: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 05:12:33.682435: Epoch time: 108.91 s\n",
      "2025-05-16 05:12:34.204492: \n",
      "2025-05-16 05:12:34.204736: Epoch 532\n",
      "2025-05-16 05:12:34.204859: Current learning rate: 0.00505\n",
      "2025-05-16 05:14:23.041126: train_loss -0.9548\n",
      "2025-05-16 05:14:23.041296: val_loss -0.9672\n",
      "2025-05-16 05:14:23.041330: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 05:14:23.041366: Epoch time: 108.84 s\n",
      "2025-05-16 05:14:23.568689: \n",
      "2025-05-16 05:14:23.569109: Epoch 533\n",
      "2025-05-16 05:14:23.569198: Current learning rate: 0.00504\n",
      "2025-05-16 05:16:12.546275: train_loss -0.9558\n",
      "2025-05-16 05:16:12.546396: val_loss -0.9714\n",
      "2025-05-16 05:16:12.546432: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 05:16:12.546466: Epoch time: 108.98 s\n",
      "2025-05-16 05:16:12.546488: Yayy! New best EMA pseudo Dice: 0.9860000014305115\n",
      "2025-05-16 05:16:13.298945: \n",
      "2025-05-16 05:16:13.299121: Epoch 534\n",
      "2025-05-16 05:16:13.299208: Current learning rate: 0.00503\n",
      "2025-05-16 05:18:02.248975: train_loss -0.9587\n",
      "2025-05-16 05:18:02.249180: val_loss -0.9688\n",
      "2025-05-16 05:18:02.249337: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 05:18:02.249381: Epoch time: 108.95 s\n",
      "2025-05-16 05:18:02.249403: Yayy! New best EMA pseudo Dice: 0.9861000180244446\n",
      "2025-05-16 05:18:03.000178: \n",
      "2025-05-16 05:18:03.000316: Epoch 535\n",
      "2025-05-16 05:18:03.000391: Current learning rate: 0.00502\n",
      "2025-05-16 05:19:51.857921: train_loss -0.9603\n",
      "2025-05-16 05:19:51.858094: val_loss -0.9631\n",
      "2025-05-16 05:19:51.858128: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-16 05:19:51.858162: Epoch time: 108.86 s\n",
      "2025-05-16 05:19:52.381120: \n",
      "2025-05-16 05:19:52.381426: Epoch 536\n",
      "2025-05-16 05:19:52.381543: Current learning rate: 0.00501\n",
      "2025-05-16 05:21:41.295750: train_loss -0.9601\n",
      "2025-05-16 05:21:41.295867: val_loss -0.9682\n",
      "2025-05-16 05:21:41.295897: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 05:21:41.295937: Epoch time: 108.92 s\n",
      "2025-05-16 05:21:41.820524: \n",
      "2025-05-16 05:21:41.820613: Epoch 537\n",
      "2025-05-16 05:21:41.820680: Current learning rate: 0.005\n",
      "2025-05-16 05:23:30.769156: train_loss -0.9581\n",
      "2025-05-16 05:23:30.769690: val_loss -0.969\n",
      "2025-05-16 05:23:30.769769: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 05:23:30.769850: Epoch time: 108.95 s\n",
      "2025-05-16 05:23:31.498196: \n",
      "2025-05-16 05:23:31.498446: Epoch 538\n",
      "2025-05-16 05:23:31.498561: Current learning rate: 0.00499\n",
      "2025-05-16 05:25:20.467318: train_loss -0.9308\n",
      "2025-05-16 05:25:20.467445: val_loss -0.9476\n",
      "2025-05-16 05:25:20.467477: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-16 05:25:20.467518: Epoch time: 108.97 s\n",
      "2025-05-16 05:25:21.002967: \n",
      "2025-05-16 05:25:21.003076: Epoch 539\n",
      "2025-05-16 05:25:21.003149: Current learning rate: 0.00498\n",
      "2025-05-16 05:27:09.961563: train_loss -0.9378\n",
      "2025-05-16 05:27:09.961718: val_loss -0.9646\n",
      "2025-05-16 05:27:09.961752: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 05:27:09.961786: Epoch time: 108.96 s\n",
      "2025-05-16 05:27:10.495827: \n",
      "2025-05-16 05:27:10.495934: Epoch 540\n",
      "2025-05-16 05:27:10.496004: Current learning rate: 0.00497\n",
      "2025-05-16 05:28:59.284478: train_loss -0.9435\n",
      "2025-05-16 05:28:59.284604: val_loss -0.9622\n",
      "2025-05-16 05:28:59.284652: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-16 05:28:59.284687: Epoch time: 108.79 s\n",
      "2025-05-16 05:28:59.814137: \n",
      "2025-05-16 05:28:59.814360: Epoch 541\n",
      "2025-05-16 05:28:59.814442: Current learning rate: 0.00496\n",
      "2025-05-16 05:30:48.802732: train_loss -0.9443\n",
      "2025-05-16 05:30:48.802913: val_loss -0.9587\n",
      "2025-05-16 05:30:48.803101: Pseudo dice [np.float32(0.9828)]\n",
      "2025-05-16 05:30:48.803286: Epoch time: 108.99 s\n",
      "2025-05-16 05:30:49.335441: \n",
      "2025-05-16 05:30:49.335557: Epoch 542\n",
      "2025-05-16 05:30:49.335629: Current learning rate: 0.00495\n",
      "2025-05-16 05:32:38.138381: train_loss -0.9008\n",
      "2025-05-16 05:32:38.138560: val_loss -0.9418\n",
      "2025-05-16 05:32:38.138608: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-16 05:32:38.138644: Epoch time: 108.8 s\n",
      "2025-05-16 05:32:38.673417: \n",
      "2025-05-16 05:32:38.673612: Epoch 543\n",
      "2025-05-16 05:32:38.673696: Current learning rate: 0.00494\n",
      "2025-05-16 05:34:27.451293: train_loss -0.9266\n",
      "2025-05-16 05:34:27.451413: val_loss -0.96\n",
      "2025-05-16 05:34:27.451447: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-16 05:34:27.451480: Epoch time: 108.78 s\n",
      "2025-05-16 05:34:27.983946: \n",
      "2025-05-16 05:34:27.984036: Epoch 544\n",
      "2025-05-16 05:34:27.984105: Current learning rate: 0.00493\n",
      "2025-05-16 05:36:16.908744: train_loss -0.9439\n",
      "2025-05-16 05:36:16.908869: val_loss -0.9628\n",
      "2025-05-16 05:36:16.908903: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-16 05:36:16.908938: Epoch time: 108.93 s\n",
      "2025-05-16 05:36:17.442739: \n",
      "2025-05-16 05:36:17.442960: Epoch 545\n",
      "2025-05-16 05:36:17.443063: Current learning rate: 0.00492\n",
      "2025-05-16 05:38:06.495322: train_loss -0.9497\n",
      "2025-05-16 05:38:06.495552: val_loss -0.9663\n",
      "2025-05-16 05:38:06.495730: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 05:38:06.495809: Epoch time: 109.05 s\n",
      "2025-05-16 05:38:07.028584: \n",
      "2025-05-16 05:38:07.028671: Epoch 546\n",
      "2025-05-16 05:38:07.028740: Current learning rate: 0.00491\n",
      "2025-05-16 05:39:56.026945: train_loss -0.9477\n",
      "2025-05-16 05:39:56.027063: val_loss -0.9685\n",
      "2025-05-16 05:39:56.027096: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 05:39:56.027129: Epoch time: 109.0 s\n",
      "2025-05-16 05:39:56.555491: \n",
      "2025-05-16 05:39:56.555594: Epoch 547\n",
      "2025-05-16 05:39:56.555666: Current learning rate: 0.0049\n",
      "2025-05-16 05:41:45.421865: train_loss -0.9514\n",
      "2025-05-16 05:41:45.422111: val_loss -0.9655\n",
      "2025-05-16 05:41:45.422352: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 05:41:45.422538: Epoch time: 108.87 s\n",
      "2025-05-16 05:41:45.960423: \n",
      "2025-05-16 05:41:45.960709: Epoch 548\n",
      "2025-05-16 05:41:45.960887: Current learning rate: 0.00489\n",
      "2025-05-16 05:43:35.065855: train_loss -0.9531\n",
      "2025-05-16 05:43:35.065976: val_loss -0.9702\n",
      "2025-05-16 05:43:35.066019: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 05:43:35.066054: Epoch time: 109.11 s\n",
      "2025-05-16 05:43:35.598608: \n",
      "2025-05-16 05:43:35.598874: Epoch 549\n",
      "2025-05-16 05:43:35.599120: Current learning rate: 0.00488\n",
      "2025-05-16 05:45:24.574973: train_loss -0.9533\n",
      "2025-05-16 05:45:24.575109: val_loss -0.9665\n",
      "2025-05-16 05:45:24.575145: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 05:45:24.575179: Epoch time: 108.98 s\n",
      "2025-05-16 05:45:25.517722: \n",
      "2025-05-16 05:45:25.517907: Epoch 550\n",
      "2025-05-16 05:45:25.518065: Current learning rate: 0.00487\n",
      "2025-05-16 05:47:14.478534: train_loss -0.9547\n",
      "2025-05-16 05:47:14.478702: val_loss -0.966\n",
      "2025-05-16 05:47:14.478738: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-16 05:47:14.478772: Epoch time: 108.96 s\n",
      "2025-05-16 05:47:15.031480: \n",
      "2025-05-16 05:47:15.031802: Epoch 551\n",
      "2025-05-16 05:47:15.031989: Current learning rate: 0.00486\n",
      "2025-05-16 05:49:04.059431: train_loss -0.9584\n",
      "2025-05-16 05:49:04.059546: val_loss -0.9686\n",
      "2025-05-16 05:49:04.059791: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 05:49:04.059894: Epoch time: 109.03 s\n",
      "2025-05-16 05:49:04.589977: \n",
      "2025-05-16 05:49:04.590162: Epoch 552\n",
      "2025-05-16 05:49:04.590257: Current learning rate: 0.00485\n",
      "2025-05-16 05:50:53.620313: train_loss -0.9569\n",
      "2025-05-16 05:50:53.620428: val_loss -0.9643\n",
      "2025-05-16 05:50:53.620459: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-16 05:50:53.620492: Epoch time: 109.03 s\n",
      "2025-05-16 05:50:54.156249: \n",
      "2025-05-16 05:50:54.156352: Epoch 553\n",
      "2025-05-16 05:50:54.156426: Current learning rate: 0.00484\n",
      "2025-05-16 05:52:43.156337: train_loss -0.9581\n",
      "2025-05-16 05:52:43.156453: val_loss -0.9657\n",
      "2025-05-16 05:52:43.156485: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-16 05:52:43.156518: Epoch time: 109.0 s\n",
      "2025-05-16 05:52:43.686870: \n",
      "2025-05-16 05:52:43.686989: Epoch 554\n",
      "2025-05-16 05:52:43.687212: Current learning rate: 0.00484\n",
      "2025-05-16 05:54:32.586901: train_loss -0.9566\n",
      "2025-05-16 05:54:32.587028: val_loss -0.9688\n",
      "2025-05-16 05:54:32.587066: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 05:54:32.587099: Epoch time: 108.9 s\n",
      "2025-05-16 05:54:33.128620: \n",
      "2025-05-16 05:54:33.128810: Epoch 555\n",
      "2025-05-16 05:54:33.128891: Current learning rate: 0.00483\n",
      "2025-05-16 05:56:22.094034: train_loss -0.957\n",
      "2025-05-16 05:56:22.094165: val_loss -0.9697\n",
      "2025-05-16 05:56:22.094196: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 05:56:22.094229: Epoch time: 108.97 s\n",
      "2025-05-16 05:56:22.621482: \n",
      "2025-05-16 05:56:22.621726: Epoch 556\n",
      "2025-05-16 05:56:22.621864: Current learning rate: 0.00482\n",
      "2025-05-16 05:58:11.435619: train_loss -0.9594\n",
      "2025-05-16 05:58:11.435739: val_loss -0.9685\n",
      "2025-05-16 05:58:11.435774: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 05:58:11.435807: Epoch time: 108.81 s\n",
      "2025-05-16 05:58:11.970534: \n",
      "2025-05-16 05:58:11.970696: Epoch 557\n",
      "2025-05-16 05:58:11.970793: Current learning rate: 0.00481\n",
      "2025-05-16 06:00:00.955551: train_loss -0.957\n",
      "2025-05-16 06:00:00.955673: val_loss -0.969\n",
      "2025-05-16 06:00:00.955717: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 06:00:00.955753: Epoch time: 108.99 s\n",
      "2025-05-16 06:00:01.483908: \n",
      "2025-05-16 06:00:01.483997: Epoch 558\n",
      "2025-05-16 06:00:01.484067: Current learning rate: 0.0048\n",
      "2025-05-16 06:01:50.342585: train_loss -0.9575\n",
      "2025-05-16 06:01:50.342752: val_loss -0.9629\n",
      "2025-05-16 06:01:50.342786: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-16 06:01:50.342818: Epoch time: 108.86 s\n",
      "2025-05-16 06:01:50.879221: \n",
      "2025-05-16 06:01:50.879311: Epoch 559\n",
      "2025-05-16 06:01:50.879381: Current learning rate: 0.00479\n",
      "2025-05-16 06:03:39.766381: train_loss -0.9573\n",
      "2025-05-16 06:03:39.766513: val_loss -0.9664\n",
      "2025-05-16 06:03:39.766824: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 06:03:39.766938: Epoch time: 108.89 s\n",
      "2025-05-16 06:03:40.300033: \n",
      "2025-05-16 06:03:40.300115: Epoch 560\n",
      "2025-05-16 06:03:40.300183: Current learning rate: 0.00478\n",
      "2025-05-16 06:05:29.305971: train_loss -0.958\n",
      "2025-05-16 06:05:29.306097: val_loss -0.9692\n",
      "2025-05-16 06:05:29.306136: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 06:05:29.306172: Epoch time: 109.01 s\n",
      "2025-05-16 06:05:29.839903: \n",
      "2025-05-16 06:05:29.839994: Epoch 561\n",
      "2025-05-16 06:05:29.840060: Current learning rate: 0.00477\n",
      "2025-05-16 06:07:18.695184: train_loss -0.9594\n",
      "2025-05-16 06:07:18.695369: val_loss -0.9694\n",
      "2025-05-16 06:07:18.695401: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 06:07:18.695434: Epoch time: 108.86 s\n",
      "2025-05-16 06:07:19.224153: \n",
      "2025-05-16 06:07:19.224377: Epoch 562\n",
      "2025-05-16 06:07:19.224468: Current learning rate: 0.00476\n",
      "2025-05-16 06:09:08.201376: train_loss -0.9565\n",
      "2025-05-16 06:09:08.201490: val_loss -0.9663\n",
      "2025-05-16 06:09:08.201523: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 06:09:08.201556: Epoch time: 108.98 s\n",
      "2025-05-16 06:09:08.932401: \n",
      "2025-05-16 06:09:08.932517: Epoch 563\n",
      "2025-05-16 06:09:08.932592: Current learning rate: 0.00475\n",
      "2025-05-16 06:10:57.947978: train_loss -0.958\n",
      "2025-05-16 06:10:57.948142: val_loss -0.9698\n",
      "2025-05-16 06:10:57.948173: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 06:10:57.948208: Epoch time: 109.02 s\n",
      "2025-05-16 06:10:58.471974: \n",
      "2025-05-16 06:10:58.472079: Epoch 564\n",
      "2025-05-16 06:10:58.472148: Current learning rate: 0.00474\n",
      "2025-05-16 06:12:47.409944: train_loss -0.9611\n",
      "2025-05-16 06:12:47.410151: val_loss -0.9668\n",
      "2025-05-16 06:12:47.410334: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 06:12:47.410443: Epoch time: 108.94 s\n",
      "2025-05-16 06:12:47.942513: \n",
      "2025-05-16 06:12:47.942626: Epoch 565\n",
      "2025-05-16 06:12:47.942697: Current learning rate: 0.00473\n",
      "2025-05-16 06:14:36.931759: train_loss -0.9576\n",
      "2025-05-16 06:14:36.931912: val_loss -0.9707\n",
      "2025-05-16 06:14:36.932003: Pseudo dice [np.float32(0.9878)]\n",
      "2025-05-16 06:14:36.932047: Epoch time: 108.99 s\n",
      "2025-05-16 06:14:37.460221: \n",
      "2025-05-16 06:14:37.460324: Epoch 566\n",
      "2025-05-16 06:14:37.460393: Current learning rate: 0.00472\n",
      "2025-05-16 06:16:26.393329: train_loss -0.9605\n",
      "2025-05-16 06:16:26.393449: val_loss -0.9695\n",
      "2025-05-16 06:16:26.393481: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 06:16:26.393513: Epoch time: 108.93 s\n",
      "2025-05-16 06:16:26.924961: \n",
      "2025-05-16 06:16:26.925076: Epoch 567\n",
      "2025-05-16 06:16:26.925152: Current learning rate: 0.00471\n",
      "2025-05-16 06:18:15.959529: train_loss -0.9597\n",
      "2025-05-16 06:18:15.959929: val_loss -0.9633\n",
      "2025-05-16 06:18:15.959988: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-16 06:18:15.960033: Epoch time: 109.04 s\n",
      "2025-05-16 06:18:16.496253: \n",
      "2025-05-16 06:18:16.496415: Epoch 568\n",
      "2025-05-16 06:18:16.496502: Current learning rate: 0.0047\n",
      "2025-05-16 06:20:05.347160: train_loss -0.961\n",
      "2025-05-16 06:20:05.347280: val_loss -0.9657\n",
      "2025-05-16 06:20:05.347313: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 06:20:05.347347: Epoch time: 108.85 s\n",
      "2025-05-16 06:20:05.872751: \n",
      "2025-05-16 06:20:05.872861: Epoch 569\n",
      "2025-05-16 06:20:05.872961: Current learning rate: 0.00469\n",
      "2025-05-16 06:21:54.905860: train_loss -0.9573\n",
      "2025-05-16 06:21:54.906003: val_loss -0.9645\n",
      "2025-05-16 06:21:54.906155: Pseudo dice [np.float32(0.9839)]\n",
      "2025-05-16 06:21:54.906211: Epoch time: 109.03 s\n",
      "2025-05-16 06:21:55.438388: \n",
      "2025-05-16 06:21:55.438587: Epoch 570\n",
      "2025-05-16 06:21:55.438730: Current learning rate: 0.00468\n",
      "2025-05-16 06:23:44.451459: train_loss -0.9592\n",
      "2025-05-16 06:23:44.451574: val_loss -0.9682\n",
      "2025-05-16 06:23:44.451607: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 06:23:44.451642: Epoch time: 109.01 s\n",
      "2025-05-16 06:23:44.981854: \n",
      "2025-05-16 06:23:44.982184: Epoch 571\n",
      "2025-05-16 06:23:44.982341: Current learning rate: 0.00467\n",
      "2025-05-16 06:25:33.963388: train_loss -0.96\n",
      "2025-05-16 06:25:33.963510: val_loss -0.9715\n",
      "2025-05-16 06:25:33.963544: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 06:25:33.963575: Epoch time: 108.98 s\n",
      "2025-05-16 06:25:34.496111: \n",
      "2025-05-16 06:25:34.496538: Epoch 572\n",
      "2025-05-16 06:25:34.496771: Current learning rate: 0.00466\n",
      "2025-05-16 06:27:23.505126: train_loss -0.9571\n",
      "2025-05-16 06:27:23.505300: val_loss -0.9647\n",
      "2025-05-16 06:27:23.505339: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 06:27:23.505373: Epoch time: 109.01 s\n",
      "2025-05-16 06:27:24.044464: \n",
      "2025-05-16 06:27:24.044765: Epoch 573\n",
      "2025-05-16 06:27:24.044866: Current learning rate: 0.00465\n",
      "2025-05-16 06:29:13.104592: train_loss -0.9568\n",
      "2025-05-16 06:29:13.104855: val_loss -0.9702\n",
      "2025-05-16 06:29:13.104917: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 06:29:13.104956: Epoch time: 109.06 s\n",
      "2025-05-16 06:29:13.641524: \n",
      "2025-05-16 06:29:13.641723: Epoch 574\n",
      "2025-05-16 06:29:13.641811: Current learning rate: 0.00464\n",
      "2025-05-16 06:31:02.559062: train_loss -0.9586\n",
      "2025-05-16 06:31:02.559183: val_loss -0.9678\n",
      "2025-05-16 06:31:02.559424: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 06:31:02.559504: Epoch time: 108.92 s\n",
      "2025-05-16 06:31:03.092024: \n",
      "2025-05-16 06:31:03.092113: Epoch 575\n",
      "2025-05-16 06:31:03.092181: Current learning rate: 0.00463\n",
      "2025-05-16 06:32:52.023664: train_loss -0.961\n",
      "2025-05-16 06:32:52.023839: val_loss -0.9709\n",
      "2025-05-16 06:32:52.023903: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 06:32:52.023942: Epoch time: 108.93 s\n",
      "2025-05-16 06:32:52.745674: \n",
      "2025-05-16 06:32:52.745784: Epoch 576\n",
      "2025-05-16 06:32:52.745854: Current learning rate: 0.00462\n",
      "2025-05-16 06:34:41.780981: train_loss -0.959\n",
      "2025-05-16 06:34:41.781096: val_loss -0.9658\n",
      "2025-05-16 06:34:41.781129: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-16 06:34:41.781164: Epoch time: 109.04 s\n",
      "2025-05-16 06:34:42.326247: \n",
      "2025-05-16 06:34:42.326359: Epoch 577\n",
      "2025-05-16 06:34:42.326434: Current learning rate: 0.00461\n",
      "2025-05-16 06:36:31.152894: train_loss -0.9606\n",
      "2025-05-16 06:36:31.153083: val_loss -0.9677\n",
      "2025-05-16 06:36:31.153127: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 06:36:31.153163: Epoch time: 108.83 s\n",
      "2025-05-16 06:36:31.684852: \n",
      "2025-05-16 06:36:31.684998: Epoch 578\n",
      "2025-05-16 06:36:31.685069: Current learning rate: 0.0046\n",
      "2025-05-16 06:38:20.662150: train_loss -0.9587\n",
      "2025-05-16 06:38:20.662266: val_loss -0.9687\n",
      "2025-05-16 06:38:20.662299: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 06:38:20.662330: Epoch time: 108.98 s\n",
      "2025-05-16 06:38:21.193926: \n",
      "2025-05-16 06:38:21.194142: Epoch 579\n",
      "2025-05-16 06:38:21.194309: Current learning rate: 0.00459\n",
      "2025-05-16 06:40:10.189499: train_loss -0.9588\n",
      "2025-05-16 06:40:10.189690: val_loss -0.9697\n",
      "2025-05-16 06:40:10.189733: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 06:40:10.189766: Epoch time: 109.0 s\n",
      "2025-05-16 06:40:10.728514: \n",
      "2025-05-16 06:40:10.728628: Epoch 580\n",
      "2025-05-16 06:40:10.728699: Current learning rate: 0.00458\n",
      "2025-05-16 06:41:59.622092: train_loss -0.9559\n",
      "2025-05-16 06:41:59.622210: val_loss -0.9678\n",
      "2025-05-16 06:41:59.622277: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 06:41:59.622452: Epoch time: 108.89 s\n",
      "2025-05-16 06:42:00.157463: \n",
      "2025-05-16 06:42:00.157571: Epoch 581\n",
      "2025-05-16 06:42:00.157645: Current learning rate: 0.00457\n",
      "2025-05-16 06:43:49.051139: train_loss -0.958\n",
      "2025-05-16 06:43:49.051311: val_loss -0.9704\n",
      "2025-05-16 06:43:49.051345: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 06:43:49.051378: Epoch time: 108.89 s\n",
      "2025-05-16 06:43:49.584630: \n",
      "2025-05-16 06:43:49.584806: Epoch 582\n",
      "2025-05-16 06:43:49.584876: Current learning rate: 0.00456\n",
      "2025-05-16 06:45:38.603099: train_loss -0.9586\n",
      "2025-05-16 06:45:38.603221: val_loss -0.969\n",
      "2025-05-16 06:45:38.603256: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 06:45:38.603291: Epoch time: 109.02 s\n",
      "2025-05-16 06:45:39.137381: \n",
      "2025-05-16 06:45:39.137477: Epoch 583\n",
      "2025-05-16 06:45:39.137544: Current learning rate: 0.00455\n",
      "2025-05-16 06:47:28.193576: train_loss -0.9587\n",
      "2025-05-16 06:47:28.193904: val_loss -0.9675\n",
      "2025-05-16 06:47:28.193947: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 06:47:28.193988: Epoch time: 109.06 s\n",
      "2025-05-16 06:47:28.731315: \n",
      "2025-05-16 06:47:28.731464: Epoch 584\n",
      "2025-05-16 06:47:28.731537: Current learning rate: 0.00454\n",
      "2025-05-16 06:49:17.827193: train_loss -0.9569\n",
      "2025-05-16 06:49:17.827315: val_loss -0.9692\n",
      "2025-05-16 06:49:17.827348: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 06:49:17.827381: Epoch time: 109.1 s\n",
      "2025-05-16 06:49:18.359937: \n",
      "2025-05-16 06:49:18.360039: Epoch 585\n",
      "2025-05-16 06:49:18.360110: Current learning rate: 0.00453\n",
      "2025-05-16 06:51:07.251061: train_loss -0.9583\n",
      "2025-05-16 06:51:07.251177: val_loss -0.957\n",
      "2025-05-16 06:51:07.251210: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-16 06:51:07.251245: Epoch time: 108.89 s\n",
      "2025-05-16 06:51:07.778848: \n",
      "2025-05-16 06:51:07.779058: Epoch 586\n",
      "2025-05-16 06:51:07.779207: Current learning rate: 0.00452\n",
      "2025-05-16 06:52:56.841318: train_loss -0.9586\n",
      "2025-05-16 06:52:56.841437: val_loss -0.9636\n",
      "2025-05-16 06:52:56.841469: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-16 06:52:56.841502: Epoch time: 109.06 s\n",
      "2025-05-16 06:52:57.378937: \n",
      "2025-05-16 06:52:57.379179: Epoch 587\n",
      "2025-05-16 06:52:57.379301: Current learning rate: 0.00451\n",
      "2025-05-16 06:54:46.359684: train_loss -0.9578\n",
      "2025-05-16 06:54:46.359800: val_loss -0.9682\n",
      "2025-05-16 06:54:46.359832: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-16 06:54:46.359873: Epoch time: 108.98 s\n",
      "2025-05-16 06:54:47.080652: \n",
      "2025-05-16 06:54:47.080763: Epoch 588\n",
      "2025-05-16 06:54:47.080842: Current learning rate: 0.0045\n",
      "2025-05-16 06:56:35.868511: train_loss -0.9592\n",
      "2025-05-16 06:56:35.868698: val_loss -0.9708\n",
      "2025-05-16 06:56:35.868733: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 06:56:35.868778: Epoch time: 108.79 s\n",
      "2025-05-16 06:56:36.411309: \n",
      "2025-05-16 06:56:36.411708: Epoch 589\n",
      "2025-05-16 06:56:36.411810: Current learning rate: 0.00449\n",
      "2025-05-16 06:58:25.299809: train_loss -0.9612\n",
      "2025-05-16 06:58:25.299926: val_loss -0.9707\n",
      "2025-05-16 06:58:25.299959: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 06:58:25.299995: Epoch time: 108.89 s\n",
      "2025-05-16 06:58:25.828625: \n",
      "2025-05-16 06:58:25.828910: Epoch 590\n",
      "2025-05-16 06:58:25.829001: Current learning rate: 0.00448\n",
      "2025-05-16 07:00:14.886597: train_loss -0.9611\n",
      "2025-05-16 07:00:14.886844: val_loss -0.9671\n",
      "2025-05-16 07:00:14.886882: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-16 07:00:14.886920: Epoch time: 109.06 s\n",
      "2025-05-16 07:00:15.419596: \n",
      "2025-05-16 07:00:15.419761: Epoch 591\n",
      "2025-05-16 07:00:15.419839: Current learning rate: 0.00447\n",
      "2025-05-16 07:02:04.250953: train_loss -0.9587\n",
      "2025-05-16 07:02:04.251150: val_loss -0.9683\n",
      "2025-05-16 07:02:04.251183: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 07:02:04.251218: Epoch time: 108.83 s\n",
      "2025-05-16 07:02:04.789131: \n",
      "2025-05-16 07:02:04.789223: Epoch 592\n",
      "2025-05-16 07:02:04.789295: Current learning rate: 0.00446\n",
      "2025-05-16 07:03:53.798821: train_loss -0.9587\n",
      "2025-05-16 07:03:53.798938: val_loss -0.9675\n",
      "2025-05-16 07:03:53.798970: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 07:03:53.799004: Epoch time: 109.01 s\n",
      "2025-05-16 07:03:54.337590: \n",
      "2025-05-16 07:03:54.337703: Epoch 593\n",
      "2025-05-16 07:03:54.337771: Current learning rate: 0.00445\n",
      "2025-05-16 07:05:43.174336: train_loss -0.9593\n",
      "2025-05-16 07:05:43.174454: val_loss -0.9703\n",
      "2025-05-16 07:05:43.174488: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 07:05:43.174562: Epoch time: 108.84 s\n",
      "2025-05-16 07:05:43.710025: \n",
      "2025-05-16 07:05:43.710280: Epoch 594\n",
      "2025-05-16 07:05:43.710529: Current learning rate: 0.00444\n",
      "2025-05-16 07:07:32.591986: train_loss -0.9603\n",
      "2025-05-16 07:07:32.592163: val_loss -0.9697\n",
      "2025-05-16 07:07:32.592198: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 07:07:32.592234: Epoch time: 108.88 s\n",
      "2025-05-16 07:07:33.117628: \n",
      "2025-05-16 07:07:33.117791: Epoch 595\n",
      "2025-05-16 07:07:33.117874: Current learning rate: 0.00443\n",
      "2025-05-16 07:09:21.959084: train_loss -0.9615\n",
      "2025-05-16 07:09:21.959205: val_loss -0.9701\n",
      "2025-05-16 07:09:21.959238: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 07:09:21.959286: Epoch time: 108.84 s\n",
      "2025-05-16 07:09:22.493565: \n",
      "2025-05-16 07:09:22.493743: Epoch 596\n",
      "2025-05-16 07:09:22.493838: Current learning rate: 0.00442\n",
      "2025-05-16 07:11:11.544164: train_loss -0.9605\n",
      "2025-05-16 07:11:11.544291: val_loss -0.9676\n",
      "2025-05-16 07:11:11.544326: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-16 07:11:11.544360: Epoch time: 109.05 s\n",
      "2025-05-16 07:11:12.074786: \n",
      "2025-05-16 07:11:12.074862: Epoch 597\n",
      "2025-05-16 07:11:12.074924: Current learning rate: 0.00441\n",
      "2025-05-16 07:13:01.153863: train_loss -0.9602\n",
      "2025-05-16 07:13:01.154089: val_loss -0.9658\n",
      "2025-05-16 07:13:01.154277: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-16 07:13:01.154443: Epoch time: 109.08 s\n",
      "2025-05-16 07:13:01.695436: \n",
      "2025-05-16 07:13:01.695534: Epoch 598\n",
      "2025-05-16 07:13:01.695608: Current learning rate: 0.0044\n",
      "2025-05-16 07:14:50.667516: train_loss -0.9612\n",
      "2025-05-16 07:14:50.667659: val_loss -0.9682\n",
      "2025-05-16 07:14:50.667696: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 07:14:50.667730: Epoch time: 108.97 s\n",
      "2025-05-16 07:14:51.211432: \n",
      "2025-05-16 07:14:51.211604: Epoch 599\n",
      "2025-05-16 07:14:51.211686: Current learning rate: 0.00439\n",
      "2025-05-16 07:16:40.056757: train_loss -0.9593\n",
      "2025-05-16 07:16:40.056900: val_loss -0.9677\n",
      "2025-05-16 07:16:40.057017: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 07:16:40.057076: Epoch time: 108.85 s\n",
      "2025-05-16 07:16:40.796532: \n",
      "2025-05-16 07:16:40.796618: Epoch 600\n",
      "2025-05-16 07:16:40.796689: Current learning rate: 0.00438\n",
      "2025-05-16 07:18:29.841108: train_loss -0.9627\n",
      "2025-05-16 07:18:29.841231: val_loss -0.9672\n",
      "2025-05-16 07:18:29.841269: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-16 07:18:29.841304: Epoch time: 109.05 s\n",
      "2025-05-16 07:18:30.560656: \n",
      "2025-05-16 07:18:30.560766: Epoch 601\n",
      "2025-05-16 07:18:30.560839: Current learning rate: 0.00437\n",
      "2025-05-16 07:20:19.486051: train_loss -0.9592\n",
      "2025-05-16 07:20:19.486176: val_loss -0.9725\n",
      "2025-05-16 07:20:19.486210: Pseudo dice [np.float32(0.9883)]\n",
      "2025-05-16 07:20:19.486244: Epoch time: 108.93 s\n",
      "2025-05-16 07:20:20.025053: \n",
      "2025-05-16 07:20:20.025147: Epoch 602\n",
      "2025-05-16 07:20:20.025213: Current learning rate: 0.00436\n",
      "2025-05-16 07:22:09.009466: train_loss -0.9618\n",
      "2025-05-16 07:22:09.009633: val_loss -0.9672\n",
      "2025-05-16 07:22:09.009675: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 07:22:09.009710: Epoch time: 108.98 s\n",
      "2025-05-16 07:22:09.551470: \n",
      "2025-05-16 07:22:09.551585: Epoch 603\n",
      "2025-05-16 07:22:09.551658: Current learning rate: 0.00435\n",
      "2025-05-16 07:23:58.610277: train_loss -0.9607\n",
      "2025-05-16 07:23:58.610507: val_loss -0.97\n",
      "2025-05-16 07:23:58.610582: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 07:23:58.610621: Epoch time: 109.06 s\n",
      "2025-05-16 07:23:59.151973: \n",
      "2025-05-16 07:23:59.152308: Epoch 604\n",
      "2025-05-16 07:23:59.152507: Current learning rate: 0.00434\n",
      "2025-05-16 07:25:48.317109: train_loss -0.9621\n",
      "2025-05-16 07:25:48.317253: val_loss -0.9697\n",
      "2025-05-16 07:25:48.317284: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 07:25:48.317318: Epoch time: 109.17 s\n",
      "2025-05-16 07:25:48.858707: \n",
      "2025-05-16 07:25:48.859135: Epoch 605\n",
      "2025-05-16 07:25:48.859254: Current learning rate: 0.00433\n",
      "2025-05-16 07:27:37.906275: train_loss -0.961\n",
      "2025-05-16 07:27:37.906396: val_loss -0.9698\n",
      "2025-05-16 07:27:37.906428: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 07:27:37.906462: Epoch time: 109.05 s\n",
      "2025-05-16 07:27:38.448576: \n",
      "2025-05-16 07:27:38.448995: Epoch 606\n",
      "2025-05-16 07:27:38.449201: Current learning rate: 0.00432\n",
      "2025-05-16 07:29:27.417971: train_loss -0.9608\n",
      "2025-05-16 07:29:27.418088: val_loss -0.9694\n",
      "2025-05-16 07:29:27.418123: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 07:29:27.418156: Epoch time: 108.97 s\n",
      "2025-05-16 07:29:27.955617: \n",
      "2025-05-16 07:29:27.955716: Epoch 607\n",
      "2025-05-16 07:29:27.955785: Current learning rate: 0.00431\n",
      "2025-05-16 07:31:17.032157: train_loss -0.9595\n",
      "2025-05-16 07:31:17.032269: val_loss -0.9693\n",
      "2025-05-16 07:31:17.032302: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 07:31:17.032470: Epoch time: 109.08 s\n",
      "2025-05-16 07:31:17.565244: \n",
      "2025-05-16 07:31:17.565370: Epoch 608\n",
      "2025-05-16 07:31:17.565441: Current learning rate: 0.0043\n",
      "2025-05-16 07:33:06.533886: train_loss -0.9598\n",
      "2025-05-16 07:33:06.534039: val_loss -0.9675\n",
      "2025-05-16 07:33:06.534074: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 07:33:06.534108: Epoch time: 108.97 s\n",
      "2025-05-16 07:33:07.071785: \n",
      "2025-05-16 07:33:07.071963: Epoch 609\n",
      "2025-05-16 07:33:07.072062: Current learning rate: 0.00429\n",
      "2025-05-16 07:34:56.101109: train_loss -0.9612\n",
      "2025-05-16 07:34:56.101223: val_loss -0.9679\n",
      "2025-05-16 07:34:56.101259: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 07:34:56.101294: Epoch time: 109.03 s\n",
      "2025-05-16 07:34:56.633817: \n",
      "2025-05-16 07:34:56.634194: Epoch 610\n",
      "2025-05-16 07:34:56.634280: Current learning rate: 0.00429\n",
      "2025-05-16 07:36:45.583721: train_loss -0.9612\n",
      "2025-05-16 07:36:45.583842: val_loss -0.9696\n",
      "2025-05-16 07:36:45.583876: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 07:36:45.583908: Epoch time: 108.95 s\n",
      "2025-05-16 07:36:46.117089: \n",
      "2025-05-16 07:36:46.117244: Epoch 611\n",
      "2025-05-16 07:36:46.117316: Current learning rate: 0.00428\n",
      "2025-05-16 07:38:34.894773: train_loss -0.9608\n",
      "2025-05-16 07:38:34.894892: val_loss -0.9674\n",
      "2025-05-16 07:38:34.894923: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 07:38:34.894957: Epoch time: 108.78 s\n",
      "2025-05-16 07:38:35.435274: \n",
      "2025-05-16 07:38:35.435466: Epoch 612\n",
      "2025-05-16 07:38:35.435556: Current learning rate: 0.00427\n",
      "2025-05-16 07:40:24.252681: train_loss -0.9607\n",
      "2025-05-16 07:40:24.252794: val_loss -0.9692\n",
      "2025-05-16 07:40:24.252830: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 07:40:24.252895: Epoch time: 108.82 s\n",
      "2025-05-16 07:40:24.984883: \n",
      "2025-05-16 07:40:24.985148: Epoch 613\n",
      "2025-05-16 07:40:24.985232: Current learning rate: 0.00426\n",
      "2025-05-16 07:42:13.840299: train_loss -0.9611\n",
      "2025-05-16 07:42:13.840505: val_loss -0.9687\n",
      "2025-05-16 07:42:13.840602: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 07:42:13.840650: Epoch time: 108.86 s\n",
      "2025-05-16 07:42:14.379536: \n",
      "2025-05-16 07:42:14.379917: Epoch 614\n",
      "2025-05-16 07:42:14.380011: Current learning rate: 0.00425\n",
      "2025-05-16 07:44:03.477990: train_loss -0.9626\n",
      "2025-05-16 07:44:03.478172: val_loss -0.9696\n",
      "2025-05-16 07:44:03.478203: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 07:44:03.478236: Epoch time: 109.1 s\n",
      "2025-05-16 07:44:03.478256: Yayy! New best EMA pseudo Dice: 0.9861000180244446\n",
      "2025-05-16 07:44:04.236268: \n",
      "2025-05-16 07:44:04.236365: Epoch 615\n",
      "2025-05-16 07:44:04.236440: Current learning rate: 0.00424\n",
      "2025-05-16 07:45:53.148164: train_loss -0.9592\n",
      "2025-05-16 07:45:53.148362: val_loss -0.9677\n",
      "2025-05-16 07:45:53.148397: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 07:45:53.148431: Epoch time: 108.91 s\n",
      "2025-05-16 07:45:53.678433: \n",
      "2025-05-16 07:45:53.678561: Epoch 616\n",
      "2025-05-16 07:45:53.678718: Current learning rate: 0.00423\n",
      "2025-05-16 07:47:42.782557: train_loss -0.9605\n",
      "2025-05-16 07:47:42.782675: val_loss -0.9654\n",
      "2025-05-16 07:47:42.782710: Pseudo dice [np.float32(0.9845)]\n",
      "2025-05-16 07:47:42.782743: Epoch time: 109.1 s\n",
      "2025-05-16 07:47:43.328347: \n",
      "2025-05-16 07:47:43.328557: Epoch 617\n",
      "2025-05-16 07:47:43.328649: Current learning rate: 0.00422\n",
      "2025-05-16 07:49:32.269134: train_loss -0.9611\n",
      "2025-05-16 07:49:32.269256: val_loss -0.9711\n",
      "2025-05-16 07:49:32.269289: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 07:49:32.269322: Epoch time: 108.94 s\n",
      "2025-05-16 07:49:32.809792: \n",
      "2025-05-16 07:49:32.809908: Epoch 618\n",
      "2025-05-16 07:49:32.809980: Current learning rate: 0.00421\n",
      "2025-05-16 07:51:21.709196: train_loss -0.9613\n",
      "2025-05-16 07:51:21.709489: val_loss -0.9702\n",
      "2025-05-16 07:51:21.709530: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 07:51:21.709565: Epoch time: 108.9 s\n",
      "2025-05-16 07:51:22.243180: \n",
      "2025-05-16 07:51:22.243378: Epoch 619\n",
      "2025-05-16 07:51:22.243460: Current learning rate: 0.0042\n",
      "2025-05-16 07:53:11.333426: train_loss -0.9607\n",
      "2025-05-16 07:53:11.333546: val_loss -0.9694\n",
      "2025-05-16 07:53:11.333579: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 07:53:11.333613: Epoch time: 109.09 s\n",
      "2025-05-16 07:53:11.872239: \n",
      "2025-05-16 07:53:11.872385: Epoch 620\n",
      "2025-05-16 07:53:11.872472: Current learning rate: 0.00419\n",
      "2025-05-16 07:55:00.946021: train_loss -0.9578\n",
      "2025-05-16 07:55:00.946160: val_loss -0.9691\n",
      "2025-05-16 07:55:00.946201: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 07:55:00.946236: Epoch time: 109.07 s\n",
      "2025-05-16 07:55:01.492052: \n",
      "2025-05-16 07:55:01.492214: Epoch 621\n",
      "2025-05-16 07:55:01.492285: Current learning rate: 0.00418\n",
      "2025-05-16 07:56:50.542270: train_loss -0.9598\n",
      "2025-05-16 07:56:50.542413: val_loss -0.9681\n",
      "2025-05-16 07:56:50.542444: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-16 07:56:50.542477: Epoch time: 109.05 s\n",
      "2025-05-16 07:56:51.086702: \n",
      "2025-05-16 07:56:51.086794: Epoch 622\n",
      "2025-05-16 07:56:51.086925: Current learning rate: 0.00417\n",
      "2025-05-16 07:58:39.930074: train_loss -0.9607\n",
      "2025-05-16 07:58:39.930194: val_loss -0.9681\n",
      "2025-05-16 07:58:39.930227: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 07:58:39.930350: Epoch time: 108.84 s\n",
      "2025-05-16 07:58:40.466921: \n",
      "2025-05-16 07:58:40.467354: Epoch 623\n",
      "2025-05-16 07:58:40.467642: Current learning rate: 0.00416\n",
      "2025-05-16 08:00:29.553495: train_loss -0.9614\n",
      "2025-05-16 08:00:29.553716: val_loss -0.9675\n",
      "2025-05-16 08:00:29.553837: Pseudo dice [np.float32(0.9845)]\n",
      "2025-05-16 08:00:29.553928: Epoch time: 109.09 s\n",
      "2025-05-16 08:00:30.097845: \n",
      "2025-05-16 08:00:30.097994: Epoch 624\n",
      "2025-05-16 08:00:30.098082: Current learning rate: 0.00415\n",
      "2025-05-16 08:02:19.205539: train_loss -0.9617\n",
      "2025-05-16 08:02:19.205744: val_loss -0.9671\n",
      "2025-05-16 08:02:19.205822: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 08:02:19.205862: Epoch time: 109.11 s\n",
      "2025-05-16 08:02:19.743898: \n",
      "2025-05-16 08:02:19.744129: Epoch 625\n",
      "2025-05-16 08:02:19.744239: Current learning rate: 0.00414\n",
      "2025-05-16 08:04:08.609117: train_loss -0.961\n",
      "2025-05-16 08:04:08.609447: val_loss -0.9703\n",
      "2025-05-16 08:04:08.609504: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 08:04:08.609571: Epoch time: 108.87 s\n",
      "2025-05-16 08:04:09.328646: \n",
      "2025-05-16 08:04:09.328752: Epoch 626\n",
      "2025-05-16 08:04:09.328821: Current learning rate: 0.00413\n",
      "2025-05-16 08:05:58.173482: train_loss -0.9606\n",
      "2025-05-16 08:05:58.173616: val_loss -0.9698\n",
      "2025-05-16 08:05:58.173651: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 08:05:58.173685: Epoch time: 108.85 s\n",
      "2025-05-16 08:05:58.712249: \n",
      "2025-05-16 08:05:58.712449: Epoch 627\n",
      "2025-05-16 08:05:58.712537: Current learning rate: 0.00412\n",
      "2025-05-16 08:07:47.744017: train_loss -0.9627\n",
      "2025-05-16 08:07:47.744200: val_loss -0.9693\n",
      "2025-05-16 08:07:47.744506: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 08:07:47.744616: Epoch time: 109.03 s\n",
      "2025-05-16 08:07:48.282984: \n",
      "2025-05-16 08:07:48.283091: Epoch 628\n",
      "2025-05-16 08:07:48.283163: Current learning rate: 0.00411\n",
      "2025-05-16 08:09:37.212927: train_loss -0.9597\n",
      "2025-05-16 08:09:37.213089: val_loss -0.9627\n",
      "2025-05-16 08:09:37.213122: Pseudo dice [np.float32(0.9822)]\n",
      "2025-05-16 08:09:37.213157: Epoch time: 108.93 s\n",
      "2025-05-16 08:09:37.748608: \n",
      "2025-05-16 08:09:37.748714: Epoch 629\n",
      "2025-05-16 08:09:37.748786: Current learning rate: 0.0041\n",
      "2025-05-16 08:11:26.810868: train_loss -0.9624\n",
      "2025-05-16 08:11:26.811010: val_loss -0.9687\n",
      "2025-05-16 08:11:26.811065: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 08:11:26.811102: Epoch time: 109.06 s\n",
      "2025-05-16 08:11:27.350722: \n",
      "2025-05-16 08:11:27.351125: Epoch 630\n",
      "2025-05-16 08:11:27.351441: Current learning rate: 0.00409\n",
      "2025-05-16 08:13:16.411762: train_loss -0.9619\n",
      "2025-05-16 08:13:16.411938: val_loss -0.9676\n",
      "2025-05-16 08:13:16.412017: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 08:13:16.412112: Epoch time: 109.06 s\n",
      "2025-05-16 08:13:16.955523: \n",
      "2025-05-16 08:13:16.955963: Epoch 631\n",
      "2025-05-16 08:13:16.956059: Current learning rate: 0.00408\n",
      "2025-05-16 08:15:06.001406: train_loss -0.9606\n",
      "2025-05-16 08:15:06.001714: val_loss -0.9608\n",
      "2025-05-16 08:15:06.001809: Pseudo dice [np.float32(0.9827)]\n",
      "2025-05-16 08:15:06.001915: Epoch time: 109.05 s\n",
      "2025-05-16 08:15:06.536452: \n",
      "2025-05-16 08:15:06.536814: Epoch 632\n",
      "2025-05-16 08:15:06.536984: Current learning rate: 0.00407\n",
      "2025-05-16 08:16:55.513230: train_loss -0.962\n",
      "2025-05-16 08:16:55.513533: val_loss -0.9678\n",
      "2025-05-16 08:16:55.513679: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 08:16:55.513757: Epoch time: 108.98 s\n",
      "2025-05-16 08:16:56.056392: \n",
      "2025-05-16 08:16:56.056639: Epoch 633\n",
      "2025-05-16 08:16:56.056756: Current learning rate: 0.00406\n",
      "2025-05-16 08:18:45.052434: train_loss -0.964\n",
      "2025-05-16 08:18:45.052582: val_loss -0.9696\n",
      "2025-05-16 08:18:45.052621: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 08:18:45.052654: Epoch time: 109.0 s\n",
      "2025-05-16 08:18:45.587316: \n",
      "2025-05-16 08:18:45.587429: Epoch 634\n",
      "2025-05-16 08:18:45.587515: Current learning rate: 0.00405\n",
      "2025-05-16 08:20:34.514887: train_loss -0.9596\n",
      "2025-05-16 08:20:34.515009: val_loss -0.9704\n",
      "2025-05-16 08:20:34.515041: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 08:20:34.515074: Epoch time: 108.93 s\n",
      "2025-05-16 08:20:35.052997: \n",
      "2025-05-16 08:20:35.053092: Epoch 635\n",
      "2025-05-16 08:20:35.053163: Current learning rate: 0.00404\n",
      "2025-05-16 08:22:24.070583: train_loss -0.9604\n",
      "2025-05-16 08:22:24.070754: val_loss -0.9695\n",
      "2025-05-16 08:22:24.070788: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 08:22:24.070821: Epoch time: 109.02 s\n",
      "2025-05-16 08:22:24.611117: \n",
      "2025-05-16 08:22:24.611288: Epoch 636\n",
      "2025-05-16 08:22:24.611367: Current learning rate: 0.00403\n",
      "2025-05-16 08:24:13.540963: train_loss -0.9578\n",
      "2025-05-16 08:24:13.541092: val_loss -0.9615\n",
      "2025-05-16 08:24:13.541127: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-16 08:24:13.541160: Epoch time: 108.93 s\n",
      "2025-05-16 08:24:14.079306: \n",
      "2025-05-16 08:24:14.079454: Epoch 637\n",
      "2025-05-16 08:24:14.079532: Current learning rate: 0.00402\n",
      "2025-05-16 08:26:03.117960: train_loss -0.9585\n",
      "2025-05-16 08:26:03.118085: val_loss -0.9702\n",
      "2025-05-16 08:26:03.118199: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 08:26:03.118331: Epoch time: 109.04 s\n",
      "2025-05-16 08:26:03.838015: \n",
      "2025-05-16 08:26:03.838177: Epoch 638\n",
      "2025-05-16 08:26:03.838262: Current learning rate: 0.00401\n",
      "2025-05-16 08:27:52.854038: train_loss -0.9622\n",
      "2025-05-16 08:27:52.854164: val_loss -0.972\n",
      "2025-05-16 08:27:52.854202: Pseudo dice [np.float32(0.9881)]\n",
      "2025-05-16 08:27:52.854236: Epoch time: 109.02 s\n",
      "2025-05-16 08:27:53.395381: \n",
      "2025-05-16 08:27:53.395575: Epoch 639\n",
      "2025-05-16 08:27:53.395665: Current learning rate: 0.004\n",
      "2025-05-16 08:29:42.318425: train_loss -0.9622\n",
      "2025-05-16 08:29:42.318582: val_loss -0.9698\n",
      "2025-05-16 08:29:42.318676: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 08:29:42.318737: Epoch time: 108.92 s\n",
      "2025-05-16 08:29:42.855471: \n",
      "2025-05-16 08:29:42.855851: Epoch 640\n",
      "2025-05-16 08:29:42.856016: Current learning rate: 0.00399\n",
      "2025-05-16 08:31:31.923661: train_loss -0.9603\n",
      "2025-05-16 08:31:31.923920: val_loss -0.9693\n",
      "2025-05-16 08:31:31.924027: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 08:31:31.924116: Epoch time: 109.07 s\n",
      "2025-05-16 08:31:32.458171: \n",
      "2025-05-16 08:31:32.458433: Epoch 641\n",
      "2025-05-16 08:31:32.458560: Current learning rate: 0.00398\n",
      "2025-05-16 08:33:21.452762: train_loss -0.9621\n",
      "2025-05-16 08:33:21.453190: val_loss -0.9692\n",
      "2025-05-16 08:33:21.453236: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 08:33:21.453271: Epoch time: 109.0 s\n",
      "2025-05-16 08:33:22.000581: \n",
      "2025-05-16 08:33:22.001049: Epoch 642\n",
      "2025-05-16 08:33:22.001155: Current learning rate: 0.00397\n",
      "2025-05-16 08:35:10.839695: train_loss -0.96\n",
      "2025-05-16 08:35:10.839965: val_loss -0.9718\n",
      "2025-05-16 08:35:10.840056: Pseudo dice [np.float32(0.9878)]\n",
      "2025-05-16 08:35:10.840160: Epoch time: 108.84 s\n",
      "2025-05-16 08:35:10.840205: Yayy! New best EMA pseudo Dice: 0.9861000180244446\n",
      "2025-05-16 08:35:11.606925: \n",
      "2025-05-16 08:35:11.607054: Epoch 643\n",
      "2025-05-16 08:35:11.607136: Current learning rate: 0.00396\n",
      "2025-05-16 08:37:00.687226: train_loss -0.9619\n",
      "2025-05-16 08:37:00.687362: val_loss -0.9701\n",
      "2025-05-16 08:37:00.687397: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 08:37:00.687432: Epoch time: 109.08 s\n",
      "2025-05-16 08:37:00.687454: Yayy! New best EMA pseudo Dice: 0.9861999750137329\n",
      "2025-05-16 08:37:01.449640: \n",
      "2025-05-16 08:37:01.450117: Epoch 644\n",
      "2025-05-16 08:37:01.450342: Current learning rate: 0.00395\n",
      "2025-05-16 08:38:50.329713: train_loss -0.9629\n",
      "2025-05-16 08:38:50.329841: val_loss -0.9689\n",
      "2025-05-16 08:38:50.329871: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 08:38:50.329904: Epoch time: 108.88 s\n",
      "2025-05-16 08:38:50.867413: \n",
      "2025-05-16 08:38:50.867516: Epoch 645\n",
      "2025-05-16 08:38:50.867596: Current learning rate: 0.00394\n",
      "2025-05-16 08:40:39.949663: train_loss -0.9628\n",
      "2025-05-16 08:40:39.949782: val_loss -0.9665\n",
      "2025-05-16 08:40:39.949815: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-16 08:40:39.949849: Epoch time: 109.08 s\n",
      "2025-05-16 08:40:40.485997: \n",
      "2025-05-16 08:40:40.486183: Epoch 646\n",
      "2025-05-16 08:40:40.486269: Current learning rate: 0.00393\n",
      "2025-05-16 08:42:29.344396: train_loss -0.962\n",
      "val_loss -0.9708:29.344517: \n",
      "2025-05-16 08:42:29.344616: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 08:42:29.344672: Epoch time: 108.86 s\n",
      "2025-05-16 08:42:29.882150: \n",
      "2025-05-16 08:42:29.882271: Epoch 647\n",
      "2025-05-16 08:42:29.882586: Current learning rate: 0.00392\n",
      "2025-05-16 08:44:18.880745: train_loss -0.964\n",
      "2025-05-16 08:44:18.881039: val_loss -0.9634\n",
      "2025-05-16 08:44:18.881144: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-16 08:44:18.881187: Epoch time: 109.0 s\n",
      "2025-05-16 08:44:19.423611: \n",
      "2025-05-16 08:44:19.423701: Epoch 648\n",
      "2025-05-16 08:44:19.423771: Current learning rate: 0.00391\n",
      "2025-05-16 08:46:08.241434: train_loss -0.9626\n",
      "2025-05-16 08:46:08.241560: val_loss -0.9665\n",
      "2025-05-16 08:46:08.241594: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-16 08:46:08.241629: Epoch time: 108.82 s\n",
      "2025-05-16 08:46:08.782106: \n",
      "2025-05-16 08:46:08.782278: Epoch 649\n",
      "2025-05-16 08:46:08.782364: Current learning rate: 0.0039\n",
      "2025-05-16 08:47:57.825950: train_loss -0.9639\n",
      "2025-05-16 08:47:57.826125: val_loss -0.9663\n",
      "2025-05-16 08:47:57.826163: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-16 08:47:57.826199: Epoch time: 109.04 s\n",
      "2025-05-16 08:47:58.773333: \n",
      "2025-05-16 08:47:58.773844: Epoch 650\n",
      "2025-05-16 08:47:58.774026: Current learning rate: 0.00389\n",
      "2025-05-16 08:49:47.883363: train_loss -0.964\n",
      "2025-05-16 08:49:47.883608: val_loss -0.968\n",
      "2025-05-16 08:49:47.883691: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 08:49:47.883737: Epoch time: 109.11 s\n",
      "2025-05-16 08:49:48.426858: \n",
      "2025-05-16 08:49:48.427219: Epoch 651\n",
      "2025-05-16 08:49:48.427306: Current learning rate: 0.00388\n",
      "2025-05-16 08:51:37.428033: train_loss -0.9624\n",
      "2025-05-16 08:51:37.428165: val_loss -0.9631\n",
      "2025-05-16 08:51:37.428200: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-16 08:51:37.428235: Epoch time: 109.0 s\n",
      "2025-05-16 08:51:37.970804: \n",
      "2025-05-16 08:51:37.970926: Epoch 652\n",
      "2025-05-16 08:51:37.971010: Current learning rate: 0.00387\n",
      "2025-05-16 08:53:27.076190: train_loss -0.9621\n",
      "2025-05-16 08:53:27.076304: val_loss -0.972\n",
      "2025-05-16 08:53:27.076338: Pseudo dice [np.float32(0.9885)]\n",
      "2025-05-16 08:53:27.076402: Epoch time: 109.11 s\n",
      "2025-05-16 08:53:27.617768: \n",
      "2025-05-16 08:53:27.617983: Epoch 653\n",
      "2025-05-16 08:53:27.618093: Current learning rate: 0.00386\n",
      "2025-05-16 08:55:16.679005: train_loss -0.9638\n",
      "2025-05-16 08:55:16.679355: val_loss -0.9697\n",
      "2025-05-16 08:55:16.679393: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 08:55:16.679425: Epoch time: 109.06 s\n",
      "2025-05-16 08:55:17.233732: \n",
      "2025-05-16 08:55:17.233921: Epoch 654\n",
      "2025-05-16 08:55:17.234000: Current learning rate: 0.00385\n",
      "2025-05-16 08:57:06.270149: train_loss -0.9609\n",
      "2025-05-16 08:57:06.270272: val_loss -0.9696\n",
      "2025-05-16 08:57:06.270306: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 08:57:06.270341: Epoch time: 109.04 s\n",
      "2025-05-16 08:57:06.810380: \n",
      "2025-05-16 08:57:06.810554: Epoch 655\n",
      "2025-05-16 08:57:06.810634: Current learning rate: 0.00384\n",
      "2025-05-16 08:58:55.706003: train_loss -0.9612\n",
      "2025-05-16 08:58:55.706184: val_loss -0.9694\n",
      "2025-05-16 08:58:55.706221: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 08:58:55.706256: Epoch time: 108.9 s\n",
      "2025-05-16 08:58:56.248944: \n",
      "2025-05-16 08:58:56.249141: Epoch 656\n",
      "2025-05-16 08:58:56.249228: Current learning rate: 0.00383\n",
      "2025-05-16 09:00:45.201997: train_loss -0.9638\n",
      "2025-05-16 09:00:45.202122: val_loss -0.9651\n",
      "2025-05-16 09:00:45.202157: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-16 09:00:45.202191: Epoch time: 108.95 s\n",
      "2025-05-16 09:00:45.744604: \n",
      "2025-05-16 09:00:45.744712: Epoch 657\n",
      "2025-05-16 09:00:45.744784: Current learning rate: 0.00382\n",
      "2025-05-16 09:02:34.786655: train_loss -0.963\n",
      "2025-05-16 09:02:34.786784: val_loss -0.9675\n",
      "2025-05-16 09:02:34.786818: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 09:02:34.786853: Epoch time: 109.04 s\n",
      "2025-05-16 09:02:35.329114: \n",
      "2025-05-16 09:02:35.329393: Epoch 658\n",
      "2025-05-16 09:02:35.329504: Current learning rate: 0.00381\n",
      "2025-05-16 09:04:24.351202: train_loss -0.9632\n",
      "2025-05-16 09:04:24.351317: val_loss -0.9696\n",
      "2025-05-16 09:04:24.351351: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 09:04:24.351385: Epoch time: 109.02 s\n",
      "2025-05-16 09:04:24.894521: \n",
      "2025-05-16 09:04:24.894852: Epoch 659\n",
      "2025-05-16 09:04:24.894943: Current learning rate: 0.0038\n",
      "2025-05-16 09:06:13.902883: train_loss -0.9628\n",
      "2025-05-16 09:06:13.903128: val_loss -0.9637\n",
      "2025-05-16 09:06:13.903214: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-16 09:06:13.903279: Epoch time: 109.01 s\n",
      "2025-05-16 09:06:14.439640: \n",
      "2025-05-16 09:06:14.439814: Epoch 660\n",
      "2025-05-16 09:06:14.439917: Current learning rate: 0.00379\n",
      "2025-05-16 09:08:03.495539: train_loss -0.9625\n",
      "2025-05-16 09:08:03.495723: val_loss -0.9688\n",
      "2025-05-16 09:08:03.495758: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 09:08:03.495802: Epoch time: 109.06 s\n",
      "2025-05-16 09:08:04.039888: \n",
      "2025-05-16 09:08:04.040081: Epoch 661\n",
      "2025-05-16 09:08:04.040170: Current learning rate: 0.00378\n",
      "2025-05-16 09:09:53.056563: train_loss -0.9613\n",
      "2025-05-16 09:09:53.056740: val_loss -0.9745\n",
      "2025-05-16 09:09:53.056865: Pseudo dice [np.float32(0.9891)]\n",
      "2025-05-16 09:09:53.056914: Epoch time: 109.02 s\n",
      "2025-05-16 09:09:53.788646: \n",
      "2025-05-16 09:09:53.788865: Epoch 662\n",
      "2025-05-16 09:09:53.788962: Current learning rate: 0.00377\n",
      "2025-05-16 09:11:42.868093: train_loss -0.963\n",
      "2025-05-16 09:11:42.868312: val_loss -0.9683\n",
      "2025-05-16 09:11:42.868401: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 09:11:42.868485: Epoch time: 109.08 s\n",
      "2025-05-16 09:11:43.411606: \n",
      "2025-05-16 09:11:43.411951: Epoch 663\n",
      "2025-05-16 09:11:43.412203: Current learning rate: 0.00376\n",
      "2025-05-16 09:13:32.286106: train_loss -0.9647\n",
      "2025-05-16 09:13:32.286245: val_loss -0.9694\n",
      "2025-05-16 09:13:32.286287: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 09:13:32.286326: Epoch time: 108.88 s\n",
      "2025-05-16 09:13:32.820945: \n",
      "2025-05-16 09:13:32.821052: Epoch 664\n",
      "2025-05-16 09:13:32.821123: Current learning rate: 0.00375\n",
      "2025-05-16 09:15:21.911803: train_loss -0.9627\n",
      "2025-05-16 09:15:21.911933: val_loss -0.9724\n",
      "2025-05-16 09:15:21.911967: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 09:15:21.912003: Epoch time: 109.09 s\n",
      "2025-05-16 09:15:22.450790: \n",
      "2025-05-16 09:15:22.451318: Epoch 665\n",
      "2025-05-16 09:15:22.451415: Current learning rate: 0.00374\n",
      "2025-05-16 09:17:11.517371: train_loss -0.9638\n",
      "2025-05-16 09:17:11.517563: val_loss -0.9684\n",
      "2025-05-16 09:17:11.517682: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 09:17:11.517749: Epoch time: 109.07 s\n",
      "2025-05-16 09:17:12.057288: \n",
      "2025-05-16 09:17:12.057480: Epoch 666\n",
      "2025-05-16 09:17:12.057562: Current learning rate: 0.00373\n",
      "2025-05-16 09:19:01.059580: train_loss -0.9639\n",
      "2025-05-16 09:19:01.059722: val_loss -0.9723\n",
      "2025-05-16 09:19:01.059761: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 09:19:01.059793: Epoch time: 109.0 s\n",
      "2025-05-16 09:19:01.059815: Yayy! New best EMA pseudo Dice: 0.986299991607666\n",
      "2025-05-16 09:19:01.823167: \n",
      "2025-05-16 09:19:01.823371: Epoch 667\n",
      "2025-05-16 09:19:01.823453: Current learning rate: 0.00372\n",
      "2025-05-16 09:20:50.896003: train_loss -0.9652\n",
      "2025-05-16 09:20:50.896122: val_loss -0.9725\n",
      "2025-05-16 09:20:50.896155: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 09:20:50.896189: Epoch time: 109.07 s\n",
      "2025-05-16 09:20:50.896208: Yayy! New best EMA pseudo Dice: 0.986299991607666\n",
      "2025-05-16 09:20:51.663504: \n",
      "2025-05-16 09:20:51.663706: Epoch 668\n",
      "2025-05-16 09:20:51.663784: Current learning rate: 0.00371\n",
      "2025-05-16 09:22:40.526881: train_loss -0.9632\n",
      "2025-05-16 09:22:40.527001: val_loss -0.9704\n",
      "2025-05-16 09:22:40.527033: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 09:22:40.527067: Epoch time: 108.86 s\n",
      "2025-05-16 09:22:41.071925: \n",
      "2025-05-16 09:22:41.072359: Epoch 669\n",
      "2025-05-16 09:22:41.072459: Current learning rate: 0.0037\n",
      "2025-05-16 09:24:30.170866: train_loss -0.9642\n",
      "2025-05-16 09:24:30.170984: val_loss -0.9716\n",
      "2025-05-16 09:24:30.171017: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 09:24:30.171049: Epoch time: 109.1 s\n",
      "2025-05-16 09:24:30.171069: Yayy! New best EMA pseudo Dice: 0.986299991607666\n",
      "2025-05-16 09:24:30.933377: \n",
      "2025-05-16 09:24:30.933540: Epoch 670\n",
      "2025-05-16 09:24:30.933627: Current learning rate: 0.00369\n",
      "2025-05-16 09:26:19.921704: train_loss -0.9642\n",
      "2025-05-16 09:26:19.921827: val_loss -0.966\n",
      "2025-05-16 09:26:19.921860: Pseudo dice [np.float32(0.9834)]\n",
      " Epoch time: 108.99 s21891:\n",
      "2025-05-16 09:26:20.472335: \n",
      "2025-05-16 09:26:20.472538: Epoch 671\n",
      "2025-05-16 09:26:20.472749: Current learning rate: 0.00368\n",
      "2025-05-16 09:28:09.497434: train_loss -0.9632\n",
      "2025-05-16 09:28:09.497563: val_loss -0.9668\n",
      "2025-05-16 09:28:09.497751: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-16 09:28:09.497820: Epoch time: 109.03 s\n",
      "2025-05-16 09:28:10.046749: \n",
      "2025-05-16 09:28:10.046924: Epoch 672\n",
      "2025-05-16 09:28:10.047005: Current learning rate: 0.00367\n",
      "2025-05-16 09:29:59.126649: train_loss -0.9644\n",
      "2025-05-16 09:29:59.126809: val_loss -0.9686\n",
      "2025-05-16 09:29:59.126843: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 09:29:59.126876: Epoch time: 109.08 s\n",
      "2025-05-16 09:29:59.672974: \n",
      "2025-05-16 09:29:59.673148: Epoch 673\n",
      "2025-05-16 09:29:59.673270: Current learning rate: 0.00366\n",
      "2025-05-16 09:31:48.668266: train_loss -0.9632\n",
      "2025-05-16 09:31:48.668414: val_loss -0.9684\n",
      "2025-05-16 09:31:48.668447: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 09:31:48.668480: Epoch time: 109.0 s\n",
      "2025-05-16 09:31:49.407552: \n",
      "2025-05-16 09:31:49.407737: Epoch 674\n",
      "2025-05-16 09:31:49.407822: Current learning rate: 0.00365\n",
      "2025-05-16 09:33:38.414872: train_loss -0.9635\n",
      "2025-05-16 09:33:38.414988: val_loss -0.9671\n",
      "2025-05-16 09:33:38.415020: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 09:33:38.415052: Epoch time: 109.01 s\n",
      "2025-05-16 09:33:38.962390: \n",
      "2025-05-16 09:33:38.962507: Epoch 675\n",
      "2025-05-16 09:33:38.962579: Current learning rate: 0.00364\n",
      "2025-05-16 09:35:28.005808: train_loss -0.9628\n",
      "2025-05-16 09:35:28.005941: val_loss -0.968\n",
      "2025-05-16 09:35:28.005979: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 09:35:28.006013: Epoch time: 109.04 s\n",
      "2025-05-16 09:35:28.547552: \n",
      "2025-05-16 09:35:28.547657: Epoch 676\n",
      "2025-05-16 09:35:28.547728: Current learning rate: 0.00363\n",
      "2025-05-16 09:37:17.680483: train_loss -0.9638\n",
      "2025-05-16 09:37:17.680612: val_loss -0.9716\n",
      "2025-05-16 09:37:17.680647: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 09:37:17.680681: Epoch time: 109.13 s\n",
      "2025-05-16 09:37:18.229824: \n",
      "2025-05-16 09:37:18.229925: Epoch 677\n",
      "2025-05-16 09:37:18.229990: Current learning rate: 0.00362\n",
      "2025-05-16 09:39:07.163845: train_loss -0.9644\n",
      "2025-05-16 09:39:07.163966: val_loss -0.9655\n",
      "2025-05-16 09:39:07.164000: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-16 09:39:07.164035: Epoch time: 108.93 s\n",
      "2025-05-16 09:39:07.705284: \n",
      "2025-05-16 09:39:07.705386: Epoch 678\n",
      "2025-05-16 09:39:07.705455: Current learning rate: 0.00361\n",
      "2025-05-16 09:40:56.782676: train_loss -0.9641\n",
      "2025-05-16 09:40:56.782801: val_loss -0.9681\n",
      "2025-05-16 09:40:56.782833: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 09:40:56.782867: Epoch time: 109.08 s\n",
      "2025-05-16 09:40:57.330599: \n",
      "2025-05-16 09:40:57.330838: Epoch 679\n",
      "2025-05-16 09:40:57.331005: Current learning rate: 0.0036\n",
      "2025-05-16 09:42:46.380375: train_loss -0.9635\n",
      "2025-05-16 09:42:46.380506: val_loss -0.9721\n",
      "2025-05-16 09:42:46.380538: Pseudo dice [np.float32(0.988)]\n",
      "2025-05-16 09:42:46.380572: Epoch time: 109.05 s\n",
      "2025-05-16 09:42:46.920059: \n",
      "2025-05-16 09:42:46.920207: Epoch 680\n",
      "2025-05-16 09:42:46.920285: Current learning rate: 0.00359\n",
      "2025-05-16 09:44:35.950838: train_loss -0.9653\n",
      "2025-05-16 09:44:35.950958: val_loss -0.9731\n",
      "2025-05-16 09:44:35.950993: Pseudo dice [np.float32(0.9878)]\n",
      "2025-05-16 09:44:35.951028: Epoch time: 109.03 s\n",
      "2025-05-16 09:44:36.497047: \n",
      "2025-05-16 09:44:36.497143: Epoch 681\n",
      "2025-05-16 09:44:36.497214: Current learning rate: 0.00358\n",
      "2025-05-16 09:46:25.504729: train_loss -0.9636\n",
      "2025-05-16 09:46:25.504856: val_loss -0.9715\n",
      "2025-05-16 09:46:25.504888: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-16 09:46:25.504920: Epoch time: 109.01 s\n",
      "2025-05-16 09:46:26.053360: \n",
      "2025-05-16 09:46:26.053470: Epoch 682\n",
      "2025-05-16 09:46:26.053549: Current learning rate: 0.00357\n",
      "2025-05-16 09:48:15.108228: train_loss -0.9645\n",
      "2025-05-16 09:48:15.108407: val_loss -0.9726\n",
      "2025-05-16 09:48:15.108441: Pseudo dice [np.float32(0.9882)]\n",
      "2025-05-16 09:48:15.108475: Epoch time: 109.06 s\n",
      "2025-05-16 09:48:15.108496: Yayy! New best EMA pseudo Dice: 0.9864000082015991\n",
      "2025-05-16 09:48:15.871136: \n",
      "2025-05-16 09:48:15.871361: Epoch 683\n",
      "2025-05-16 09:48:15.871440: Current learning rate: 0.00356\n",
      "2025-05-16 09:50:04.717662: train_loss -0.9659\n",
      "2025-05-16 09:50:04.717777: val_loss -0.9715\n",
      "2025-05-16 09:50:04.717810: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 09:50:04.717840: Epoch time: 108.85 s\n",
      "2025-05-16 09:50:04.717860: Yayy! New best EMA pseudo Dice: 0.9864000082015991\n",
      "2025-05-16 09:50:05.481194: \n",
      "2025-05-16 09:50:05.481302: Epoch 684\n",
      "2025-05-16 09:50:05.481378: Current learning rate: 0.00355\n",
      "2025-05-16 09:51:54.336186: train_loss -0.9647\n",
      "2025-05-16 09:51:54.336395: val_loss -0.9709\n",
      "2025-05-16 09:51:54.336443: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 09:51:54.336529: Epoch time: 108.86 s\n",
      "2025-05-16 09:51:54.336596: Yayy! New best EMA pseudo Dice: 0.9865000247955322\n",
      "2025-05-16 09:51:55.099109: \n",
      "2025-05-16 09:51:55.099228: Epoch 685\n",
      "2025-05-16 09:51:55.099460: Current learning rate: 0.00354\n",
      "2025-05-16 09:53:43.996528: train_loss -0.9659\n",
      "2025-05-16 09:53:43.996677: val_loss -0.9666\n",
      "2025-05-16 09:53:43.996792: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 09:53:43.996872: Epoch time: 108.9 s\n",
      "2025-05-16 09:53:44.732182: \n",
      "2025-05-16 09:53:44.732447: Epoch 686\n",
      "2025-05-16 09:53:44.732566: Current learning rate: 0.00353\n",
      "2025-05-16 09:55:33.645193: train_loss -0.9643\n",
      "2025-05-16 09:55:33.645314: val_loss -0.9681\n",
      "2025-05-16 09:55:33.645347: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 09:55:33.645380: Epoch time: 108.91 s\n",
      "2025-05-16 09:55:34.196355: \n",
      "2025-05-16 09:55:34.196472: Epoch 687\n",
      "2025-05-16 09:55:34.196539: Current learning rate: 0.00352\n",
      "2025-05-16 09:57:23.025479: train_loss -0.9637\n",
      "2025-05-16 09:57:23.025598: val_loss -0.9675\n",
      "2025-05-16 09:57:23.025808: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 09:57:23.025901: Epoch time: 108.83 s\n",
      "2025-05-16 09:57:23.570668: \n",
      "2025-05-16 09:57:23.570776: Epoch 688\n",
      "2025-05-16 09:57:23.570851: Current learning rate: 0.00351\n",
      "2025-05-16 09:59:12.395602: train_loss -0.9646\n",
      "2025-05-16 09:59:12.395777: val_loss -0.9661\n",
      "2025-05-16 09:59:12.395817: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-16 09:59:12.395857: Epoch time: 108.83 s\n",
      "2025-05-16 09:59:12.936724: \n",
      "2025-05-16 09:59:12.936830: Epoch 689\n",
      "2025-05-16 09:59:12.936902: Current learning rate: 0.0035\n",
      "2025-05-16 10:01:01.821148: train_loss -0.9642\n",
      "2025-05-16 10:01:01.821270: val_loss -0.9691\n",
      "2025-05-16 10:01:01.821303: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 10:01:01.821337: Epoch time: 108.89 s\n",
      "2025-05-16 10:01:02.369383: \n",
      "2025-05-16 10:01:02.370014: Epoch 690\n",
      "2025-05-16 10:01:02.370132: Current learning rate: 0.00349\n",
      "2025-05-16 10:02:51.388400: train_loss -0.9632\n",
      "2025-05-16 10:02:51.388577: val_loss -0.9707\n",
      "2025-05-16 10:02:51.388611: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 10:02:51.388644: Epoch time: 109.02 s\n",
      "2025-05-16 10:02:51.933338: \n",
      "2025-05-16 10:02:51.933517: Epoch 691\n",
      "2025-05-16 10:02:51.933623: Current learning rate: 0.00348\n",
      "2025-05-16 10:04:40.797492: train_loss -0.9646\n",
      "2025-05-16 10:04:40.797673: val_loss -0.9649\n",
      "2025-05-16 10:04:40.797706: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-16 10:04:40.797737: Epoch time: 108.86 s\n",
      "2025-05-16 10:04:41.338867: \n",
      "2025-05-16 10:04:41.339308: Epoch 692\n",
      "2025-05-16 10:04:41.339394: Current learning rate: 0.00346\n",
      "2025-05-16 10:06:30.200922: train_loss -0.9652\n",
      "2025-05-16 10:06:30.201047: val_loss -0.9688\n",
      "2025-05-16 10:06:30.201081: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 10:06:30.201115: Epoch time: 108.86 s\n",
      "2025-05-16 10:06:30.746706: \n",
      "2025-05-16 10:06:30.747260: Epoch 693\n",
      "2025-05-16 10:06:30.747373: Current learning rate: 0.00345\n",
      "2025-05-16 10:08:19.582390: train_loss -0.9652\n",
      "2025-05-16 10:08:19.582524: val_loss -0.9652\n",
      "2025-05-16 10:08:19.582565: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-16 10:08:19.582598: Epoch time: 108.84 s\n",
      "2025-05-16 10:08:20.125261: \n",
      "2025-05-16 10:08:20.125359: Epoch 694\n",
      "2025-05-16 10:08:20.125433: Current learning rate: 0.00344\n",
      "2025-05-16 10:10:09.152296: train_loss -0.9641\n",
      "2025-05-16 10:10:09.152429: val_loss -0.9699\n",
      "2025-05-16 10:10:09.152468: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 10:10:09.152501: Epoch time: 109.03 s\n",
      "2025-05-16 10:10:09.694284: \n",
      "2025-05-16 10:10:09.694451: Epoch 695\n",
      "2025-05-16 10:10:09.694615: Current learning rate: 0.00343\n",
      "2025-05-16 10:11:58.683484: train_loss -0.9647\n",
      "2025-05-16 10:11:58.683722: val_loss -0.9697\n",
      "2025-05-16 10:11:58.683829: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 10:11:58.683879: Epoch time: 108.99 s\n",
      "2025-05-16 10:11:59.224647: \n",
      "2025-05-16 10:11:59.224838: Epoch 696\n",
      "2025-05-16 10:11:59.224954: Current learning rate: 0.00342\n",
      "2025-05-16 10:13:48.052834: train_loss -0.9657\n",
      "2025-05-16 10:13:48.052958: val_loss -0.9637\n",
      "2025-05-16 10:13:48.052994: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-16 10:13:48.053029: Epoch time: 108.83 s\n",
      "2025-05-16 10:13:48.596617: \n",
      "2025-05-16 10:13:48.596821: Epoch 697\n",
      "2025-05-16 10:13:48.596893: Current learning rate: 0.00341\n",
      "2025-05-16 10:15:37.406758: train_loss -0.963\n",
      "2025-05-16 10:15:37.407052: val_loss -0.9702\n",
      "2025-05-16 10:15:37.407102: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 10:15:37.407140: Epoch time: 108.81 s\n",
      "2025-05-16 10:15:38.147379: \n",
      "2025-05-16 10:15:38.147482: Epoch 698\n",
      "2025-05-16 10:15:38.147556: Current learning rate: 0.0034\n",
      "2025-05-16 10:17:27.000830: train_loss -0.9634\n",
      "2025-05-16 10:17:27.000951: val_loss -0.969\n",
      "2025-05-16 10:17:27.001057: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 10:17:27.001124: Epoch time: 108.85 s\n",
      "2025-05-16 10:17:27.552011: \n",
      "2025-05-16 10:17:27.552246: Epoch 699\n",
      "2025-05-16 10:17:27.552441: Current learning rate: 0.00339\n",
      "2025-05-16 10:19:16.391547: train_loss -0.9639\n",
      "2025-05-16 10:19:16.391729: val_loss -0.9669\n",
      "2025-05-16 10:19:16.391767: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 10:19:16.391803: Epoch time: 108.84 s\n",
      "2025-05-16 10:19:17.150267: \n",
      "2025-05-16 10:19:17.150445: Epoch 700\n",
      "2025-05-16 10:19:17.150549: Current learning rate: 0.00338\n",
      "2025-05-16 10:21:07.262437: train_loss -0.9652\n",
      "2025-05-16 10:21:07.262667: val_loss -0.9685\n",
      "2025-05-16 10:21:07.262757: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 10:21:07.262797: Epoch time: 110.11 s\n",
      "2025-05-16 10:21:07.811623: \n",
      "2025-05-16 10:21:07.811729: Epoch 701\n",
      "2025-05-16 10:21:07.811800: Current learning rate: 0.00337\n",
      "2025-05-16 10:22:57.120739: train_loss -0.9662\n",
      "2025-05-16 10:22:57.120859: val_loss -0.9652\n",
      "2025-05-16 10:22:57.120892: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 10:22:57.120926: Epoch time: 109.31 s\n",
      "2025-05-16 10:22:57.680439: \n",
      "2025-05-16 10:22:57.680569: Epoch 702\n",
      "2025-05-16 10:22:57.680645: Current learning rate: 0.00336\n",
      "2025-05-16 10:24:46.907458: train_loss -0.9663\n",
      "2025-05-16 10:24:46.907581: val_loss -0.9639\n",
      "2025-05-16 10:24:46.907614: Pseudo dice [np.float32(0.9832)]\n",
      "2025-05-16 10:24:46.907649: Epoch time: 109.23 s\n",
      "2025-05-16 10:24:47.450289: \n",
      "2025-05-16 10:24:47.450393: Epoch 703\n",
      "2025-05-16 10:24:47.450474: Current learning rate: 0.00335\n",
      "2025-05-16 10:26:36.635202: train_loss -0.9642\n",
      "2025-05-16 10:26:36.635410: val_loss -0.9716\n",
      "2025-05-16 10:26:36.635453: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 10:26:36.635488: Epoch time: 109.19 s\n",
      "2025-05-16 10:26:37.176944: \n",
      "2025-05-16 10:26:37.177086: Epoch 704\n",
      "2025-05-16 10:26:37.177164: Current learning rate: 0.00334\n",
      "2025-05-16 10:28:26.462698: train_loss -0.9641\n",
      "2025-05-16 10:28:26.462844: val_loss -0.9714\n",
      "2025-05-16 10:28:26.462879: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 10:28:26.462914: Epoch time: 109.29 s\n",
      "2025-05-16 10:28:27.012599: \n",
      "2025-05-16 10:28:27.012875: Epoch 705\n",
      "2025-05-16 10:28:27.013005: Current learning rate: 0.00333\n",
      "2025-05-16 10:30:16.212313: train_loss -0.9645\n",
      "2025-05-16 10:30:16.212438: val_loss -0.9723\n",
      "2025-05-16 10:30:16.212580: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 10:30:16.212635: Epoch time: 109.2 s\n",
      "2025-05-16 10:30:16.757187: \n",
      "2025-05-16 10:30:16.757303: Epoch 706\n",
      "2025-05-16 10:30:16.757435: Current learning rate: 0.00332\n",
      "2025-05-16 10:32:06.123008: train_loss -0.9658\n",
      "2025-05-16 10:32:06.123191: val_loss -0.9695\n",
      "2025-05-16 10:32:06.123232: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 10:32:06.123265: Epoch time: 109.37 s\n",
      "2025-05-16 10:32:06.667388: \n",
      "2025-05-16 10:32:06.668346: Epoch 707\n",
      "2025-05-16 10:32:06.668539: Current learning rate: 0.00331\n",
      "2025-05-16 10:33:55.899139: train_loss -0.9661\n",
      "2025-05-16 10:33:55.899262: val_loss -0.9648\n",
      "2025-05-16 10:33:55.899301: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 10:33:55.899339: Epoch time: 109.23 s\n",
      "2025-05-16 10:33:56.447905: \n",
      "2025-05-16 10:33:56.448078: Epoch 708\n",
      "2025-05-16 10:33:56.448163: Current learning rate: 0.0033\n",
      "2025-05-16 10:35:46.344272: train_loss -0.966\n",
      "2025-05-16 10:35:46.344462: val_loss -0.9717\n",
      "2025-05-16 10:35:46.344495: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 10:35:46.344528: Epoch time: 109.9 s\n",
      "2025-05-16 10:35:46.878394: \n",
      "2025-05-16 10:35:46.878589: Epoch 709\n",
      "2025-05-16 10:35:46.878663: Current learning rate: 0.00329\n",
      "2025-05-16 10:37:36.294090: train_loss -0.9652\n",
      "2025-05-16 10:37:36.294250: val_loss -0.9694\n",
      "2025-05-16 10:37:36.294283: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 10:37:36.294319: Epoch time: 109.42 s\n",
      "2025-05-16 10:37:37.038530: \n",
      "2025-05-16 10:37:37.038886: Epoch 710\n",
      "2025-05-16 10:37:37.039012: Current learning rate: 0.00328\n",
      "2025-05-16 10:39:26.283103: train_loss -0.9643\n",
      "2025-05-16 10:39:26.283252: val_loss -0.9731\n",
      "2025-05-16 10:39:26.283287: Pseudo dice [np.float32(0.9877)]\n",
      "2025-05-16 10:39:26.283322: Epoch time: 109.25 s\n",
      "2025-05-16 10:39:26.823905: \n",
      "2025-05-16 10:39:26.824179: Epoch 711\n",
      "2025-05-16 10:39:26.824393: Current learning rate: 0.00327\n",
      "2025-05-16 10:41:16.216784: train_loss -0.9649\n",
      "2025-05-16 10:41:16.216914: val_loss -0.9714\n",
      "2025-05-16 10:41:16.216949: Pseudo dice [np.float32(0.988)]\n",
      "2025-05-16 10:41:16.216986: Epoch time: 109.39 s\n",
      "2025-05-16 10:41:16.782561: \n",
      "2025-05-16 10:41:16.782705: Epoch 712\n",
      "2025-05-16 10:41:16.782788: Current learning rate: 0.00326\n",
      "2025-05-16 10:43:06.330326: train_loss -0.9645\n",
      "2025-05-16 10:43:06.330446: val_loss -0.9692\n",
      "2025-05-16 10:43:06.330479: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 10:43:06.330524: Epoch time: 109.55 s\n",
      "2025-05-16 10:43:06.878037: \n",
      "2025-05-16 10:43:06.878352: Epoch 713\n",
      "2025-05-16 10:43:06.878515: Current learning rate: 0.00325\n",
      "2025-05-16 10:44:56.153423: train_loss -0.9645\n",
      "2025-05-16 10:44:56.153570: val_loss -0.9669\n",
      "2025-05-16 10:44:56.153610: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 10:44:56.153652: Epoch time: 109.28 s\n",
      "2025-05-16 10:44:56.696750: \n",
      "2025-05-16 10:44:56.696928: Epoch 714\n",
      "2025-05-16 10:44:56.696998: Current learning rate: 0.00324\n",
      "2025-05-16 10:46:45.873597: train_loss -0.9643\n",
      "2025-05-16 10:46:45.873735: val_loss -0.9687\n",
      "2025-05-16 10:46:45.873768: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 10:46:45.873804: Epoch time: 109.18 s\n",
      "2025-05-16 10:46:46.414225: \n",
      "2025-05-16 10:46:46.414326: Epoch 715\n",
      "2025-05-16 10:46:46.414392: Current learning rate: 0.00323\n",
      "2025-05-16 10:48:35.753923: train_loss -0.9651\n",
      "2025-05-16 10:48:35.754119: val_loss -0.9674\n",
      "2025-05-16 10:48:35.754157: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 10:48:35.754194: Epoch time: 109.34 s\n",
      "2025-05-16 10:48:36.294510: \n",
      "2025-05-16 10:48:36.294699: Epoch 716\n",
      "2025-05-16 10:48:36.294794: Current learning rate: 0.00322\n",
      "2025-05-16 10:50:25.557499: train_loss -0.9675\n",
      "2025-05-16 10:50:25.557697: val_loss -0.9672\n",
      "2025-05-16 10:50:25.557731: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 10:50:25.557764: Epoch time: 109.26 s\n",
      "2025-05-16 10:50:26.093374: \n",
      "2025-05-16 10:50:26.093462: Epoch 717\n",
      "2025-05-16 10:50:26.093532: Current learning rate: 0.00321\n",
      "2025-05-16 10:52:15.231037: train_loss -0.9665\n",
      "2025-05-16 10:52:15.231326: val_loss -0.9699\n",
      "2025-05-16 10:52:15.231369: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 10:52:15.231404: Epoch time: 109.14 s\n",
      "2025-05-16 10:52:15.768161: \n",
      "2025-05-16 10:52:15.768525: Epoch 718\n",
      "2025-05-16 10:52:15.768684: Current learning rate: 0.0032\n",
      "2025-05-16 10:54:04.577224: train_loss -0.9659\n",
      "2025-05-16 10:54:04.577355: val_loss -0.9649\n",
      "2025-05-16 10:54:04.577388: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-16 10:54:04.577422: Epoch time: 108.81 s\n",
      "2025-05-16 10:54:05.119232: \n",
      "2025-05-16 10:54:05.119706: Epoch 719\n",
      "2025-05-16 10:54:05.119817: Current learning rate: 0.00319\n",
      "2025-05-16 10:55:53.933128: train_loss -0.9648\n",
      "2025-05-16 10:55:53.933301: val_loss -0.9697\n",
      "2025-05-16 10:55:53.933343: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 10:55:53.933380: Epoch time: 108.81 s\n",
      "2025-05-16 10:55:54.481896: \n",
      "2025-05-16 10:55:54.482046: Epoch 720\n",
      "2025-05-16 10:55:54.482148: Current learning rate: 0.00318\n",
      "2025-05-16 10:57:43.327118: train_loss -0.9668\n",
      "2025-05-16 10:57:43.327295: val_loss -0.9699\n",
      "2025-05-16 10:57:43.327331: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 10:57:43.327365: Epoch time: 108.85 s\n",
      "2025-05-16 10:57:43.864931: \n",
      "2025-05-16 10:57:43.865123: Epoch 721\n",
      "2025-05-16 10:57:43.865191: Current learning rate: 0.00317\n",
      "2025-05-16 10:59:32.688566: train_loss -0.9658\n",
      "2025-05-16 10:59:32.688938: val_loss -0.9671\n",
      "2025-05-16 10:59:32.689045: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 10:59:32.689090: Epoch time: 108.82 s\n",
      "2025-05-16 10:59:33.434298: \n",
      "2025-05-16 10:59:33.434526: Epoch 722\n",
      "2025-05-16 10:59:33.434664: Current learning rate: 0.00316\n",
      "2025-05-16 11:01:22.275698: train_loss -0.9655\n",
      "2025-05-16 11:01:22.275817: val_loss -0.9692\n",
      "2025-05-16 11:01:22.275850: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 11:01:22.275884: Epoch time: 108.84 s\n",
      "2025-05-16 11:01:22.813905: \n",
      "2025-05-16 11:01:22.814009: Epoch 723\n",
      "2025-05-16 11:01:22.814079: Current learning rate: 0.00315\n",
      "2025-05-16 11:03:11.692867: train_loss -0.9662\n",
      "2025-05-16 11:03:11.692998: val_loss -0.9693\n",
      "2025-05-16 11:03:11.693033: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 11:03:11.693066: Epoch time: 108.88 s\n",
      "2025-05-16 11:03:12.231776: \n",
      "2025-05-16 11:03:12.231886: Epoch 724\n",
      "2025-05-16 11:03:12.231954: Current learning rate: 0.00314\n",
      "2025-05-16 11:05:01.120090: train_loss -0.9655\n",
      "2025-05-16 11:05:01.120263: val_loss -0.9656\n",
      "2025-05-16 11:05:01.120297: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 11:05:01.120332: Epoch time: 108.89 s\n",
      "2025-05-16 11:05:01.663016: \n",
      "2025-05-16 11:05:01.663124: Epoch 725\n",
      "2025-05-16 11:05:01.663189: Current learning rate: 0.00313\n",
      "2025-05-16 11:06:50.549348: train_loss -0.9676\n",
      "2025-05-16 11:06:50.549460: val_loss -0.9673\n",
      "2025-05-16 11:06:50.549490: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 11:06:50.549523: Epoch time: 108.89 s\n",
      "2025-05-16 11:06:51.090303: \n",
      "2025-05-16 11:06:51.090413: Epoch 726\n",
      "2025-05-16 11:06:51.090480: Current learning rate: 0.00312\n",
      "2025-05-16 11:08:40.017419: train_loss -0.9664\n",
      "2025-05-16 11:08:40.017536: val_loss -0.9683\n",
      "2025-05-16 11:08:40.017570: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 11:08:40.017603: Epoch time: 108.93 s\n",
      "2025-05-16 11:08:40.554740: \n",
      "2025-05-16 11:08:40.555001: Epoch 727\n",
      "2025-05-16 11:08:40.555158: Current learning rate: 0.00311\n",
      "2025-05-16 11:10:29.348187: train_loss -0.9654\n",
      "2025-05-16 11:10:29.348324: val_loss -0.9668\n",
      "2025-05-16 11:10:29.348359: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-16 11:10:29.348429: Epoch time: 108.79 s\n",
      "2025-05-16 11:10:29.881810: \n",
      "2025-05-16 11:10:29.882181: Epoch 728\n",
      "2025-05-16 11:10:29.882272: Current learning rate: 0.0031\n",
      "2025-05-16 11:12:18.650047: train_loss -0.9654\n",
      "2025-05-16 11:12:18.650161: val_loss -0.9705\n",
      "2025-05-16 11:12:18.650194: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 11:12:18.650227: Epoch time: 108.77 s\n",
      "2025-05-16 11:12:19.191688: \n",
      "2025-05-16 11:12:19.191863: Epoch 729\n",
      "2025-05-16 11:12:19.191945: Current learning rate: 0.00309\n",
      "2025-05-16 11:14:08.122925: train_loss -0.9654\n",
      "2025-05-16 11:14:08.123090: val_loss -0.9625\n",
      "2025-05-16 11:14:08.123123: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-16 11:14:08.123155: Epoch time: 108.93 s\n",
      "2025-05-16 11:14:08.660219: \n",
      "2025-05-16 11:14:08.660334: Epoch 730\n",
      "2025-05-16 11:14:08.660418: Current learning rate: 0.00308\n",
      "2025-05-16 11:15:57.640895: train_loss -0.9656\n",
      "2025-05-16 11:15:57.641005: val_loss -0.9678\n",
      "2025-05-16 11:15:57.641038: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-16 11:15:57.641183: Epoch time: 108.98 s\n",
      "2025-05-16 11:15:58.183158: \n",
      "2025-05-16 11:15:58.183333: Epoch 731\n",
      "2025-05-16 11:15:58.183497: Current learning rate: 0.00307\n",
      "2025-05-16 11:17:46.980754: train_loss -0.9669\n",
      "2025-05-16 11:17:46.980872: val_loss -0.9718\n",
      "2025-05-16 11:17:46.980903: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 11:17:46.980937: Epoch time: 108.8 s\n",
      "2025-05-16 11:17:47.518656: \n",
      "2025-05-16 11:17:47.519041: Epoch 732\n",
      "2025-05-16 11:17:47.519149: Current learning rate: 0.00306\n",
      "2025-05-16 11:19:36.487971: train_loss -0.966\n",
      "2025-05-16 11:19:36.488173: val_loss -0.97\n",
      "2025-05-16 11:19:36.488210: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 11:19:36.488244: Epoch time: 108.97 s\n",
      "2025-05-16 11:19:37.030734: \n",
      "2025-05-16 11:19:37.030831: Epoch 733\n",
      "2025-05-16 11:19:37.030902: Current learning rate: 0.00305\n",
      "2025-05-16 11:21:25.984002: train_loss -0.9657\n",
      "2025-05-16 11:21:25.984128: val_loss -0.9708\n",
      "2025-05-16 11:21:25.984162: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 11:21:25.984194: Epoch time: 108.95 s\n",
      "2025-05-16 11:21:26.723954: \n",
      "2025-05-16 11:21:26.724091: Epoch 734\n",
      "2025-05-16 11:21:26.724165: Current learning rate: 0.00304\n",
      "2025-05-16 11:23:15.557552: train_loss -0.9663\n",
      "2025-05-16 11:23:15.557735: val_loss -0.9693\n",
      "2025-05-16 11:23:15.557769: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 11:23:15.557801: Epoch time: 108.83 s\n",
      "2025-05-16 11:23:16.103785: \n",
      "2025-05-16 11:23:16.104096: Epoch 735\n",
      "2025-05-16 11:23:16.104229: Current learning rate: 0.00303\n",
      "2025-05-16 11:25:04.992980: train_loss -0.966\n",
      "2025-05-16 11:25:04.993109: val_loss -0.9695\n",
      "2025-05-16 11:25:04.993140: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 11:25:04.993174: Epoch time: 108.89 s\n",
      "2025-05-16 11:25:05.536705: \n",
      "2025-05-16 11:25:05.536855: Epoch 736\n",
      "2025-05-16 11:25:05.536934: Current learning rate: 0.00302\n",
      "2025-05-16 11:26:54.445197: train_loss -0.9637\n",
      "2025-05-16 11:26:54.445313: val_loss -0.9688\n",
      "2025-05-16 11:26:54.445347: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 11:26:54.445380: Epoch time: 108.91 s\n",
      "2025-05-16 11:26:54.990579: \n",
      "2025-05-16 11:26:54.990802: Epoch 737\n",
      "2025-05-16 11:26:54.990885: Current learning rate: 0.00301\n",
      "2025-05-16 11:28:43.768397: train_loss -0.9643\n",
      "2025-05-16 11:28:43.768507: val_loss -0.9719\n",
      "2025-05-16 11:28:43.768540: Pseudo dice [np.float32(0.9879)]\n",
      "2025-05-16 11:28:43.768573: Epoch time: 108.78 s\n",
      "2025-05-16 11:28:44.314409: \n",
      "2025-05-16 11:28:44.314713: Epoch 738\n",
      "2025-05-16 11:28:44.314945: Current learning rate: 0.003\n",
      "2025-05-16 11:30:33.233386: train_loss -0.9658\n",
      "2025-05-16 11:30:33.233517: val_loss -0.9693\n",
      "2025-05-16 11:30:33.233555: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 11:30:33.233588: Epoch time: 108.92 s\n",
      "2025-05-16 11:30:33.772838: \n",
      "2025-05-16 11:30:33.772996: Epoch 739\n",
      "2025-05-16 11:30:33.773377: Current learning rate: 0.00299\n",
      "2025-05-16 11:32:22.619381: train_loss -0.964\n",
      "2025-05-16 11:32:22.619517: val_loss -0.9722\n",
      "2025-05-16 11:32:22.619552: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 11:32:22.619586: Epoch time: 108.85 s\n",
      "2025-05-16 11:32:23.168265: \n",
      "2025-05-16 11:32:23.168693: Epoch 740\n",
      "2025-05-16 11:32:23.168814: Current learning rate: 0.00297\n",
      "2025-05-16 11:34:11.953081: train_loss -0.9649\n",
      "2025-05-16 11:34:11.953241: val_loss -0.9641\n",
      "2025-05-16 11:34:11.953273: Pseudo dice [np.float32(0.9838)]\n",
      "2025-05-16 11:34:11.953305: Epoch time: 108.79 s\n",
      "2025-05-16 11:34:12.495330: \n",
      "2025-05-16 11:34:12.495425: Epoch 741\n",
      "2025-05-16 11:34:12.495501: Current learning rate: 0.00296\n",
      "2025-05-16 11:36:01.463668: train_loss -0.966\n",
      "2025-05-16 11:36:01.463782: val_loss -0.9708\n",
      "2025-05-16 11:36:01.463815: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 11:36:01.463849: Epoch time: 108.97 s\n",
      "2025-05-16 11:36:02.004049: \n",
      "2025-05-16 11:36:02.004150: Epoch 742\n",
      "2025-05-16 11:36:02.004222: Current learning rate: 0.00295\n",
      "2025-05-16 11:37:50.880445: train_loss -0.9656\n",
      "2025-05-16 11:37:50.880563: val_loss -0.9673\n",
      "2025-05-16 11:37:50.880595: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-16 11:37:50.880627: Epoch time: 108.88 s\n",
      "2025-05-16 11:37:51.420781: \n",
      "2025-05-16 11:37:51.420926: Epoch 743\n",
      "2025-05-16 11:37:51.421002: Current learning rate: 0.00294\n",
      "2025-05-16 11:39:40.223378: train_loss -0.9667\n",
      "2025-05-16 11:39:40.223517: val_loss -0.9697\n",
      "2025-05-16 11:39:40.223550: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 11:39:40.223581: Epoch time: 108.8 s\n",
      "2025-05-16 11:39:40.767058: \n",
      "2025-05-16 11:39:40.767227: Epoch 744\n",
      "2025-05-16 11:39:40.767332: Current learning rate: 0.00293\n",
      "2025-05-16 11:41:29.674699: train_loss -0.9668\n",
      "2025-05-16 11:41:29.674877: val_loss -0.9673\n",
      "2025-05-16 11:41:29.674909: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 11:41:29.674950: Epoch time: 108.91 s\n",
      "2025-05-16 11:41:30.222054: \n",
      "2025-05-16 11:41:30.222342: Epoch 745\n",
      "2025-05-16 11:41:30.222826: Current learning rate: 0.00292\n",
      "2025-05-16 11:43:19.077509: train_loss -0.9665\n",
      "2025-05-16 11:43:19.077630: val_loss -0.9688\n",
      "2025-05-16 11:43:19.077663: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 11:43:19.077696: Epoch time: 108.86 s\n",
      "2025-05-16 11:43:19.799449: \n",
      "2025-05-16 11:43:19.799645: Epoch 746\n",
      "2025-05-16 11:43:19.799722: Current learning rate: 0.00291\n",
      "2025-05-16 11:45:08.771957: train_loss -0.9669\n",
      "2025-05-16 11:45:08.772079: val_loss -0.9704\n",
      "2025-05-16 11:45:08.772113: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 11:45:08.772146: Epoch time: 108.97 s\n",
      "2025-05-16 11:45:09.318545: \n",
      "2025-05-16 11:45:09.318756: Epoch 747\n",
      "2025-05-16 11:45:09.318882: Current learning rate: 0.0029\n",
      "2025-05-16 11:46:58.113933: train_loss -0.9659\n",
      "2025-05-16 11:46:58.114041: val_loss -0.9699\n",
      "2025-05-16 11:46:58.114072: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 11:46:58.114105: Epoch time: 108.8 s\n",
      "2025-05-16 11:46:58.658848: \n",
      "2025-05-16 11:46:58.659213: Epoch 748\n",
      "2025-05-16 11:46:58.659357: Current learning rate: 0.00289\n",
      "2025-05-16 11:48:47.515515: train_loss -0.9673\n",
      "2025-05-16 11:48:47.515635: val_loss -0.9725\n",
      "2025-05-16 11:48:47.515668: Pseudo dice [np.float32(0.9877)]\n",
      "2025-05-16 11:48:47.515702: Epoch time: 108.86 s\n",
      "2025-05-16 11:48:48.059476: \n",
      "2025-05-16 11:48:48.059592: Epoch 749\n",
      "2025-05-16 11:48:48.059664: Current learning rate: 0.00288\n",
      "2025-05-16 11:50:36.975122: train_loss -0.9663\n",
      "2025-05-16 11:50:36.975299: val_loss -0.9726\n",
      "2025-05-16 11:50:36.975333: Pseudo dice [np.float32(0.9879)]\n",
      "2025-05-16 11:50:36.975365: Epoch time: 108.92 s\n",
      "2025-05-16 11:50:37.736938: \n",
      "2025-05-16 11:50:37.737180: Epoch 750\n",
      "2025-05-16 11:50:37.737327: Current learning rate: 0.00287\n",
      "2025-05-16 11:52:26.527950: train_loss -0.966\n",
      "2025-05-16 11:52:26.528066: val_loss -0.9671\n",
      "2025-05-16 11:52:26.528099: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-16 11:52:26.528182: Epoch time: 108.79 s\n",
      "2025-05-16 11:52:27.071682: \n",
      "2025-05-16 11:52:27.071899: Epoch 751\n",
      "2025-05-16 11:52:27.072036: Current learning rate: 0.00286\n",
      "2025-05-16 11:54:15.982285: train_loss -0.9648\n",
      "2025-05-16 11:54:15.982421: val_loss -0.9678\n",
      "2025-05-16 11:54:15.982455: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 11:54:15.982489: Epoch time: 108.91 s\n",
      "2025-05-16 11:54:16.522842: \n",
      "2025-05-16 11:54:16.523029: Epoch 752\n",
      "2025-05-16 11:54:16.523131: Current learning rate: 0.00285\n",
      "2025-05-16 11:56:05.496124: train_loss -0.9651\n",
      "2025-05-16 11:56:05.496243: val_loss -0.9678\n",
      "2025-05-16 11:56:05.496366: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 11:56:05.496452: Epoch time: 108.97 s\n",
      "2025-05-16 11:56:06.033100: \n",
      "2025-05-16 11:56:06.033207: Epoch 753\n",
      "2025-05-16 11:56:06.033278: Current learning rate: 0.00284\n",
      "2025-05-16 11:57:55.011549: train_loss -0.9672\n",
      "2025-05-16 11:57:55.011730: val_loss -0.9667\n",
      "2025-05-16 11:57:55.011761: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 11:57:55.011795: Epoch time: 108.98 s\n",
      "2025-05-16 11:57:55.552614: \n",
      "2025-05-16 11:57:55.552706: Epoch 754\n",
      "2025-05-16 11:57:55.552777: Current learning rate: 0.00283\n",
      "2025-05-16 11:59:44.513900: train_loss -0.9663\n",
      "2025-05-16 11:59:44.514015: val_loss -0.9674\n",
      "2025-05-16 11:59:44.514047: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 11:59:44.514114: Epoch time: 108.96 s\n",
      "2025-05-16 11:59:45.055710: \n",
      "2025-05-16 11:59:45.056121: Epoch 755\n",
      "2025-05-16 11:59:45.056235: Current learning rate: 0.00282\n",
      "2025-05-16 12:01:34.051186: train_loss -0.9659\n",
      "2025-05-16 12:01:34.051357: val_loss -0.9704\n",
      "2025-05-16 12:01:34.051389: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 12:01:34.051423: Epoch time: 109.0 s\n",
      "2025-05-16 12:01:34.597715: \n",
      "2025-05-16 12:01:34.597835: Epoch 756\n",
      "2025-05-16 12:01:34.597965: Current learning rate: 0.00281\n",
      "2025-05-16 12:03:23.390260: train_loss -0.9659\n",
      "2025-05-16 12:03:23.390426: val_loss -0.9695\n",
      "2025-05-16 12:03:23.390559: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 12:03:23.390611: Epoch time: 108.79 s\n",
      "2025-05-16 12:03:23.938879: \n",
      "2025-05-16 12:03:23.938966: Epoch 757\n",
      "2025-05-16 12:03:23.939032: Current learning rate: 0.0028\n",
      "2025-05-16 12:05:12.788791: train_loss -0.9673\n",
      "2025-05-16 12:05:12.788909: val_loss -0.9681\n",
      "2025-05-16 12:05:12.789013: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 12:05:12.789136: Epoch time: 108.85 s\n",
      "2025-05-16 12:05:13.499809: \n",
      "2025-05-16 12:05:13.499994: Epoch 758\n",
      "2025-05-16 12:05:13.500091: Current learning rate: 0.00279\n",
      "2025-05-16 12:07:02.347374: train_loss -0.9656\n",
      "2025-05-16 12:07:02.347581: val_loss -0.9724\n",
      "2025-05-16 12:07:02.347632: Pseudo dice [np.float32(0.9879)]\n",
      "2025-05-16 12:07:02.347672: Epoch time: 108.85 s\n",
      "2025-05-16 12:07:02.891645: \n",
      "2025-05-16 12:07:02.892096: Epoch 759\n",
      "2025-05-16 12:07:02.892233: Current learning rate: 0.00278\n",
      "2025-05-16 12:08:51.760014: train_loss -0.9664\n",
      "2025-05-16 12:08:51.760135: val_loss -0.9723\n",
      "2025-05-16 12:08:51.760167: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 12:08:51.760226: Epoch time: 108.87 s\n",
      "2025-05-16 12:08:52.305165: \n",
      "2025-05-16 12:08:52.305300: Epoch 760\n",
      "2025-05-16 12:08:52.305396: Current learning rate: 0.00277\n",
      "2025-05-16 12:10:41.176186: train_loss -0.9674\n",
      "2025-05-16 12:10:41.176296: val_loss -0.9647\n",
      "2025-05-16 12:10:41.176331: Pseudo dice [np.float32(0.9836)]\n",
      "2025-05-16 12:10:41.176377: Epoch time: 108.87 s\n",
      "2025-05-16 12:10:41.723575: \n",
      "2025-05-16 12:10:41.723680: Epoch 761\n",
      "2025-05-16 12:10:41.723751: Current learning rate: 0.00276\n",
      "2025-05-16 12:12:30.702058: train_loss -0.9652\n",
      "2025-05-16 12:12:30.702282: val_loss -0.9673\n",
      "2025-05-16 12:12:30.702319: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-16 12:12:30.702350: Epoch time: 108.98 s\n",
      "2025-05-16 12:12:31.246488: \n",
      "2025-05-16 12:12:31.246662: Epoch 762\n",
      "2025-05-16 12:12:31.246745: Current learning rate: 0.00275\n",
      "2025-05-16 12:14:20.242023: train_loss -0.9668\n",
      "2025-05-16 12:14:20.242149: val_loss -0.9718\n",
      "2025-05-16 12:14:20.242194: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 12:14:20.242252: Epoch time: 109.0 s\n",
      "2025-05-16 12:14:20.797765: \n",
      "2025-05-16 12:14:20.797969: Epoch 763\n",
      "2025-05-16 12:14:20.798088: Current learning rate: 0.00274\n",
      "2025-05-16 12:16:09.809463: train_loss -0.9658\n",
      "2025-05-16 12:16:09.809583: val_loss -0.9711\n",
      "2025-05-16 12:16:09.809618: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 12:16:09.809700: Epoch time: 109.01 s\n",
      "2025-05-16 12:16:10.356769: \n",
      "2025-05-16 12:16:10.356877: Epoch 764\n",
      "2025-05-16 12:16:10.356961: Current learning rate: 0.00273\n",
      "2025-05-16 12:17:59.221634: train_loss -0.9674\n",
      "2025-05-16 12:17:59.221828: val_loss -0.9687\n",
      "2025-05-16 12:17:59.221958: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 12:17:59.222010: Epoch time: 108.87 s\n",
      "2025-05-16 12:17:59.764941: \n",
      "2025-05-16 12:17:59.765044: Epoch 765\n",
      "2025-05-16 12:17:59.765117: Current learning rate: 0.00272\n",
      "2025-05-16 12:19:48.618024: train_loss -0.9659\n",
      "2025-05-16 12:19:48.618168: val_loss -0.9708\n",
      "2025-05-16 12:19:48.618205: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 12:19:48.618241: Epoch time: 108.85 s\n",
      "2025-05-16 12:19:49.163636: \n",
      "2025-05-16 12:19:49.163753: Epoch 766\n",
      "2025-05-16 12:19:49.163825: Current learning rate: 0.00271\n",
      "2025-05-16 12:21:38.122477: train_loss -0.968\n",
      "2025-05-16 12:21:38.122607: val_loss -0.9723\n",
      "2025-05-16 12:21:38.122640: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 12:21:38.122674: Epoch time: 108.96 s\n",
      "2025-05-16 12:21:38.670613: \n",
      "2025-05-16 12:21:38.670707: Epoch 767\n",
      "2025-05-16 12:21:38.670775: Current learning rate: 0.0027\n",
      "2025-05-16 12:23:27.503712: train_loss -0.9671\n",
      "2025-05-16 12:23:27.503829: val_loss -0.967\n",
      "2025-05-16 12:23:27.504031: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-16 12:23:27.504096: Epoch time: 108.83 s\n",
      "2025-05-16 12:23:28.043355: \n",
      "2025-05-16 12:23:28.043505: Epoch 768\n",
      "2025-05-16 12:23:28.043667: Current learning rate: 0.00268\n",
      "2025-05-16 12:25:16.967882: train_loss -0.9674\n",
      "2025-05-16 12:25:16.968069: val_loss -0.9693\n",
      "2025-05-16 12:25:16.968103: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 12:25:16.968136: Epoch time: 108.93 s\n",
      "2025-05-16 12:25:17.521683: \n",
      "2025-05-16 12:25:17.521906: Epoch 769\n",
      "2025-05-16 12:25:17.522002: Current learning rate: 0.00267\n",
      "2025-05-16 12:27:06.287065: train_loss -0.9673\n",
      "2025-05-16 12:27:06.287185: val_loss -0.9711\n",
      "2025-05-16 12:27:06.287218: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 12:27:06.287251: Epoch time: 108.77 s\n",
      "2025-05-16 12:27:07.021858: \n",
      "2025-05-16 12:27:07.022040: Epoch 770\n",
      "2025-05-16 12:27:07.022114: Current learning rate: 0.00266\n",
      "2025-05-16 12:28:55.885748: train_loss -0.9649\n",
      "2025-05-16 12:28:55.885923: val_loss -0.9614\n",
      "2025-05-16 12:28:55.885957: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-16 12:28:55.885992: Epoch time: 108.86 s\n",
      "2025-05-16 12:28:56.427167: \n",
      "2025-05-16 12:28:56.427519: Epoch 771\n",
      "2025-05-16 12:28:56.427813: Current learning rate: 0.00265\n",
      "2025-05-16 12:30:45.253536: train_loss -0.9618\n",
      "2025-05-16 12:30:45.253656: val_loss -0.9663\n",
      "2025-05-16 12:30:45.253687: Pseudo dice [np.float32(0.984)]\n",
      "2025-05-16 12:30:45.253721: Epoch time: 108.83 s\n",
      "2025-05-16 12:30:45.796974: \n",
      "2025-05-16 12:30:45.797091: Epoch 772\n",
      "2025-05-16 12:30:45.797160: Current learning rate: 0.00264\n",
      "2025-05-16 12:32:34.556601: train_loss -0.9598\n",
      "2025-05-16 12:32:34.556774: val_loss -0.9678\n",
      "2025-05-16 12:32:34.556807: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 12:32:34.556839: Epoch time: 108.76 s\n",
      "2025-05-16 12:32:35.105044: \n",
      "2025-05-16 12:32:35.105273: Epoch 773\n",
      "2025-05-16 12:32:35.105415: Current learning rate: 0.00263\n",
      "2025-05-16 12:34:23.865701: train_loss -0.9515\n",
      "2025-05-16 12:34:23.865818: val_loss -0.8813\n",
      "2025-05-16 12:34:23.865850: Pseudo dice [np.float32(0.9545)]\n",
      "2025-05-16 12:34:23.865889: Epoch time: 108.76 s\n",
      "2025-05-16 12:34:24.423416: \n",
      "2025-05-16 12:34:24.423550: Epoch 774\n",
      "2025-05-16 12:34:24.423631: Current learning rate: 0.00262\n",
      "2025-05-16 12:36:13.224850: train_loss -0.9378\n",
      "2025-05-16 12:36:13.224986: val_loss -0.9624\n",
      "2025-05-16 12:36:13.225041: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-16 12:36:13.225092: Epoch time: 108.8 s\n",
      "2025-05-16 12:36:13.774574: \n",
      "2025-05-16 12:36:13.774680: Epoch 775\n",
      "2025-05-16 12:36:13.774803: Current learning rate: 0.00261\n",
      "2025-05-16 12:38:02.784018: train_loss -0.9405\n",
      "2025-05-16 12:38:02.784138: val_loss -0.9582\n",
      "2025-05-16 12:38:02.784172: Pseudo dice [np.float32(0.9816)]\n",
      "2025-05-16 12:38:02.784205: Epoch time: 109.01 s\n",
      "2025-05-16 12:38:03.329846: \n",
      "2025-05-16 12:38:03.330008: Epoch 776\n",
      "2025-05-16 12:38:03.330079: Current learning rate: 0.0026\n",
      "2025-05-16 12:39:52.288845: train_loss -0.9299\n",
      "2025-05-16 12:39:52.289002: val_loss -0.9509\n",
      "2025-05-16 12:39:52.289036: Pseudo dice [np.float32(0.9811)]\n",
      "2025-05-16 12:39:52.289068: Epoch time: 108.96 s\n",
      "2025-05-16 12:39:52.840846: \n",
      "2025-05-16 12:39:52.841225: Epoch 777\n",
      "2025-05-16 12:39:52.841310: Current learning rate: 0.00259\n",
      "2025-05-16 12:41:41.654179: train_loss -0.921\n",
      "2025-05-16 12:41:41.654297: val_loss -0.9537\n",
      "2025-05-16 12:41:41.654328: Pseudo dice [np.float32(0.9821)]\n",
      "2025-05-16 12:41:41.654359: Epoch time: 108.81 s\n",
      "2025-05-16 12:41:42.204579: \n",
      "2025-05-16 12:41:42.204784: Epoch 778\n",
      "2025-05-16 12:41:42.204865: Current learning rate: 0.00258\n",
      "2025-05-16 12:43:31.227718: train_loss -0.9373\n",
      "2025-05-16 12:43:31.227836: val_loss -0.9627\n",
      "2025-05-16 12:43:31.227869: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 12:43:31.227901: Epoch time: 109.02 s\n",
      "2025-05-16 12:43:31.775114: \n",
      "2025-05-16 12:43:31.775358: Epoch 779\n",
      "2025-05-16 12:43:31.775706: Current learning rate: 0.00257\n",
      "2025-05-16 12:45:20.526973: train_loss -0.9499\n",
      "2025-05-16 12:45:20.527179: val_loss -0.9649\n",
      "2025-05-16 12:45:20.527224: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-16 12:45:20.527256: Epoch time: 108.75 s\n",
      "2025-05-16 12:45:21.073743: \n",
      "2025-05-16 12:45:21.073932: Epoch 780\n",
      "2025-05-16 12:45:21.074009: Current learning rate: 0.00256\n",
      "2025-05-16 12:47:09.846900: train_loss -0.9461\n",
      "2025-05-16 12:47:09.847026: val_loss -0.9627\n",
      "2025-05-16 12:47:09.847057: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-16 12:47:09.847090: Epoch time: 108.77 s\n",
      "2025-05-16 12:47:10.392280: \n",
      "2025-05-16 12:47:10.392745: Epoch 781\n",
      "2025-05-16 12:47:10.392859: Current learning rate: 0.00255\n",
      "2025-05-16 12:48:59.175308: train_loss -0.9464\n",
      "2025-05-16 12:48:59.176063: val_loss -0.9617\n",
      "2025-05-16 12:48:59.176129: Pseudo dice [np.float32(0.9835)]\n",
      "2025-05-16 12:48:59.176168: Epoch time: 108.78 s\n",
      "2025-05-16 12:48:59.914753: \n",
      "2025-05-16 12:48:59.914868: Epoch 782\n",
      "2025-05-16 12:48:59.914939: Current learning rate: 0.00254\n",
      "2025-05-16 12:50:48.726730: train_loss -0.9536\n",
      "2025-05-16 12:50:48.726884: val_loss -0.9656\n",
      "2025-05-16 12:50:48.726919: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 12:50:48.726952: Epoch time: 108.81 s\n",
      "2025-05-16 12:50:49.274875: \n",
      "2025-05-16 12:50:49.274997: Epoch 783\n",
      "2025-05-16 12:50:49.275078: Current learning rate: 0.00253\n",
      "2025-05-16 12:52:38.226338: train_loss -0.9553\n",
      "2025-05-16 12:52:38.226544: val_loss -0.9649\n",
      "2025-05-16 12:52:38.226589: Pseudo dice [np.float32(0.9834)]\n",
      "2025-05-16 12:52:38.226624: Epoch time: 108.95 s\n",
      "2025-05-16 12:52:38.779225: \n",
      "2025-05-16 12:52:38.779452: Epoch 784\n",
      "2025-05-16 12:52:38.779592: Current learning rate: 0.00252\n",
      "2025-05-16 12:54:27.592269: train_loss -0.9597\n",
      "2025-05-16 12:54:27.592409: val_loss -0.9699\n",
      "2025-05-16 12:54:27.592442: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 12:54:27.592475: Epoch time: 108.81 s\n",
      "2025-05-16 12:54:28.136513: \n",
      "2025-05-16 12:54:28.136632: Epoch 785\n",
      "2025-05-16 12:54:28.136739: Current learning rate: 0.00251\n",
      "2025-05-16 12:56:16.974188: train_loss -0.9625\n",
      "2025-05-16 12:56:16.974333: val_loss -0.9663\n",
      "2025-05-16 12:56:16.974366: Pseudo dice [np.float32(0.9846)]\n",
      "2025-05-16 12:56:16.974400: Epoch time: 108.84 s\n",
      "2025-05-16 12:56:17.529500: \n",
      "2025-05-16 12:56:17.529621: Epoch 786\n",
      "2025-05-16 12:56:17.529694: Current learning rate: 0.0025\n",
      "2025-05-16 12:58:06.428114: train_loss -0.9622\n",
      "2025-05-16 12:58:06.428363: val_loss -0.9705\n",
      "2025-05-16 12:58:06.428522: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 12:58:06.428626: Epoch time: 108.9 s\n",
      "2025-05-16 12:58:06.976843: \n",
      "2025-05-16 12:58:06.977117: Epoch 787\n",
      "2025-05-16 12:58:06.977216: Current learning rate: 0.00249\n",
      "2025-05-16 12:59:55.797494: train_loss -0.959\n",
      "2025-05-16 12:59:55.797627: val_loss -0.9668\n",
      "2025-05-16 12:59:55.797658: Pseudo dice [np.float32(0.9844)]\n",
      "2025-05-16 12:59:55.797690: Epoch time: 108.82 s\n",
      "2025-05-16 12:59:56.343498: \n",
      "2025-05-16 12:59:56.343597: Epoch 788\n",
      "2025-05-16 12:59:56.343664: Current learning rate: 0.00248\n",
      "2025-05-16 13:01:45.183750: train_loss -0.9621\n",
      "2025-05-16 13:01:45.183898: val_loss -0.9713\n",
      "2025-05-16 13:01:45.183933: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 13:01:45.183989: Epoch time: 108.84 s\n",
      "2025-05-16 13:01:45.730932: \n",
      "2025-05-16 13:01:45.731372: Epoch 789\n",
      "2025-05-16 13:01:45.731490: Current learning rate: 0.00247\n",
      "2025-05-16 13:03:34.706702: train_loss -0.9618\n",
      "2025-05-16 13:03:34.706834: val_loss -0.9708\n",
      "2025-05-16 13:03:34.706867: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 13:03:34.706899: Epoch time: 108.98 s\n",
      "2025-05-16 13:03:35.252044: \n",
      "2025-05-16 13:03:35.252196: Epoch 790\n",
      "2025-05-16 13:03:35.252269: Current learning rate: 0.00245\n",
      "2025-05-16 13:05:24.044922: train_loss -0.9605\n",
      "2025-05-16 13:05:24.045043: val_loss -0.9687\n",
      "2025-05-16 13:05:24.045077: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 13:05:24.045111: Epoch time: 108.79 s\n",
      "2025-05-16 13:05:24.589550: \n",
      "2025-05-16 13:05:24.589841: Epoch 791\n",
      "2025-05-16 13:05:24.589953: Current learning rate: 0.00244\n",
      "2025-05-16 13:07:13.576927: train_loss -0.9616\n",
      "2025-05-16 13:07:13.577090: val_loss -0.9688\n",
      "2025-05-16 13:07:13.577132: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 13:07:13.577170: Epoch time: 108.99 s\n",
      "2025-05-16 13:07:14.118529: \n",
      "2025-05-16 13:07:14.118773: Epoch 792\n",
      "2025-05-16 13:07:14.118933: Current learning rate: 0.00243\n",
      "2025-05-16 13:09:02.877809: train_loss -0.9637\n",
      "2025-05-16 13:09:02.877990: val_loss -0.9675\n",
      "2025-05-16 13:09:02.878027: Pseudo dice [np.float32(0.9848)]\n",
      "2025-05-16 13:09:02.878061: Epoch time: 108.76 s\n",
      "2025-05-16 13:09:03.421835: \n",
      "2025-05-16 13:09:03.421926: Epoch 793\n",
      "2025-05-16 13:09:03.422005: Current learning rate: 0.00242\n",
      "2025-05-16 13:10:52.255383: train_loss -0.9646\n",
      "2025-05-16 13:10:52.255515: val_loss -0.9588\n",
      "2025-05-16 13:10:52.255549: Pseudo dice [np.float32(0.98)]\n",
      "2025-05-16 13:10:52.255592: Epoch time: 108.83 s\n",
      "2025-05-16 13:10:52.992385: \n",
      "2025-05-16 13:10:52.992582: Epoch 794\n",
      "2025-05-16 13:10:52.992668: Current learning rate: 0.00241\n",
      "2025-05-16 13:12:41.909307: train_loss -0.9618\n",
      "2025-05-16 13:12:41.909506: val_loss -0.9684\n",
      "2025-05-16 13:12:41.909546: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 13:12:41.909587: Epoch time: 108.92 s\n",
      "2025-05-16 13:12:42.458064: \n",
      "2025-05-16 13:12:42.458196: Epoch 795\n",
      "2025-05-16 13:12:42.458273: Current learning rate: 0.0024\n",
      "2025-05-16 13:14:31.318639: train_loss -0.9627\n",
      "2025-05-16 13:14:31.318848: val_loss -0.9695\n",
      "2025-05-16 13:14:31.318889: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 13:14:31.318957: Epoch time: 108.86 s\n",
      "2025-05-16 13:14:31.862832: \n",
      "2025-05-16 13:14:31.863021: Epoch 796\n",
      "2025-05-16 13:14:31.863105: Current learning rate: 0.00239\n",
      "2025-05-16 13:16:20.752476: train_loss -0.9652\n",
      "2025-05-16 13:16:20.752701: val_loss -0.9732\n",
      "2025-05-16 13:16:20.752769: Pseudo dice [np.float32(0.9878)]\n",
      "2025-05-16 13:16:20.752810: Epoch time: 108.89 s\n",
      "2025-05-16 13:16:21.296685: \n",
      "2025-05-16 13:16:21.296858: Epoch 797\n",
      "2025-05-16 13:16:21.296932: Current learning rate: 0.00238\n",
      "2025-05-16 13:18:10.232260: train_loss -0.9632\n",
      "2025-05-16 13:18:10.232434: val_loss -0.9708\n",
      "2025-05-16 13:18:10.232466: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 13:18:10.232499: Epoch time: 108.94 s\n",
      "2025-05-16 13:18:10.783513: \n",
      "2025-05-16 13:18:10.783793: Epoch 798\n",
      "2025-05-16 13:18:10.783951: Current learning rate: 0.00237\n",
      "2025-05-16 13:19:59.625116: train_loss -0.9644\n",
      "2025-05-16 13:19:59.625234: val_loss -0.9702\n",
      "2025-05-16 13:19:59.625268: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 13:19:59.625302: Epoch time: 108.84 s\n",
      "2025-05-16 13:20:00.174920: \n",
      "2025-05-16 13:20:00.175159: Epoch 799\n",
      "2025-05-16 13:20:00.175255: Current learning rate: 0.00236\n",
      "2025-05-16 13:21:49.119230: train_loss -0.9663\n",
      "2025-05-16 13:21:49.119344: val_loss -0.9707\n",
      "2025-05-16 13:21:49.119378: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 13:21:49.119414: Epoch time: 108.94 s\n",
      "2025-05-16 13:21:49.885437: \n",
      "2025-05-16 13:21:49.885588: Epoch 800\n",
      "2025-05-16 13:21:49.885669: Current learning rate: 0.00235\n",
      "2025-05-16 13:23:38.707298: train_loss -0.9659\n",
      "2025-05-16 13:23:38.707420: val_loss -0.9709\n",
      "2025-05-16 13:23:38.707452: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-16 13:23:38.707484: Epoch time: 108.82 s\n",
      "2025-05-16 13:23:39.248529: \n",
      "2025-05-16 13:23:39.248692: Epoch 801\n",
      "2025-05-16 13:23:39.248782: Current learning rate: 0.00234\n",
      "2025-05-16 13:25:28.113979: train_loss -0.9669\n",
      "2025-05-16 13:25:28.114153: val_loss -0.9723\n",
      "2025-05-16 13:25:28.114280: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 13:25:28.114327: Epoch time: 108.87 s\n",
      "2025-05-16 13:25:28.665355: \n",
      "2025-05-16 13:25:28.665475: Epoch 802\n",
      "2025-05-16 13:25:28.665556: Current learning rate: 0.00233\n",
      "2025-05-16 13:27:17.479265: train_loss -0.9648\n",
      "2025-05-16 13:27:17.479421: val_loss -0.9675\n",
      "2025-05-16 13:27:17.479556: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 13:27:17.479612: Epoch time: 108.81 s\n",
      "2025-05-16 13:27:18.030429: \n",
      "2025-05-16 13:27:18.030524: Epoch 803\n",
      "2025-05-16 13:27:18.030600: Current learning rate: 0.00232\n",
      "2025-05-16 13:29:07.007499: train_loss -0.9643\n",
      "2025-05-16 13:29:07.007668: val_loss -0.9678\n",
      "2025-05-16 13:29:07.007700: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 13:29:07.007732: Epoch time: 108.98 s\n",
      "2025-05-16 13:29:07.559036: \n",
      "2025-05-16 13:29:07.559122: Epoch 804\n",
      "2025-05-16 13:29:07.559189: Current learning rate: 0.00231\n",
      "2025-05-16 13:30:56.390282: train_loss -0.9664\n",
      "2025-05-16 13:30:56.390540: val_loss -0.9734\n",
      "2025-05-16 13:30:56.390580: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 13:30:56.390617: Epoch time: 108.83 s\n",
      "2025-05-16 13:30:56.943301: \n",
      "2025-05-16 13:30:56.943477: Epoch 805\n",
      "2025-05-16 13:30:56.943560: Current learning rate: 0.0023\n",
      "2025-05-16 13:32:45.693303: train_loss -0.9644\n",
      "2025-05-16 13:32:45.693428: val_loss -0.9711\n",
      "2025-05-16 13:32:45.693463: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 13:32:45.693495: Epoch time: 108.75 s\n",
      "2025-05-16 13:32:46.419565: \n",
      "2025-05-16 13:32:46.419697: Epoch 806\n",
      "2025-05-16 13:32:46.419783: Current learning rate: 0.00229\n",
      "2025-05-16 13:34:35.287687: train_loss -0.9647\n",
      "2025-05-16 13:34:35.287823: val_loss -0.9724\n",
      "2025-05-16 13:34:35.287856: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 13:34:35.287889: Epoch time: 108.87 s\n",
      "2025-05-16 13:34:35.841217: \n",
      "2025-05-16 13:34:35.841418: Epoch 807\n",
      "2025-05-16 13:34:35.841530: Current learning rate: 0.00228\n",
      "2025-05-16 13:36:24.640641: train_loss -0.9645\n",
      "2025-05-16 13:36:24.640767: val_loss -0.9711\n",
      "2025-05-16 13:36:24.640801: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 13:36:24.640835: Epoch time: 108.8 s\n",
      "2025-05-16 13:36:25.188311: \n",
      "2025-05-16 13:36:25.188727: Epoch 808\n",
      "2025-05-16 13:36:25.188805: Current learning rate: 0.00226\n",
      "2025-05-16 13:38:14.047550: train_loss -0.9654\n",
      "2025-05-16 13:38:14.047670: val_loss -0.9701\n",
      "2025-05-16 13:38:14.047703: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 13:38:14.047739: Epoch time: 108.86 s\n",
      "2025-05-16 13:38:14.600103: \n",
      "2025-05-16 13:38:14.600430: Epoch 809\n",
      "2025-05-16 13:38:14.600507: Current learning rate: 0.00225\n",
      "2025-05-16 13:40:03.406978: train_loss -0.9641\n",
      "2025-05-16 13:40:03.407101: val_loss -0.9632\n",
      "2025-05-16 13:40:03.407134: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-16 13:40:03.407167: Epoch time: 108.81 s\n",
      "2025-05-16 13:40:03.956914: \n",
      "2025-05-16 13:40:03.957029: Epoch 810\n",
      "2025-05-16 13:40:03.957106: Current learning rate: 0.00224\n",
      "2025-05-16 13:41:52.862107: train_loss -0.9642\n",
      "2025-05-16 13:41:52.862499: val_loss -0.9684\n",
      "2025-05-16 13:41:52.862535: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 13:41:52.862568: Epoch time: 108.91 s\n",
      "2025-05-16 13:41:53.407594: \n",
      "2025-05-16 13:41:53.407711: Epoch 811\n",
      "2025-05-16 13:41:53.407794: Current learning rate: 0.00223\n",
      "2025-05-16 13:43:42.261766: train_loss -0.9675\n",
      "2025-05-16 13:43:42.261898: val_loss -0.9685\n",
      "2025-05-16 13:43:42.261934: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 13:43:42.261969: Epoch time: 108.85 s\n",
      "2025-05-16 13:43:42.810785: \n",
      "2025-05-16 13:43:42.810954: Epoch 812\n",
      "2025-05-16 13:43:42.811025: Current learning rate: 0.00222\n",
      "2025-05-16 13:45:31.751060: train_loss -0.9665\n",
      "2025-05-16 13:45:31.751179: val_loss -0.9669\n",
      "2025-05-16 13:45:31.751213: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-16 13:45:31.751250: Epoch time: 108.94 s\n",
      "2025-05-16 13:45:32.296774: \n",
      "2025-05-16 13:45:32.296910: Epoch 813\n",
      "2025-05-16 13:45:32.297000: Current learning rate: 0.00221\n",
      "2025-05-16 13:47:21.148615: train_loss -0.9667\n",
      "2025-05-16 13:47:21.148771: val_loss -0.9698\n",
      "2025-05-16 13:47:21.148916: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 13:47:21.149000: Epoch time: 108.85 s\n",
      "2025-05-16 13:47:21.697967: \n",
      "2025-05-16 13:47:21.698441: Epoch 814\n",
      "2025-05-16 13:47:21.698603: Current learning rate: 0.0022\n",
      "2025-05-16 13:49:10.622530: train_loss -0.9664\n",
      "2025-05-16 13:49:10.622654: val_loss -0.9688\n",
      "2025-05-16 13:49:10.622689: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 13:49:10.622722: Epoch time: 108.93 s\n",
      "2025-05-16 13:49:11.169908: \n",
      "2025-05-16 13:49:11.169997: Epoch 815\n",
      "2025-05-16 13:49:11.170069: Current learning rate: 0.00219\n",
      "2025-05-16 13:50:59.969073: train_loss -0.9666\n",
      "2025-05-16 13:50:59.969190: val_loss -0.9672\n",
      "2025-05-16 13:50:59.969222: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 13:50:59.969254: Epoch time: 108.8 s\n",
      "2025-05-16 13:51:00.522451: \n",
      "2025-05-16 13:51:00.522626: Epoch 816\n",
      "2025-05-16 13:51:00.522723: Current learning rate: 0.00218\n",
      "2025-05-16 13:52:49.366908: train_loss -0.9668\n",
      "2025-05-16 13:52:49.367111: val_loss -0.9711\n",
      "2025-05-16 13:52:49.367156: Pseudo dice [np.float32(0.9877)]\n",
      "2025-05-16 13:52:49.367194: Epoch time: 108.85 s\n",
      "2025-05-16 13:52:50.108451: \n",
      "2025-05-16 13:52:50.108646: Epoch 817\n",
      "2025-05-16 13:52:50.108740: Current learning rate: 0.00217\n",
      "2025-05-16 13:54:39.086082: train_loss -0.9645\n",
      "2025-05-16 13:54:39.086291: val_loss -0.968\n",
      "2025-05-16 13:54:39.086338: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 13:54:39.086385: Epoch time: 108.98 s\n",
      "2025-05-16 13:54:39.630466: \n",
      "2025-05-16 13:54:39.630618: Epoch 818\n",
      "2025-05-16 13:54:39.630694: Current learning rate: 0.00216\n",
      "2025-05-16 13:56:28.441399: train_loss -0.9659\n",
      "2025-05-16 13:56:28.441564: val_loss -0.9621\n",
      "2025-05-16 13:56:28.441594: Pseudo dice [np.float32(0.9826)]\n",
      "2025-05-16 13:56:28.441627: Epoch time: 108.81 s\n",
      "2025-05-16 13:56:28.999237: \n",
      "2025-05-16 13:56:28.999403: Epoch 819\n",
      "2025-05-16 13:56:28.999505: Current learning rate: 0.00215\n",
      "2025-05-16 13:58:17.851781: train_loss -0.9668\n",
      "2025-05-16 13:58:17.851898: val_loss -0.9674\n",
      "2025-05-16 13:58:17.851960: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 13:58:17.851996: Epoch time: 108.85 s\n",
      "2025-05-16 13:58:18.383653: \n",
      "2025-05-16 13:58:18.383845: Epoch 820\n",
      "2025-05-16 13:58:18.383924: Current learning rate: 0.00214\n",
      "2025-05-16 14:00:07.220098: train_loss -0.9668\n",
      "2025-05-16 14:00:07.220374: val_loss -0.9719\n",
      "2025-05-16 14:00:07.220412: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 14:00:07.220445: Epoch time: 108.84 s\n",
      "2025-05-16 14:00:07.751436: \n",
      "2025-05-16 14:00:07.751814: Epoch 821\n",
      "2025-05-16 14:00:07.751925: Current learning rate: 0.00213\n",
      "2025-05-16 14:01:56.609128: train_loss -0.9673\n",
      "2025-05-16 14:01:56.609288: val_loss -0.9731\n",
      "2025-05-16 14:01:56.609437: Pseudo dice [np.float32(0.9879)]\n",
      "2025-05-16 14:01:56.609483: Epoch time: 108.86 s\n",
      "2025-05-16 14:01:57.142003: \n",
      "2025-05-16 14:01:57.142273: Epoch 822\n",
      "2025-05-16 14:01:57.142359: Current learning rate: 0.00212\n",
      "2025-05-16 14:03:46.031222: train_loss -0.9678\n",
      "2025-05-16 14:03:46.031344: val_loss -0.9702\n",
      "2025-05-16 14:03:46.031376: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 14:03:46.031409: Epoch time: 108.89 s\n",
      "2025-05-16 14:03:46.561345: \n",
      "2025-05-16 14:03:46.561739: Epoch 823\n",
      "2025-05-16 14:03:46.561883: Current learning rate: 0.0021\n",
      "2025-05-16 14:05:35.380888: train_loss -0.9665\n",
      "2025-05-16 14:05:35.381013: val_loss -0.9654\n",
      "2025-05-16 14:05:35.381046: Pseudo dice [np.float32(0.9841)]\n",
      "2025-05-16 14:05:35.381078: Epoch time: 108.82 s\n",
      "2025-05-16 14:05:35.907874: \n",
      "2025-05-16 14:05:35.908072: Epoch 824\n",
      "2025-05-16 14:05:35.908192: Current learning rate: 0.00209\n",
      "2025-05-16 14:07:24.746297: train_loss -0.9663\n",
      "2025-05-16 14:07:24.746440: val_loss -0.9709\n",
      "2025-05-16 14:07:24.746485: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 14:07:24.746522: Epoch time: 108.84 s\n",
      "2025-05-16 14:07:25.280124: \n",
      "2025-05-16 14:07:25.280301: Epoch 825\n",
      "2025-05-16 14:07:25.280402: Current learning rate: 0.00208\n",
      "2025-05-16 14:09:14.232586: train_loss -0.9673\n",
      "2025-05-16 14:09:14.232702: val_loss -0.9701\n",
      "2025-05-16 14:09:14.232734: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 14:09:14.232768: Epoch time: 108.95 s\n",
      "2025-05-16 14:09:14.770574: \n",
      "2025-05-16 14:09:14.770897: Epoch 826\n",
      "2025-05-16 14:09:14.770999: Current learning rate: 0.00207\n",
      "2025-05-16 14:11:03.631940: train_loss -0.9656\n",
      "2025-05-16 14:11:03.632233: val_loss -0.9694\n",
      "2025-05-16 14:11:03.632277: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 14:11:03.632312: Epoch time: 108.86 s\n",
      "2025-05-16 14:11:04.162892: \n",
      "2025-05-16 14:11:04.163097: Epoch 827\n",
      "2025-05-16 14:11:04.163186: Current learning rate: 0.00206\n",
      "2025-05-16 14:12:53.026512: train_loss -0.967\n",
      "2025-05-16 14:12:53.026636: val_loss -0.9686\n",
      "2025-05-16 14:12:53.026668: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 14:12:53.026701: Epoch time: 108.86 s\n",
      "2025-05-16 14:12:53.560880: \n",
      "2025-05-16 14:12:53.561061: Epoch 828\n",
      "2025-05-16 14:12:53.561142: Current learning rate: 0.00205\n",
      "2025-05-16 14:14:42.348020: train_loss -0.9667\n",
      "2025-05-16 14:14:42.348137: val_loss -0.9689\n",
      "2025-05-16 14:14:42.348205: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 14:14:42.348340: Epoch time: 108.79 s\n",
      "2025-05-16 14:14:42.876434: \n",
      "2025-05-16 14:14:42.876621: Epoch 829\n",
      "2025-05-16 14:14:42.876704: Current learning rate: 0.00204\n",
      "2025-05-16 14:16:31.709780: train_loss -0.9674\n",
      "2025-05-16 14:16:31.710003: val_loss -0.9681\n",
      "2025-05-16 14:16:31.710162: Pseudo dice [np.float32(0.9849)]\n",
      "2025-05-16 14:16:31.710268: Epoch time: 108.83 s\n",
      "2025-05-16 14:16:32.421498: \n",
      "2025-05-16 14:16:32.421618: Epoch 830\n",
      "2025-05-16 14:16:32.421689: Current learning rate: 0.00203\n",
      "2025-05-16 14:18:21.288038: train_loss -0.9648\n",
      "2025-05-16 14:18:21.288162: val_loss -0.9712\n",
      "2025-05-16 14:18:21.288198: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 14:18:21.288233: Epoch time: 108.87 s\n",
      "2025-05-16 14:18:21.809984: \n",
      "2025-05-16 14:18:21.810409: Epoch 831\n",
      "2025-05-16 14:18:21.810494: Current learning rate: 0.00202\n",
      "2025-05-16 14:20:10.746901: train_loss -0.9667\n",
      "2025-05-16 14:20:10.747022: val_loss -0.9716\n",
      "2025-05-16 14:20:10.747058: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 14:20:10.747092: Epoch time: 108.94 s\n",
      "2025-05-16 14:20:11.274451: \n",
      "2025-05-16 14:20:11.274665: Epoch 832\n",
      "2025-05-16 14:20:11.274743: Current learning rate: 0.00201\n",
      "2025-05-16 14:22:00.219456: train_loss -0.9672\n",
      "2025-05-16 14:22:00.219571: val_loss -0.9676\n",
      "2025-05-16 14:22:00.219604: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 14:22:00.219635: Epoch time: 108.95 s\n",
      "2025-05-16 14:22:00.745768: \n",
      "2025-05-16 14:22:00.745948: Epoch 833\n",
      "2025-05-16 14:22:00.746025: Current learning rate: 0.002\n",
      "2025-05-16 14:23:49.733360: train_loss -0.9684\n",
      "2025-05-16 14:23:49.733485: val_loss -0.9694\n",
      "2025-05-16 14:23:49.733521: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 14:23:49.733556: Epoch time: 108.99 s\n",
      "2025-05-16 14:23:50.272184: \n",
      "2025-05-16 14:23:50.272421: Epoch 834\n",
      "2025-05-16 14:23:50.272503: Current learning rate: 0.00199\n",
      "2025-05-16 14:25:39.138388: train_loss -0.9678\n",
      "2025-05-16 14:25:39.138501: val_loss -0.9679\n",
      "2025-05-16 14:25:39.138532: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 14:25:39.138598: Epoch time: 108.87 s\n",
      "2025-05-16 14:25:39.666216: \n",
      "2025-05-16 14:25:39.666325: Epoch 835\n",
      "2025-05-16 14:25:39.666393: Current learning rate: 0.00198\n",
      "2025-05-16 14:27:28.617638: train_loss -0.9674\n",
      "2025-05-16 14:27:28.617812: val_loss -0.9737\n",
      "2025-05-16 14:27:28.617853: Pseudo dice [np.float32(0.9883)]\n",
      "2025-05-16 14:27:28.617888: Epoch time: 108.95 s\n",
      "2025-05-16 14:27:29.145028: \n",
      "2025-05-16 14:27:29.145146: Epoch 836\n",
      "2025-05-16 14:27:29.145216: Current learning rate: 0.00196\n",
      "2025-05-16 14:29:18.098010: train_loss -0.9644\n",
      "2025-05-16 14:29:18.098126: val_loss -0.9694\n",
      "2025-05-16 14:29:18.098160: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 14:29:18.098194: Epoch time: 108.95 s\n",
      "2025-05-16 14:29:18.628792: \n",
      "2025-05-16 14:29:18.629126: Epoch 837\n",
      "2025-05-16 14:29:18.629309: Current learning rate: 0.00195\n",
      "2025-05-16 14:31:07.660519: train_loss -0.9672\n",
      "2025-05-16 14:31:07.660652: val_loss -0.9691\n",
      "2025-05-16 14:31:07.660684: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 14:31:07.660718: Epoch time: 109.03 s\n",
      "2025-05-16 14:31:08.188399: \n",
      "2025-05-16 14:31:08.188606: Epoch 838\n",
      "2025-05-16 14:31:08.188733: Current learning rate: 0.00194\n",
      "2025-05-16 14:32:57.108769: train_loss -0.9659\n",
      "2025-05-16 14:32:57.108892: val_loss -0.969\n",
      "2025-05-16 14:32:57.108977: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 14:32:57.109081: Epoch time: 108.92 s\n",
      "2025-05-16 14:32:57.633208: \n",
      "2025-05-16 14:32:57.633298: Epoch 839\n",
      "2025-05-16 14:32:57.633368: Current learning rate: 0.00193\n",
      "2025-05-16 14:34:46.610742: train_loss -0.9669\n",
      "2025-05-16 14:34:46.610914: val_loss -0.9708\n",
      "2025-05-16 14:34:46.610946: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 14:34:46.610978: Epoch time: 108.98 s\n",
      "2025-05-16 14:34:47.142560: \n",
      "2025-05-16 14:34:47.142659: Epoch 840\n",
      "2025-05-16 14:34:47.142735: Current learning rate: 0.00192\n",
      "2025-05-16 14:36:35.966903: train_loss -0.9694\n",
      "2025-05-16 14:36:35.967021: val_loss -0.9709\n",
      "2025-05-16 14:36:35.967056: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-16 14:36:35.967090: Epoch time: 108.82 s\n",
      "2025-05-16 14:36:36.492693: \n",
      "2025-05-16 14:36:36.492866: Epoch 841\n",
      "2025-05-16 14:36:36.492951: Current learning rate: 0.00191\n",
      "2025-05-16 14:38:25.359142: train_loss -0.9688\n",
      "2025-05-16 14:38:25.359260: val_loss -0.9696\n",
      "2025-05-16 14:38:25.359294: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 14:38:25.359325: Epoch time: 108.87 s\n",
      "2025-05-16 14:38:25.888240: \n",
      "2025-05-16 14:38:25.888382: Epoch 842\n",
      "2025-05-16 14:38:25.888459: Current learning rate: 0.0019\n",
      "2025-05-16 14:40:14.731030: train_loss -0.9689\n",
      "2025-05-16 14:40:14.731289: val_loss -0.9679\n",
      "2025-05-16 14:40:14.731560: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 14:40:14.731648: Epoch time: 108.84 s\n",
      "2025-05-16 14:40:15.437141: \n",
      "2025-05-16 14:40:15.437404: Epoch 843\n",
      "2025-05-16 14:40:15.437554: Current learning rate: 0.00189\n",
      "2025-05-16 14:42:04.278902: train_loss -0.9685\n",
      "2025-05-16 14:42:04.279021: val_loss -0.9712\n",
      "2025-05-16 14:42:04.279052: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 14:42:04.279086: Epoch time: 108.84 s\n",
      "2025-05-16 14:42:04.810673: \n",
      "2025-05-16 14:42:04.810786: Epoch 844\n",
      "2025-05-16 14:42:04.810856: Current learning rate: 0.00188\n",
      "2025-05-16 14:43:53.765219: train_loss -0.9685\n",
      "2025-05-16 14:43:53.765414: val_loss -0.9662\n",
      "2025-05-16 14:43:53.765446: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 14:43:53.765479: Epoch time: 108.96 s\n",
      "2025-05-16 14:43:54.299086: \n",
      "2025-05-16 14:43:54.299215: Epoch 845\n",
      "2025-05-16 14:43:54.299294: Current learning rate: 0.00187\n",
      "2025-05-16 14:45:43.234421: train_loss -0.9671\n",
      "2025-05-16 14:45:43.234535: val_loss -0.9718\n",
      "2025-05-16 14:45:43.234566: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 14:45:43.234599: Epoch time: 108.94 s\n",
      "2025-05-16 14:45:43.770312: \n",
      "2025-05-16 14:45:43.770450: Epoch 846\n",
      "2025-05-16 14:45:43.770608: Current learning rate: 0.00186\n",
      "2025-05-16 14:47:32.642269: train_loss -0.9666\n",
      "2025-05-16 14:47:32.642433: val_loss -0.9725\n",
      "2025-05-16 14:47:32.642473: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 14:47:32.642508: Epoch time: 108.87 s\n",
      "2025-05-16 14:47:33.170383: \n",
      "2025-05-16 14:47:33.170664: Epoch 847\n",
      "2025-05-16 14:47:33.170777: Current learning rate: 0.00185\n",
      "2025-05-16 14:49:22.081693: train_loss -0.9693\n",
      "2025-05-16 14:49:22.081826: val_loss -0.965\n",
      "2025-05-16 14:49:22.081865: Pseudo dice [np.float32(0.9837)]\n",
      "2025-05-16 14:49:22.081897: Epoch time: 108.91 s\n",
      "2025-05-16 14:49:22.608533: \n",
      "2025-05-16 14:49:22.608640: Epoch 848\n",
      "2025-05-16 14:49:22.608711: Current learning rate: 0.00184\n",
      "2025-05-16 14:51:11.573159: train_loss -0.968\n",
      "2025-05-16 14:51:11.573353: val_loss -0.9646\n",
      "2025-05-16 14:51:11.573430: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-16 14:51:11.573501: Epoch time: 108.97 s\n",
      "2025-05-16 14:51:12.102189: \n",
      "2025-05-16 14:51:12.102540: Epoch 849\n",
      "2025-05-16 14:51:12.102701: Current learning rate: 0.00182\n",
      "2025-05-16 14:53:00.975570: train_loss -0.9677\n",
      "2025-05-16 14:53:00.975686: val_loss -0.9676\n",
      "2025-05-16 14:53:00.975717: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 14:53:00.975750: Epoch time: 108.87 s\n",
      "2025-05-16 14:53:01.729854: \n",
      "2025-05-16 14:53:01.729973: Epoch 850\n",
      "2025-05-16 14:53:01.730056: Current learning rate: 0.00181\n",
      "2025-05-16 14:54:50.718893: train_loss -0.9682\n",
      "2025-05-16 14:54:50.719052: val_loss -0.9689\n",
      "2025-05-16 14:54:50.719083: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 14:54:50.719115: Epoch time: 108.99 s\n",
      "2025-05-16 14:54:51.239967: \n",
      "2025-05-16 14:54:51.240085: Epoch 851\n",
      "2025-05-16 14:54:51.240274: Current learning rate: 0.0018\n",
      "2025-05-16 14:56:40.116557: train_loss -0.9676\n",
      "2025-05-16 14:56:40.116677: val_loss -0.9713\n",
      "2025-05-16 14:56:40.116710: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 14:56:40.116743: Epoch time: 108.88 s\n",
      "2025-05-16 14:56:40.636685: \n",
      "2025-05-16 14:56:40.636982: Epoch 852\n",
      "2025-05-16 14:56:40.637054: Current learning rate: 0.00179\n",
      "2025-05-16 14:58:29.466742: train_loss -0.9681\n",
      "2025-05-16 14:58:29.466930: val_loss -0.9714\n",
      "2025-05-16 14:58:29.466990: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 14:58:29.467038: Epoch time: 108.83 s\n",
      "2025-05-16 14:58:29.988103: \n",
      "2025-05-16 14:58:29.988333: Epoch 853\n",
      "2025-05-16 14:58:29.988422: Current learning rate: 0.00178\n",
      "2025-05-16 15:00:18.884655: train_loss -0.9679\n",
      "2025-05-16 15:00:18.884775: val_loss -0.9693\n",
      "2025-05-16 15:00:18.884808: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 15:00:18.884841: Epoch time: 108.9 s\n",
      "2025-05-16 15:00:19.406623: \n",
      "2025-05-16 15:00:19.406718: Epoch 854\n",
      "2025-05-16 15:00:19.406795: Current learning rate: 0.00177\n",
      "2025-05-16 15:02:08.285009: train_loss -0.968\n",
      "2025-05-16 15:02:08.285232: val_loss -0.9683\n",
      "2025-05-16 15:02:08.285323: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 15:02:08.285405: Epoch time: 108.88 s\n",
      "2025-05-16 15:02:08.816214: \n",
      "2025-05-16 15:02:08.816312: Epoch 855\n",
      "2025-05-16 15:02:08.816384: Current learning rate: 0.00176\n",
      "2025-05-16 15:03:57.617160: train_loss -0.9676\n",
      "2025-05-16 15:03:57.617276: val_loss -0.9693\n",
      "2025-05-16 15:03:57.617311: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 15:03:57.617344: Epoch time: 108.8 s\n",
      "2025-05-16 15:03:58.321003: \n",
      "2025-05-16 15:03:58.321109: Epoch 856\n",
      "2025-05-16 15:03:58.321177: Current learning rate: 0.00175\n",
      "2025-05-16 15:05:47.133102: train_loss -0.9681\n",
      "2025-05-16 15:05:47.133224: val_loss -0.9709\n",
      "2025-05-16 15:05:47.133257: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 15:05:47.133290: Epoch time: 108.81 s\n",
      "2025-05-16 15:05:47.651184: \n",
      "2025-05-16 15:05:47.651471: Epoch 857\n",
      "2025-05-16 15:05:47.651592: Current learning rate: 0.00174\n",
      "2025-05-16 15:07:36.540560: train_loss -0.9675\n",
      "2025-05-16 15:07:36.540683: val_loss -0.9719\n",
      "2025-05-16 15:07:36.540716: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 15:07:36.540790: Epoch time: 108.89 s\n",
      "2025-05-16 15:07:37.071154: \n",
      "2025-05-16 15:07:37.071334: Epoch 858\n",
      "2025-05-16 15:07:37.071416: Current learning rate: 0.00173\n",
      "2025-05-16 15:09:26.037008: train_loss -0.9677\n",
      "2025-05-16 15:09:26.037153: val_loss -0.9701\n",
      "2025-05-16 15:09:26.037415: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 15:09:26.037483: Epoch time: 108.97 s\n",
      "2025-05-16 15:09:26.564594: \n",
      "2025-05-16 15:09:26.564820: Epoch 859\n",
      "2025-05-16 15:09:26.564907: Current learning rate: 0.00172\n",
      "2025-05-16 15:11:15.414342: train_loss -0.9672\n",
      "2025-05-16 15:11:15.414462: val_loss -0.9697\n",
      "2025-05-16 15:11:15.414496: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 15:11:15.414530: Epoch time: 108.85 s\n",
      "2025-05-16 15:11:15.933206: \n",
      "2025-05-16 15:11:15.933546: Epoch 860\n",
      "2025-05-16 15:11:15.933650: Current learning rate: 0.0017\n",
      "2025-05-16 15:13:04.891315: train_loss -0.9667\n",
      "2025-05-16 15:13:04.891461: val_loss -0.9676\n",
      "2025-05-16 15:13:04.891496: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 15:13:04.891576: Epoch time: 108.96 s\n",
      "2025-05-16 15:13:05.415475: \n",
      "2025-05-16 15:13:05.415722: Epoch 861\n",
      "2025-05-16 15:13:05.415814: Current learning rate: 0.00169\n",
      "2025-05-16 15:14:54.373997: train_loss -0.9676\n",
      "2025-05-16 15:14:54.374113: val_loss -0.9743\n",
      "2025-05-16 15:14:54.374143: Pseudo dice [np.float32(0.9884)]\n",
      "2025-05-16 15:14:54.374175: Epoch time: 108.96 s\n",
      "2025-05-16 15:14:54.374195: Yayy! New best EMA pseudo Dice: 0.9865999817848206\n",
      "2025-05-16 15:14:55.141150: \n",
      "2025-05-16 15:14:55.141262: Epoch 862\n",
      "2025-05-16 15:14:55.141341: Current learning rate: 0.00168\n",
      "2025-05-16 15:16:44.172960: train_loss -0.9684\n",
      "2025-05-16 15:16:44.173088: val_loss -0.9718\n",
      "2025-05-16 15:16:44.175722: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 15:16:44.175817: Epoch time: 109.03 s\n",
      "2025-05-16 15:16:44.175848: Yayy! New best EMA pseudo Dice: 0.9866999983787537\n",
      "2025-05-16 15:16:44.918230: \n",
      "2025-05-16 15:16:44.918592: Epoch 863\n",
      "2025-05-16 15:16:44.918881: Current learning rate: 0.00167\n",
      "2025-05-16 15:18:33.902328: train_loss -0.9685\n",
      "2025-05-16 15:18:33.902457: val_loss -0.9714\n",
      "2025-05-16 15:18:33.902524: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 15:18:33.902595: Epoch time: 108.98 s\n",
      "2025-05-16 15:18:33.902743: Yayy! New best EMA pseudo Dice: 0.9866999983787537\n",
      "2025-05-16 15:18:34.650971: \n",
      "2025-05-16 15:18:34.651082: Epoch 864\n",
      "2025-05-16 15:18:34.651155: Current learning rate: 0.00166\n",
      "2025-05-16 15:20:23.648563: train_loss -0.9699\n",
      "2025-05-16 15:20:23.648692: val_loss -0.9716\n",
      "2025-05-16 15:20:23.648837: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 15:20:23.648918: Epoch time: 109.0 s\n",
      "2025-05-16 15:20:23.648951: Yayy! New best EMA pseudo Dice: 0.9868000149726868\n",
      "2025-05-16 15:20:24.385243: \n",
      "2025-05-16 15:20:24.385414: Epoch 865\n",
      "2025-05-16 15:20:24.385489: Current learning rate: 0.00165\n",
      "2025-05-16 15:22:13.342422: train_loss -0.9698\n",
      "2025-05-16 15:22:13.342544: val_loss -0.9688\n",
      "2025-05-16 15:22:13.342578: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 15:22:13.342610: Epoch time: 108.96 s\n",
      "2025-05-16 15:22:13.866190: \n",
      "2025-05-16 15:22:13.866521: Epoch 866\n",
      "2025-05-16 15:22:13.866682: Current learning rate: 0.00164\n",
      "2025-05-16 15:24:02.745387: train_loss -0.9683\n",
      "2025-05-16 15:24:02.745514: val_loss -0.9704\n",
      "2025-05-16 15:24:02.745545: Pseudo dice [np.float32(0.987)]\n",
      "2025-05-16 15:24:02.745578: Epoch time: 108.88 s\n",
      "2025-05-16 15:24:03.263582: \n",
      "2025-05-16 15:24:03.263756: Epoch 867\n",
      "2025-05-16 15:24:03.263880: Current learning rate: 0.00163\n",
      "2025-05-16 15:25:52.104306: train_loss -0.9688\n",
      "2025-05-16 15:25:52.104429: val_loss -0.9724\n",
      "2025-05-16 15:25:52.104460: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 15:25:52.104492: Epoch time: 108.84 s\n",
      "2025-05-16 15:25:52.104511: Yayy! New best EMA pseudo Dice: 0.9868999719619751\n",
      "2025-05-16 15:25:53.018877: \n",
      "2025-05-16 15:25:53.019112: Epoch 868\n",
      "2025-05-16 15:25:53.019234: Current learning rate: 0.00162\n",
      "2025-05-16 15:27:41.840873: train_loss -0.9689\n",
      "2025-05-16 15:27:41.840996: val_loss -0.9695\n",
      "2025-05-16 15:27:41.841029: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 15:27:41.841062: Epoch time: 108.82 s\n",
      "2025-05-16 15:27:42.375517: \n",
      "2025-05-16 15:27:42.375817: Epoch 869\n",
      "2025-05-16 15:27:42.375973: Current learning rate: 0.00161\n",
      "2025-05-16 15:29:31.208701: train_loss -0.9696\n",
      "2025-05-16 15:29:31.208877: val_loss -0.9733\n",
      "2025-05-16 15:29:31.208908: Pseudo dice [np.float32(0.9888)]\n",
      "2025-05-16 15:29:31.208940: Epoch time: 108.83 s\n",
      "2025-05-16 15:29:31.208961: Yayy! New best EMA pseudo Dice: 0.9868999719619751\n",
      "2025-05-16 15:29:31.960805: \n",
      "2025-05-16 15:29:31.960906: Epoch 870\n",
      "2025-05-16 15:29:31.960973: Current learning rate: 0.00159\n",
      "2025-05-16 15:31:20.854127: train_loss -0.9693\n",
      "2025-05-16 15:31:20.854241: val_loss -0.9698\n",
      "2025-05-16 15:31:20.854273: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 15:31:20.854306: Epoch time: 108.89 s\n",
      "2025-05-16 15:31:21.378551: \n",
      "2025-05-16 15:31:21.378814: Epoch 871\n",
      "2025-05-16 15:31:21.378890: Current learning rate: 0.00158\n",
      "2025-05-16 15:33:10.378318: train_loss -0.967\n",
      "2025-05-16 15:33:10.378530: val_loss -0.9715\n",
      "2025-05-16 15:33:10.378574: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 15:33:10.378608: Epoch time: 109.0 s\n",
      "2025-05-16 15:33:10.905105: \n",
      "2025-05-16 15:33:10.905212: Epoch 872\n",
      "2025-05-16 15:33:10.905281: Current learning rate: 0.00157\n",
      "2025-05-16 15:34:59.755798: train_loss -0.9692\n",
      "2025-05-16 15:34:59.755985: val_loss -0.9708\n",
      "2025-05-16 15:34:59.756176: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 15:34:59.756296: Epoch time: 108.85 s\n",
      "2025-05-16 15:35:00.274756: \n",
      "2025-05-16 15:35:00.274928: Epoch 873\n",
      "2025-05-16 15:35:00.275015: Current learning rate: 0.00156\n",
      "2025-05-16 15:36:49.182705: train_loss -0.9681\n",
      "2025-05-16 15:36:49.182871: val_loss -0.972\n",
      "2025-05-16 15:36:49.182904: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 15:36:49.182938: Epoch time: 108.91 s\n",
      "2025-05-16 15:36:49.183139: Yayy! New best EMA pseudo Dice: 0.9868999719619751\n",
      "2025-05-16 15:36:49.929237: \n",
      "2025-05-16 15:36:49.929423: Epoch 874\n",
      "2025-05-16 15:36:49.929502: Current learning rate: 0.00155\n",
      "2025-05-16 15:38:38.823058: train_loss -0.9704\n",
      "2025-05-16 15:38:38.823204: val_loss -0.9672\n",
      "2025-05-16 15:38:38.823243: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 15:38:38.823274: Epoch time: 108.89 s\n",
      "2025-05-16 15:38:39.346361: \n",
      "2025-05-16 15:38:39.346453: Epoch 875\n",
      "2025-05-16 15:38:39.346523: Current learning rate: 0.00154\n",
      "2025-05-16 15:40:28.241815: train_loss -0.9686\n",
      "2025-05-16 15:40:28.241930: val_loss -0.9672\n",
      "2025-05-16 15:40:28.241965: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 15:40:28.242000: Epoch time: 108.9 s\n",
      "2025-05-16 15:40:28.766596: \n",
      "2025-05-16 15:40:28.766687: Epoch 876\n",
      "2025-05-16 15:40:28.766755: Current learning rate: 0.00153\n",
      "2025-05-16 15:42:17.608375: train_loss -0.9678\n",
      "2025-05-16 15:42:17.608499: val_loss -0.9696\n",
      "2025-05-16 15:42:17.608535: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 15:42:17.608569: Epoch time: 108.84 s\n",
      "2025-05-16 15:42:18.126969: \n",
      "2025-05-16 15:42:18.127317: Epoch 877\n",
      "2025-05-16 15:42:18.127428: Current learning rate: 0.00152\n",
      "2025-05-16 15:44:07.067340: train_loss -0.9711\n",
      "2025-05-16 15:44:07.067463: val_loss -0.9651\n",
      "2025-05-16 15:44:07.067496: Pseudo dice [np.float32(0.9842)]\n",
      "2025-05-16 15:44:07.067537: Epoch time: 108.94 s\n",
      "2025-05-16 15:44:07.586913: \n",
      "2025-05-16 15:44:07.587187: Epoch 878\n",
      "2025-05-16 15:44:07.587289: Current learning rate: 0.00151\n",
      "2025-05-16 15:45:56.412736: train_loss -0.9704\n",
      "2025-05-16 15:45:56.412858: val_loss -0.9642\n",
      "2025-05-16 15:45:56.412892: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-16 15:45:56.412926: Epoch time: 108.83 s\n",
      "2025-05-16 15:45:56.933222: \n",
      "2025-05-16 15:45:56.933405: Epoch 879\n",
      "2025-05-16 15:45:56.933484: Current learning rate: 0.00149\n",
      "2025-05-16 15:47:45.845743: train_loss -0.9695\n",
      "2025-05-16 15:47:45.845860: val_loss -0.9699\n",
      "2025-05-16 15:47:45.845894: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 15:47:45.845928: Epoch time: 108.91 s\n",
      "2025-05-16 15:47:46.363039: \n",
      "2025-05-16 15:47:46.363227: Epoch 880\n",
      "2025-05-16 15:47:46.363395: Current learning rate: 0.00148\n",
      "2025-05-16 15:49:35.250990: train_loss -0.9699\n",
      "2025-05-16 15:49:35.251116: val_loss -0.9685\n",
      "2025-05-16 15:49:35.251150: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 15:49:35.251181: Epoch time: 108.89 s\n",
      "2025-05-16 15:49:35.955168: \n",
      "2025-05-16 15:49:35.955288: Epoch 881\n",
      "2025-05-16 15:49:35.955356: Current learning rate: 0.00147\n",
      "2025-05-16 15:51:24.799179: train_loss -0.9698\n",
      "2025-05-16 15:51:24.799298: val_loss -0.9714\n",
      "2025-05-16 15:51:24.799334: Pseudo dice [np.float32(0.9879)]\n",
      "2025-05-16 15:51:24.799366: Epoch time: 108.84 s\n",
      "2025-05-16 15:51:25.330345: \n",
      "2025-05-16 15:51:25.330705: Epoch 882\n",
      "2025-05-16 15:51:25.330790: Current learning rate: 0.00146\n",
      "2025-05-16 15:53:14.394214: train_loss -0.9689\n",
      "2025-05-16 15:53:14.394366: val_loss -0.9688\n",
      "2025-05-16 15:53:14.394402: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 15:53:14.394437: Epoch time: 109.06 s\n",
      "2025-05-16 15:53:14.921900: \n",
      "2025-05-16 15:53:14.922020: Epoch 883\n",
      "2025-05-16 15:53:14.922108: Current learning rate: 0.00145\n",
      "2025-05-16 15:55:03.980479: train_loss -0.9671\n",
      "2025-05-16 15:55:03.980601: val_loss -0.9705\n",
      "2025-05-16 15:55:03.980633: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 15:55:03.980665: Epoch time: 109.06 s\n",
      "2025-05-16 15:55:04.503053: \n",
      "2025-05-16 15:55:04.503212: Epoch 884\n",
      "2025-05-16 15:55:04.503283: Current learning rate: 0.00144\n",
      "2025-05-16 15:56:53.522586: train_loss -0.9692\n",
      "2025-05-16 15:56:53.522760: val_loss -0.972\n",
      "2025-05-16 15:56:53.522974: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 15:56:53.523043: Epoch time: 109.02 s\n",
      "2025-05-16 15:56:54.044999: \n",
      "2025-05-16 15:56:54.045092: Epoch 885\n",
      "2025-05-16 15:56:54.045165: Current learning rate: 0.00143\n",
      "2025-05-16 15:58:42.894931: train_loss -0.9687\n",
      "2025-05-16 15:58:42.895106: val_loss -0.9644\n",
      "2025-05-16 15:58:42.895138: Pseudo dice [np.float32(0.9829)]\n",
      "2025-05-16 15:58:42.895171: Epoch time: 108.85 s\n",
      "2025-05-16 15:58:43.421680: \n",
      "2025-05-16 15:58:43.421973: Epoch 886\n",
      "2025-05-16 15:58:43.422087: Current learning rate: 0.00142\n",
      "2025-05-16 16:00:32.246627: train_loss -0.9681\n",
      "2025-05-16 16:00:32.246995: val_loss -0.9637\n",
      "2025-05-16 16:00:32.247034: Pseudo dice [np.float32(0.9833)]\n",
      "2025-05-16 16:00:32.247066: Epoch time: 108.83 s\n",
      "2025-05-16 16:00:32.770362: \n",
      "2025-05-16 16:00:32.770461: Epoch 887\n",
      "2025-05-16 16:00:32.770534: Current learning rate: 0.00141\n",
      "2025-05-16 16:02:21.610720: train_loss -0.9698\n",
      "2025-05-16 16:02:21.610932: val_loss -0.9703\n",
      "2025-05-16 16:02:21.611063: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 16:02:21.611108: Epoch time: 108.84 s\n",
      "2025-05-16 16:02:22.131658: \n",
      "2025-05-16 16:02:22.131758: Epoch 888\n",
      "2025-05-16 16:02:22.131840: Current learning rate: 0.00139\n",
      "2025-05-16 16:04:11.059369: train_loss -0.9701\n",
      "2025-05-16 16:04:11.059586: val_loss -0.967\n",
      "2025-05-16 16:04:11.059627: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 16:04:11.059659: Epoch time: 108.93 s\n",
      "2025-05-16 16:04:11.577273: \n",
      "2025-05-16 16:04:11.577510: Epoch 889\n",
      "2025-05-16 16:04:11.577652: Current learning rate: 0.00138\n",
      "2025-05-16 16:06:00.590613: train_loss -0.9689\n",
      "2025-05-16 16:06:00.590772: val_loss -0.9707\n",
      "2025-05-16 16:06:00.590807: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 16:06:00.590840: Epoch time: 109.01 s\n",
      "2025-05-16 16:06:01.114609: \n",
      "2025-05-16 16:06:01.114954: Epoch 890\n",
      "2025-05-16 16:06:01.115156: Current learning rate: 0.00137\n",
      "2025-05-16 16:07:49.959046: train_loss -0.9689\n",
      "2025-05-16 16:07:49.959321: val_loss -0.9692\n",
      "2025-05-16 16:07:49.959437: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 16:07:49.959561: Epoch time: 108.84 s\n",
      "2025-05-16 16:07:50.480758: \n",
      "2025-05-16 16:07:50.481040: Epoch 891\n",
      "2025-05-16 16:07:50.481137: Current learning rate: 0.00136\n",
      "2025-05-16 16:09:39.412940: train_loss -0.9692\n",
      "2025-05-16 16:09:39.413055: val_loss -0.9714\n",
      "2025-05-16 16:09:39.413088: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 16:09:39.413120: Epoch time: 108.93 s\n",
      "2025-05-16 16:09:39.936270: \n",
      "2025-05-16 16:09:39.936363: Epoch 892\n",
      "2025-05-16 16:09:39.936430: Current learning rate: 0.00135\n",
      "2025-05-16 16:11:28.783210: train_loss -0.9698\n",
      "2025-05-16 16:11:28.783355: val_loss -0.9712\n",
      "2025-05-16 16:11:28.783479: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 16:11:28.783592: Epoch time: 108.85 s\n",
      "2025-05-16 16:11:29.300821: \n",
      "2025-05-16 16:11:29.300993: Epoch 893\n",
      "2025-05-16 16:11:29.301081: Current learning rate: 0.00134\n",
      "-0.9685-16 16:13:18.174460: train_loss \n",
      "2025-05-16 16:13:18.174887: val_loss -0.9744\n",
      "2025-05-16 16:13:18.175053: Pseudo dice [np.float32(0.9881)]\n",
      "2025-05-16 16:13:18.175251: Epoch time: 108.87 s\n",
      "2025-05-16 16:13:18.887945: \n",
      "2025-05-16 16:13:18.888056: Epoch 894\n",
      "2025-05-16 16:13:18.888134: Current learning rate: 0.00133\n",
      "2025-05-16 16:15:07.839933: train_loss -0.9709\n",
      "2025-05-16 16:15:07.840122: val_loss -0.9697\n",
      "2025-05-16 16:15:07.840155: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 16:15:07.840215: Epoch time: 108.95 s\n",
      "2025-05-16 16:15:08.362305: \n",
      "2025-05-16 16:15:08.362502: Epoch 895\n",
      "2025-05-16 16:15:08.362599: Current learning rate: 0.00132\n",
      "2025-05-16 16:16:57.241520: train_loss -0.9698\n",
      "2025-05-16 16:16:57.241640: val_loss -0.9685\n",
      "2025-05-16 16:16:57.241759: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 16:16:57.241836: Epoch time: 108.88 s\n",
      "2025-05-16 16:16:57.769816: \n",
      "2025-05-16 16:16:57.770224: Epoch 896\n",
      "2025-05-16 16:16:57.770316: Current learning rate: 0.0013\n",
      "2025-05-16 16:18:48.014680: train_loss -0.9688\n",
      "2025-05-16 16:18:48.014807: val_loss -0.97\n",
      "2025-05-16 16:18:48.014842: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 16:18:48.014874: Epoch time: 110.25 s\n",
      "2025-05-16 16:18:48.544686: \n",
      "2025-05-16 16:18:48.544934: Epoch 897\n",
      "2025-05-16 16:18:48.545028: Current learning rate: 0.00129\n",
      "2025-05-16 16:20:37.731644: train_loss -0.9696\n",
      "2025-05-16 16:20:37.731773: val_loss -0.9698\n",
      "2025-05-16 16:20:37.731809: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 16:20:37.731846: Epoch time: 109.19 s\n",
      "2025-05-16 16:20:38.260490: \n",
      "2025-05-16 16:20:38.260768: Epoch 898\n",
      "2025-05-16 16:20:38.260854: Current learning rate: 0.00128\n",
      "2025-05-16 16:22:27.622783: train_loss -0.9693\n",
      "2025-05-16 16:22:27.622914: val_loss -0.9717\n",
      "2025-05-16 16:22:27.623005: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 16:22:27.623047: Epoch time: 109.36 s\n",
      "2025-05-16 16:22:28.154540: \n",
      "2025-05-16 16:22:28.154665: Epoch 899\n",
      "2025-05-16 16:22:28.154737: Current learning rate: 0.00127\n",
      "2025-05-16 16:24:17.362268: train_loss -0.9692\n",
      "2025-05-16 16:24:17.362423: val_loss -0.9714\n",
      "2025-05-16 16:24:17.362499: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 16:24:17.362542: Epoch time: 109.21 s\n",
      "2025-05-16 16:24:18.132230: \n",
      "2025-05-16 16:24:18.132333: Epoch 900\n",
      "2025-05-16 16:24:18.132400: Current learning rate: 0.00126\n",
      "2025-05-16 16:26:07.566746: train_loss -0.97\n",
      "2025-05-16 16:26:07.566911: val_loss -0.9689\n",
      "2025-05-16 16:26:07.566988: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 16:26:07.567031: Epoch time: 109.44 s\n",
      "2025-05-16 16:26:08.094627: \n",
      "2025-05-16 16:26:08.094723: Epoch 901\n",
      "2025-05-16 16:26:08.094795: Current learning rate: 0.00125\n",
      "2025-05-16 16:27:57.456626: train_loss -0.9695\n",
      "2025-05-16 16:27:57.456752: val_loss -0.9688\n",
      "2025-05-16 16:27:57.456786: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 16:27:57.456818: Epoch time: 109.36 s\n",
      "2025-05-16 16:27:57.978639: \n",
      "2025-05-16 16:27:57.979082: Epoch 902\n",
      "2025-05-16 16:27:57.979217: Current learning rate: 0.00124\n",
      "2025-05-16 16:29:47.321242: train_loss -0.9693\n",
      "2025-05-16 16:29:47.321431: val_loss -0.9712\n",
      "2025-05-16 16:29:47.321485: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 16:29:47.321523: Epoch time: 109.34 s\n",
      "2025-05-16 16:29:47.846813: \n",
      "2025-05-16 16:29:47.846920: Epoch 903\n",
      "2025-05-16 16:29:47.846990: Current learning rate: 0.00122\n",
      "2025-05-16 16:31:37.215542: train_loss -0.969\n",
      "2025-05-16 16:31:37.215752: val_loss -0.9704\n",
      "2025-05-16 16:31:37.215788: Pseudo dice [np.float32(0.9878)]\n",
      "2025-05-16 16:31:37.215823: Epoch time: 109.37 s\n",
      "2025-05-16 16:31:37.746648: \n",
      "2025-05-16 16:31:37.746853: Epoch 904\n",
      "2025-05-16 16:31:37.746929: Current learning rate: 0.00121\n",
      "2025-05-16 16:33:26.980844: train_loss -0.9687\n",
      "2025-05-16 16:33:26.981034: val_loss -0.9693\n",
      "2025-05-16 16:33:26.981102: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 16:33:26.981204: Epoch time: 109.23 s\n",
      "2025-05-16 16:33:27.515414: \n",
      "2025-05-16 16:33:27.515883: Epoch 905\n",
      "2025-05-16 16:33:27.516183: Current learning rate: 0.0012\n",
      "2025-05-16 16:35:16.850855: train_loss -0.9694\n",
      "2025-05-16 16:35:16.851021: val_loss -0.9683\n",
      "2025-05-16 16:35:16.851055: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 16:35:16.851088: Epoch time: 109.34 s\n",
      "2025-05-16 16:35:17.373319: \n",
      "2025-05-16 16:35:17.373407: Epoch 906\n",
      "2025-05-16 16:35:17.373480: Current learning rate: 0.00119\n",
      "2025-05-16 16:37:06.522201: train_loss -0.9696\n",
      "2025-05-16 16:37:06.522396: val_loss -0.9676\n",
      "2025-05-16 16:37:06.522429: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 16:37:06.522461: Epoch time: 109.15 s\n",
      "2025-05-16 16:37:07.252508: \n",
      "2025-05-16 16:37:07.252872: Epoch 907\n",
      "2025-05-16 16:37:07.252947: Current learning rate: 0.00118\n",
      "2025-05-16 16:38:56.510463: train_loss -0.9702\n",
      "2025-05-16 16:38:56.510670: val_loss -0.9708\n",
      "2025-05-16 16:38:56.510706: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 16:38:56.510740: Epoch time: 109.26 s\n",
      "2025-05-16 16:38:57.041971: \n",
      "2025-05-16 16:38:57.042082: Epoch 908\n",
      "2025-05-16 16:38:57.042148: Current learning rate: 0.00117\n",
      "2025-05-16 16:40:46.382570: train_loss -0.9691\n",
      "2025-05-16 16:40:46.382764: val_loss -0.9703\n",
      "2025-05-16 16:40:46.382796: Pseudo dice [np.float32(0.9853)]\n",
      "2025-05-16 16:40:46.382828: Epoch time: 109.34 s\n",
      "2025-05-16 16:40:46.912440: \n",
      "2025-05-16 16:40:46.912570: Epoch 909\n",
      "2025-05-16 16:40:46.912647: Current learning rate: 0.00116\n",
      "2025-05-16 16:42:36.227795: train_loss -0.9709\n",
      "2025-05-16 16:42:36.227949: val_loss -0.9699\n",
      "2025-05-16 16:42:36.228100: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 16:42:36.228215: Epoch time: 109.32 s\n",
      "2025-05-16 16:42:36.750151: \n",
      "2025-05-16 16:42:36.750457: Epoch 910\n",
      "2025-05-16 16:42:36.750796: Current learning rate: 0.00115\n",
      "2025-05-16 16:44:26.052734: train_loss -0.9713\n",
      "2025-05-16 16:44:26.052858: val_loss -0.9643\n",
      "2025-05-16 16:44:26.052892: Pseudo dice [np.float32(0.9831)]\n",
      "2025-05-16 16:44:26.052925: Epoch time: 109.3 s\n",
      "2025-05-16 16:44:26.576492: \n",
      "2025-05-16 16:44:26.576733: Epoch 911\n",
      "2025-05-16 16:44:26.576820: Current learning rate: 0.00113\n",
      "2025-05-16 16:46:15.748589: train_loss -0.9706\n",
      "2025-05-16 16:46:15.748703: val_loss -0.9712\n",
      "2025-05-16 16:46:15.748767: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 16:46:15.748888: Epoch time: 109.17 s\n",
      "2025-05-16 16:46:16.268339: \n",
      "2025-05-16 16:46:16.268651: Epoch 912\n",
      "2025-05-16 16:46:16.268806: Current learning rate: 0.00112\n",
      "2025-05-16 16:48:05.627158: train_loss -0.9709\n",
      "2025-05-16 16:48:05.627377: val_loss -0.9711\n",
      "2025-05-16 16:48:05.627412: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 16:48:05.627446: Epoch time: 109.36 s\n",
      "2025-05-16 16:48:06.153884: \n",
      "2025-05-16 16:48:06.153987: Epoch 913\n",
      "2025-05-16 16:48:06.154059: Current learning rate: 0.00111\n",
      "2025-05-16 16:49:55.366264: train_loss -0.971\n",
      "2025-05-16 16:49:55.366476: val_loss -0.9695\n",
      "2025-05-16 16:49:55.366576: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 16:49:55.366736: Epoch time: 109.21 s\n",
      "2025-05-16 16:49:55.896421: \n",
      "2025-05-16 16:49:55.896608: Epoch 914\n",
      "2025-05-16 16:49:55.896687: Current learning rate: 0.0011\n",
      "2025-05-16 16:51:45.146640: train_loss -0.9701\n",
      "2025-05-16 16:51:45.146801: val_loss -0.966\n",
      "2025-05-16 16:51:45.146837: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-16 16:51:45.146871: Epoch time: 109.25 s\n",
      "2025-05-16 16:51:45.670230: \n",
      "2025-05-16 16:51:45.670414: Epoch 915\n",
      "2025-05-16 16:51:45.670503: Current learning rate: 0.00109\n",
      "2025-05-16 16:53:34.989824: train_loss -0.9705\n",
      "2025-05-16 16:53:34.989969: val_loss -0.9697\n",
      "2025-05-16 16:53:34.990051: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 16:53:34.990098: Epoch time: 109.32 s\n",
      "2025-05-16 16:53:35.515941: \n",
      "2025-05-16 16:53:35.516154: Epoch 916\n",
      "2025-05-16 16:53:35.516276: Current learning rate: 0.00108\n",
      "2025-05-16 16:55:24.844270: train_loss -0.97\n",
      "2025-05-16 16:55:24.844390: val_loss -0.9693\n",
      "2025-05-16 16:55:24.844424: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 16:55:24.844459: Epoch time: 109.33 s\n",
      "2025-05-16 16:55:25.360631: \n",
      "2025-05-16 16:55:25.360805: Epoch 917\n",
      "2025-05-16 16:55:25.360886: Current learning rate: 0.00106\n",
      "2025-05-16 16:57:14.544442: train_loss -0.9706\n",
      "2025-05-16 16:57:14.544615: val_loss -0.9675\n",
      "2025-05-16 16:57:14.544704: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 16:57:14.544749: Epoch time: 109.18 s\n",
      "2025-05-16 16:57:15.062258: \n",
      "2025-05-16 16:57:15.062463: Epoch 918\n",
      "2025-05-16 16:57:15.062560: Current learning rate: 0.00105\n",
      "-0.9694-16 16:59:04.294245: train_loss \n",
      "2025-05-16 16:59:04.294439: val_loss -0.9689\n",
      "2025-05-16 16:59:04.294488: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 16:59:04.294527: Epoch time: 109.23 s\n",
      "2025-05-16 16:59:04.836768: \n",
      "2025-05-16 16:59:04.836864: Epoch 919\n",
      "2025-05-16 16:59:04.837075: Current learning rate: 0.00104\n",
      "2025-05-16 17:00:54.326054: train_loss -0.971\n",
      "2025-05-16 17:00:54.326292: val_loss -0.9699\n",
      "2025-05-16 17:00:54.326331: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 17:00:54.326438: Epoch time: 109.49 s\n",
      "2025-05-16 17:00:55.038163: \n",
      "2025-05-16 17:00:55.038293: Epoch 920\n",
      "2025-05-16 17:00:55.038365: Current learning rate: 0.00103\n",
      "2025-05-16 17:02:44.357112: train_loss -0.9715\n",
      "2025-05-16 17:02:44.357396: val_loss -0.9727\n",
      "2025-05-16 17:02:44.357525: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 17:02:44.357603: Epoch time: 109.32 s\n",
      "2025-05-16 17:02:44.884666: \n",
      "2025-05-16 17:02:44.884779: Epoch 921\n",
      "2025-05-16 17:02:44.884887: Current learning rate: 0.00102\n",
      "2025-05-16 17:04:34.233018: train_loss -0.9708\n",
      "2025-05-16 17:04:34.233190: val_loss -0.9677\n",
      "2025-05-16 17:04:34.233223: Pseudo dice [np.float32(0.9851)]\n",
      "2025-05-16 17:04:34.233257: Epoch time: 109.35 s\n",
      "2025-05-16 17:04:34.759639: \n",
      "2025-05-16 17:04:34.759757: Epoch 922\n",
      "2025-05-16 17:04:34.759835: Current learning rate: 0.00101\n",
      "2025-05-16 17:06:24.066306: train_loss -0.9708\n",
      "2025-05-16 17:06:24.066501: val_loss -0.9719\n",
      "2025-05-16 17:06:24.066544: Pseudo dice [np.float32(0.9877)]\n",
      "2025-05-16 17:06:24.066580: Epoch time: 109.31 s\n",
      "2025-05-16 17:06:24.590544: \n",
      "2025-05-16 17:06:24.590760: Epoch 923\n",
      "2025-05-16 17:06:24.590850: Current learning rate: 0.001\n",
      "2025-05-16 17:08:13.806725: train_loss -0.9713\n",
      "2025-05-16 17:08:13.806853: val_loss -0.9701\n",
      "2025-05-16 17:08:13.806887: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 17:08:13.806921: Epoch time: 109.22 s\n",
      "2025-05-16 17:08:14.327404: \n",
      "2025-05-16 17:08:14.327508: Epoch 924\n",
      "2025-05-16 17:08:14.327575: Current learning rate: 0.00098\n",
      "2025-05-16 17:10:03.535980: train_loss -0.9703\n",
      "2025-05-16 17:10:03.536107: val_loss -0.9651\n",
      "2025-05-16 17:10:03.536143: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 17:10:03.536177: Epoch time: 109.21 s\n",
      "2025-05-16 17:10:04.062267: \n",
      "2025-05-16 17:10:04.062489: Epoch 925\n",
      "2025-05-16 17:10:04.062572: Current learning rate: 0.00097\n",
      "2025-05-16 17:11:53.280615: train_loss -0.9709\n",
      "2025-05-16 17:11:53.280764: val_loss -0.9697\n",
      "2025-05-16 17:11:53.280802: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-16 17:11:53.280840: Epoch time: 109.22 s\n",
      "2025-05-16 17:11:53.806921: \n",
      "2025-05-16 17:11:53.807267: Epoch 926\n",
      "2025-05-16 17:11:53.807351: Current learning rate: 0.00096\n",
      "2025-05-16 17:13:43.187197: train_loss -0.971\n",
      "2025-05-16 17:13:43.187421: val_loss -0.9701\n",
      "2025-05-16 17:13:43.187599: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 17:13:43.187707: Epoch time: 109.38 s\n",
      "2025-05-16 17:13:43.713920: \n",
      "2025-05-16 17:13:43.714130: Epoch 927\n",
      "2025-05-16 17:13:43.714214: Current learning rate: 0.00095\n",
      "2025-05-16 17:15:33.144946: train_loss -0.9714\n",
      "2025-05-16 17:15:33.145068: val_loss -0.9682\n",
      "2025-05-16 17:15:33.145103: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 17:15:33.145136: Epoch time: 109.43 s\n",
      "2025-05-16 17:15:33.677328: \n",
      "2025-05-16 17:15:33.677522: Epoch 928\n",
      "2025-05-16 17:15:33.677706: Current learning rate: 0.00094\n",
      "2025-05-16 17:17:22.907922: train_loss -0.9709\n",
      "2025-05-16 17:17:22.908146: val_loss -0.9681\n",
      "2025-05-16 17:17:22.908327: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 17:17:22.908388: Epoch time: 109.23 s\n",
      "2025-05-16 17:17:23.431884: \n",
      "2025-05-16 17:17:23.432069: Epoch 929\n",
      "2025-05-16 17:17:23.432168: Current learning rate: 0.00092\n",
      "2025-05-16 17:19:12.661195: train_loss -0.9712\n",
      "2025-05-16 17:19:12.661479: val_loss -0.969\n",
      "2025-05-16 17:19:12.661521: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 17:19:12.661556: Epoch time: 109.23 s\n",
      "2025-05-16 17:19:13.188793: \n",
      "2025-05-16 17:19:13.189073: Epoch 930\n",
      "2025-05-16 17:19:13.189368: Current learning rate: 0.00091\n",
      "2025-05-16 17:21:02.489689: train_loss -0.9708\n",
      "2025-05-16 17:21:02.489817: val_loss -0.9686\n",
      "2025-05-16 17:21:02.489849: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 17:21:02.489882: Epoch time: 109.3 s\n",
      "2025-05-16 17:21:03.018332: \n",
      "2025-05-16 17:21:03.018711: Epoch 931\n",
      "2025-05-16 17:21:03.018897: Current learning rate: 0.0009\n",
      "2025-05-16 17:22:52.243424: train_loss -0.9701\n",
      "2025-05-16 17:22:52.243680: val_loss -0.9723\n",
      "2025-05-16 17:22:52.243723: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 17:22:52.243757: Epoch time: 109.23 s\n",
      "2025-05-16 17:22:52.776052: \n",
      "2025-05-16 17:22:52.776218: Epoch 932\n",
      "2025-05-16 17:22:52.776331: Current learning rate: 0.00089\n",
      "2025-05-16 17:24:42.105766: train_loss -0.9711\n",
      "2025-05-16 17:24:42.105885: val_loss -0.9705\n",
      "2025-05-16 17:24:42.105918: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 17:24:42.105952: Epoch time: 109.33 s\n",
      "2025-05-16 17:24:42.624382: \n",
      "2025-05-16 17:24:42.624588: Epoch 933\n",
      "2025-05-16 17:24:42.624676: Current learning rate: 0.00088\n",
      "2025-05-16 17:26:31.774588: train_loss -0.9707\n",
      "2025-05-16 17:26:31.774722: val_loss -0.9699\n",
      "2025-05-16 17:26:31.774757: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 17:26:31.774790: Epoch time: 109.15 s\n",
      "2025-05-16 17:26:32.497028: \n",
      "2025-05-16 17:26:32.497138: Epoch 934\n",
      "2025-05-16 17:26:32.497208: Current learning rate: 0.00087\n",
      "2025-05-16 17:28:22.305625: train_loss -0.9716\n",
      "2025-05-16 17:28:22.305744: val_loss -0.9694\n",
      "2025-05-16 17:28:22.305778: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 17:28:22.305813: Epoch time: 109.81 s\n",
      "2025-05-16 17:28:22.829802: \n",
      "2025-05-16 17:28:22.830190: Epoch 935\n",
      "2025-05-16 17:28:22.830333: Current learning rate: 0.00085\n",
      "2025-05-16 17:30:12.045771: train_loss -0.97\n",
      "2025-05-16 17:30:12.046052: val_loss -0.9723\n",
      "2025-05-16 17:30:12.046123: Pseudo dice [np.float32(0.9876)]\n",
      "2025-05-16 17:30:12.046167: Epoch time: 109.22 s\n",
      "2025-05-16 17:30:12.576478: \n",
      "2025-05-16 17:30:12.576677: Epoch 936\n",
      "2025-05-16 17:30:12.576764: Current learning rate: 0.00084\n",
      "2025-05-16 17:32:01.964757: train_loss -0.9711\n",
      "2025-05-16 17:32:01.964923: val_loss -0.9697\n",
      "2025-05-16 17:32:01.965073: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 17:32:01.965152: Epoch time: 109.39 s\n",
      "2025-05-16 17:32:02.496758: \n",
      "2025-05-16 17:32:02.496878: Epoch 937\n",
      "2025-05-16 17:32:02.496969: Current learning rate: 0.00083\n",
      "2025-05-16 17:33:51.698691: train_loss -0.9703\n",
      "2025-05-16 17:33:51.698862: val_loss -0.9736\n",
      "2025-05-16 17:33:51.698905: Pseudo dice [np.float32(0.9877)]\n",
      "2025-05-16 17:33:51.698976: Epoch time: 109.2 s\n",
      "2025-05-16 17:33:52.227284: \n",
      "2025-05-16 17:33:52.227553: Epoch 938\n",
      "2025-05-16 17:33:52.227670: Current learning rate: 0.00082\n",
      "2025-05-16 17:35:41.551780: train_loss -0.9714\n",
      "2025-05-16 17:35:41.551924: val_loss -0.9664\n",
      "2025-05-16 17:35:41.551957: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 17:35:41.551992: Epoch time: 109.33 s\n",
      "2025-05-16 17:35:42.077348: \n",
      "2025-05-16 17:35:42.077555: Epoch 939\n",
      "2025-05-16 17:35:42.077675: Current learning rate: 0.00081\n",
      "2025-05-16 17:37:31.385718: train_loss -0.9711\n",
      "2025-05-16 17:37:31.385861: val_loss -0.9696\n",
      "2025-05-16 17:37:31.385898: Pseudo dice [np.float32(0.9865)]\n",
      "2025-05-16 17:37:31.385935: Epoch time: 109.31 s\n",
      "2025-05-16 17:37:31.907992: \n",
      "2025-05-16 17:37:31.908205: Epoch 940\n",
      "2025-05-16 17:37:31.908350: Current learning rate: 0.00079\n",
      "2025-05-16 17:39:21.174898: train_loss -0.9709\n",
      "2025-05-16 17:39:21.175127: val_loss -0.9699\n",
      "2025-05-16 17:39:21.175245: Pseudo dice [np.float32(0.9873)]\n",
      "2025-05-16 17:39:21.175408: Epoch time: 109.27 s\n",
      "2025-05-16 17:39:21.701890: \n",
      "2025-05-16 17:39:21.701993: Epoch 941\n",
      "2025-05-16 17:39:21.702062: Current learning rate: 0.00078\n",
      "2025-05-16 17:41:11.005789: train_loss -0.971\n",
      "2025-05-16 17:41:11.005905: val_loss -0.9682\n",
      "2025-05-16 17:41:11.005938: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 17:41:11.005970: Epoch time: 109.3 s\n",
      "2025-05-16 17:41:11.531574: \n",
      "2025-05-16 17:41:11.531679: Epoch 942\n",
      "2025-05-16 17:41:11.531765: Current learning rate: 0.00077\n",
      "2025-05-16 17:43:00.862691: train_loss -0.9709\n",
      "2025-05-16 17:43:00.862907: val_loss -0.9666\n",
      "2025-05-16 17:43:00.862998: Pseudo dice [np.float32(0.9847)]\n",
      "2025-05-16 17:43:00.863174: Epoch time: 109.33 s\n",
      "2025-05-16 17:43:01.400061: \n",
      "2025-05-16 17:43:01.400161: Epoch 943\n",
      "2025-05-16 17:43:01.400253: Current learning rate: 0.00076\n",
      "-0.9706-16 17:44:50.561556: train_loss \n",
      "2025-05-16 17:44:50.561788: val_loss -0.9709\n",
      "2025-05-16 17:44:50.561838: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 17:44:50.561877: Epoch time: 109.16 s\n",
      "2025-05-16 17:44:51.088545: \n",
      "2025-05-16 17:44:51.088752: Epoch 944\n",
      "2025-05-16 17:44:51.088845: Current learning rate: 0.00075\n",
      "2025-05-16 17:46:40.428054: train_loss -0.9699\n",
      "2025-05-16 17:46:40.428400: val_loss -0.9723\n",
      "2025-05-16 17:46:40.428443: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-16 17:46:40.428478: Epoch time: 109.34 s\n",
      "2025-05-16 17:46:40.949110: \n",
      "2025-05-16 17:46:40.949308: Epoch 945\n",
      "2025-05-16 17:46:40.949398: Current learning rate: 0.00074\n",
      "2025-05-16 17:48:30.388418: train_loss -0.9708\n",
      "2025-05-16 17:48:30.388651: val_loss -0.9688\n",
      "2025-05-16 17:48:30.388754: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 17:48:30.388796: Epoch time: 109.44 s\n",
      "2025-05-16 17:48:30.910369: \n",
      "2025-05-16 17:48:30.910533: Epoch 946\n",
      "2025-05-16 17:48:30.910621: Current learning rate: 0.00072\n",
      "2025-05-16 17:50:20.115991: train_loss -0.9715\n",
      "2025-05-16 17:50:20.116166: val_loss -0.9686\n",
      "2025-05-16 17:50:20.116199: Pseudo dice [np.float32(0.9866)]\n",
      "2025-05-16 17:50:20.116235: Epoch time: 109.21 s\n",
      "2025-05-16 17:50:20.832659: \n",
      "2025-05-16 17:50:20.832819: Epoch 947\n",
      "2025-05-16 17:50:20.832908: Current learning rate: 0.00071\n",
      "2025-05-16 17:52:10.194421: train_loss -0.9705\n",
      "2025-05-16 17:52:10.194637: val_loss -0.9675\n",
      "2025-05-16 17:52:10.194731: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 17:52:10.194777: Epoch time: 109.36 s\n",
      "2025-05-16 17:52:10.727482: \n",
      "2025-05-16 17:52:10.727714: Epoch 948\n",
      "2025-05-16 17:52:10.727808: Current learning rate: 0.0007\n",
      "2025-05-16 17:54:00.073369: train_loss -0.9709\n",
      "2025-05-16 17:54:00.073521: val_loss -0.97\n",
      "2025-05-16 17:54:00.073557: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 17:54:00.073593: Epoch time: 109.35 s\n",
      "2025-05-16 17:54:00.594609: \n",
      "2025-05-16 17:54:00.594720: Epoch 949\n",
      "2025-05-16 17:54:00.594798: Current learning rate: 0.00069\n",
      "2025-05-16 17:55:50.005597: train_loss -0.9709\n",
      "2025-05-16 17:55:50.005725: val_loss -0.9675\n",
      "2025-05-16 17:55:50.005760: Pseudo dice [np.float32(0.9857)]\n",
      "2025-05-16 17:55:50.005794: Epoch time: 109.41 s\n",
      "2025-05-16 17:55:50.771736: \n",
      "2025-05-16 17:55:50.771953: Epoch 950\n",
      "2025-05-16 17:55:50.772047: Current learning rate: 0.00067\n",
      "2025-05-16 17:57:40.081718: train_loss -0.972\n",
      "2025-05-16 17:57:40.081875: val_loss -0.9673\n",
      "2025-05-16 17:57:40.081908: Pseudo dice [np.float32(0.986)]\n",
      "2025-05-16 17:57:40.081942: Epoch time: 109.31 s\n",
      "2025-05-16 17:57:40.610017: \n",
      "2025-05-16 17:57:40.610198: Epoch 951\n",
      "2025-05-16 17:57:40.610296: Current learning rate: 0.00066\n",
      "2025-05-16 17:59:29.960247: train_loss -0.9711\n",
      "2025-05-16 17:59:29.960371: val_loss -0.9677\n",
      "2025-05-16 17:59:29.960405: Pseudo dice [np.float32(0.9858)]\n",
      "2025-05-16 17:59:29.960438: Epoch time: 109.35 s\n",
      "2025-05-16 17:59:30.478625: \n",
      "2025-05-16 17:59:30.478710: Epoch 952\n",
      "2025-05-16 17:59:30.478773: Current learning rate: 0.00065\n",
      "2025-05-16 18:01:19.881601: train_loss -0.9712\n",
      "2025-05-16 18:01:19.881781: val_loss -0.9693\n",
      "2025-05-16 18:01:19.881816: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 18:01:19.881850: Epoch time: 109.4 s\n",
      "2025-05-16 18:01:20.407559: \n",
      "2025-05-16 18:01:20.407653: Epoch 953\n",
      "2025-05-16 18:01:20.407722: Current learning rate: 0.00064\n",
      "2025-05-16 18:03:09.656601: train_loss -0.9714\n",
      "2025-05-16 18:03:09.656759: val_loss -0.97\n",
      "2025-05-16 18:03:09.656788: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 18:03:09.656821: Epoch time: 109.25 s\n",
      "2025-05-16 18:03:10.186671: \n",
      "2025-05-16 18:03:10.186762: Epoch 954\n",
      "2025-05-16 18:03:10.186829: Current learning rate: 0.00063\n",
      "2025-05-16 18:04:59.443558: train_loss -0.9727\n",
      "2025-05-16 18:04:59.443701: val_loss -0.975\n",
      "2025-05-16 18:04:59.443735: Pseudo dice [np.float32(0.9891)]\n",
      "2025-05-16 18:04:59.443766: Epoch time: 109.26 s\n",
      "2025-05-16 18:04:59.971043: \n",
      "2025-05-16 18:04:59.971179: Epoch 955\n",
      "2025-05-16 18:04:59.971248: Current learning rate: 0.00061\n",
      "2025-05-16 18:06:49.190108: train_loss -0.9709\n",
      "2025-05-16 18:06:49.190234: val_loss -0.9671\n",
      "2025-05-16 18:06:49.190267: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 18:06:49.190312: Epoch time: 109.22 s\n",
      "2025-05-16 18:06:49.715147: \n",
      "2025-05-16 18:06:49.715264: Epoch 956\n",
      "2025-05-16 18:06:49.715353: Current learning rate: 0.0006\n",
      "2025-05-16 18:08:38.978517: train_loss -0.9715\n",
      "2025-05-16 18:08:38.978653: val_loss -0.9729\n",
      "2025-05-16 18:08:38.978685: Pseudo dice [np.float32(0.9882)]\n",
      "2025-05-16 18:08:38.978717: Epoch time: 109.26 s\n",
      "2025-05-16 18:08:39.514081: \n",
      "2025-05-16 18:08:39.514445: Epoch 957\n",
      "2025-05-16 18:08:39.514595: Current learning rate: 0.00059\n",
      "2025-05-16 18:10:28.843711: train_loss -0.9705\n",
      "2025-05-16 18:10:28.843828: val_loss -0.9704\n",
      "2025-05-16 18:10:28.843860: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 18:10:28.843893: Epoch time: 109.33 s\n",
      "2025-05-16 18:10:29.379390: \n",
      "2025-05-16 18:10:29.379521: Epoch 958\n",
      "2025-05-16 18:10:29.379589: Current learning rate: 0.00058\n",
      "2025-05-16 18:12:18.655581: train_loss -0.9715\n",
      "2025-05-16 18:12:18.655868: val_loss -0.9651\n",
      "2025-05-16 18:12:18.655909: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 18:12:18.655945: Epoch time: 109.28 s\n",
      "2025-05-16 18:12:19.185469: \n",
      "2025-05-16 18:12:19.185717: Epoch 959\n",
      "2025-05-16 18:12:19.185875: Current learning rate: 0.00056\n",
      "2025-05-16 18:14:08.345526: train_loss -0.9717\n",
      "2025-05-16 18:14:08.345819: val_loss -0.9691\n",
      "2025-05-16 18:14:08.345855: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 18:14:08.345887: Epoch time: 109.16 s\n",
      "2025-05-16 18:14:09.071960: \n",
      "2025-05-16 18:14:09.072160: Epoch 960\n",
      "2025-05-16 18:14:09.072266: Current learning rate: 0.00055\n",
      "2025-05-16 18:15:58.364596: train_loss -0.9704\n",
      "2025-05-16 18:15:58.364711: val_loss -0.9729\n",
      "2025-05-16 18:15:58.364743: Pseudo dice [np.float32(0.9878)]\n",
      "2025-05-16 18:15:58.364775: Epoch time: 109.29 s\n",
      "2025-05-16 18:15:58.903821: \n",
      "2025-05-16 18:15:58.904037: Epoch 961\n",
      "2025-05-16 18:15:58.904291: Current learning rate: 0.00054\n",
      "2025-05-16 18:17:48.246897: train_loss -0.9727\n",
      "2025-05-16 18:17:48.247014: val_loss -0.9734\n",
      "2025-05-16 18:17:48.247224: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 18:17:48.247392: Epoch time: 109.34 s\n",
      "2025-05-16 18:17:48.780084: \n",
      "2025-05-16 18:17:48.780184: Epoch 962\n",
      "2025-05-16 18:17:48.780252: Current learning rate: 0.00053\n",
      "2025-05-16 18:19:38.091843: train_loss -0.972\n",
      "2025-05-16 18:19:38.091964: val_loss -0.9709\n",
      "2025-05-16 18:19:38.091998: Pseudo dice [np.float32(0.9867)]\n",
      "2025-05-16 18:19:38.092031: Epoch time: 109.31 s\n",
      "2025-05-16 18:19:38.616019: \n",
      "2025-05-16 18:19:38.616313: Epoch 963\n",
      "2025-05-16 18:19:38.616399: Current learning rate: 0.00051\n",
      "2025-05-16 18:21:27.850180: train_loss -0.9729\n",
      "2025-05-16 18:21:27.850404: val_loss -0.9673\n",
      "2025-05-16 18:21:27.850440: Pseudo dice [np.float32(0.9852)]\n",
      "2025-05-16 18:21:27.850473: Epoch time: 109.23 s\n",
      "2025-05-16 18:21:28.369812: \n",
      "2025-05-16 18:21:28.369913: Epoch 964\n",
      "2025-05-16 18:21:28.369999: Current learning rate: 0.0005\n",
      "2025-05-16 18:23:17.557237: train_loss -0.9712\n",
      "2025-05-16 18:23:17.557406: val_loss -0.9724\n",
      "2025-05-16 18:23:17.557474: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 18:23:17.557515: Epoch time: 109.19 s\n",
      "2025-05-16 18:23:18.083085: \n",
      "2025-05-16 18:23:18.083187: Epoch 965\n",
      "2025-05-16 18:23:18.083256: Current learning rate: 0.00049\n",
      "2025-05-16 18:25:07.384669: train_loss -0.972\n",
      "2025-05-16 18:25:07.384844: val_loss -0.9681\n",
      "2025-05-16 18:25:07.384879: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 18:25:07.384913: Epoch time: 109.3 s\n",
      "2025-05-16 18:25:07.912004: \n",
      "2025-05-16 18:25:07.912100: Epoch 966\n",
      "2025-05-16 18:25:07.912169: Current learning rate: 0.00048\n",
      "2025-05-16 18:26:57.033475: train_loss -0.97\n",
      "2025-05-16 18:26:57.033585: val_loss -0.9686\n",
      "2025-05-16 18:26:57.033618: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 18:26:57.033755: Epoch time: 109.12 s\n",
      "2025-05-16 18:26:57.561262: \n",
      "2025-05-16 18:26:57.561358: Epoch 967\n",
      "2025-05-16 18:26:57.561424: Current learning rate: 0.00046\n",
      "2025-05-16 18:28:46.837339: train_loss -0.9717\n",
      "2025-05-16 18:28:46.837457: val_loss -0.968\n",
      "2025-05-16 18:28:46.837489: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 18:28:46.837522: Epoch time: 109.28 s\n",
      "2025-05-16 18:28:47.368046: \n",
      "2025-05-16 18:28:47.368303: Epoch 968\n",
      "2025-05-16 18:28:47.368453: Current learning rate: 0.00045\n",
      "2025-05-16 18:30:36.474134: train_loss -0.9714\n",
      "2025-05-16 18:30:36.474307: val_loss -0.9718\n",
      "2025-05-16 18:30:36.474510: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 18:30:36.474578: Epoch time: 109.11 s\n",
      "2025-05-16 18:30:37.014123: \n",
      "2025-05-16 18:30:37.014557: Epoch 969\n",
      "2025-05-16 18:30:37.014736: Current learning rate: 0.00044\n",
      "2025-05-16 18:32:25.973818: train_loss -0.9727\n",
      "2025-05-16 18:32:25.973942: val_loss -0.9672\n",
      "2025-05-16 18:32:25.973975: Pseudo dice [np.float32(0.9845)]\n",
      "2025-05-16 18:32:25.974007: Epoch time: 108.96 s\n",
      "2025-05-16 18:32:26.505614: \n",
      "2025-05-16 18:32:26.505715: Epoch 970\n",
      "2025-05-16 18:32:26.505793: Current learning rate: 0.00043\n",
      "2025-05-16 18:34:15.409937: train_loss -0.9716\n",
      "2025-05-16 18:34:15.410077: val_loss -0.9717\n",
      "2025-05-16 18:34:15.410237: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 18:34:15.410349: Epoch time: 108.9 s\n",
      "2025-05-16 18:34:15.943475: \n",
      "2025-05-16 18:34:15.943562: Epoch 971\n",
      "2025-05-16 18:34:15.943630: Current learning rate: 0.00041\n",
      "2025-05-16 18:36:04.801621: train_loss -0.9724\n",
      "2025-05-16 18:36:04.801774: val_loss -0.9678\n",
      "2025-05-16 18:36:04.801807: Pseudo dice [np.float32(0.9855)]\n",
      "2025-05-16 18:36:04.801838: Epoch time: 108.86 s\n",
      "2025-05-16 18:36:05.331369: \n",
      "2025-05-16 18:36:05.331453: Epoch 972\n",
      "2025-05-16 18:36:05.331518: Current learning rate: 0.0004\n",
      "2025-05-16 18:37:54.134336: train_loss -0.9718\n",
      "2025-05-16 18:37:54.134450: val_loss -0.9701\n",
      "2025-05-16 18:37:54.134482: Pseudo dice [np.float32(0.9863)]\n",
      "2025-05-16 18:37:54.134514: Epoch time: 108.8 s\n",
      "2025-05-16 18:37:54.853335: \n",
      "2025-05-16 18:37:54.853534: Epoch 973\n",
      "2025-05-16 18:37:54.853629: Current learning rate: 0.00039\n",
      "2025-05-16 18:39:43.803048: train_loss -0.9718\n",
      "2025-05-16 18:39:43.803167: val_loss -0.9716\n",
      "2025-05-16 18:39:43.803201: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 18:39:43.803236: Epoch time: 108.95 s\n",
      "2025-05-16 18:39:44.335493: \n",
      "2025-05-16 18:39:44.335621: Epoch 974\n",
      "2025-05-16 18:39:44.335702: Current learning rate: 0.00037\n",
      "2025-05-16 18:41:33.285637: train_loss -0.9718\n",
      "2025-05-16 18:41:33.285765: val_loss -0.9703\n",
      "2025-05-16 18:41:33.285799: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 18:41:33.285832: Epoch time: 108.95 s\n",
      "2025-05-16 18:41:33.814014: \n",
      "2025-05-16 18:41:33.814109: Epoch 975\n",
      "2025-05-16 18:41:33.814176: Current learning rate: 0.00036\n",
      "2025-05-16 18:43:22.727399: train_loss -0.972\n",
      "2025-05-16 18:43:22.727517: val_loss -0.971\n",
      "2025-05-16 18:43:22.727551: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-16 18:43:22.727591: Epoch time: 108.91 s\n",
      "2025-05-16 18:43:23.258034: \n",
      "2025-05-16 18:43:23.258191: Epoch 976\n",
      "2025-05-16 18:43:23.258266: Current learning rate: 0.00035\n",
      "2025-05-16 18:45:12.081210: train_loss -0.9715\n",
      "2025-05-16 18:45:12.081327: val_loss -0.9708\n",
      "2025-05-16 18:45:12.081359: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 18:45:12.081392: Epoch time: 108.82 s\n",
      "2025-05-16 18:45:12.611591: \n",
      "2025-05-16 18:45:12.611769: Epoch 977\n",
      "2025-05-16 18:45:12.611854: Current learning rate: 0.00034\n",
      "2025-05-16 18:47:01.521250: train_loss -0.9715\n",
      "2025-05-16 18:47:01.521378: val_loss -0.9745\n",
      "2025-05-16 18:47:01.521422: Pseudo dice [np.float32(0.9885)]\n",
      "2025-05-16 18:47:01.521462: Epoch time: 108.91 s\n",
      "2025-05-16 18:47:02.049119: \n",
      "2025-05-16 18:47:02.049228: Epoch 978\n",
      "2025-05-16 18:47:02.049304: Current learning rate: 0.00032\n",
      "2025-05-16 18:48:50.998899: train_loss -0.9721\n",
      "2025-05-16 18:48:50.999025: val_loss -0.9695\n",
      "2025-05-16 18:48:50.999059: Pseudo dice [np.float32(0.9868)]\n",
      "2025-05-16 18:48:50.999092: Epoch time: 108.95 s\n",
      "2025-05-16 18:48:51.528935: \n",
      "2025-05-16 18:48:51.529090: Epoch 979\n",
      "2025-05-16 18:48:51.529179: Current learning rate: 0.00031\n",
      "2025-05-16 18:50:40.368903: train_loss -0.9726\n",
      "2025-05-16 18:50:40.369079: val_loss -0.9739\n",
      "2025-05-16 18:50:40.369113: Pseudo dice [np.float32(0.9884)]\n",
      "2025-05-16 18:50:40.369145: Epoch time: 108.84 s\n",
      "2025-05-16 18:50:40.897609: \n",
      "2025-05-16 18:50:40.897709: Epoch 980\n",
      "2025-05-16 18:50:40.897781: Current learning rate: 0.0003\n",
      "2025-05-16 18:52:29.696974: train_loss -0.9717\n",
      "2025-05-16 18:52:29.697094: val_loss -0.9709\n",
      "2025-05-16 18:52:29.697125: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 18:52:29.697158: Epoch time: 108.8 s\n",
      "2025-05-16 18:52:30.225634: \n",
      "2025-05-16 18:52:30.225896: Epoch 981\n",
      "2025-05-16 18:52:30.226013: Current learning rate: 0.00028\n",
      "2025-05-16 18:54:19.156867: train_loss -0.9722\n",
      "2025-05-16 18:54:19.157005: val_loss -0.968\n",
      "2025-05-16 18:54:19.157181: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 18:54:19.157272: Epoch time: 108.93 s\n",
      "2025-05-16 18:54:19.695188: \n",
      "2025-05-16 18:54:19.695446: Epoch 982\n",
      "2025-05-16 18:54:19.695538: Current learning rate: 0.00027\n",
      "2025-05-16 18:56:08.679880: train_loss -0.9718\n",
      "2025-05-16 18:56:08.679995: val_loss -0.9673\n",
      "2025-05-16 18:56:08.680122: Pseudo dice [np.float32(0.9862)]\n",
      "2025-05-16 18:56:08.680168: Epoch time: 108.99 s\n",
      "2025-05-16 18:56:09.216224: \n",
      "2025-05-16 18:56:09.216612: Epoch 983\n",
      "2025-05-16 18:56:09.216790: Current learning rate: 0.00026\n",
      "2025-05-16 18:57:58.020134: train_loss -0.9714\n",
      "2025-05-16 18:57:58.020257: val_loss -0.9712\n",
      "2025-05-16 18:57:58.020292: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 18:57:58.020326: Epoch time: 108.8 s\n",
      "2025-05-16 18:57:58.555850: \n",
      "2025-05-16 18:57:58.555990: Epoch 984\n",
      "2025-05-16 18:57:58.556072: Current learning rate: 0.00024\n",
      "2025-05-16 18:59:47.377425: train_loss -0.9715\n",
      "2025-05-16 18:59:47.377551: val_loss -0.9676\n",
      "2025-05-16 18:59:47.377603: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 18:59:47.377639: Epoch time: 108.82 s\n",
      "2025-05-16 18:59:48.093826: \n",
      "2025-05-16 18:59:48.093944: Epoch 985\n",
      "2025-05-16 18:59:48.094028: Current learning rate: 0.00023\n",
      "2025-05-16 19:01:36.973864: train_loss -0.9729\n",
      "2025-05-16 19:01:36.973988: val_loss -0.9674\n",
      "2025-05-16 19:01:36.974026: Pseudo dice [np.float32(0.9856)]\n",
      "2025-05-16 19:01:36.974059: Epoch time: 108.88 s\n",
      "2025-05-16 19:01:37.519959: \n",
      "2025-05-16 19:01:37.520194: Epoch 986\n",
      "2025-05-16 19:01:37.520314: Current learning rate: 0.00021\n",
      "2025-05-16 19:03:26.466009: train_loss -0.9723\n",
      "2025-05-16 19:03:26.466121: val_loss -0.9666\n",
      "2025-05-16 19:03:26.466155: Pseudo dice [np.float32(0.9843)]\n",
      "2025-05-16 19:03:26.466188: Epoch time: 108.95 s\n",
      "2025-05-16 19:03:27.005340: \n",
      "2025-05-16 19:03:27.005519: Epoch 987\n",
      "2025-05-16 19:03:27.005640: Current learning rate: 0.0002\n",
      "2025-05-16 19:05:15.910439: train_loss -0.9724\n",
      "2025-05-16 19:05:15.910553: val_loss -0.971\n",
      "2025-05-16 19:05:15.910585: Pseudo dice [np.float32(0.9861)]\n",
      "2025-05-16 19:05:15.910616: Epoch time: 108.91 s\n",
      "2025-05-16 19:05:16.449383: \n",
      "2025-05-16 19:05:16.449481: Epoch 988\n",
      "2025-05-16 19:05:16.449545: Current learning rate: 0.00019\n",
      "2025-05-16 19:07:05.301522: train_loss -0.9719\n",
      "2025-05-16 19:07:05.301887: val_loss -0.9669\n",
      "2025-05-16 19:07:05.301954: Pseudo dice [np.float32(0.9854)]\n",
      "2025-05-16 19:07:05.301994: Epoch time: 108.85 s\n",
      "2025-05-16 19:07:05.837213: \n",
      "2025-05-16 19:07:05.837306: Epoch 989\n",
      "2025-05-16 19:07:05.837378: Current learning rate: 0.00017\n",
      "2025-05-16 19:08:54.771872: train_loss -0.9715\n",
      "2025-05-16 19:08:54.772041: val_loss -0.971\n",
      "2025-05-16 19:08:54.772072: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 19:08:54.772103: Epoch time: 108.94 s\n",
      "2025-05-16 19:08:55.295480: \n",
      "2025-05-16 19:08:55.295630: Epoch 990\n",
      "2025-05-16 19:08:55.295694: Current learning rate: 0.00016\n",
      "2025-05-16 19:10:44.170874: train_loss -0.9728\n",
      "2025-05-16 19:10:44.170998: val_loss -0.9673\n",
      "2025-05-16 19:10:44.171032: Pseudo dice [np.float32(0.985)]\n",
      "2025-05-16 19:10:44.171071: Epoch time: 108.88 s\n",
      "2025-05-16 19:10:44.711234: \n",
      "2025-05-16 19:10:44.711334: Epoch 991\n",
      "2025-05-16 19:10:44.711408: Current learning rate: 0.00014\n",
      "2025-05-16 19:12:33.541259: train_loss -0.9729\n",
      "2025-05-16 19:12:33.541388: val_loss -0.9723\n",
      "2025-05-16 19:12:33.541424: Pseudo dice [np.float32(0.9875)]\n",
      "2025-05-16 19:12:33.541457: Epoch time: 108.83 s\n",
      "2025-05-16 19:12:34.071532: \n",
      "2025-05-16 19:12:34.071656: Epoch 992\n",
      "2025-05-16 19:12:34.071725: Current learning rate: 0.00013\n",
      "2025-05-16 19:14:22.925147: train_loss -0.9723\n",
      "2025-05-16 19:14:22.925318: val_loss -0.9694\n",
      "2025-05-16 19:14:22.925351: Pseudo dice [np.float32(0.9864)]\n",
      "2025-05-16 19:14:22.925386: Epoch time: 108.85 s\n",
      "2025-05-16 19:14:23.459458: \n",
      "2025-05-16 19:14:23.459626: Epoch 993\n",
      "2025-05-16 19:14:23.459719: Current learning rate: 0.00011\n",
      "2025-05-16 19:16:12.277657: train_loss -0.9715\n",
      "2025-05-16 19:16:12.277907: val_loss -0.9741\n",
      "2025-05-16 19:16:12.278097: Pseudo dice [np.float32(0.9884)]\n",
      "2025-05-16 19:16:12.278205: Epoch time: 108.82 s\n",
      "2025-05-16 19:16:12.809750: \n",
      "2025-05-16 19:16:12.809855: Epoch 994\n",
      "2025-05-16 19:16:12.809938: Current learning rate: 0.0001\n",
      "2025-05-16 19:18:01.655917: train_loss -0.9727\n",
      "2025-05-16 19:18:01.656034: val_loss -0.9718\n",
      "2025-05-16 19:18:01.656068: Pseudo dice [np.float32(0.9871)]\n",
      "2025-05-16 19:18:01.656102: Epoch time: 108.85 s\n",
      "2025-05-16 19:18:02.182079: \n",
      "2025-05-16 19:18:02.182169: Epoch 995\n",
      "2025-05-16 19:18:02.182338: Current learning rate: 8e-05\n",
      "2025-05-16 19:19:51.023681: train_loss -0.9722\n",
      "2025-05-16 19:19:51.023866: val_loss -0.9686\n",
      "2025-05-16 19:19:51.023907: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 19:19:51.023943: Epoch time: 108.84 s\n",
      "2025-05-16 19:19:51.551576: \n",
      "2025-05-16 19:19:51.551662: Epoch 996\n",
      "2025-05-16 19:19:51.551731: Current learning rate: 7e-05\n",
      "2025-05-16 19:21:40.497968: train_loss -0.9731\n",
      "2025-05-16 19:21:40.498306: val_loss -0.9683\n",
      "2025-05-16 19:21:40.498346: Pseudo dice [np.float32(0.9859)]\n",
      "2025-05-16 19:21:40.498382: Epoch time: 108.95 s\n",
      "2025-05-16 19:21:41.022135: \n",
      "2025-05-16 19:21:41.022302: Epoch 997\n",
      "2025-05-16 19:21:41.022384: Current learning rate: 5e-05\n",
      "2025-05-16 19:23:30.050349: train_loss -0.9726\n",
      "2025-05-16 19:23:30.050484: val_loss -0.9703\n",
      "2025-05-16 19:23:30.050516: Pseudo dice [np.float32(0.9874)]\n",
      "2025-05-16 19:23:30.050550: Epoch time: 109.03 s\n",
      "2025-05-16 19:23:30.766350: \n",
      "2025-05-16 19:23:30.766570: Epoch 998\n",
      "2025-05-16 19:23:30.766652: Current learning rate: 4e-05\n",
      "2025-05-16 19:25:19.671123: train_loss -0.9731\n",
      "2025-05-16 19:25:19.671336: val_loss -0.971\n",
      "2025-05-16 19:25:19.671377: Pseudo dice [np.float32(0.9869)]\n",
      "2025-05-16 19:25:19.671411: Epoch time: 108.91 s\n",
      "2025-05-16 19:25:20.206071: \n",
      "2025-05-16 19:25:20.206208: Epoch 999\n",
      "2025-05-16 19:25:20.206282: Current learning rate: 2e-05\n",
      "2025-05-16 19:27:09.137076: train_loss -0.9722\n",
      "2025-05-16 19:27:09.137196: val_loss -0.9702\n",
      "2025-05-16 19:27:09.137228: Pseudo dice [np.float32(0.9872)]\n",
      "2025-05-16 19:27:09.137262: Epoch time: 108.93 s\n",
      "2025-05-16 19:27:09.949565: Training done.\n",
      "2025-05-16 19:27:09.955140: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-16 19:27:09.955372: The split file contains 5 splits.\n",
      "2025-05-16 19:27:09.955404: Desired fold for training: 2\n",
      "2025-05-16 19:27:09.955418: This split has 29 training and 7 validation cases.\n",
      "2025-05-16 19:27:09.955476: predicting LCTSC-Train-S1-007\n",
      "2025-05-16 19:27:09.955986: LCTSC-Train-S1-007, shape torch.Size([1, 178, 512, 512]), rank 0\n",
      "2025-05-16 19:28:28.018824: predicting LCTSC-Train-S1-008\n",
      "2025-05-16 19:28:28.021985: LCTSC-Train-S1-008, shape torch.Size([1, 161, 512, 512]), rank 0\n",
      "2025-05-16 19:29:28.963237: predicting LCTSC-Train-S1-011\n",
      "2025-05-16 19:29:28.965895: LCTSC-Train-S1-011, shape torch.Size([1, 194, 512, 512]), rank 0\n",
      "2025-05-16 19:30:40.024430: predicting LCTSC-Train-S1-012\n",
      "2025-05-16 19:30:40.027028: LCTSC-Train-S1-012, shape torch.Size([1, 157, 512, 512]), rank 0\n",
      "2025-05-16 19:31:39.230340: predicting LCTSC-Train-S2-002\n",
      "2025-05-16 19:31:39.233410: LCTSC-Train-S2-002, shape torch.Size([1, 152, 512, 512]), rank 0\n",
      "2025-05-16 19:32:38.337162: predicting LCTSC-Train-S3-003\n",
      "2025-05-16 19:32:38.339527: LCTSC-Train-S3-003, shape torch.Size([1, 162, 717, 717]), rank 0\n",
      "2025-05-16 19:34:34.656051: predicting LCTSC-Train-S3-009\n",
      "2025-05-16 19:34:34.661492: LCTSC-Train-S3-009, shape torch.Size([1, 138, 614, 614]), rank 0\n",
      "2025-05-16 19:35:52.118710: Validation complete\n",
      "2025-05-16 19:35:52.118777: Mean Validation Dice:  0.9862769373110419\n",
      "🔁 Running Fold 3: nnUNetv2_train 3 3d_fullres 3 --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "WARNING: Cannot continue training because there seems to be no checkpoint available to continue from. Starting a new training...\n",
      "2025-05-16 19:35:56.423051: do_dummy_2d_data_aug: True\n",
      "2025-05-16 19:35:56.423311: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-16 19:35:56.423398: The split file contains 5 splits.\n",
      "2025-05-16 19:35:56.423426: Desired fold for training: 3\n",
      "2025-05-16 19:35:56.423443: This split has 29 training and 7 validation cases.\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pin_memory on device 0\n",
      "2025-05-16 19:36:02.879629: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [56, 192, 192], 'median_image_size_in_voxels': [162.5, 512.0, 512.0], 'spacing': [2.5, 0.9765625, 0.9765625], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset003_Lung_only', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.9765625, 0.9765625], 'original_median_shape_after_transp': [156, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1910.0, 'mean': -742.67333984375, 'median': -794.0, 'min': -1023.0, 'percentile_00_5': -981.0, 'percentile_99_5': -51.0, 'std': 175.66612243652344}}} \n",
      "\n",
      "2025-05-16 19:36:03.658951: unpacking dataset...\n",
      "2025-05-16 19:36:07.695984: unpacking done...\n",
      "2025-05-16 19:36:07.696888: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-05-16 19:36:07.699733: \n",
      "2025-05-16 19:36:07.699856: Epoch 0\n",
      "2025-05-16 19:36:07.699937: Current learning rate: 0.01\n",
      "2025-05-16 19:38:15.603526: train_loss -0.4178\n",
      "2025-05-16 19:38:15.603967: val_loss -0.6964\n",
      "2025-05-16 19:38:15.604073: Pseudo dice [np.float32(0.8619)]\n",
      "2025-05-16 19:38:15.604123: Epoch time: 127.9 s\n",
      "2025-05-16 19:38:15.604147: Yayy! New best EMA pseudo Dice: 0.8618999719619751\n",
      "2025-05-16 19:38:16.257756: \n",
      "2025-05-16 19:38:16.257895: Epoch 1\n",
      "2025-05-16 19:38:16.257962: Current learning rate: 0.00999\n",
      "2025-05-16 19:40:05.308027: train_loss -0.672\n",
      "2025-05-16 19:40:05.308149: val_loss -0.7524\n",
      "2025-05-16 19:40:05.308181: Pseudo dice [np.float32(0.9093)]\n",
      "2025-05-16 19:40:05.308234: Epoch time: 109.05 s\n",
      "2025-05-16 19:40:05.308324: Yayy! New best EMA pseudo Dice: 0.8666999936103821\n",
      "2025-05-16 19:40:06.172719: \n",
      "2025-05-16 19:40:06.173026: Epoch 2\n",
      "2025-05-16 19:40:06.173097: Current learning rate: 0.00998\n",
      "2025-05-16 19:41:54.835312: train_loss -0.7372\n",
      "2025-05-16 19:41:54.835495: val_loss -0.8346\n",
      "2025-05-16 19:41:54.835532: Pseudo dice [np.float32(0.932)]\n",
      "2025-05-16 19:41:54.835568: Epoch time: 108.66 s\n",
      "2025-05-16 19:41:54.835588: Yayy! New best EMA pseudo Dice: 0.873199999332428\n",
      "2025-05-16 19:41:55.550309: \n",
      "2025-05-16 19:41:55.550542: Epoch 3\n",
      "2025-05-16 19:41:55.550618: Current learning rate: 0.00997\n",
      "2025-05-16 19:43:44.251040: train_loss -0.81\n",
      "2025-05-16 19:43:44.251171: val_loss -0.8871\n",
      "2025-05-16 19:43:44.251206: Pseudo dice [np.float32(0.9571)]\n",
      "2025-05-16 19:43:44.251241: Epoch time: 108.7 s\n",
      "2025-05-16 19:43:44.251262: Yayy! New best EMA pseudo Dice: 0.881600022315979\n",
      "2025-05-16 19:43:44.960388: \n",
      "2025-05-16 19:43:44.960655: Epoch 4\n",
      "2025-05-16 19:43:44.960737: Current learning rate: 0.00996\n",
      "2025-05-16 19:45:33.577077: train_loss -0.7956\n",
      "2025-05-16 19:45:33.577222: val_loss -0.8594\n",
      "2025-05-16 19:45:33.577264: Pseudo dice [np.float32(0.948)]\n",
      "2025-05-16 19:45:33.577302: Epoch time: 108.62 s\n",
      "2025-05-16 19:45:33.577343: Yayy! New best EMA pseudo Dice: 0.8881999850273132\n",
      "2025-05-16 19:45:34.291834: \n",
      "2025-05-16 19:45:34.291981: Epoch 5\n",
      "2025-05-16 19:45:34.292052: Current learning rate: 0.00995\n",
      "2025-05-16 19:47:22.986821: train_loss -0.8318\n",
      "2025-05-16 19:47:22.986940: val_loss -0.885\n",
      "2025-05-16 19:47:22.986982: Pseudo dice [np.float32(0.9539)]\n",
      "2025-05-16 19:47:22.987019: Epoch time: 108.7 s\n",
      "2025-05-16 19:47:22.987055: Yayy! New best EMA pseudo Dice: 0.8948000073432922\n",
      "2025-05-16 19:47:23.680442: \n",
      "2025-05-16 19:47:23.680522: Epoch 6\n",
      "2025-05-16 19:47:23.680585: Current learning rate: 0.00995\n",
      "2025-05-16 19:49:12.492470: train_loss -0.8202\n",
      "2025-05-16 19:49:12.492599: val_loss -0.8722\n",
      "2025-05-16 19:49:12.492635: Pseudo dice [np.float32(0.9507)]\n",
      "2025-05-16 19:49:12.492669: Epoch time: 108.81 s\n",
      "2025-05-16 19:49:12.492786: Yayy! New best EMA pseudo Dice: 0.9003999829292297\n",
      "2025-05-16 19:49:13.196561: \n",
      "2025-05-16 19:49:13.196646: Epoch 7\n",
      "2025-05-16 19:49:13.196710: Current learning rate: 0.00994\n",
      "2025-05-16 19:51:01.874273: train_loss -0.8558\n",
      "2025-05-16 19:51:01.874465: val_loss -0.8668\n",
      "2025-05-16 19:51:01.874498: Pseudo dice [np.float32(0.9491)]\n",
      "2025-05-16 19:51:01.874530: Epoch time: 108.68 s\n",
      "2025-05-16 19:51:01.874549: Yayy! New best EMA pseudo Dice: 0.9053000211715698\n",
      "2025-05-16 19:51:02.587286: \n",
      "2025-05-16 19:51:02.587366: Epoch 8\n",
      "2025-05-16 19:51:02.587431: Current learning rate: 0.00993\n",
      "2025-05-16 19:52:51.266435: train_loss -0.8523\n",
      "2025-05-16 19:52:51.266580: val_loss -0.8704\n",
      "2025-05-16 19:52:51.269269: Pseudo dice [np.float32(0.9426)]\n",
      "2025-05-16 19:52:51.269334: Epoch time: 108.68 s\n",
      "2025-05-16 19:52:51.269361: Yayy! New best EMA pseudo Dice: 0.9089999794960022\n",
      "2025-05-16 19:52:51.990233: \n",
      "2025-05-16 19:52:51.990621: Epoch 9\n",
      "2025-05-16 19:52:51.990696: Current learning rate: 0.00992\n",
      "2025-05-16 19:54:40.752410: train_loss -0.8445\n",
      "2025-05-16 19:54:40.752592: val_loss -0.8803\n",
      "2025-05-16 19:54:40.752639: Pseudo dice [np.float32(0.9449)]\n",
      "2025-05-16 19:54:40.752672: Epoch time: 108.76 s\n",
      "2025-05-16 19:54:40.752698: Yayy! New best EMA pseudo Dice: 0.9125999808311462\n",
      "2025-05-16 19:54:41.456068: \n",
      "2025-05-16 19:54:41.456360: Epoch 10\n",
      "2025-05-16 19:54:41.456597: Current learning rate: 0.00991\n",
      "2025-05-16 19:56:30.263572: train_loss -0.8504\n",
      "2025-05-16 19:56:30.264106: val_loss -0.8784\n",
      "2025-05-16 19:56:30.264223: Pseudo dice [np.float32(0.9559)]\n",
      "2025-05-16 19:56:30.264274: Epoch time: 108.81 s\n",
      "2025-05-16 19:56:30.264335: Yayy! New best EMA pseudo Dice: 0.9168999791145325\n",
      "2025-05-16 19:56:30.963642: \n",
      "2025-05-16 19:56:30.963720: Epoch 11\n",
      "2025-05-16 19:56:30.963785: Current learning rate: 0.0099\n",
      "2025-05-16 19:58:19.611820: train_loss -0.8783\n",
      "2025-05-16 19:58:19.611944: val_loss -0.8883\n",
      "2025-05-16 19:58:19.611989: Pseudo dice [np.float32(0.9528)]\n",
      "2025-05-16 19:58:19.612032: Epoch time: 108.65 s\n",
      "2025-05-16 19:58:19.612080: Yayy! New best EMA pseudo Dice: 0.9204999804496765\n",
      "2025-05-16 19:58:20.323611: \n",
      "2025-05-16 19:58:20.323685: Epoch 12\n",
      "2025-05-16 19:58:20.323821: Current learning rate: 0.00989\n",
      "2025-05-16 20:00:09.180385: train_loss -0.8353\n",
      "2025-05-16 20:00:09.180589: val_loss -0.8897\n",
      "2025-05-16 20:00:09.180636: Pseudo dice [np.float32(0.9565)]\n",
      "2025-05-16 20:00:09.180671: Epoch time: 108.86 s\n",
      "2025-05-16 20:00:09.180692: Yayy! New best EMA pseudo Dice: 0.9240999817848206\n",
      "2025-05-16 20:00:09.890030: \n",
      "2025-05-16 20:00:09.890202: Epoch 13\n",
      "2025-05-16 20:00:09.890428: Current learning rate: 0.00988\n",
      "2025-05-16 20:01:58.599553: train_loss -0.8749\n",
      "2025-05-16 20:01:58.599679: val_loss -0.9071\n",
      "2025-05-16 20:01:58.599717: Pseudo dice [np.float32(0.9608)]\n",
      "2025-05-16 20:01:58.599751: Epoch time: 108.71 s\n",
      "2025-05-16 20:01:58.599771: Yayy! New best EMA pseudo Dice: 0.9277999997138977\n",
      "2025-05-16 20:01:59.497923: \n",
      "2025-05-16 20:01:59.498024: Epoch 14\n",
      "2025-05-16 20:01:59.498188: Current learning rate: 0.00987\n",
      "2025-05-16 20:03:48.210881: train_loss -0.8849\n",
      "2025-05-16 20:03:48.211020: val_loss -0.8999\n",
      "2025-05-16 20:03:48.211055: Pseudo dice [np.float32(0.9586)]\n",
      "2025-05-16 20:03:48.211089: Epoch time: 108.71 s\n",
      "2025-05-16 20:03:48.211216: Yayy! New best EMA pseudo Dice: 0.930899977684021\n",
      "2025-05-16 20:03:48.929757: \n",
      "2025-05-16 20:03:48.929926: Epoch 15\n",
      "2025-05-16 20:03:48.929989: Current learning rate: 0.00986\n",
      "2025-05-16 20:05:37.782352: train_loss -0.8683\n",
      "2025-05-16 20:05:37.782492: val_loss -0.9055\n",
      "2025-05-16 20:05:37.782526: Pseudo dice [np.float32(0.9626)]\n",
      "2025-05-16 20:05:37.782558: Epoch time: 108.85 s\n",
      "2025-05-16 20:05:37.782578: Yayy! New best EMA pseudo Dice: 0.9340000152587891\n",
      "2025-05-16 20:05:38.510972: \n",
      "2025-05-16 20:05:38.511070: Epoch 16\n",
      "2025-05-16 20:05:38.511134: Current learning rate: 0.00986\n",
      "2025-05-16 20:07:27.220125: train_loss -0.8916\n",
      "2025-05-16 20:07:27.220327: val_loss -0.9093\n",
      "2025-05-16 20:07:27.220363: Pseudo dice [np.float32(0.9632)]\n",
      "2025-05-16 20:07:27.220397: Epoch time: 108.71 s\n",
      "2025-05-16 20:07:27.220418: Yayy! New best EMA pseudo Dice: 0.9369000196456909\n",
      "2025-05-16 20:07:27.947484: \n",
      "2025-05-16 20:07:27.947653: Epoch 17\n",
      "2025-05-16 20:07:27.947726: Current learning rate: 0.00985\n",
      "2025-05-16 20:09:16.807479: train_loss -0.8965\n",
      "2025-05-16 20:09:16.807690: val_loss -0.8985\n",
      "2025-05-16 20:09:16.807729: Pseudo dice [np.float32(0.955)]\n",
      "2025-05-16 20:09:16.807764: Epoch time: 108.86 s\n",
      "2025-05-16 20:09:16.807785: Yayy! New best EMA pseudo Dice: 0.9387000203132629\n",
      "2025-05-16 20:09:17.532962: \n",
      "2025-05-16 20:09:17.533053: Epoch 18\n",
      "2025-05-16 20:09:17.533118: Current learning rate: 0.00984\n",
      "2025-05-16 20:11:06.412687: train_loss -0.8793\n",
      "2025-05-16 20:11:06.412973: val_loss -0.9056\n",
      "2025-05-16 20:11:06.413072: Pseudo dice [np.float32(0.9618)]\n",
      "2025-05-16 20:11:06.413155: Epoch time: 108.88 s\n",
      "2025-05-16 20:11:06.413185: Yayy! New best EMA pseudo Dice: 0.9409999847412109\n",
      "2025-05-16 20:11:07.147080: \n",
      "2025-05-16 20:11:07.147365: Epoch 19\n",
      "2025-05-16 20:11:07.147465: Current learning rate: 0.00983\n",
      "2025-05-16 20:12:55.972886: train_loss -0.8806\n",
      "2025-05-16 20:12:55.973011: val_loss -0.8396\n",
      "2025-05-16 20:12:55.973213: Pseudo dice [np.float32(0.9312)]\n",
      "2025-05-16 20:12:55.973284: Epoch time: 108.83 s\n",
      "2025-05-16 20:12:56.497584: \n",
      "2025-05-16 20:12:56.497835: Epoch 20\n",
      "2025-05-16 20:12:56.498000: Current learning rate: 0.00982\n",
      "2025-05-16 20:14:45.264660: train_loss -0.8613\n",
      "2025-05-16 20:14:45.264778: val_loss -0.9078\n",
      "2025-05-16 20:14:45.264812: Pseudo dice [np.float32(0.9628)]\n",
      "2025-05-16 20:14:45.264845: Epoch time: 108.77 s\n",
      "2025-05-16 20:14:45.264866: Yayy! New best EMA pseudo Dice: 0.942300021648407\n",
      "2025-05-16 20:14:45.989348: \n",
      "2025-05-16 20:14:45.989430: Epoch 21\n",
      "2025-05-16 20:14:45.989496: Current learning rate: 0.00981\n",
      "2025-05-16 20:16:34.884232: train_loss -0.8978\n",
      "2025-05-16 20:16:34.884411: val_loss -0.9143\n",
      "2025-05-16 20:16:34.884448: Pseudo dice [np.float32(0.9615)]\n",
      "2025-05-16 20:16:34.884484: Epoch time: 108.9 s\n",
      "2025-05-16 20:16:34.884511: Yayy! New best EMA pseudo Dice: 0.9442999958992004\n",
      "2025-05-16 20:16:35.598468: \n",
      "2025-05-16 20:16:35.598619: Epoch 22\n",
      "2025-05-16 20:16:35.598723: Current learning rate: 0.0098\n",
      "2025-05-16 20:18:24.455011: train_loss -0.9081\n",
      "2025-05-16 20:18:24.455133: val_loss -0.9239\n",
      "2025-05-16 20:18:24.455168: Pseudo dice [np.float32(0.967)]\n",
      "2025-05-16 20:18:24.455202: Epoch time: 108.86 s\n",
      "2025-05-16 20:18:24.455222: Yayy! New best EMA pseudo Dice: 0.9465000033378601\n",
      "2025-05-16 20:18:25.160009: \n",
      "2025-05-16 20:18:25.160181: Epoch 23\n",
      "2025-05-16 20:18:25.160251: Current learning rate: 0.00979\n",
      "2025-05-16 20:20:13.838448: train_loss -0.9104\n",
      "2025-05-16 20:20:13.838566: val_loss -0.9259\n",
      "2025-05-16 20:20:13.838614: Pseudo dice [np.float32(0.9679)]\n",
      "2025-05-16 20:20:13.838648: Epoch time: 108.68 s\n",
      "2025-05-16 20:20:13.838685: Yayy! New best EMA pseudo Dice: 0.9487000107765198\n",
      "2025-05-16 20:20:14.543699: \n",
      "2025-05-16 20:20:14.543783: Epoch 24\n",
      "2025-05-16 20:20:14.543846: Current learning rate: 0.00978\n",
      "2025-05-16 20:22:03.212835: train_loss -0.912\n",
      "2025-05-16 20:22:03.212960: val_loss -0.909\n",
      "2025-05-16 20:22:03.212995: Pseudo dice [np.float32(0.9624)]\n",
      "2025-05-16 20:22:03.213173: Epoch time: 108.67 s\n",
      "2025-05-16 20:22:03.213271: Yayy! New best EMA pseudo Dice: 0.949999988079071\n",
      "2025-05-16 20:22:03.923812: \n",
      "2025-05-16 20:22:03.923950: Epoch 25\n",
      "2025-05-16 20:22:03.924046: Current learning rate: 0.00977\n",
      "2025-05-16 20:23:52.629667: train_loss -0.9207\n",
      "2025-05-16 20:23:52.629791: val_loss -0.9292\n",
      "2025-05-16 20:23:52.629826: Pseudo dice [np.float32(0.9699)]\n",
      "2025-05-16 20:23:52.629857: Epoch time: 108.71 s\n",
      "2025-05-16 20:23:52.629881: Yayy! New best EMA pseudo Dice: 0.9520000219345093\n",
      "2025-05-16 20:23:53.523489: \n",
      "2025-05-16 20:23:53.523673: Epoch 26\n",
      "2025-05-16 20:23:53.523771: Current learning rate: 0.00977\n",
      "2025-05-16 20:25:42.306595: train_loss -0.9136\n",
      "2025-05-16 20:25:42.306946: val_loss -0.9039\n",
      "2025-05-16 20:25:42.306994: Pseudo dice [np.float32(0.9599)]\n",
      "2025-05-16 20:25:42.307030: Epoch time: 108.78 s\n",
      "2025-05-16 20:25:42.307052: Yayy! New best EMA pseudo Dice: 0.9527999758720398\n",
      "2025-05-16 20:25:43.015584: \n",
      "2025-05-16 20:25:43.015857: Epoch 27\n",
      "2025-05-16 20:25:43.015952: Current learning rate: 0.00976\n",
      "2025-05-16 20:27:32.000489: train_loss -0.9028\n",
      "2025-05-16 20:27:32.000659: val_loss -0.922\n",
      "2025-05-16 20:27:32.000844: Pseudo dice [np.float32(0.9662)]\n",
      "2025-05-16 20:27:32.000937: Epoch time: 108.99 s\n",
      "2025-05-16 20:27:32.001069: Yayy! New best EMA pseudo Dice: 0.954200029373169\n",
      "2025-05-16 20:27:32.723119: \n",
      "2025-05-16 20:27:32.723418: Epoch 28\n",
      "2025-05-16 20:27:32.723490: Current learning rate: 0.00975\n",
      "2025-05-16 20:29:21.673431: train_loss -0.9095\n",
      "2025-05-16 20:29:21.673558: val_loss -0.9273\n",
      "2025-05-16 20:29:21.673593: Pseudo dice [np.float32(0.9683)]\n",
      "2025-05-16 20:29:21.673625: Epoch time: 108.95 s\n",
      "2025-05-16 20:29:21.673646: Yayy! New best EMA pseudo Dice: 0.9556000232696533\n",
      "2025-05-16 20:29:22.382639: \n",
      "2025-05-16 20:29:22.382786: Epoch 29\n",
      "2025-05-16 20:29:22.382857: Current learning rate: 0.00974\n",
      "2025-05-16 20:31:11.217572: train_loss -0.9109\n",
      "2025-05-16 20:31:11.217706: val_loss -0.9198\n",
      "2025-05-16 20:31:11.217740: Pseudo dice [np.float32(0.9649)]\n",
      "2025-05-16 20:31:11.217772: Epoch time: 108.84 s\n",
      "2025-05-16 20:31:11.217793: Yayy! New best EMA pseudo Dice: 0.9564999938011169\n",
      "2025-05-16 20:31:11.933050: \n",
      "2025-05-16 20:31:11.933139: Epoch 30\n",
      "2025-05-16 20:31:11.933204: Current learning rate: 0.00973\n",
      "2025-05-16 20:33:00.711870: train_loss -0.9203\n",
      "2025-05-16 20:33:00.711994: val_loss -0.9138\n",
      "2025-05-16 20:33:00.712028: Pseudo dice [np.float32(0.9617)]\n",
      "2025-05-16 20:33:00.712072: Epoch time: 108.78 s\n",
      "2025-05-16 20:33:00.712100: Yayy! New best EMA pseudo Dice: 0.9570000171661377\n",
      "2025-05-16 20:33:01.439859: \n",
      "2025-05-16 20:33:01.440142: Epoch 31\n",
      "2025-05-16 20:33:01.440217: Current learning rate: 0.00972\n",
      "2025-05-16 20:34:50.168434: train_loss -0.9238\n",
      "2025-05-16 20:34:50.168576: val_loss -0.9263\n",
      "2025-05-16 20:34:50.168610: Pseudo dice [np.float32(0.9682)]\n",
      "2025-05-16 20:34:50.168649: Epoch time: 108.73 s\n",
      "2025-05-16 20:34:50.168670: Yayy! New best EMA pseudo Dice: 0.9581000208854675\n",
      "2025-05-16 20:34:50.890399: \n",
      "2025-05-16 20:34:50.890591: Epoch 32\n",
      "2025-05-16 20:34:50.890677: Current learning rate: 0.00971\n",
      "2025-05-16 20:36:39.767539: train_loss -0.9304\n",
      "2025-05-16 20:36:39.767659: val_loss -0.9335\n",
      "2025-05-16 20:36:39.767694: Pseudo dice [np.float32(0.9695)]\n",
      "2025-05-16 20:36:39.767725: Epoch time: 108.88 s\n",
      "2025-05-16 20:36:39.767746: Yayy! New best EMA pseudo Dice: 0.9592999815940857\n",
      "2025-05-16 20:36:40.481778: \n",
      "2025-05-16 20:36:40.482081: Epoch 33\n",
      "2025-05-16 20:36:40.482191: Current learning rate: 0.0097\n",
      "2025-05-16 20:38:29.260184: train_loss -0.9192\n",
      "2025-05-16 20:38:29.260368: val_loss -0.9254\n",
      "2025-05-16 20:38:29.260404: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-16 20:38:29.260440: Epoch time: 108.78 s\n",
      "2025-05-16 20:38:29.260461: Yayy! New best EMA pseudo Dice: 0.9603999853134155\n",
      "2025-05-16 20:38:29.977057: \n",
      "2025-05-16 20:38:29.977253: Epoch 34\n",
      "2025-05-16 20:38:29.977358: Current learning rate: 0.00969\n",
      "2025-05-16 20:40:18.682949: train_loss -0.9096\n",
      "2025-05-16 20:40:18.683187: val_loss -0.9198\n",
      "2025-05-16 20:40:18.683292: Pseudo dice [np.float32(0.9644)]\n",
      "2025-05-16 20:40:18.683333: Epoch time: 108.71 s\n",
      "2025-05-16 20:40:18.683356: Yayy! New best EMA pseudo Dice: 0.9607999920845032\n",
      "2025-05-16 20:40:19.409075: \n",
      "2025-05-16 20:40:19.409200: Epoch 35\n",
      "2025-05-16 20:40:19.409333: Current learning rate: 0.00968\n",
      "2025-05-16 20:42:08.314635: train_loss -0.9145\n",
      "2025-05-16 20:42:08.314773: val_loss -0.9289\n",
      "2025-05-16 20:42:08.314807: Pseudo dice [np.float32(0.9685)]\n",
      "2025-05-16 20:42:08.314842: Epoch time: 108.91 s\n",
      "2025-05-16 20:42:08.314863: Yayy! New best EMA pseudo Dice: 0.9614999890327454\n",
      "2025-05-16 20:42:09.036896: \n",
      "2025-05-16 20:42:09.036980: Epoch 36\n",
      "2025-05-16 20:42:09.037055: Current learning rate: 0.00968\n",
      "2025-05-16 20:43:57.950014: train_loss -0.9189\n",
      "2025-05-16 20:43:57.950201: val_loss -0.9352\n",
      "2025-05-16 20:43:57.950235: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-16 20:43:57.950267: Epoch time: 108.91 s\n",
      "2025-05-16 20:43:57.950289: Yayy! New best EMA pseudo Dice: 0.9624999761581421\n",
      "2025-05-16 20:43:58.679713: \n",
      "2025-05-16 20:43:58.679872: Epoch 37\n",
      "2025-05-16 20:43:58.679953: Current learning rate: 0.00967\n",
      "2025-05-16 20:45:47.502171: train_loss -0.9286\n",
      "2025-05-16 20:45:47.502299: val_loss -0.9309\n",
      "2025-05-16 20:45:47.502335: Pseudo dice [np.float32(0.9706)]\n",
      "2025-05-16 20:45:47.502370: Epoch time: 108.82 s\n",
      "2025-05-16 20:45:47.502394: Yayy! New best EMA pseudo Dice: 0.9632999897003174\n",
      "2025-05-16 20:45:48.411444: \n",
      "2025-05-16 20:45:48.411617: Epoch 38\n",
      "2025-05-16 20:45:48.411714: Current learning rate: 0.00966\n",
      "2025-05-16 20:47:37.117379: train_loss -0.8916\n",
      "2025-05-16 20:47:37.117501: val_loss -0.9032\n",
      "2025-05-16 20:47:37.117536: Pseudo dice [np.float32(0.9631)]\n",
      "2025-05-16 20:47:37.117568: Epoch time: 108.71 s\n",
      "2025-05-16 20:47:37.641855: \n",
      "2025-05-16 20:47:37.642019: Epoch 39\n",
      "2025-05-16 20:47:37.642164: Current learning rate: 0.00965\n",
      "2025-05-16 20:49:26.565412: train_loss -0.906\n",
      "2025-05-16 20:49:26.565604: val_loss -0.8777\n",
      "2025-05-16 20:49:26.565638: Pseudo dice [np.float32(0.939)]\n",
      "2025-05-16 20:49:26.565672: Epoch time: 108.92 s\n",
      "2025-05-16 20:49:27.090953: \n",
      "2025-05-16 20:49:27.091108: Epoch 40\n",
      "2025-05-16 20:49:27.091185: Current learning rate: 0.00964\n",
      "2025-05-16 20:51:15.862918: train_loss -0.9054\n",
      "2025-05-16 20:51:15.863084: val_loss -0.9221\n",
      "[np.float32(0.9654)]863249: Pseudo dice \n",
      "2025-05-16 20:51:15.863459: Epoch time: 108.77 s\n",
      "2025-05-16 20:51:16.397232: \n",
      "2025-05-16 20:51:16.397398: Epoch 41\n",
      "2025-05-16 20:51:16.397491: Current learning rate: 0.00963\n",
      "2025-05-16 20:53:05.297990: train_loss -0.9133\n",
      "2025-05-16 20:53:05.298200: val_loss -0.9171\n",
      "2025-05-16 20:53:05.298257: Pseudo dice [np.float32(0.966)]\n",
      "2025-05-16 20:53:05.298316: Epoch time: 108.9 s\n",
      "2025-05-16 20:53:05.797745: \n",
      "2025-05-16 20:53:05.797894: Epoch 42\n",
      "2025-05-16 20:53:05.797986: Current learning rate: 0.00962\n",
      "2025-05-16 20:54:54.745758: train_loss -0.9135\n",
      "2025-05-16 20:54:54.745875: val_loss -0.9267\n",
      "2025-05-16 20:54:54.745909: Pseudo dice [np.float32(0.9674)]\n",
      "2025-05-16 20:54:54.745941: Epoch time: 108.95 s\n",
      "2025-05-16 20:54:55.256118: \n",
      "2025-05-16 20:54:55.256207: Epoch 43\n",
      "2025-05-16 20:54:55.256269: Current learning rate: 0.00961\n",
      "2025-05-16 20:56:44.160721: train_loss -0.9328\n",
      "2025-05-16 20:56:44.161054: val_loss -0.9329\n",
      "2025-05-16 20:56:44.161122: Pseudo dice [np.float32(0.9688)]\n",
      "2025-05-16 20:56:44.161157: Epoch time: 108.91 s\n",
      "2025-05-16 20:56:44.672346: \n",
      "2025-05-16 20:56:44.672614: Epoch 44\n",
      "2025-05-16 20:56:44.672685: Current learning rate: 0.0096\n",
      "2025-05-16 20:58:33.603610: train_loss -0.9325\n",
      "2025-05-16 20:58:33.603835: val_loss -0.9336\n",
      "2025-05-16 20:58:33.603941: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-16 20:58:33.604023: Epoch time: 108.93 s\n",
      "2025-05-16 20:58:33.604129: Yayy! New best EMA pseudo Dice: 0.9639000296592712\n",
      "2025-05-16 20:58:34.322387: \n",
      "2025-05-16 20:58:34.322474: Epoch 45\n",
      "2025-05-16 20:58:34.322595: Current learning rate: 0.00959\n",
      "2025-05-16 21:00:23.235433: train_loss -0.9297\n",
      "2025-05-16 21:00:23.235567: val_loss -0.9322\n",
      "2025-05-16 21:00:23.235601: Pseudo dice [np.float32(0.9704)]\n",
      "2025-05-16 21:00:23.235634: Epoch time: 108.91 s\n",
      "2025-05-16 21:00:23.235655: Yayy! New best EMA pseudo Dice: 0.9645000100135803\n",
      "2025-05-16 21:00:23.938643: \n",
      "2025-05-16 21:00:23.938773: Epoch 46\n",
      "2025-05-16 21:00:23.938839: Current learning rate: 0.00959\n",
      "2025-05-16 21:02:12.701180: train_loss -0.9401\n",
      "2025-05-16 21:02:12.701311: val_loss -0.9284\n",
      "2025-05-16 21:02:12.701346: Pseudo dice [np.float32(0.9671)]\n",
      "2025-05-16 21:02:12.701486: Epoch time: 108.76 s\n",
      "2025-05-16 21:02:12.701570: Yayy! New best EMA pseudo Dice: 0.9648000001907349\n",
      "2025-05-16 21:02:13.408536: \n",
      "2025-05-16 21:02:13.408617: Epoch 47\n",
      "2025-05-16 21:02:13.408692: Current learning rate: 0.00958\n",
      "2025-05-16 21:04:02.273877: train_loss -0.9372\n",
      "2025-05-16 21:04:02.274047: val_loss -0.9303\n",
      "2025-05-16 21:04:02.274079: Pseudo dice [np.float32(0.9688)]\n",
      "2025-05-16 21:04:02.274112: Epoch time: 108.87 s\n",
      "2025-05-16 21:04:02.274132: Yayy! New best EMA pseudo Dice: 0.9652000069618225\n",
      "2025-05-16 21:04:02.978815: \n",
      "2025-05-16 21:04:02.978960: Epoch 48\n",
      "2025-05-16 21:04:02.979028: Current learning rate: 0.00957\n",
      "2025-05-16 21:05:51.798851: train_loss -0.9383\n",
      "2025-05-16 21:05:51.799027: val_loss -0.9344\n",
      "2025-05-16 21:05:51.799137: Pseudo dice [np.float32(0.9699)]\n",
      "2025-05-16 21:05:51.799195: Epoch time: 108.82 s\n",
      "2025-05-16 21:05:51.799224: Yayy! New best EMA pseudo Dice: 0.9656999707221985\n",
      "2025-05-16 21:05:52.513753: \n",
      "2025-05-16 21:05:52.513832: Epoch 49\n",
      "2025-05-16 21:05:52.513897: Current learning rate: 0.00956\n",
      "2025-05-16 21:07:41.530426: train_loss -0.935\n",
      "2025-05-16 21:07:41.530562: val_loss -0.9335\n",
      "2025-05-16 21:07:41.530601: Pseudo dice [np.float32(0.9703)]\n",
      "2025-05-16 21:07:41.530682: Epoch time: 109.02 s\n",
      "2025-05-16 21:07:41.719884: Yayy! New best EMA pseudo Dice: 0.9660999774932861\n",
      "2025-05-16 21:07:42.445401: \n",
      "2025-05-16 21:07:42.445785: Epoch 50\n",
      "2025-05-16 21:07:42.445931: Current learning rate: 0.00955\n",
      "2025-05-16 21:09:31.259181: train_loss -0.9258\n",
      "2025-05-16 21:09:31.259308: val_loss -0.9281\n",
      "2025-05-16 21:09:31.259340: Pseudo dice [np.float32(0.9672)]\n",
      "2025-05-16 21:09:31.259372: Epoch time: 108.81 s\n",
      "2025-05-16 21:09:31.259473: Yayy! New best EMA pseudo Dice: 0.9661999940872192\n",
      "2025-05-16 21:09:32.166625: \n",
      "2025-05-16 21:09:32.166902: Epoch 51\n",
      "2025-05-16 21:09:32.166977: Current learning rate: 0.00954\n",
      "2025-05-16 21:11:21.035045: train_loss -0.9344\n",
      "2025-05-16 21:11:21.035166: val_loss -0.9289\n",
      "2025-05-16 21:11:21.035200: Pseudo dice [np.float32(0.9677)]\n",
      "2025-05-16 21:11:21.035232: Epoch time: 108.87 s\n",
      "2025-05-16 21:11:21.035251: Yayy! New best EMA pseudo Dice: 0.9664000272750854\n",
      "2025-05-16 21:11:21.764086: \n",
      "2025-05-16 21:11:21.764283: Epoch 52\n",
      "2025-05-16 21:11:21.764415: Current learning rate: 0.00953\n",
      "2025-05-16 21:13:10.558474: train_loss -0.9361\n",
      "2025-05-16 21:13:10.558604: val_loss -0.9239\n",
      "2025-05-16 21:13:10.558637: Pseudo dice [np.float32(0.9651)]\n",
      "2025-05-16 21:13:10.558676: Epoch time: 108.79 s\n",
      "2025-05-16 21:13:11.063537: \n",
      "2025-05-16 21:13:11.063689: Epoch 53\n",
      "2025-05-16 21:13:11.063760: Current learning rate: 0.00952\n",
      "2025-05-16 21:14:59.806422: train_loss -0.9222\n",
      "2025-05-16 21:14:59.806547: val_loss -0.9229\n",
      "2025-05-16 21:14:59.806860: Pseudo dice [np.float32(0.9673)]\n",
      "2025-05-16 21:14:59.806987: Epoch time: 108.74 s\n",
      "2025-05-16 21:15:00.319031: \n",
      "2025-05-16 21:15:00.319123: Epoch 54\n",
      "2025-05-16 21:15:00.319186: Current learning rate: 0.00951\n",
      "2025-05-16 21:16:49.187729: train_loss -0.9352\n",
      "2025-05-16 21:16:49.187943: val_loss -0.9357\n",
      "2025-05-16 21:16:49.188059: Pseudo dice [np.float32(0.9701)]\n",
      "2025-05-16 21:16:49.188112: Epoch time: 108.87 s\n",
      "2025-05-16 21:16:49.188134: Yayy! New best EMA pseudo Dice: 0.96670001745224\n",
      "2025-05-16 21:16:49.903622: \n",
      "2025-05-16 21:16:49.903719: Epoch 55\n",
      "2025-05-16 21:16:49.903783: Current learning rate: 0.0095\n",
      "2025-05-16 21:18:38.640838: train_loss -0.9121\n",
      "2025-05-16 21:18:38.640963: val_loss -0.8785\n",
      "2025-05-16 21:18:38.640998: Pseudo dice [np.float32(0.9437)]\n",
      "2025-05-16 21:18:38.641029: Epoch time: 108.74 s\n",
      "2025-05-16 21:18:39.144676: \n",
      "2025-05-16 21:18:39.144838: Epoch 56\n",
      "2025-05-16 21:18:39.144902: Current learning rate: 0.00949\n",
      "2025-05-16 21:20:28.019860: train_loss -0.9273\n",
      "2025-05-16 21:20:28.020038: val_loss -0.9289\n",
      "2025-05-16 21:20:28.020069: Pseudo dice [np.float32(0.9673)]\n",
      "2025-05-16 21:20:28.020103: Epoch time: 108.88 s\n",
      "2025-05-16 21:20:28.525249: \n",
      "2025-05-16 21:20:28.525335: Epoch 57\n",
      "2025-05-16 21:20:28.525429: Current learning rate: 0.00949\n",
      "2025-05-16 21:22:17.290412: train_loss -0.9372\n",
      "2025-05-16 21:22:17.290544: val_loss -0.9373\n",
      "2025-05-16 21:22:17.290577: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-16 21:22:17.290785: Epoch time: 108.77 s\n",
      "2025-05-16 21:22:17.796284: \n",
      "2025-05-16 21:22:17.796418: Epoch 58\n",
      "2025-05-16 21:22:17.796577: Current learning rate: 0.00948\n",
      "2025-05-16 21:24:06.745227: train_loss -0.9376\n",
      "2025-05-16 21:24:06.745362: val_loss -0.9365\n",
      "2025-05-16 21:24:06.745397: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-16 21:24:06.745431: Epoch time: 108.95 s\n",
      "2025-05-16 21:24:07.245698: \n",
      "2025-05-16 21:24:07.245791: Epoch 59\n",
      "2025-05-16 21:24:07.246000: Current learning rate: 0.00947\n",
      "2025-05-16 21:25:56.210206: train_loss -0.9392\n",
      "2025-05-16 21:25:56.210336: val_loss -0.9371\n",
      "2025-05-16 21:25:56.210372: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-16 21:25:56.210404: Epoch time: 108.97 s\n",
      "2025-05-16 21:25:56.720623: \n",
      "2025-05-16 21:25:56.720707: Epoch 60\n",
      "2025-05-16 21:25:56.720773: Current learning rate: 0.00946\n",
      "2025-05-16 21:27:45.464895: train_loss -0.9418\n",
      "2025-05-16 21:27:45.465024: val_loss -0.9379\n",
      "2025-05-16 21:27:45.465059: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-16 21:27:45.465091: Epoch time: 108.74 s\n",
      "2025-05-16 21:27:45.465112: Yayy! New best EMA pseudo Dice: 0.9667999744415283\n",
      "2025-05-16 21:27:46.184801: \n",
      "2025-05-16 21:27:46.185165: Epoch 61\n",
      "2025-05-16 21:27:46.185248: Current learning rate: 0.00945\n",
      "2025-05-16 21:29:35.127571: train_loss -0.9409\n",
      "2025-05-16 21:29:35.127721: val_loss -0.9404\n",
      "2025-05-16 21:29:35.127756: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-16 21:29:35.127789: Epoch time: 108.94 s\n",
      "2025-05-16 21:29:35.127810: Yayy! New best EMA pseudo Dice: 0.9672999978065491\n",
      "2025-05-16 21:29:35.861684: \n",
      "2025-05-16 21:29:35.861909: Epoch 62\n",
      "2025-05-16 21:29:35.861988: Current learning rate: 0.00944\n",
      "2025-05-16 21:31:24.790741: train_loss -0.9332\n",
      "2025-05-16 21:31:24.790864: val_loss -0.9345\n",
      "2025-05-16 21:31:24.790897: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-16 21:31:24.790929: Epoch time: 108.93 s\n",
      "Yayy! New best EMA pseudo Dice: 0.9675999879837036\n",
      "2025-05-16 21:31:25.700934: \n",
      "2025-05-16 21:31:25.701071: Epoch 63\n",
      "2025-05-16 21:31:25.701150: Current learning rate: 0.00943\n",
      "2025-05-16 21:33:14.583023: train_loss -0.9426\n",
      "2025-05-16 21:33:14.583154: val_loss -0.9377\n",
      "2025-05-16 21:33:14.583188: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-16 21:33:14.583221: Epoch time: 108.88 s\n",
      "2025-05-16 21:33:14.583243: Yayy! New best EMA pseudo Dice: 0.9679999947547913\n",
      "2025-05-16 21:33:15.300444: \n",
      "2025-05-16 21:33:15.300629: Epoch 64\n",
      "2025-05-16 21:33:15.300697: Current learning rate: 0.00942\n",
      "2025-05-16 21:35:04.271888: train_loss -0.9413\n",
      "2025-05-16 21:35:04.272075: val_loss -0.9361\n",
      "2025-05-16 21:35:04.272109: Pseudo dice [np.float32(0.971)]\n",
      "2025-05-16 21:35:04.272143: Epoch time: 108.97 s\n",
      "2025-05-16 21:35:04.272163: Yayy! New best EMA pseudo Dice: 0.9682999849319458\n",
      "2025-05-16 21:35:04.999824: \n",
      "2025-05-16 21:35:04.999981: Epoch 65\n",
      "2025-05-16 21:35:05.000170: Current learning rate: 0.00941\n",
      "2025-05-16 21:36:53.935135: train_loss -0.9407\n",
      "2025-05-16 21:36:53.935314: val_loss -0.9338\n",
      "2025-05-16 21:36:53.935350: Pseudo dice [np.float32(0.9688)]\n",
      "2025-05-16 21:36:53.935382: Epoch time: 108.94 s\n",
      "2025-05-16 21:36:53.935404: Yayy! New best EMA pseudo Dice: 0.9684000015258789\n",
      "2025-05-16 21:36:54.654773: \n",
      "2025-05-16 21:36:54.654949: Epoch 66\n",
      "2025-05-16 21:36:54.655082: Current learning rate: 0.0094\n",
      "2025-05-16 21:38:43.521538: train_loss -0.9435\n",
      "2025-05-16 21:38:43.521723: val_loss -0.9361\n",
      "2025-05-16 21:38:43.521759: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-16 21:38:43.521792: Epoch time: 108.87 s\n",
      "2025-05-16 21:38:43.521812: Yayy! New best EMA pseudo Dice: 0.968500018119812\n",
      "2025-05-16 21:38:44.256979: \n",
      "2025-05-16 21:38:44.257074: Epoch 67\n",
      "2025-05-16 21:38:44.257135: Current learning rate: 0.00939\n",
      "2025-05-16 21:40:32.966131: train_loss -0.9406\n",
      "2025-05-16 21:40:32.966269: val_loss -0.9267\n",
      "2025-05-16 21:40:32.966305: Pseudo dice [np.float32(0.9678)]\n",
      "2025-05-16 21:40:32.966339: Epoch time: 108.71 s\n",
      "2025-05-16 21:40:33.485927: \n",
      "2025-05-16 21:40:33.486024: Epoch 68\n",
      "2025-05-16 21:40:33.486092: Current learning rate: 0.00939\n",
      "2025-05-16 21:42:22.171181: train_loss -0.9184\n",
      "2025-05-16 21:42:22.171323: val_loss -0.9319\n",
      "2025-05-16 21:42:22.171360: Pseudo dice [np.float32(0.9701)]\n",
      "2025-05-16 21:42:22.171393: Epoch time: 108.69 s\n",
      "2025-05-16 21:42:22.171416: Yayy! New best EMA pseudo Dice: 0.9685999751091003\n",
      "2025-05-16 21:42:22.900480: \n",
      "2025-05-16 21:42:22.900574: Epoch 69\n",
      "2025-05-16 21:42:22.900646: Current learning rate: 0.00938\n",
      "2025-05-16 21:44:11.768090: train_loss -0.9347\n",
      "2025-05-16 21:44:11.768292: val_loss -0.9335\n",
      "2025-05-16 21:44:11.768349: Pseudo dice [np.float32(0.9699)]\n",
      "2025-05-16 21:44:11.768387: Epoch time: 108.87 s\n",
      "2025-05-16 21:44:11.768409: Yayy! New best EMA pseudo Dice: 0.9686999917030334\n",
      "2025-05-16 21:44:12.503138: \n",
      "2025-05-16 21:44:12.503226: Epoch 70\n",
      "2025-05-16 21:44:12.503294: Current learning rate: 0.00937\n",
      "2025-05-16 21:46:01.413452: train_loss -0.9389\n",
      "2025-05-16 21:46:01.413652: val_loss -0.9368\n",
      "2025-05-16 21:46:01.413699: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-16 21:46:01.413735: Epoch time: 108.91 s\n",
      "2025-05-16 21:46:01.413758: Yayy! New best EMA pseudo Dice: 0.9690999984741211\n",
      "2025-05-16 21:46:02.145249: \n",
      "2025-05-16 21:46:02.145579: Epoch 71\n",
      "2025-05-16 21:46:02.145651: Current learning rate: 0.00936\n",
      "2025-05-16 21:47:51.035613: train_loss -0.9396\n",
      "2025-05-16 21:47:51.035776: val_loss -0.932\n",
      "2025-05-16 21:47:51.035808: Pseudo dice [np.float32(0.9676)]\n",
      "2025-05-16 21:47:51.035842: Epoch time: 108.89 s\n",
      "2025-05-16 21:47:51.548670: \n",
      "2025-05-16 21:47:51.548815: Epoch 72\n",
      "2025-05-16 21:47:51.548881: Current learning rate: 0.00935\n",
      "2025-05-16 21:49:40.493958: train_loss -0.9415\n",
      "2025-05-16 21:49:40.494374: val_loss -0.9369\n",
      "2025-05-16 21:49:40.494435: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-16 21:49:40.494477: Epoch time: 108.95 s\n",
      "2025-05-16 21:49:40.494501: Yayy! New best EMA pseudo Dice: 0.9692000150680542\n",
      "2025-05-16 21:49:41.218933: \n",
      "2025-05-16 21:49:41.219095: Epoch 73\n",
      "2025-05-16 21:49:41.219323: Current learning rate: 0.00934\n",
      "2025-05-16 21:51:30.032043: train_loss -0.9408\n",
      "2025-05-16 21:51:30.032197: val_loss -0.9313\n",
      "2025-05-16 21:51:30.032233: Pseudo dice [np.float32(0.9689)]\n",
      "2025-05-16 21:51:30.032267: Epoch time: 108.81 s\n",
      "2025-05-16 21:51:30.549615: \n",
      "2025-05-16 21:51:30.549696: Epoch 74\n",
      "2025-05-16 21:51:30.549766: Current learning rate: 0.00933\n",
      "2025-05-16 21:53:19.389025: train_loss -0.9189\n",
      "2025-05-16 21:53:19.389160: val_loss -0.9112\n",
      "2025-05-16 21:53:19.389195: Pseudo dice [np.float32(0.9658)]\n",
      "2025-05-16 21:53:19.389228: Epoch time: 108.84 s\n",
      "2025-05-16 21:53:20.113643: \n",
      "2025-05-16 21:53:20.113737: Epoch 75\n",
      "2025-05-16 21:53:20.113801: Current learning rate: 0.00932\n",
      "2025-05-16 21:55:08.874686: train_loss -0.8569\n",
      "2025-05-16 21:55:08.874816: val_loss -0.9112\n",
      "2025-05-16 21:55:08.875015: Pseudo dice [np.float32(0.9638)]\n",
      "2025-05-16 21:55:08.875092: Epoch time: 108.76 s\n",
      "2025-05-16 21:55:09.394766: \n",
      "2025-05-16 21:55:09.394904: Epoch 76\n",
      "2025-05-16 21:55:09.394969: Current learning rate: 0.00931\n",
      "2025-05-16 21:56:58.361833: train_loss -0.9081\n",
      "2025-05-16 21:56:58.361960: val_loss -0.926\n",
      "2025-05-16 21:56:58.361996: Pseudo dice [np.float32(0.9672)]\n",
      "2025-05-16 21:56:58.362029: Epoch time: 108.97 s\n",
      "2025-05-16 21:56:58.917818: \n",
      "2025-05-16 21:56:58.918005: Epoch 77\n",
      "2025-05-16 21:56:58.918091: Current learning rate: 0.0093\n",
      "2025-05-16 21:58:47.681425: train_loss -0.9214\n",
      "2025-05-16 21:58:47.681648: val_loss -0.9268\n",
      "2025-05-16 21:58:47.681682: Pseudo dice [np.float32(0.969)]\n",
      "2025-05-16 21:58:47.681715: Epoch time: 108.76 s\n",
      "2025-05-16 21:58:48.213269: \n",
      "2025-05-16 21:58:48.213367: Epoch 78\n",
      "2025-05-16 21:58:48.213428: Current learning rate: 0.0093\n",
      "2025-05-16 22:00:37.175575: train_loss -0.9273\n",
      "2025-05-16 22:00:37.175706: val_loss -0.9254\n",
      "2025-05-16 22:00:37.175741: Pseudo dice [np.float32(0.968)]\n",
      "2025-05-16 22:00:37.175774: Epoch time: 108.96 s\n",
      "2025-05-16 22:00:37.706785: \n",
      "2025-05-16 22:00:37.706879: Epoch 79\n",
      "2025-05-16 22:00:37.707030: Current learning rate: 0.00929\n",
      "2025-05-16 22:02:26.561516: train_loss -0.9325\n",
      "2025-05-16 22:02:26.561725: val_loss -0.9265\n",
      "2025-05-16 22:02:26.561762: Pseudo dice [np.float32(0.9686)]\n",
      "2025-05-16 22:02:26.561795: Epoch time: 108.86 s\n",
      "2025-05-16 22:02:27.086676: \n",
      "2025-05-16 22:02:27.086765: Epoch 80\n",
      "2025-05-16 22:02:27.086829: Current learning rate: 0.00928\n",
      "2025-05-16 22:04:16.027470: train_loss -0.9412\n",
      "2025-05-16 22:04:16.027652: val_loss -0.93\n",
      "2025-05-16 22:04:16.027861: Pseudo dice [np.float32(0.9674)]\n",
      "2025-05-16 22:04:16.027997: Epoch time: 108.94 s\n",
      "2025-05-16 22:04:16.554041: \n",
      "2025-05-16 22:04:16.554610: Epoch 81\n",
      "2025-05-16 22:04:16.554715: Current learning rate: 0.00927\n",
      "2025-05-16 22:06:05.452971: train_loss -0.9388\n",
      "2025-05-16 22:06:05.453095: val_loss -0.9346\n",
      "2025-05-16 22:06:05.453130: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-16 22:06:05.453182: Epoch time: 108.9 s\n",
      "2025-05-16 22:06:05.976999: \n",
      "2025-05-16 22:06:05.977268: Epoch 82\n",
      "2025-05-16 22:06:05.977418: Current learning rate: 0.00926\n",
      "2025-05-16 22:07:54.746620: train_loss -0.9443\n",
      "2025-05-16 22:07:54.746759: val_loss -0.936\n",
      "2025-05-16 22:07:54.746798: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-16 22:07:54.746832: Epoch time: 108.77 s\n",
      "2025-05-16 22:07:55.249570: \n",
      "2025-05-16 22:07:55.249659: Epoch 83\n",
      "2025-05-16 22:07:55.249723: Current learning rate: 0.00925\n",
      "2025-05-16 22:09:44.079310: train_loss -0.9466\n",
      "2025-05-16 22:09:44.079441: val_loss -0.9331\n",
      "2025-05-16 22:09:44.079473: Pseudo dice [np.float32(0.9691)]\n",
      "2025-05-16 22:09:44.079505: Epoch time: 108.83 s\n",
      "2025-05-16 22:09:44.584178: \n",
      "2025-05-16 22:09:44.584329: Epoch 84\n",
      "2025-05-16 22:09:44.584493: Current learning rate: 0.00924\n",
      "2025-05-16 22:11:33.521379: train_loss -0.9452\n",
      "2025-05-16 22:11:33.521506: val_loss -0.9361\n",
      "2025-05-16 22:11:33.521540: Pseudo dice [np.float32(0.9711)]\n",
      "2025-05-16 22:11:33.521574: Epoch time: 108.94 s\n",
      "2025-05-16 22:11:34.024969: \n",
      "2025-05-16 22:11:34.025098: Epoch 85\n",
      "2025-05-16 22:11:34.025169: Current learning rate: 0.00923\n",
      "2025-05-16 22:13:22.959430: train_loss -0.9251\n",
      "2025-05-16 22:13:22.959573: val_loss -0.9171\n",
      "2025-05-16 22:13:22.959611: Pseudo dice [np.float32(0.9635)]\n",
      "2025-05-16 22:13:22.959646: Epoch time: 108.94 s\n",
      "2025-05-16 22:13:23.461570: \n",
      "2025-05-16 22:13:23.461695: Epoch 86\n",
      "2025-05-16 22:13:23.461765: Current learning rate: 0.00922\n",
      "2025-05-16 22:15:12.396994: train_loss -0.9205\n",
      "2025-05-16 22:15:12.397126: val_loss -0.9359\n",
      "2025-05-16 22:15:12.397160: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-16 22:15:12.397201: Epoch time: 108.94 s\n",
      "2025-05-16 22:15:13.099958: \n",
      "2025-05-16 22:15:13.100460: Epoch 87\n",
      "2025-05-16 22:15:13.100620: Current learning rate: 0.00921\n",
      "2025-05-16 22:17:01.895689: train_loss -0.9328\n",
      "2025-05-16 22:17:01.895843: val_loss -0.9105\n",
      "2025-05-16 22:17:01.895931: Pseudo dice [np.float32(0.959)]\n",
      "2025-05-16 22:17:01.895972: Epoch time: 108.8 s\n",
      "2025-05-16 22:17:02.402038: \n",
      "2025-05-16 22:17:02.402131: Epoch 88\n",
      "2025-05-16 22:17:02.402229: Current learning rate: 0.0092\n",
      "2025-05-16 22:18:51.333527: train_loss -0.9331\n",
      "2025-05-16 22:18:51.333833: val_loss -0.9178\n",
      "2025-05-16 22:18:51.333897: Pseudo dice [np.float32(0.9628)]\n",
      "2025-05-16 22:18:51.333937: Epoch time: 108.93 s\n",
      "2025-05-16 22:18:51.845404: \n",
      "2025-05-16 22:18:51.845599: Epoch 89\n",
      "2025-05-16 22:18:51.845709: Current learning rate: 0.0092\n",
      "2025-05-16 22:20:40.795197: train_loss -0.9342\n",
      "2025-05-16 22:20:40.795656: val_loss -0.9388\n",
      "2025-05-16 22:20:40.795802: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-16 22:20:40.795860: Epoch time: 108.95 s\n",
      "2025-05-16 22:20:41.307955: \n",
      "2025-05-16 22:20:41.308384: Epoch 90\n",
      "2025-05-16 22:20:41.308485: Current learning rate: 0.00919\n",
      "2025-05-16 22:22:30.067872: train_loss -0.9277\n",
      "2025-05-16 22:22:30.068001: val_loss -0.9341\n",
      "2025-05-16 22:22:30.068037: Pseudo dice [np.float32(0.9714)]\n",
      "2025-05-16 22:22:30.068071: Epoch time: 108.76 s\n",
      "2025-05-16 22:22:30.581635: \n",
      "2025-05-16 22:22:30.582033: Epoch 91\n",
      "2025-05-16 22:22:30.582163: Current learning rate: 0.00918\n",
      "2025-05-16 22:24:19.454985: train_loss -0.9212\n",
      "2025-05-16 22:24:19.455158: val_loss -0.9359\n",
      "2025-05-16 22:24:19.455314: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-16 22:24:19.455433: Epoch time: 108.87 s\n",
      "2025-05-16 22:24:19.955666: \n",
      "2025-05-16 22:24:19.955815: Epoch 92\n",
      "2025-05-16 22:24:19.955921: Current learning rate: 0.00917\n",
      "2025-05-16 22:26:08.989966: train_loss -0.9208\n",
      "2025-05-16 22:26:08.990226: val_loss -0.9264\n",
      "2025-05-16 22:26:08.990330: Pseudo dice [np.float32(0.9683)]\n",
      "2025-05-16 22:26:08.990399: Epoch time: 109.03 s\n",
      "2025-05-16 22:26:09.502539: \n",
      "2025-05-16 22:26:09.502631: Epoch 93\n",
      "2025-05-16 22:26:09.502694: Current learning rate: 0.00916\n",
      "2025-05-16 22:27:58.288871: train_loss -0.9219\n",
      "2025-05-16 22:27:58.288996: val_loss -0.931\n",
      "2025-05-16 22:27:58.289044: Pseudo dice [np.float32(0.9697)]\n",
      "2025-05-16 22:27:58.289082: Epoch time: 108.79 s\n",
      "2025-05-16 22:27:58.781265: \n",
      "2025-05-16 22:27:58.781352: Epoch 94\n",
      "2025-05-16 22:27:58.781416: Current learning rate: 0.00915\n",
      "2025-05-16 22:29:47.585104: train_loss -0.9432\n",
      "2025-05-16 22:29:47.585234: val_loss -0.9377\n",
      "2025-05-16 22:29:47.585267: Pseudo dice [np.float32(0.9717)]\n",
      "2025-05-16 22:29:47.585299: Epoch time: 108.8 s\n",
      "2025-05-16 22:29:48.086652: \n",
      "2025-05-16 22:29:48.086803: Epoch 95\n",
      "2025-05-16 22:29:48.086871: Current learning rate: 0.00914\n",
      "2025-05-16 22:31:36.937085: train_loss -0.9429\n",
      "2025-05-16 22:31:36.937207: val_loss -0.9395\n",
      "2025-05-16 22:31:36.937470: Pseudo dice [np.float32(0.9705)]\n",
      "2025-05-16 22:31:36.937680: Epoch time: 108.85 s\n",
      "2025-05-16 22:31:37.435897: \n",
      "2025-05-16 22:31:37.436115: Epoch 96\n",
      "2025-05-16 22:31:37.436199: Current learning rate: 0.00913\n",
      "2025-05-16 22:33:26.410018: train_loss -0.9404\n",
      "2025-05-16 22:33:26.410170: val_loss -0.9358\n",
      "2025-05-16 22:33:26.410207: Pseudo dice [np.float32(0.9697)]\n",
      "2025-05-16 22:33:26.410241: Epoch time: 108.97 s\n",
      "2025-05-16 22:33:26.916946: \n",
      "2025-05-16 22:33:26.917043: Epoch 97\n",
      "2025-05-16 22:33:26.917104: Current learning rate: 0.00912\n",
      "2025-05-16 22:35:15.933455: train_loss -0.9376\n",
      "2025-05-16 22:35:15.933579: val_loss -0.9297\n",
      "2025-05-16 22:35:15.933613: Pseudo dice [np.float32(0.9689)]\n",
      "2025-05-16 22:35:15.933647: Epoch time: 109.02 s\n",
      "2025-05-16 22:35:16.443152: \n",
      "2025-05-16 22:35:16.443231: Epoch 98\n",
      "2025-05-16 22:35:16.443297: Current learning rate: 0.00911\n",
      "2025-05-16 22:37:05.432962: train_loss -0.9415\n",
      "2025-05-16 22:37:05.433084: val_loss -0.9357\n",
      "2025-05-16 22:37:05.433280: Pseudo dice [np.float32(0.9693)]\n",
      "2025-05-16 22:37:05.433353: Epoch time: 108.99 s\n",
      "2025-05-16 22:37:05.946681: \n",
      "2025-05-16 22:37:05.946844: Epoch 99\n",
      "2025-05-16 22:37:05.946996: Current learning rate: 0.0091\n",
      "2025-05-16 22:38:54.708621: train_loss -0.9456\n",
      "2025-05-16 22:38:54.708746: val_loss -0.9377\n",
      "2025-05-16 22:38:54.708781: Pseudo dice [np.float32(0.971)]\n",
      "2025-05-16 22:38:54.708812: Epoch time: 108.76 s\n",
      "2025-05-16 22:38:55.602168: \n",
      "2025-05-16 22:38:55.602259: Epoch 100\n",
      "2025-05-16 22:38:55.602322: Current learning rate: 0.0091\n",
      "2025-05-16 22:40:44.505778: train_loss -0.9442\n",
      "2025-05-16 22:40:44.505915: val_loss -0.9324\n",
      "2025-05-16 22:40:44.505950: Pseudo dice [np.float32(0.9704)]\n",
      "2025-05-16 22:40:44.505983: Epoch time: 108.9 s\n",
      "2025-05-16 22:40:44.506002: Yayy! New best EMA pseudo Dice: 0.9692999720573425\n",
      "2025-05-16 22:40:45.225035: \n",
      "2025-05-16 22:40:45.225386: Epoch 101\n",
      "2025-05-16 22:40:45.225465: Current learning rate: 0.00909\n",
      "2025-05-16 22:42:34.087135: train_loss -0.9457\n",
      "2025-05-16 22:42:34.087415: val_loss -0.9365\n",
      "2025-05-16 22:42:34.087459: Pseudo dice [np.float32(0.9708)]\n",
      "2025-05-16 22:42:34.087492: Epoch time: 108.86 s\n",
      "2025-05-16 22:42:34.087513: Yayy! New best EMA pseudo Dice: 0.9695000052452087\n",
      "2025-05-16 22:42:34.809642: \n",
      "2025-05-16 22:42:34.809741: Epoch 102\n",
      "2025-05-16 22:42:34.809805: Current learning rate: 0.00908\n",
      "2025-05-16 22:44:23.754915: train_loss -0.9494\n",
      "2025-05-16 22:44:23.755057: val_loss -0.9385\n",
      "2025-05-16 22:44:23.755101: Pseudo dice [np.float32(0.9714)]\n",
      "2025-05-16 22:44:23.755134: Epoch time: 108.95 s\n",
      "2025-05-16 22:44:23.755156: Yayy! New best EMA pseudo Dice: 0.9696999788284302\n",
      "2025-05-16 22:44:24.508147: \n",
      "2025-05-16 22:44:24.508357: Epoch 103\n",
      "2025-05-16 22:44:24.508456: Current learning rate: 0.00907\n",
      "2025-05-16 22:46:13.448581: train_loss -0.9463\n",
      "2025-05-16 22:46:13.448705: val_loss -0.9344\n",
      "2025-05-16 22:46:13.448738: Pseudo dice [np.float32(0.9686)]\n",
      "2025-05-16 22:46:13.448772: Epoch time: 108.94 s\n",
      "2025-05-16 22:46:13.959367: \n",
      "2025-05-16 22:46:13.959554: Epoch 104\n",
      "2025-05-16 22:46:13.959642: Current learning rate: 0.00906\n",
      "2025-05-16 22:48:02.934255: train_loss -0.949\n",
      "2025-05-16 22:48:02.934379: val_loss -0.9409\n",
      "2025-05-16 22:48:02.934547: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-16 22:48:02.934616: Epoch time: 108.98 s\n",
      "2025-05-16 22:48:02.934646: Yayy! New best EMA pseudo Dice: 0.9697999954223633\n",
      "2025-05-16 22:48:03.655529: \n",
      "2025-05-16 22:48:03.655769: Epoch 105\n",
      "2025-05-16 22:48:03.655945: Current learning rate: 0.00905\n",
      "2025-05-16 22:49:52.429087: train_loss -0.9504\n",
      "2025-05-16 22:49:52.429266: val_loss -0.9407\n",
      "2025-05-16 22:49:52.429299: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-16 22:49:52.429332: Epoch time: 108.77 s\n",
      "2025-05-16 22:49:52.429353: Yayy! New best EMA pseudo Dice: 0.9700999855995178\n",
      "2025-05-16 22:49:53.150792: \n",
      "2025-05-16 22:49:53.151044: Epoch 106\n",
      "2025-05-16 22:49:53.151113: Current learning rate: 0.00904\n",
      "2025-05-16 22:51:41.895562: train_loss -0.9507\n",
      "2025-05-16 22:51:41.895688: val_loss -0.9397\n",
      "2025-05-16 22:51:41.895723: Pseudo dice [np.float32(0.9717)]\n",
      "2025-05-16 22:51:41.895756: Epoch time: 108.75 s\n",
      "2025-05-16 22:51:41.895776: Yayy! New best EMA pseudo Dice: 0.970300018787384\n",
      "2025-05-16 22:51:42.622820: \n",
      "2025-05-16 22:51:42.623134: Epoch 107\n",
      "2025-05-16 22:51:42.623206: Current learning rate: 0.00903\n",
      "2025-05-16 22:53:31.517395: train_loss -0.9536\n",
      "2025-05-16 22:53:31.517602: val_loss -0.9436\n",
      "2025-05-16 22:53:31.517690: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-16 22:53:31.517799: Epoch time: 108.9 s\n",
      "2025-05-16 22:53:31.517908: Yayy! New best EMA pseudo Dice: 0.9704999923706055\n",
      "2025-05-16 22:53:32.238783: \n",
      "2025-05-16 22:53:32.238925: Epoch 108\n",
      "2025-05-16 22:53:32.239006: Current learning rate: 0.00902\n",
      "2025-05-16 22:55:21.154011: train_loss -0.9506\n",
      "2025-05-16 22:55:21.154192: val_loss -0.9291\n",
      "2025-05-16 22:55:21.154227: Pseudo dice [np.float32(0.9683)]\n",
      "2025-05-16 22:55:21.154261: Epoch time: 108.92 s\n",
      "2025-05-16 22:55:21.659248: \n",
      "2025-05-16 22:55:21.659506: Epoch 109\n",
      "Current learning rate: 0.00901\n",
      "2025-05-16 22:57:10.639276: train_loss -0.9536\n",
      "2025-05-16 22:57:10.639411: val_loss -0.944\n",
      "2025-05-16 22:57:10.639448: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-16 22:57:10.639482: Epoch time: 108.98 s\n",
      "2025-05-16 22:57:10.639503: Yayy! New best EMA pseudo Dice: 0.9707000255584717\n",
      "2025-05-16 22:57:11.357643: \n",
      "2025-05-16 22:57:11.357779: Epoch 110\n",
      "2025-05-16 22:57:11.357845: Current learning rate: 0.009\n",
      "2025-05-16 22:59:00.333170: train_loss -0.9535\n",
      "2025-05-16 22:59:00.333350: val_loss -0.9392\n",
      "2025-05-16 22:59:00.333385: Pseudo dice [np.float32(0.9706)]\n",
      "2025-05-16 22:59:00.333420: Epoch time: 108.98 s\n",
      "2025-05-16 22:59:00.833140: \n",
      "2025-05-16 22:59:00.833390: Epoch 111\n",
      "2025-05-16 22:59:00.833501: Current learning rate: 0.009\n",
      "2025-05-16 23:00:49.806402: train_loss -0.9521\n",
      "2025-05-16 23:00:49.806526: val_loss -0.9398\n",
      "2025-05-16 23:00:49.806561: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-16 23:00:49.806593: Epoch time: 108.97 s\n",
      "2025-05-16 23:00:49.806614: Yayy! New best EMA pseudo Dice: 0.97079998254776\n",
      "2025-05-16 23:00:50.729776: \n",
      "2025-05-16 23:00:50.730094: Epoch 112\n",
      "2025-05-16 23:00:50.730191: Current learning rate: 0.00899\n",
      "2025-05-16 23:02:39.533894: train_loss -0.9485\n",
      "2025-05-16 23:02:39.534066: val_loss -0.9313\n",
      "2025-05-16 23:02:39.534116: Pseudo dice [np.float32(0.9677)]\n",
      "2025-05-16 23:02:39.534220: Epoch time: 108.8 s\n",
      "2025-05-16 23:02:40.042420: \n",
      "2025-05-16 23:02:40.042652: Epoch 113\n",
      "2025-05-16 23:02:40.042727: Current learning rate: 0.00898\n",
      "2025-05-16 23:04:28.959328: train_loss -0.9511\n",
      "2025-05-16 23:04:28.959461: val_loss -0.936\n",
      "2025-05-16 23:04:28.959507: Pseudo dice [np.float32(0.9703)]\n",
      "2025-05-16 23:04:28.959547: Epoch time: 108.92 s\n",
      "2025-05-16 23:04:29.469297: \n",
      "2025-05-16 23:04:29.469418: Epoch 114\n",
      "2025-05-16 23:04:29.469541: Current learning rate: 0.00897\n",
      "2025-05-16 23:06:18.411426: train_loss -0.9484\n",
      "2025-05-16 23:06:18.411558: val_loss -0.9399\n",
      "2025-05-16 23:06:18.411594: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-16 23:06:18.411627: Epoch time: 108.94 s\n",
      "2025-05-16 23:06:18.930349: \n",
      "2025-05-16 23:06:18.930500: Epoch 115\n",
      "2025-05-16 23:06:18.930567: Current learning rate: 0.00896\n",
      "2025-05-16 23:08:07.764911: train_loss -0.9497\n",
      "2025-05-16 23:08:07.765033: val_loss -0.9419\n",
      "2025-05-16 23:08:07.765065: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-16 23:08:07.765099: Epoch time: 108.84 s\n",
      "2025-05-16 23:08:08.281016: \n",
      "2025-05-16 23:08:08.281108: Epoch 116\n",
      "2025-05-16 23:08:08.281172: Current learning rate: 0.00895\n",
      "2025-05-16 23:09:57.161016: train_loss -0.9487\n",
      "2025-05-16 23:09:57.161139: val_loss -0.9447\n",
      "2025-05-16 23:09:57.161172: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-16 23:09:57.161205: Epoch time: 108.88 s\n",
      "2025-05-16 23:09:57.161226: Yayy! New best EMA pseudo Dice: 0.9710999727249146\n",
      "2025-05-16 23:09:57.891708: \n",
      "2025-05-16 23:09:57.892283: Epoch 117\n",
      "2025-05-16 23:09:57.892378: Current learning rate: 0.00894\n",
      "2025-05-16 23:11:46.794578: train_loss -0.9484\n",
      "2025-05-16 23:11:46.794779: val_loss -0.9349\n",
      "2025-05-16 23:11:46.794823: Pseudo dice [np.float32(0.9681)]\n",
      "2025-05-16 23:11:46.794858: Epoch time: 108.9 s\n",
      "2025-05-16 23:11:47.319595: \n",
      "2025-05-16 23:11:47.319788: Epoch 118\n",
      "2025-05-16 23:11:47.320019: Current learning rate: 0.00893\n",
      "2025-05-16 23:13:36.168227: train_loss -0.9537\n",
      "2025-05-16 23:13:36.168448: val_loss -0.94\n",
      "2025-05-16 23:13:36.168485: Pseudo dice [np.float32(0.9717)]\n",
      "2025-05-16 23:13:36.168520: Epoch time: 108.85 s\n",
      "2025-05-16 23:13:36.677432: \n",
      "2025-05-16 23:13:36.677519: Epoch 119\n",
      "2025-05-16 23:13:36.677613: Current learning rate: 0.00892\n",
      "2025-05-16 23:15:25.510623: train_loss -0.9542\n",
      "2025-05-16 23:15:25.510818: val_loss -0.9403\n",
      "2025-05-16 23:15:25.510854: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-16 23:15:25.510889: Epoch time: 108.83 s\n",
      "2025-05-16 23:15:26.019042: \n",
      "2025-05-16 23:15:26.019236: Epoch 120\n",
      "2025-05-16 23:15:26.019363: Current learning rate: 0.00891\n",
      "2025-05-16 23:17:14.974539: train_loss -0.9526\n",
      "2025-05-16 23:17:14.974678: val_loss -0.9399\n",
      "2025-05-16 23:17:14.974711: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-16 23:17:14.974745: Epoch time: 108.96 s\n",
      "2025-05-16 23:17:14.974768: Yayy! New best EMA pseudo Dice: 0.9711999893188477\n",
      "2025-05-16 23:17:15.703010: \n",
      "2025-05-16 23:17:15.703093: Epoch 121\n",
      "2025-05-16 23:17:15.703156: Current learning rate: 0.0089\n",
      "2025-05-16 23:19:04.579007: train_loss -0.9563\n",
      "2025-05-16 23:19:04.579126: val_loss -0.9403\n",
      "2025-05-16 23:19:04.579158: Pseudo dice [np.float32(0.971)]\n",
      "2025-05-16 23:19:04.579189: Epoch time: 108.88 s\n",
      "2025-05-16 23:19:05.090813: \n",
      "2025-05-16 23:19:05.090892: Epoch 122\n",
      "2025-05-16 23:19:05.090955: Current learning rate: 0.00889\n",
      "2025-05-16 23:20:53.819758: train_loss -0.9505\n",
      "2025-05-16 23:20:53.819932: val_loss -0.945\n",
      "2025-05-16 23:20:53.819969: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-16 23:20:53.820004: Epoch time: 108.73 s\n",
      "2025-05-16 23:20:53.820026: Yayy! New best EMA pseudo Dice: 0.9714999794960022\n",
      "2025-05-16 23:20:54.542988: \n",
      "2025-05-16 23:20:54.543069: Epoch 123\n",
      "2025-05-16 23:20:54.543132: Current learning rate: 0.00889\n",
      "2025-05-16 23:22:43.341097: train_loss -0.9344\n",
      "2025-05-16 23:22:43.341280: val_loss -0.9182\n",
      "2025-05-16 23:22:43.341384: Pseudo dice [np.float32(0.9657)]\n",
      "2025-05-16 23:22:43.341437: Epoch time: 108.8 s\n",
      "2025-05-16 23:22:43.858137: \n",
      "2025-05-16 23:22:43.858333: Epoch 124\n",
      "2025-05-16 23:22:43.858442: Current learning rate: 0.00888\n",
      "2025-05-16 23:24:32.712658: train_loss -0.9312\n",
      "2025-05-16 23:24:32.712893: val_loss -0.9285\n",
      "2025-05-16 23:24:32.712968: Pseudo dice [np.float32(0.9686)]\n",
      "2025-05-16 23:24:32.713008: Epoch time: 108.86 s\n",
      "2025-05-16 23:24:33.425589: \n",
      "2025-05-16 23:24:33.425678: Epoch 125\n",
      "2025-05-16 23:24:33.425750: Current learning rate: 0.00887\n",
      "2025-05-16 23:26:22.308084: train_loss -0.9461\n",
      "2025-05-16 23:26:22.308215: val_loss -0.9382\n",
      "2025-05-16 23:26:22.308249: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-16 23:26:22.308301: Epoch time: 108.88 s\n",
      "2025-05-16 23:26:22.832908: \n",
      "2025-05-16 23:26:22.833003: Epoch 126\n",
      "2025-05-16 23:26:22.833067: Current learning rate: 0.00886\n",
      "2025-05-16 23:28:11.846899: train_loss -0.9339\n",
      "2025-05-16 23:28:11.847279: val_loss -0.9233\n",
      "2025-05-16 23:28:11.847376: Pseudo dice [np.float32(0.9673)]\n",
      "2025-05-16 23:28:11.847430: Epoch time: 109.01 s\n",
      "2025-05-16 23:28:12.361714: \n",
      "2025-05-16 23:28:12.361839: Epoch 127\n",
      "2025-05-16 23:28:12.362015: Current learning rate: 0.00885\n",
      "2025-05-16 23:30:01.383236: train_loss -0.9365\n",
      "2025-05-16 23:30:01.383670: val_loss -0.9357\n",
      "2025-05-16 23:30:01.383833: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-16 23:30:01.383902: Epoch time: 109.02 s\n",
      "2025-05-16 23:30:01.902647: \n",
      "2025-05-16 23:30:01.902984: Epoch 128\n",
      "2025-05-16 23:30:01.903068: Current learning rate: 0.00884\n",
      "2025-05-16 23:31:50.926898: train_loss -0.9433\n",
      "2025-05-16 23:31:50.927017: val_loss -0.9323\n",
      "2025-05-16 23:31:50.927052: Pseudo dice [np.float32(0.9697)]\n",
      "2025-05-16 23:31:50.927085: Epoch time: 109.02 s\n",
      "2025-05-16 23:31:51.448200: \n",
      "2025-05-16 23:31:51.448345: Epoch 129\n",
      "2025-05-16 23:31:51.448417: Current learning rate: 0.00883\n",
      "2025-05-16 23:33:40.400431: train_loss -0.944\n",
      "2025-05-16 23:33:40.400565: val_loss -0.9413\n",
      "2025-05-16 23:33:40.400599: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-16 23:33:40.400632: Epoch time: 108.95 s\n",
      "2025-05-16 23:33:40.913340: \n",
      "2025-05-16 23:33:40.913493: Epoch 130\n",
      "2025-05-16 23:33:40.913567: Current learning rate: 0.00882\n",
      "2025-05-16 23:35:29.815670: train_loss -0.9358\n",
      "2025-05-16 23:35:29.815822: val_loss -0.9201\n",
      "2025-05-16 23:35:29.815878: Pseudo dice [np.float32(0.9649)]\n",
      "2025-05-16 23:35:29.815911: Epoch time: 108.9 s\n",
      "2025-05-16 23:35:30.333390: \n",
      "2025-05-16 23:35:30.333479: Epoch 131\n",
      "2025-05-16 23:35:30.333544: Current learning rate: 0.00881\n",
      "2025-05-16 23:37:19.394139: train_loss -0.9102\n",
      "2025-05-16 23:37:19.394354: val_loss -0.9192\n",
      "2025-05-16 23:37:19.394399: Pseudo dice [np.float32(0.9647)]\n",
      "2025-05-16 23:37:19.394433: Epoch time: 109.06 s\n",
      "2025-05-16 23:37:19.912615: \n",
      "2025-05-16 23:37:19.912704: Epoch 132\n",
      "2025-05-16 23:37:19.912766: Current learning rate: 0.0088\n",
      "2025-05-16 23:39:08.821718: train_loss -0.9304\n",
      "2025-05-16 23:39:08.821842: val_loss -0.9319\n",
      "2025-05-16 23:39:08.821875: Pseudo dice [np.float32(0.9705)]\n",
      "2025-05-16 23:39:08.821908: Epoch time: 108.91 s\n",
      "2025-05-16 23:39:09.342031: \n",
      "2025-05-16 23:39:09.342240: Epoch 133\n",
      "2025-05-16 23:39:09.342480: Current learning rate: 0.00879\n",
      "2025-05-16 23:40:58.262940: train_loss -0.9427\n",
      "2025-05-16 23:40:58.263067: val_loss -0.9351\n",
      "2025-05-16 23:40:58.263100: Pseudo dice [np.float32(0.9714)]\n",
      "2025-05-16 23:40:58.263134: Epoch time: 108.92 s\n",
      "2025-05-16 23:40:58.780779: \n",
      "2025-05-16 23:40:58.780985: Epoch 134\n",
      "2025-05-16 23:40:58.781065: Current learning rate: 0.00879\n",
      "2025-05-16 23:42:47.793247: train_loss -0.9478\n",
      "2025-05-16 23:42:47.793369: val_loss -0.9396\n",
      "2025-05-16 23:42:47.793428: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-16 23:42:47.793494: Epoch time: 109.01 s\n",
      "2025-05-16 23:42:48.315861: \n",
      "2025-05-16 23:42:48.316015: Epoch 135\n",
      "2025-05-16 23:42:48.316153: Current learning rate: 0.00878\n",
      "2025-05-16 23:44:37.280901: train_loss -0.9422\n",
      "2025-05-16 23:44:37.281024: val_loss -0.905\n",
      "2025-05-16 23:44:37.281059: Pseudo dice [np.float32(0.9563)]\n",
      "2025-05-16 23:44:37.281095: Epoch time: 108.97 s\n",
      "2025-05-16 23:44:37.799118: \n",
      "2025-05-16 23:44:37.799270: Epoch 136\n",
      "2025-05-16 23:44:37.799345: Current learning rate: 0.00877\n",
      "2025-05-16 23:46:26.722002: train_loss -0.9311\n",
      "2025-05-16 23:46:26.722129: val_loss -0.9343\n",
      "2025-05-16 23:46:26.722162: Pseudo dice [np.float32(0.9702)]\n",
      "2025-05-16 23:46:26.722196: Epoch time: 108.92 s\n",
      "2025-05-16 23:46:27.439127: \n",
      "2025-05-16 23:46:27.439213: Epoch 137\n",
      "2025-05-16 23:46:27.439281: Current learning rate: 0.00876\n",
      "2025-05-16 23:48:16.295130: train_loss -0.9438\n",
      "2025-05-16 23:48:16.295281: val_loss -0.9198\n",
      "2025-05-16 23:48:16.295317: Pseudo dice [np.float32(0.9643)]\n",
      "2025-05-16 23:48:16.295352: Epoch time: 108.86 s\n",
      "2025-05-16 23:48:16.815253: \n",
      "2025-05-16 23:48:16.815421: Epoch 138\n",
      "2025-05-16 23:48:16.815496: Current learning rate: 0.00875\n",
      "2025-05-16 23:50:05.732751: train_loss -0.9383\n",
      "2025-05-16 23:50:05.732922: val_loss -0.9247\n",
      "2025-05-16 23:50:05.733109: Pseudo dice [np.float32(0.9655)]\n",
      "2025-05-16 23:50:05.733184: Epoch time: 108.92 s\n",
      "2025-05-16 23:50:06.265036: \n",
      "2025-05-16 23:50:06.265295: Epoch 139\n",
      "2025-05-16 23:50:06.265409: Current learning rate: 0.00874\n",
      "2025-05-16 23:51:55.292702: train_loss -0.9375\n",
      "2025-05-16 23:51:55.292921: val_loss -0.941\n",
      "2025-05-16 23:51:55.292966: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-16 23:51:55.293003: Epoch time: 109.03 s\n",
      "2025-05-16 23:51:55.816166: \n",
      "2025-05-16 23:51:55.816318: Epoch 140\n",
      "2025-05-16 23:51:55.816384: Current learning rate: 0.00873\n",
      "2025-05-16 23:53:44.804844: train_loss -0.9501\n",
      "2025-05-16 23:53:44.805075: val_loss -0.9459\n",
      "2025-05-16 23:53:44.805120: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-16 23:53:44.805184: Epoch time: 108.99 s\n",
      "2025-05-16 23:53:45.340947: \n",
      "2025-05-16 23:53:45.341042: Epoch 141\n",
      "2025-05-16 23:53:45.341108: Current learning rate: 0.00872\n",
      "2025-05-16 23:55:34.329661: train_loss -0.95\n",
      "2025-05-16 23:55:34.329879: val_loss -0.9429\n",
      "2025-05-16 23:55:34.329959: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-16 23:55:34.329999: Epoch time: 108.99 s\n",
      "2025-05-16 23:55:34.854976: \n",
      "2025-05-16 23:55:34.855086: Epoch 142\n",
      "2025-05-16 23:55:34.855152: Current learning rate: 0.00871\n",
      "2025-05-16 23:57:23.802117: train_loss -0.9509\n",
      "2025-05-16 23:57:23.802244: val_loss -0.9404\n",
      "2025-05-16 23:57:23.802278: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-16 23:57:23.802311: Epoch time: 108.95 s\n",
      "2025-05-16 23:57:24.326843: \n",
      "2025-05-16 23:57:24.327006: Epoch 143\n",
      "2025-05-16 23:57:24.327102: Current learning rate: 0.0087\n",
      "2025-05-16 23:59:13.212742: train_loss -0.9479\n",
      "2025-05-16 23:59:13.212906: val_loss -0.9317\n",
      "2025-05-16 23:59:13.213026: Pseudo dice [np.float32(0.9681)]\n",
      "2025-05-16 23:59:13.213128: Epoch time: 108.89 s\n",
      "2025-05-16 23:59:13.734567: \n",
      "2025-05-16 23:59:13.734656: Epoch 144\n",
      "2025-05-16 23:59:13.734718: Current learning rate: 0.00869\n",
      "2025-05-17 00:01:02.717303: train_loss -0.9494\n",
      "2025-05-17 00:01:02.717461: val_loss -0.9379\n",
      "2025-05-17 00:01:02.717656: Pseudo dice [np.float32(0.9716)]\n",
      "2025-05-17 00:01:02.717718: Epoch time: 108.98 s\n",
      "2025-05-17 00:01:03.249336: \n",
      "2025-05-17 00:01:03.249508: Epoch 145\n",
      "2025-05-17 00:01:03.249592: Current learning rate: 0.00868\n",
      "2025-05-17 00:02:52.176168: train_loss -0.9504\n",
      "2025-05-17 00:02:52.176360: val_loss -0.9481\n",
      "2025-05-17 00:02:52.176468: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 00:02:52.176545: Epoch time: 108.93 s\n",
      "2025-05-17 00:02:52.699785: \n",
      "2025-05-17 00:02:52.699952: Epoch 146\n",
      "2025-05-17 00:02:52.700099: Current learning rate: 0.00868\n",
      "2025-05-17 00:04:41.669506: train_loss -0.9506\n",
      "2025-05-17 00:04:41.669631: val_loss -0.9438\n",
      "2025-05-17 00:04:41.669667: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 00:04:41.669700: Epoch time: 108.97 s\n",
      "2025-05-17 00:04:42.190005: \n",
      "2025-05-17 00:04:42.190353: Epoch 147\n",
      "2025-05-17 00:04:42.190441: Current learning rate: 0.00867\n",
      "2025-05-17 00:06:30.937373: train_loss -0.953\n",
      "2025-05-17 00:06:30.937559: val_loss -0.9454\n",
      "2025-05-17 00:06:30.937595: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 00:06:30.937628: Epoch time: 108.75 s\n",
      "2025-05-17 00:06:31.452269: \n",
      "2025-05-17 00:06:31.452473: Epoch 148\n",
      "2025-05-17 00:06:31.452554: Current learning rate: 0.00866\n",
      "2025-05-17 00:08:20.371181: train_loss -0.9513\n",
      "2025-05-17 00:08:20.371295: val_loss -0.9457\n",
      "2025-05-17 00:08:20.371328: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 00:08:20.371362: Epoch time: 108.92 s\n",
      "2025-05-17 00:08:20.371382: Yayy! New best EMA pseudo Dice: 0.9715999960899353\n",
      "2025-05-17 00:08:21.125870: \n",
      "2025-05-17 00:08:21.126061: Epoch 149\n",
      "2025-05-17 00:08:21.126213: Current learning rate: 0.00865\n",
      "2025-05-17 00:10:10.067184: train_loss -0.9549\n",
      "2025-05-17 00:10:10.067905: val_loss -0.944\n",
      "2025-05-17 00:10:10.067951: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-17 00:10:10.067986: Epoch time: 108.94 s\n",
      "2025-05-17 00:10:10.474340: Yayy! New best EMA pseudo Dice: 0.9717000126838684\n",
      "2025-05-17 00:10:11.197125: \n",
      "2025-05-17 00:10:11.197360: Epoch 150\n",
      "2025-05-17 00:10:11.197438: Current learning rate: 0.00864\n",
      "2025-05-17 00:12:00.060676: train_loss -0.9551\n",
      "2025-05-17 00:12:00.060889: val_loss -0.9455\n",
      "2025-05-17 00:12:00.060926: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 00:12:00.060961: Epoch time: 108.86 s\n",
      "2025-05-17 00:12:00.060982: Yayy! New best EMA pseudo Dice: 0.9718999862670898\n",
      "2025-05-17 00:12:00.793955: \n",
      "2025-05-17 00:12:00.794058: Epoch 151\n",
      "2025-05-17 00:12:00.794122: Current learning rate: 0.00863\n",
      "2025-05-17 00:13:49.763679: train_loss -0.9522\n",
      "2025-05-17 00:13:49.763874: val_loss -0.9397\n",
      "2025-05-17 00:13:49.764003: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-17 00:13:49.764094: Epoch time: 108.97 s\n",
      "2025-05-17 00:13:50.299596: \n",
      "2025-05-17 00:13:50.299767: Epoch 152\n",
      "2025-05-17 00:13:50.299946: Current learning rate: 0.00862\n",
      "2025-05-17 00:15:39.222711: train_loss -0.9484\n",
      "2025-05-17 00:15:39.222831: val_loss -0.943\n",
      "2025-05-17 00:15:39.222866: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 00:15:39.222897: Epoch time: 108.92 s\n",
      "2025-05-17 00:15:39.223060: Yayy! New best EMA pseudo Dice: 0.972100019454956\n",
      "2025-05-17 00:15:39.966607: \n",
      "2025-05-17 00:15:39.966699: Epoch 153\n",
      "2025-05-17 00:15:39.966763: Current learning rate: 0.00861\n",
      "2025-05-17 00:17:28.897557: train_loss -0.9505\n",
      "2025-05-17 00:17:28.897688: val_loss -0.9352\n",
      "2025-05-17 00:17:28.897721: Pseudo dice [np.float32(0.9687)]\n",
      "2025-05-17 00:17:28.897754: Epoch time: 108.93 s\n",
      "2025-05-17 00:17:29.434466: \n",
      "2025-05-17 00:17:29.434677: Epoch 154\n",
      "2025-05-17 00:17:29.434838: Current learning rate: 0.0086\n",
      "2025-05-17 00:19:18.439210: train_loss -0.9473\n",
      "2025-05-17 00:19:18.439331: val_loss -0.9335\n",
      "2025-05-17 00:19:18.439470: Pseudo dice [np.float32(0.9693)]\n",
      "2025-05-17 00:19:18.439578: Epoch time: 109.01 s\n",
      "2025-05-17 00:19:18.972555: \n",
      "2025-05-17 00:19:18.972716: Epoch 155\n",
      "2025-05-17 00:19:18.972785: Current learning rate: 0.00859\n",
      "2025-05-17 00:21:07.948699: train_loss -0.9531\n",
      "2025-05-17 00:21:07.948947: val_loss -0.9413\n",
      "2025-05-17 00:21:07.948992: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-17 00:21:07.949025: Epoch time: 108.98 s\n",
      "2025-05-17 00:21:08.487951: \n",
      "2025-05-17 00:21:08.488040: Epoch 156\n",
      "2025-05-17 00:21:08.488106: Current learning rate: 0.00858\n",
      "2025-05-17 00:22:57.512137: train_loss -0.9537\n",
      "2025-05-17 00:22:57.512275: val_loss -0.9401\n",
      "2025-05-17 00:22:57.512322: Pseudo dice [np.float32(0.971)]\n",
      "2025-05-17 00:22:57.512359: Epoch time: 109.02 s\n",
      "2025-05-17 00:22:58.050771: \n",
      "2025-05-17 00:22:58.050905: Epoch 157\n",
      "2025-05-17 00:22:58.051019: Current learning rate: 0.00858\n",
      "2025-05-17 00:24:47.067597: train_loss -0.9562\n",
      "2025-05-17 00:24:47.067871: val_loss -0.9413\n",
      "2025-05-17 00:24:47.068126: Pseudo dice [np.float32(0.9707)]\n",
      "2025-05-17 00:24:47.068251: Epoch time: 109.02 s\n",
      "2025-05-17 00:24:47.606433: \n",
      "2025-05-17 00:24:47.606590: Epoch 158\n",
      "2025-05-17 00:24:47.606665: Current learning rate: 0.00857\n",
      "2025-05-17 00:26:36.500437: train_loss -0.9558\n",
      "2025-05-17 00:26:36.500572: val_loss -0.9429\n",
      "2025-05-17 00:26:36.500608: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 00:26:36.500644: Epoch time: 108.89 s\n",
      "2025-05-17 00:26:37.031859: \n",
      "2025-05-17 00:26:37.032139: Epoch 159\n",
      "2025-05-17 00:26:37.032267: Current learning rate: 0.00856\n",
      "2025-05-17 00:28:25.925348: train_loss -0.9579\n",
      "2025-05-17 00:28:25.925480: val_loss -0.9392\n",
      "2025-05-17 00:28:25.925515: Pseudo dice [np.float32(0.9711)]\n",
      "2025-05-17 00:28:25.925671: Epoch time: 108.89 s\n",
      "2025-05-17 00:28:26.451055: \n",
      "2025-05-17 00:28:26.451179: Epoch 160\n",
      "2025-05-17 00:28:26.451279: Current learning rate: 0.00855\n",
      "2025-05-17 00:30:15.396766: train_loss -0.9586\n",
      "2025-05-17 00:30:15.396892: val_loss -0.9435\n",
      "2025-05-17 00:30:15.396927: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 00:30:15.396960: Epoch time: 108.95 s\n",
      "2025-05-17 00:30:16.132089: \n",
      "2025-05-17 00:30:16.132444: Epoch 161\n",
      "2025-05-17 00:30:16.132538: Current learning rate: 0.00854\n",
      "2025-05-17 00:32:05.094338: train_loss -0.9551\n",
      "2025-05-17 00:32:05.094486: val_loss -0.9451\n",
      "2025-05-17 00:32:05.094520: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 00:32:05.094554: Epoch time: 108.96 s\n",
      "2025-05-17 00:32:05.621628: \n",
      "2025-05-17 00:32:05.621879: Epoch 162\n",
      "2025-05-17 00:32:05.621953: Current learning rate: 0.00853\n",
      "2025-05-17 00:33:54.555378: train_loss -0.9567\n",
      "2025-05-17 00:33:54.555507: val_loss -0.9461\n",
      "2025-05-17 00:33:54.555544: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 00:33:54.555578: Epoch time: 108.93 s\n",
      "2025-05-17 00:33:54.555600: Yayy! New best EMA pseudo Dice: 0.9721999764442444\n",
      "2025-05-17 00:33:55.305546: \n",
      "2025-05-17 00:33:55.305636: Epoch 163\n",
      "2025-05-17 00:33:55.305699: Current learning rate: 0.00852\n",
      "2025-05-17 00:35:44.180388: train_loss -0.9469\n",
      "2025-05-17 00:35:44.180510: val_loss -0.9448\n",
      "2025-05-17 00:35:44.180812: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 00:35:44.180911: Epoch time: 108.88 s\n",
      "2025-05-17 00:35:44.180986: Yayy! New best EMA pseudo Dice: 0.9724000096321106\n",
      "2025-05-17 00:35:44.937280: \n",
      "2025-05-17 00:35:44.937567: Epoch 164\n",
      "2025-05-17 00:35:44.937649: Current learning rate: 0.00851\n",
      "2025-05-17 00:37:33.811118: train_loss -0.9542\n",
      "2025-05-17 00:37:33.811240: val_loss -0.9362\n",
      "2025-05-17 00:37:33.811274: Pseudo dice [np.float32(0.9692)]\n",
      "2025-05-17 00:37:33.811308: Epoch time: 108.87 s\n",
      "2025-05-17 00:37:34.337969: \n",
      "2025-05-17 00:37:34.338363: Epoch 165\n",
      "2025-05-17 00:37:34.338463: Current learning rate: 0.0085\n",
      "2025-05-17 00:39:23.341185: train_loss -0.9534\n",
      "2025-05-17 00:39:23.341324: val_loss -0.9449\n",
      "2025-05-17 00:39:23.341355: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 00:39:23.341385: Epoch time: 109.0 s\n",
      "2025-05-17 00:39:23.860601: \n",
      "2025-05-17 00:39:23.860880: Epoch 166\n",
      "2025-05-17 00:39:23.861040: Current learning rate: 0.00849\n",
      "2025-05-17 00:41:12.757026: train_loss -0.9563\n",
      "2025-05-17 00:41:12.757149: val_loss -0.9426\n",
      "2025-05-17 00:41:12.757182: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-17 00:41:12.757213: Epoch time: 108.9 s\n",
      "2025-05-17 00:41:13.286145: \n",
      "2025-05-17 00:41:13.286277: Epoch 167\n",
      "2025-05-17 00:41:13.286343: Current learning rate: 0.00848\n",
      "2025-05-17 00:43:02.199219: train_loss -0.9573\n",
      "2025-05-17 00:43:02.199357: val_loss -0.9419\n",
      "2025-05-17 00:43:02.199397: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 00:43:02.199432: Epoch time: 108.91 s\n",
      "2025-05-17 00:43:02.727965: \n",
      "2025-05-17 00:43:02.728109: Epoch 168\n",
      "2025-05-17 00:43:02.728189: Current learning rate: 0.00847\n",
      "2025-05-17 00:44:51.600798: train_loss -0.9542\n",
      "2025-05-17 00:44:51.600924: val_loss -0.9473\n",
      "2025-05-17 00:44:51.600961: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 00:44:51.600993: Epoch time: 108.87 s\n",
      "2025-05-17 00:44:51.601014: Yayy! New best EMA pseudo Dice: 0.9725000262260437\n",
      "2025-05-17 00:44:52.351955: \n",
      "2025-05-17 00:44:52.352043: Epoch 169\n",
      "2025-05-17 00:44:52.352109: Current learning rate: 0.00847\n",
      "2025-05-17 00:46:41.353796: train_loss -0.9543\n",
      "2025-05-17 00:46:41.354012: val_loss -0.9341\n",
      "2025-05-17 00:46:41.354153: Pseudo dice [np.float32(0.9671)]\n",
      "2025-05-17 00:46:41.354208: Epoch time: 109.0 s\n",
      "2025-05-17 00:46:41.882768: \n",
      "2025-05-17 00:46:41.882857: Epoch 170\n",
      "2025-05-17 00:46:41.882951: Current learning rate: 0.00846\n",
      "2025-05-17 00:48:30.886305: train_loss -0.9408\n",
      "2025-05-17 00:48:30.886605: val_loss -0.9365\n",
      "2025-05-17 00:48:30.886837: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-17 00:48:30.886918: Epoch time: 109.0 s\n",
      "2025-05-17 00:48:31.411928: \n",
      "2025-05-17 00:48:31.412129: Epoch 171\n",
      "2025-05-17 00:48:31.412339: Current learning rate: 0.00845\n",
      "2025-05-17 00:50:20.326457: train_loss -0.9326\n",
      "2025-05-17 00:50:20.326582: val_loss -0.934\n",
      "2025-05-17 00:50:20.326616: Pseudo dice [np.float32(0.9703)]\n",
      "2025-05-17 00:50:20.326647: Epoch time: 108.92 s\n",
      "2025-05-17 00:50:20.852176: \n",
      "2025-05-17 00:50:20.852397: Epoch 172\n",
      "2025-05-17 00:50:20.852490: Current learning rate: 0.00844\n",
      "2025-05-17 00:52:09.726592: train_loss -0.9468\n",
      "2025-05-17 00:52:09.726701: val_loss -0.9427\n",
      "2025-05-17 00:52:09.726735: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 00:52:09.726779: Epoch time: 108.88 s\n",
      "2025-05-17 00:52:10.458989: \n",
      "2025-05-17 00:52:10.459089: Epoch 173\n",
      "2025-05-17 00:52:10.459161: Current learning rate: 0.00843\n",
      "2025-05-17 00:53:59.358430: train_loss -0.9288\n",
      "2025-05-17 00:53:59.358575: val_loss -0.9346\n",
      "2025-05-17 00:53:59.358610: Pseudo dice [np.float32(0.9688)]\n",
      "2025-05-17 00:53:59.358644: Epoch time: 108.9 s\n",
      "2025-05-17 00:53:59.905429: \n",
      "2025-05-17 00:53:59.905657: Epoch 174\n",
      "2025-05-17 00:53:59.905753: Current learning rate: 0.00842\n",
      "2025-05-17 00:55:48.833022: train_loss -0.939\n",
      "2025-05-17 00:55:48.833146: val_loss -0.9377\n",
      "2025-05-17 00:55:48.833178: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-17 00:55:48.833211: Epoch time: 108.93 s\n",
      "2025-05-17 00:55:49.356496: \n",
      "2025-05-17 00:55:49.356785: Epoch 175\n",
      "2025-05-17 00:55:49.356965: Current learning rate: 0.00841\n",
      "2025-05-17 00:57:38.299790: train_loss -0.941\n",
      "2025-05-17 00:57:38.299935: val_loss -0.9372\n",
      "2025-05-17 00:57:38.299967: Pseudo dice [np.float32(0.9711)]\n",
      "2025-05-17 00:57:38.300000: Epoch time: 108.94 s\n",
      "2025-05-17 00:57:38.834811: \n",
      "2025-05-17 00:57:38.834959: Epoch 176\n",
      "2025-05-17 00:57:38.835032: Current learning rate: 0.0084\n",
      "2025-05-17 00:59:27.868588: train_loss -0.9482\n",
      "2025-05-17 00:59:27.868760: val_loss -0.9411\n",
      "2025-05-17 00:59:27.868793: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 00:59:27.868827: Epoch time: 109.03 s\n",
      "2025-05-17 00:59:28.392927: \n",
      "2025-05-17 00:59:28.393040: Epoch 177\n",
      "2025-05-17 00:59:28.393104: Current learning rate: 0.00839\n",
      "2025-05-17 01:01:17.426205: train_loss -0.9484\n",
      "2025-05-17 01:01:17.426329: val_loss -0.9389\n",
      "2025-05-17 01:01:17.426361: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-17 01:01:17.426394: Epoch time: 109.03 s\n",
      "2025-05-17 01:01:17.959832: \n",
      "2025-05-17 01:01:17.959934: Epoch 178\n",
      "2025-05-17 01:01:17.960000: Current learning rate: 0.00838\n",
      "2025-05-17 01:03:06.696309: train_loss -0.9518\n",
      "2025-05-17 01:03:06.696434: val_loss -0.9404\n",
      "2025-05-17 01:03:06.696469: Pseudo dice [np.float32(0.9714)]\n",
      "2025-05-17 01:03:06.696503: Epoch time: 108.74 s\n",
      "2025-05-17 01:03:07.223775: \n",
      "2025-05-17 01:03:07.223879: Epoch 179\n",
      "2025-05-17 01:03:07.223944: Current learning rate: 0.00837\n",
      "2025-05-17 01:04:56.140755: train_loss -0.9544\n",
      "2025-05-17 01:04:56.140938: val_loss -0.9449\n",
      "2025-05-17 01:04:56.140972: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 01:04:56.141005: Epoch time: 108.92 s\n",
      "2025-05-17 01:04:56.674593: \n",
      "2025-05-17 01:04:56.674687: Epoch 180\n",
      "2025-05-17 01:04:56.674762: Current learning rate: 0.00836\n",
      "2025-05-17 01:06:45.502742: train_loss -0.9558\n",
      "2025-05-17 01:06:45.503033: val_loss -0.9425\n",
      "2025-05-17 01:06:45.503320: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-17 01:06:45.503525: Epoch time: 108.83 s\n",
      "2025-05-17 01:06:46.025404: \n",
      "2025-05-17 01:06:46.025495: Epoch 181\n",
      "Current learning rate: 0.00836\n",
      "2025-05-17 01:08:34.997983: train_loss -0.9561\n",
      "2025-05-17 01:08:34.998107: val_loss -0.9423\n",
      "2025-05-17 01:08:34.998233: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 01:08:34.998354: Epoch time: 108.97 s\n",
      "2025-05-17 01:08:35.522776: \n",
      "2025-05-17 01:08:35.523109: Epoch 182\n",
      "2025-05-17 01:08:35.523189: Current learning rate: 0.00835\n",
      "2025-05-17 01:10:24.401064: train_loss -0.956\n",
      "2025-05-17 01:10:24.401193: val_loss -0.9429\n",
      "2025-05-17 01:10:24.401225: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-17 01:10:24.401261: Epoch time: 108.88 s\n",
      "2025-05-17 01:10:24.930220: \n",
      "2025-05-17 01:10:24.930431: Epoch 183\n",
      "2025-05-17 01:10:24.930620: Current learning rate: 0.00834\n",
      "2025-05-17 01:12:13.874636: train_loss -0.9586\n",
      "2025-05-17 01:12:13.874995: val_loss -0.9433\n",
      "2025-05-17 01:12:13.875039: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 01:12:13.875072: Epoch time: 108.94 s\n",
      "2025-05-17 01:12:14.392896: \n",
      "2025-05-17 01:12:14.392976: Epoch 184\n",
      "2025-05-17 01:12:14.393039: Current learning rate: 0.00833\n",
      "2025-05-17 01:14:03.279628: train_loss -0.9563\n",
      "2025-05-17 01:14:03.279801: val_loss -0.9448\n",
      "2025-05-17 01:14:03.279834: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 01:14:03.279868: Epoch time: 108.89 s\n",
      "2025-05-17 01:14:03.279889: Yayy! New best EMA pseudo Dice: 0.9725000262260437\n",
      "2025-05-17 01:14:04.227470: \n",
      "2025-05-17 01:14:04.227832: Epoch 185\n",
      "2025-05-17 01:14:04.228120: Current learning rate: 0.00832\n",
      "2025-05-17 01:15:53.051193: train_loss -0.9521\n",
      "2025-05-17 01:15:53.051465: val_loss -0.9446\n",
      "2025-05-17 01:15:53.051770: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 01:15:53.051851: Epoch time: 108.82 s\n",
      "2025-05-17 01:15:53.051881: Yayy! New best EMA pseudo Dice: 0.9725000262260437\n",
      "2025-05-17 01:15:53.792809: \n",
      "2025-05-17 01:15:53.793118: Epoch 186\n",
      "2025-05-17 01:15:53.793199: Current learning rate: 0.00831\n",
      "2025-05-17 01:17:42.786309: train_loss -0.9326\n",
      "2025-05-17 01:17:42.786437: val_loss -0.9204\n",
      "2025-05-17 01:17:42.786474: Pseudo dice [np.float32(0.9614)]\n",
      "2025-05-17 01:17:42.786508: Epoch time: 108.99 s\n",
      "2025-05-17 01:17:43.320875: \n",
      "2025-05-17 01:17:43.321049: Epoch 187\n",
      "2025-05-17 01:17:43.321125: Current learning rate: 0.0083\n",
      "2025-05-17 01:19:32.349189: train_loss -0.9211\n",
      "2025-05-17 01:19:32.349314: val_loss -0.9222\n",
      "2025-05-17 01:19:32.349359: Pseudo dice [np.float32(0.9684)]\n",
      "2025-05-17 01:19:32.349395: Epoch time: 109.03 s\n",
      "2025-05-17 01:19:32.877906: \n",
      "2025-05-17 01:19:32.878126: Epoch 188\n",
      "2025-05-17 01:19:32.878252: Current learning rate: 0.00829\n",
      "2025-05-17 01:21:21.719865: train_loss -0.9396\n",
      "2025-05-17 01:21:21.720096: val_loss -0.9354\n",
      "2025-05-17 01:21:21.720183: Pseudo dice [np.float32(0.9703)]\n",
      "2025-05-17 01:21:21.720245: Epoch time: 108.84 s\n",
      "2025-05-17 01:21:22.247741: \n",
      "2025-05-17 01:21:22.247828: Epoch 189\n",
      "2025-05-17 01:21:22.247906: Current learning rate: 0.00828\n",
      "2025-05-17 01:23:11.097520: train_loss -0.9508\n",
      "2025-05-17 01:23:11.097687: val_loss -0.9417\n",
      "2025-05-17 01:23:11.097729: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-17 01:23:11.097764: Epoch time: 108.85 s\n",
      "2025-05-17 01:23:11.630316: \n",
      "2025-05-17 01:23:11.630529: Epoch 190\n",
      "2025-05-17 01:23:11.630605: Current learning rate: 0.00827\n",
      "2025-05-17 01:25:00.530721: train_loss -0.9502\n",
      "2025-05-17 01:25:00.530851: val_loss -0.942\n",
      "2025-05-17 01:25:00.530888: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 01:25:00.530921: Epoch time: 108.9 s\n",
      "2025-05-17 01:25:01.061485: \n",
      "2025-05-17 01:25:01.061644: Epoch 191\n",
      "2025-05-17 01:25:01.061710: Current learning rate: 0.00826\n",
      "2025-05-17 01:26:49.825783: train_loss -0.9515\n",
      "2025-05-17 01:26:49.825947: val_loss -0.9452\n",
      "2025-05-17 01:26:49.826026: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 01:26:49.826080: Epoch time: 108.76 s\n",
      "2025-05-17 01:26:50.353811: \n",
      "2025-05-17 01:26:50.354142: Epoch 192\n",
      "2025-05-17 01:26:50.354255: Current learning rate: 0.00825\n",
      "2025-05-17 01:28:39.282462: train_loss -0.9524\n",
      "2025-05-17 01:28:39.282659: val_loss -0.9394\n",
      "2025-05-17 01:28:39.282695: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 01:28:39.282729: Epoch time: 108.93 s\n",
      "2025-05-17 01:28:39.815476: \n",
      "2025-05-17 01:28:39.815743: Epoch 193\n",
      "2025-05-17 01:28:39.815856: Current learning rate: 0.00824\n",
      "2025-05-17 01:30:28.718388: train_loss -0.9535\n",
      "2025-05-17 01:30:28.718534: val_loss -0.9437\n",
      "2025-05-17 01:30:28.718569: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 01:30:28.718601: Epoch time: 108.9 s\n",
      "2025-05-17 01:30:29.250576: \n",
      "2025-05-17 01:30:29.250867: Epoch 194\n",
      "2025-05-17 01:30:29.250942: Current learning rate: 0.00824\n",
      "2025-05-17 01:32:18.180867: train_loss -0.953\n",
      "2025-05-17 01:32:18.180996: val_loss -0.9423\n",
      "2025-05-17 01:32:18.181033: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 01:32:18.181066: Epoch time: 108.93 s\n",
      "2025-05-17 01:32:18.710877: \n",
      "2025-05-17 01:32:18.710962: Epoch 195\n",
      "2025-05-17 01:32:18.711025: Current learning rate: 0.00823\n",
      "2025-05-17 01:34:07.442767: train_loss -0.9511\n",
      "2025-05-17 01:34:07.442939: val_loss -0.9373\n",
      "2025-05-17 01:34:07.442975: Pseudo dice [np.float32(0.9713)]\n",
      "2025-05-17 01:34:07.443010: Epoch time: 108.73 s\n",
      "2025-05-17 01:34:07.972409: \n",
      "2025-05-17 01:34:07.972623: Epoch 196\n",
      "2025-05-17 01:34:07.972720: Current learning rate: 0.00822\n",
      "2025-05-17 01:35:56.912854: train_loss -0.9439\n",
      "2025-05-17 01:35:56.913084: val_loss -0.9411\n",
      "2025-05-17 01:35:56.913124: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 01:35:56.913156: Epoch time: 108.94 s\n",
      "2025-05-17 01:35:57.642003: \n",
      "2025-05-17 01:35:57.642163: Epoch 197\n",
      "2025-05-17 01:35:57.642280: Current learning rate: 0.00821\n",
      "2025-05-17 01:37:46.636579: train_loss -0.9555\n",
      "2025-05-17 01:37:46.636710: val_loss -0.9393\n",
      "2025-05-17 01:37:46.636764: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-17 01:37:46.636798: Epoch time: 109.0 s\n",
      "2025-05-17 01:37:47.176060: \n",
      "2025-05-17 01:37:47.176225: Epoch 198\n",
      "2025-05-17 01:37:47.176311: Current learning rate: 0.0082\n",
      "2025-05-17 01:39:36.039732: train_loss -0.9548\n",
      "2025-05-17 01:39:36.039928: val_loss -0.9452\n",
      "2025-05-17 01:39:36.040073: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-17 01:39:36.040184: Epoch time: 108.86 s\n",
      "2025-05-17 01:39:36.573743: \n",
      "2025-05-17 01:39:36.573869: Epoch 199\n",
      "2025-05-17 01:39:36.574166: Current learning rate: 0.00819\n",
      "2025-05-17 01:41:25.582286: train_loss -0.9534\n",
      "2025-05-17 01:41:25.582422: val_loss -0.9469\n",
      "2025-05-17 01:41:25.582455: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 01:41:25.582490: Epoch time: 109.01 s\n",
      "2025-05-17 01:41:26.319960: \n",
      "2025-05-17 01:41:26.320321: Epoch 200\n",
      "2025-05-17 01:41:26.320411: Current learning rate: 0.00818\n",
      "2025-05-17 01:43:15.285494: train_loss -0.9551\n",
      "2025-05-17 01:43:15.285738: val_loss -0.9436\n",
      "2025-05-17 01:43:15.285866: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 01:43:15.285918: Epoch time: 108.97 s\n",
      "2025-05-17 01:43:15.823332: \n",
      "2025-05-17 01:43:15.823469: Epoch 201\n",
      "2025-05-17 01:43:15.823539: Current learning rate: 0.00817\n",
      "2025-05-17 01:45:04.748858: train_loss -0.9576\n",
      "2025-05-17 01:45:04.748987: val_loss -0.9414\n",
      "2025-05-17 01:45:04.749022: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 01:45:04.749055: Epoch time: 108.93 s\n",
      "2025-05-17 01:45:05.285871: \n",
      "2025-05-17 01:45:05.286090: Epoch 202\n",
      "2025-05-17 01:45:05.286168: Current learning rate: 0.00816\n",
      "2025-05-17 01:46:54.163349: train_loss -0.9564\n",
      "2025-05-17 01:46:54.163489: val_loss -0.9387\n",
      "2025-05-17 01:46:54.163539: Pseudo dice [np.float32(0.9717)]\n",
      "2025-05-17 01:46:54.163576: Epoch time: 108.88 s\n",
      "2025-05-17 01:46:54.696919: \n",
      "2025-05-17 01:46:54.697167: Epoch 203\n",
      "2025-05-17 01:46:54.697310: Current learning rate: 0.00815\n",
      "2025-05-17 01:48:43.490025: train_loss -0.9539\n",
      "2025-05-17 01:48:43.490147: val_loss -0.9435\n",
      "2025-05-17 01:48:43.490182: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 01:48:43.490216: Epoch time: 108.79 s\n",
      "2025-05-17 01:48:44.019562: \n",
      "2025-05-17 01:48:44.019656: Epoch 204\n",
      "2025-05-17 01:48:44.019736: Current learning rate: 0.00814\n",
      "2025-05-17 01:50:32.735759: train_loss -0.9571\n",
      "2025-05-17 01:50:32.735927: val_loss -0.9417\n",
      "2025-05-17 01:50:32.735960: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-17 01:50:32.735993: Epoch time: 108.72 s\n",
      "2025-05-17 01:50:33.269955: \n",
      "2025-05-17 01:50:33.270071: Epoch 205\n",
      "2025-05-17 01:50:33.270237: Current learning rate: 0.00813\n",
      "2025-05-17 01:52:22.185303: train_loss -0.9566\n",
      "2025-05-17 01:52:22.185593: val_loss -0.942\n",
      "[np.float32(0.9728)]185664: Pseudo dice \n",
      "2025-05-17 01:52:22.185805: Epoch time: 108.92 s\n",
      "2025-05-17 01:52:22.698067: \n",
      "2025-05-17 01:52:22.698152: Epoch 206\n",
      "2025-05-17 01:52:22.698298: Current learning rate: 0.00813\n",
      "2025-05-17 01:54:11.671137: train_loss -0.9554\n",
      "2025-05-17 01:54:11.671357: val_loss -0.9417\n",
      "2025-05-17 01:54:11.671512: Pseudo dice [np.float32(0.9706)]\n",
      "2025-05-17 01:54:11.671592: Epoch time: 108.97 s\n",
      "2025-05-17 01:54:12.182196: \n",
      "2025-05-17 01:54:12.182279: Epoch 207\n",
      "2025-05-17 01:54:12.182342: Current learning rate: 0.00812\n",
      "2025-05-17 01:56:01.212321: train_loss -0.9559\n",
      "2025-05-17 01:56:01.212457: val_loss -0.9373\n",
      "2025-05-17 01:56:01.212496: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-17 01:56:01.212619: Epoch time: 109.03 s\n",
      "2025-05-17 01:56:01.717951: \n",
      "2025-05-17 01:56:01.718099: Epoch 208\n",
      "2025-05-17 01:56:01.718166: Current learning rate: 0.00811\n",
      "2025-05-17 01:57:50.620526: train_loss -0.9366\n",
      "2025-05-17 01:57:50.620657: val_loss -0.9236\n",
      "2025-05-17 01:57:50.620692: Pseudo dice [np.float32(0.969)]\n",
      "2025-05-17 01:57:50.620725: Epoch time: 108.9 s\n",
      "2025-05-17 01:57:51.337674: \n",
      "2025-05-17 01:57:51.337768: Epoch 209\n",
      "2025-05-17 01:57:51.337831: Current learning rate: 0.0081\n",
      "2025-05-17 01:59:40.338686: train_loss -0.9473\n",
      "2025-05-17 01:59:40.338881: val_loss -0.9408\n",
      "2025-05-17 01:59:40.338916: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-17 01:59:40.338950: Epoch time: 109.0 s\n",
      "2025-05-17 01:59:40.852238: \n",
      "2025-05-17 01:59:40.852331: Epoch 210\n",
      "2025-05-17 01:59:40.852399: Current learning rate: 0.00809\n",
      "2025-05-17 02:01:29.745313: train_loss -0.951\n",
      "2025-05-17 02:01:29.745488: val_loss -0.9376\n",
      "2025-05-17 02:01:29.745540: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-17 02:01:29.745575: Epoch time: 108.89 s\n",
      "2025-05-17 02:01:30.256527: \n",
      "2025-05-17 02:01:30.256621: Epoch 211\n",
      "2025-05-17 02:01:30.256757: Current learning rate: 0.00808\n",
      "2025-05-17 02:03:19.169072: train_loss -0.9546\n",
      "2025-05-17 02:03:19.169207: val_loss -0.9419\n",
      "2025-05-17 02:03:19.169315: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-17 02:03:19.169357: Epoch time: 108.91 s\n",
      "2025-05-17 02:03:19.686251: \n",
      "2025-05-17 02:03:19.686348: Epoch 212\n",
      "2025-05-17 02:03:19.686413: Current learning rate: 0.00807\n",
      "2025-05-17 02:05:08.683002: train_loss -0.9529\n",
      "2025-05-17 02:05:08.683153: val_loss -0.9285\n",
      "2025-05-17 02:05:08.683188: Pseudo dice [np.float32(0.9679)]\n",
      "2025-05-17 02:05:08.683275: Epoch time: 109.0 s\n",
      "2025-05-17 02:05:09.198142: \n",
      "2025-05-17 02:05:09.198237: Epoch 213\n",
      "2025-05-17 02:05:09.198308: Current learning rate: 0.00806\n",
      "2025-05-17 02:06:58.208083: train_loss -0.9506\n",
      "2025-05-17 02:06:58.208211: val_loss -0.9394\n",
      "2025-05-17 02:06:58.208246: Pseudo dice [np.float32(0.9717)]\n",
      "2025-05-17 02:06:58.208309: Epoch time: 109.01 s\n",
      "2025-05-17 02:06:58.720633: \n",
      "2025-05-17 02:06:58.720795: Epoch 214\n",
      "2025-05-17 02:06:58.720861: Current learning rate: 0.00805\n",
      "2025-05-17 02:08:47.645229: train_loss -0.9503\n",
      "2025-05-17 02:08:47.645347: val_loss -0.9461\n",
      "2025-05-17 02:08:47.645380: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-17 02:08:47.645413: Epoch time: 108.93 s\n",
      "2025-05-17 02:08:48.161408: \n",
      "2025-05-17 02:08:48.161556: Epoch 215\n",
      "2025-05-17 02:08:48.161621: Current learning rate: 0.00804\n",
      "2025-05-17 02:10:37.039685: train_loss -0.9545\n",
      "2025-05-17 02:10:37.039829: val_loss -0.9441\n",
      "2025-05-17 02:10:37.039956: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 02:10:37.040009: Epoch time: 108.88 s\n",
      "2025-05-17 02:10:37.550483: \n",
      "2025-05-17 02:10:37.550736: Epoch 216\n",
      "2025-05-17 02:10:37.550853: Current learning rate: 0.00803\n",
      "2025-05-17 02:12:26.333169: train_loss -0.9548\n",
      "2025-05-17 02:12:26.333305: val_loss -0.9421\n",
      "2025-05-17 02:12:26.333353: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-17 02:12:26.333390: Epoch time: 108.78 s\n",
      "2025-05-17 02:12:26.848795: \n",
      "2025-05-17 02:12:26.848878: Epoch 217\n",
      "2025-05-17 02:12:26.848944: Current learning rate: 0.00802\n",
      "2025-05-17 02:14:15.766407: train_loss -0.9572\n",
      "2025-05-17 02:14:15.766594: val_loss -0.941\n",
      "2025-05-17 02:14:15.766630: Pseudo dice [np.float32(0.9709)]\n",
      "2025-05-17 02:14:15.766662: Epoch time: 108.92 s\n",
      "2025-05-17 02:14:16.286488: \n",
      "2025-05-17 02:14:16.286575: Epoch 218\n",
      "2025-05-17 02:14:16.286639: Current learning rate: 0.00801\n",
      "2025-05-17 02:16:05.217345: train_loss -0.955\n",
      "2025-05-17 02:16:05.217463: val_loss -0.9387\n",
      "2025-05-17 02:16:05.217495: Pseudo dice [np.float32(0.9704)]\n",
      "2025-05-17 02:16:05.217564: Epoch time: 108.93 s\n",
      "2025-05-17 02:16:05.730118: \n",
      "2025-05-17 02:16:05.730362: Epoch 219\n",
      "2025-05-17 02:16:05.730557: Current learning rate: 0.00801\n",
      "2025-05-17 02:17:54.716316: train_loss -0.9515\n",
      "2025-05-17 02:17:54.716439: val_loss -0.9334\n",
      "2025-05-17 02:17:54.716473: Pseudo dice [np.float32(0.9701)]\n",
      "2025-05-17 02:17:54.716505: Epoch time: 108.99 s\n",
      "2025-05-17 02:17:55.221767: \n",
      "2025-05-17 02:17:55.221855: Epoch 220\n",
      "2025-05-17 02:17:55.221921: Current learning rate: 0.008\n",
      "2025-05-17 02:19:44.119422: train_loss -0.9563\n",
      "2025-05-17 02:19:44.119579: val_loss -0.9419\n",
      "2025-05-17 02:19:44.119669: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-17 02:19:44.119750: Epoch time: 108.9 s\n",
      "2025-05-17 02:19:44.633221: \n",
      "2025-05-17 02:19:44.633301: Epoch 221\n",
      "2025-05-17 02:19:44.633365: Current learning rate: 0.00799\n",
      "2025-05-17 02:21:33.588955: train_loss -0.9572\n",
      "2025-05-17 02:21:33.589122: val_loss -0.9412\n",
      "2025-05-17 02:21:33.589155: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-17 02:21:33.589188: Epoch time: 108.96 s\n",
      "2025-05-17 02:21:34.300000: \n",
      "2025-05-17 02:21:34.300202: Epoch 222\n",
      "2025-05-17 02:21:34.300286: Current learning rate: 0.00798\n",
      "2025-05-17 02:23:23.273841: train_loss -0.957\n",
      "2025-05-17 02:23:23.274128: val_loss -0.9474\n",
      "2025-05-17 02:23:23.274173: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 02:23:23.274207: Epoch time: 108.97 s\n",
      "2025-05-17 02:23:23.792314: \n",
      "2025-05-17 02:23:23.792465: Epoch 223\n",
      "2025-05-17 02:23:23.792548: Current learning rate: 0.00797\n",
      "2025-05-17 02:25:12.766867: train_loss -0.9566\n",
      "2025-05-17 02:25:12.767001: val_loss -0.9461\n",
      "2025-05-17 02:25:12.767036: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 02:25:12.767069: Epoch time: 108.98 s\n",
      "2025-05-17 02:25:13.272120: \n",
      "2025-05-17 02:25:13.272370: Epoch 224\n",
      "2025-05-17 02:25:13.272443: Current learning rate: 0.00796\n",
      "2025-05-17 02:27:02.032532: train_loss -0.9568\n",
      "2025-05-17 02:27:02.032661: val_loss -0.9434\n",
      "2025-05-17 02:27:02.032820: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 02:27:02.032922: Epoch time: 108.76 s\n",
      "2025-05-17 02:27:02.549628: \n",
      "2025-05-17 02:27:02.549722: Epoch 225\n",
      "2025-05-17 02:27:02.549787: Current learning rate: 0.00795\n",
      "2025-05-17 02:28:51.559497: train_loss -0.9578\n",
      "2025-05-17 02:28:51.559628: val_loss -0.9403\n",
      "2025-05-17 02:28:51.559661: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-17 02:28:51.559696: Epoch time: 109.01 s\n",
      "2025-05-17 02:28:52.068969: \n",
      "2025-05-17 02:28:52.069060: Epoch 226\n",
      "2025-05-17 02:28:52.069125: Current learning rate: 0.00794\n",
      "2025-05-17 02:30:41.053999: train_loss -0.9578\n",
      "2025-05-17 02:30:41.054224: val_loss -0.9481\n",
      "2025-05-17 02:30:41.054420: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 02:30:41.054483: Epoch time: 108.99 s\n",
      "2025-05-17 02:30:41.552516: \n",
      "2025-05-17 02:30:41.552696: Epoch 227\n",
      "2025-05-17 02:30:41.552855: Current learning rate: 0.00793\n",
      "2025-05-17 02:32:30.513200: train_loss -0.9571\n",
      "2025-05-17 02:32:30.513407: val_loss -0.9456\n",
      "2025-05-17 02:32:30.513446: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 02:32:30.513484: Epoch time: 108.96 s\n",
      "2025-05-17 02:32:30.513508: Yayy! New best EMA pseudo Dice: 0.972599983215332\n",
      "2025-05-17 02:32:31.219191: \n",
      "2025-05-17 02:32:31.219396: Epoch 228\n",
      "2025-05-17 02:32:31.219469: Current learning rate: 0.00792\n",
      "2025-05-17 02:34:20.255387: train_loss -0.9572\n",
      "2025-05-17 02:34:20.255520: val_loss -0.941\n",
      "2025-05-17 02:34:20.255555: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 02:34:20.255589: Epoch time: 109.04 s\n",
      "2025-05-17 02:34:20.759562: \n",
      "2025-05-17 02:34:20.759657: Epoch 229\n",
      "2025-05-17 02:34:20.759723: Current learning rate: 0.00791\n",
      "2025-05-17 02:36:09.677963: train_loss -0.9579\n",
      "2025-05-17 02:36:09.678406: val_loss -0.9456\n",
      "2025-05-17 02:36:09.678474: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 02:36:09.678514: Epoch time: 108.92 s\n",
      "2025-05-17 02:36:09.678535: Yayy! New best EMA pseudo Dice: 0.9726999998092651\n",
      "2025-05-17 02:36:10.397317: \n",
      "2025-05-17 02:36:10.397595: Epoch 230\n",
      "Current learning rate: 0.0079\n",
      "2025-05-17 02:37:59.410494: train_loss -0.9578\n",
      "2025-05-17 02:37:59.410618: val_loss -0.9461\n",
      "2025-05-17 02:37:59.410789: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 02:37:59.410833: Epoch time: 109.01 s\n",
      "2025-05-17 02:37:59.410907: Yayy! New best EMA pseudo Dice: 0.9728999733924866\n",
      "2025-05-17 02:38:00.138189: \n",
      "2025-05-17 02:38:00.138396: Epoch 231\n",
      "2025-05-17 02:38:00.138474: Current learning rate: 0.00789\n",
      "2025-05-17 02:39:49.061555: train_loss -0.9557\n",
      "2025-05-17 02:39:49.061907: val_loss -0.9468\n",
      "2025-05-17 02:39:49.061948: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-17 02:39:49.061984: Epoch time: 108.92 s\n",
      "2025-05-17 02:39:49.062006: Yayy! New best EMA pseudo Dice: 0.9731000065803528\n",
      "2025-05-17 02:39:49.786568: \n",
      "2025-05-17 02:39:49.786647: Epoch 232\n",
      "2025-05-17 02:39:49.786708: Current learning rate: 0.00789\n",
      "2025-05-17 02:41:38.615325: train_loss -0.9589\n",
      "2025-05-17 02:41:38.615447: val_loss -0.9449\n",
      "2025-05-17 02:41:38.615481: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 02:41:38.615515: Epoch time: 108.83 s\n",
      "2025-05-17 02:41:38.615534: Yayy! New best EMA pseudo Dice: 0.9732000231742859\n",
      "2025-05-17 02:41:39.341419: \n",
      "2025-05-17 02:41:39.341588: Epoch 233\n",
      "2025-05-17 02:41:39.341671: Current learning rate: 0.00788\n",
      "2025-05-17 02:43:28.377856: train_loss -0.957\n",
      "2025-05-17 02:43:28.377978: val_loss -0.9438\n",
      "2025-05-17 02:43:28.378083: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 02:43:28.378126: Epoch time: 109.04 s\n",
      "2025-05-17 02:43:28.882486: \n",
      "2025-05-17 02:43:28.882567: Epoch 234\n",
      "2025-05-17 02:43:28.882629: Current learning rate: 0.00787\n",
      "2025-05-17 02:45:17.845093: train_loss -0.9599\n",
      "2025-05-17 02:45:17.845219: val_loss -0.942\n",
      "2025-05-17 02:45:17.845253: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-17 02:45:17.845316: Epoch time: 108.96 s\n",
      "2025-05-17 02:45:18.534861: \n",
      "2025-05-17 02:45:18.534957: Epoch 235\n",
      "2025-05-17 02:45:18.535079: Current learning rate: 0.00786\n",
      "2025-05-17 02:47:07.581239: train_loss -0.9581\n",
      "2025-05-17 02:47:07.581432: val_loss -0.9352\n",
      "2025-05-17 02:47:07.581466: Pseudo dice [np.float32(0.9698)]\n",
      "2025-05-17 02:47:07.581500: Epoch time: 109.05 s\n",
      "2025-05-17 02:47:08.091822: \n",
      "2025-05-17 02:47:08.092032: Epoch 236\n",
      "2025-05-17 02:47:08.092338: Current learning rate: 0.00785\n",
      "2025-05-17 02:48:57.122849: train_loss -0.9588\n",
      "2025-05-17 02:48:57.123100: val_loss -0.9461\n",
      "2025-05-17 02:48:57.123228: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 02:48:57.123323: Epoch time: 109.03 s\n",
      "2025-05-17 02:48:57.630960: \n",
      "2025-05-17 02:48:57.631058: Epoch 237\n",
      "2025-05-17 02:48:57.631124: Current learning rate: 0.00784\n",
      "2025-05-17 02:50:46.505453: train_loss -0.9598\n",
      "2025-05-17 02:50:46.505589: val_loss -0.946\n",
      "2025-05-17 02:50:46.505623: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 02:50:46.505656: Epoch time: 108.88 s\n",
      "2025-05-17 02:50:47.022899: \n",
      "2025-05-17 02:50:47.023019: Epoch 238\n",
      "2025-05-17 02:50:47.023110: Current learning rate: 0.00783\n",
      "2025-05-17 02:52:36.016870: train_loss -0.959\n",
      "2025-05-17 02:52:36.016993: val_loss -0.9418\n",
      "2025-05-17 02:52:36.017026: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 02:52:36.017060: Epoch time: 108.99 s\n",
      "2025-05-17 02:52:36.530112: \n",
      "2025-05-17 02:52:36.530210: Epoch 239\n",
      "2025-05-17 02:52:36.530274: Current learning rate: 0.00782\n",
      "2025-05-17 02:54:25.468432: train_loss -0.9577\n",
      "2025-05-17 02:54:25.468602: val_loss -0.9409\n",
      "2025-05-17 02:54:25.468635: Pseudo dice [np.float32(0.9708)]\n",
      "2025-05-17 02:54:25.468675: Epoch time: 108.94 s\n",
      "2025-05-17 02:54:25.973898: \n",
      "2025-05-17 02:54:25.974061: Epoch 240\n",
      "2025-05-17 02:54:25.974175: Current learning rate: 0.00781\n",
      "2025-05-17 02:56:14.736555: train_loss -0.9583\n",
      "2025-05-17 02:56:14.736686: val_loss -0.9471\n",
      "2025-05-17 02:56:14.736734: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-17 02:56:14.736771: Epoch time: 108.76 s\n",
      "2025-05-17 02:56:15.248368: \n",
      "2025-05-17 02:56:15.248534: Epoch 241\n",
      "2025-05-17 02:56:15.248769: Current learning rate: 0.0078\n",
      "2025-05-17 02:58:04.163140: train_loss -0.9581\n",
      "2025-05-17 02:58:04.163277: val_loss -0.9465\n",
      "2025-05-17 02:58:04.163313: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-17 02:58:04.163503: Epoch time: 108.92 s\n",
      "2025-05-17 02:58:04.692190: \n",
      "2025-05-17 02:58:04.692356: Epoch 242\n",
      "2025-05-17 02:58:04.692434: Current learning rate: 0.00779\n",
      "2025-05-17 02:59:53.495629: train_loss -0.9589\n",
      "2025-05-17 02:59:53.495767: val_loss -0.9455\n",
      "2025-05-17 02:59:53.495804: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 02:59:53.495839: Epoch time: 108.8 s\n",
      "2025-05-17 02:59:54.004458: \n",
      "2025-05-17 02:59:54.004809: Epoch 243\n",
      "2025-05-17 02:59:54.004911: Current learning rate: 0.00778\n",
      "2025-05-17 03:01:42.991802: train_loss -0.9588\n",
      "2025-05-17 03:01:42.991930: val_loss -0.9421\n",
      "2025-05-17 03:01:42.991965: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 03:01:42.991998: Epoch time: 108.99 s\n",
      "2025-05-17 03:01:43.498621: \n",
      "2025-05-17 03:01:43.498765: Epoch 244\n",
      "2025-05-17 03:01:43.498839: Current learning rate: 0.00777\n",
      "2025-05-17 03:03:32.474153: train_loss -0.934\n",
      "2025-05-17 03:03:32.474338: val_loss -0.9311\n",
      "2025-05-17 03:03:32.474374: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-17 03:03:32.474407: Epoch time: 108.98 s\n",
      "2025-05-17 03:03:32.983476: \n",
      "2025-05-17 03:03:32.983630: Epoch 245\n",
      "2025-05-17 03:03:32.983738: Current learning rate: 0.00777\n",
      "2025-05-17 03:05:22.001188: train_loss -0.9136\n",
      "2025-05-17 03:05:22.001364: val_loss -0.9237\n",
      "2025-05-17 03:05:22.001398: Pseudo dice [np.float32(0.9667)]\n",
      "2025-05-17 03:05:22.001474: Epoch time: 109.02 s\n",
      "2025-05-17 03:05:22.512238: \n",
      "2025-05-17 03:05:22.512336: Epoch 246\n",
      "2025-05-17 03:05:22.512402: Current learning rate: 0.00776\n",
      "2025-05-17 03:07:11.316504: train_loss -0.9393\n",
      "2025-05-17 03:07:11.316687: val_loss -0.9236\n",
      "2025-05-17 03:07:11.316718: Pseudo dice [np.float32(0.9664)]\n",
      "2025-05-17 03:07:11.316751: Epoch time: 108.8 s\n",
      "2025-05-17 03:07:11.823767: \n",
      "2025-05-17 03:07:11.823906: Epoch 247\n",
      "2025-05-17 03:07:11.823981: Current learning rate: 0.00775\n",
      "2025-05-17 03:09:00.799825: train_loss -0.9462\n",
      "2025-05-17 03:09:00.800050: val_loss -0.9349\n",
      "2025-05-17 03:09:00.800086: Pseudo dice [np.float32(0.9702)]\n",
      "2025-05-17 03:09:00.800119: Epoch time: 108.98 s\n",
      "2025-05-17 03:09:01.514802: \n",
      "2025-05-17 03:09:01.515072: Epoch 248\n",
      "2025-05-17 03:09:01.515162: Current learning rate: 0.00774\n",
      "2025-05-17 03:10:50.397829: train_loss -0.9439\n",
      "2025-05-17 03:10:50.397963: val_loss -0.9374\n",
      "2025-05-17 03:10:50.398000: Pseudo dice [np.float32(0.971)]\n",
      "2025-05-17 03:10:50.398045: Epoch time: 108.88 s\n",
      "2025-05-17 03:10:50.900998: \n",
      "2025-05-17 03:10:50.901128: Epoch 249\n",
      "2025-05-17 03:10:50.901505: Current learning rate: 0.00773\n",
      "2025-05-17 03:12:39.883769: train_loss -0.9521\n",
      "2025-05-17 03:12:39.883914: val_loss -0.9412\n",
      "2025-05-17 03:12:39.883949: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-17 03:12:39.883982: Epoch time: 108.98 s\n",
      "2025-05-17 03:12:40.604154: \n",
      "2025-05-17 03:12:40.604330: Epoch 250\n",
      "2025-05-17 03:12:40.604407: Current learning rate: 0.00772\n",
      "2025-05-17 03:14:29.592521: train_loss -0.951\n",
      "2025-05-17 03:14:29.592651: val_loss -0.9385\n",
      "2025-05-17 03:14:29.592684: Pseudo dice [np.float32(0.9705)]\n",
      "2025-05-17 03:14:29.592716: Epoch time: 108.99 s\n",
      "2025-05-17 03:14:30.107002: \n",
      "2025-05-17 03:14:30.107374: Epoch 251\n",
      "2025-05-17 03:14:30.107487: Current learning rate: 0.00771\n",
      "2025-05-17 03:16:19.021569: train_loss -0.9537\n",
      "2025-05-17 03:16:19.021698: val_loss -0.9463\n",
      "2025-05-17 03:16:19.021746: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 03:16:19.021782: Epoch time: 108.92 s\n",
      "2025-05-17 03:16:19.535929: \n",
      "2025-05-17 03:16:19.536475: Epoch 252\n",
      "2025-05-17 03:16:19.536602: Current learning rate: 0.0077\n",
      "2025-05-17 03:18:08.481457: train_loss -0.9514\n",
      "2025-05-17 03:18:08.481599: val_loss -0.9442\n",
      "2025-05-17 03:18:08.481632: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-17 03:18:08.481664: Epoch time: 108.95 s\n",
      "2025-05-17 03:18:08.994502: \n",
      "2025-05-17 03:18:08.994691: Epoch 253\n",
      "2025-05-17 03:18:08.994878: Current learning rate: 0.00769\n",
      "2025-05-17 03:19:57.966695: train_loss -0.9463\n",
      "2025-05-17 03:19:57.966825: val_loss -0.8937\n",
      "2025-05-17 03:19:57.966858: Pseudo dice [np.float32(0.9502)]\n",
      "2025-05-17 03:19:57.966892: Epoch time: 108.97 s\n",
      "2025-05-17 03:19:58.481577: \n",
      "2025-05-17 03:19:58.481863: Epoch 254\n",
      "2025-05-17 03:19:58.482021: Current learning rate: 0.00768\n",
      "2025-05-17 03:21:47.503403: train_loss -0.9338\n",
      "2025-05-17 03:21:47.503586: val_loss -0.9243\n",
      "2025-05-17 03:21:47.503679: Pseudo dice [np.float32(0.9645)]\n",
      "2025-05-17 03:21:47.503735: Epoch time: 109.02 s\n",
      "2025-05-17 03:21:48.014738: \n",
      "2025-05-17 03:21:48.014911: Epoch 255\n",
      "2025-05-17 03:21:48.015019: Current learning rate: 0.00767\n",
      "2025-05-17 03:23:37.036710: train_loss -0.9457\n",
      "2025-05-17 03:23:37.036841: val_loss -0.9395\n",
      "2025-05-17 03:23:37.036875: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 03:23:37.036908: Epoch time: 109.02 s\n",
      "2025-05-17 03:23:37.555112: \n",
      "2025-05-17 03:23:37.555198: Epoch 256\n",
      "2025-05-17 03:23:37.555261: Current learning rate: 0.00766\n",
      "2025-05-17 03:25:26.460107: train_loss -0.9519\n",
      "2025-05-17 03:25:26.460306: val_loss -0.9454\n",
      "2025-05-17 03:25:26.460409: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 03:25:26.460491: Epoch time: 108.91 s\n",
      "2025-05-17 03:25:26.974430: \n",
      "2025-05-17 03:25:26.974516: Epoch 257\n",
      "2025-05-17 03:25:26.974581: Current learning rate: 0.00765\n",
      "2025-05-17 03:27:15.840195: train_loss -0.9458\n",
      "2025-05-17 03:27:15.840468: val_loss -0.937\n",
      "2025-05-17 03:27:15.840509: Pseudo dice [np.float32(0.9707)]\n",
      "2025-05-17 03:27:15.840544: Epoch time: 108.87 s\n",
      "2025-05-17 03:27:16.355322: \n",
      "2025-05-17 03:27:16.355465: Epoch 258\n",
      "2025-05-17 03:27:16.355530: Current learning rate: 0.00764\n",
      "2025-05-17 03:29:05.267792: train_loss -0.9516\n",
      "2025-05-17 03:29:05.267915: val_loss -0.9433\n",
      "2025-05-17 03:29:05.267947: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 03:29:05.267980: Epoch time: 108.91 s\n",
      "2025-05-17 03:29:05.773703: \n",
      "2025-05-17 03:29:05.773786: Epoch 259\n",
      "2025-05-17 03:29:05.773849: Current learning rate: 0.00764\n",
      "2025-05-17 03:30:54.763344: train_loss -0.952\n",
      "2025-05-17 03:30:54.763477: val_loss -0.9275\n",
      "2025-05-17 03:30:54.763513: Pseudo dice [np.float32(0.9687)]\n",
      "2025-05-17 03:30:54.763548: Epoch time: 108.99 s\n",
      "2025-05-17 03:30:55.274795: \n",
      "2025-05-17 03:30:55.274950: Epoch 260\n",
      "2025-05-17 03:30:55.275016: Current learning rate: 0.00763\n",
      "2025-05-17 03:32:43.982434: train_loss -0.9355\n",
      "2025-05-17 03:32:43.982757: val_loss -0.9301\n",
      "2025-05-17 03:32:43.982802: Pseudo dice [np.float32(0.9693)]\n",
      "2025-05-17 03:32:43.982837: Epoch time: 108.71 s\n",
      "2025-05-17 03:32:44.684790: \n",
      "2025-05-17 03:32:44.684883: Epoch 261\n",
      "2025-05-17 03:32:44.684948: Current learning rate: 0.00762\n",
      "2025-05-17 03:34:33.631053: train_loss -0.9423\n",
      "2025-05-17 03:34:33.631188: val_loss -0.9387\n",
      "2025-05-17 03:34:33.631224: Pseudo dice [np.float32(0.9711)]\n",
      "2025-05-17 03:34:33.631258: Epoch time: 108.95 s\n",
      "2025-05-17 03:34:34.148230: \n",
      "2025-05-17 03:34:34.148416: Epoch 262\n",
      "2025-05-17 03:34:34.148486: Current learning rate: 0.00761\n",
      "2025-05-17 03:36:23.048977: train_loss -0.9473\n",
      "2025-05-17 03:36:23.049099: val_loss -0.9388\n",
      "2025-05-17 03:36:23.049133: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-17 03:36:23.049167: Epoch time: 108.9 s\n",
      "2025-05-17 03:36:23.558950: \n",
      "2025-05-17 03:36:23.559109: Epoch 263\n",
      "2025-05-17 03:36:23.559173: Current learning rate: 0.0076\n",
      "2025-05-17 03:38:12.525444: train_loss -0.952\n",
      "2025-05-17 03:38:12.525613: val_loss -0.938\n",
      "2025-05-17 03:38:12.525647: Pseudo dice [np.float32(0.9701)]\n",
      "2025-05-17 03:38:12.525682: Epoch time: 108.97 s\n",
      "2025-05-17 03:38:13.044050: \n",
      "2025-05-17 03:38:13.044217: Epoch 264\n",
      "2025-05-17 03:38:13.044291: Current learning rate: 0.00759\n",
      "2025-05-17 03:40:01.865098: train_loss -0.956\n",
      "2025-05-17 03:40:01.865321: val_loss -0.9426\n",
      "2025-05-17 03:40:01.865441: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 03:40:01.865516: Epoch time: 108.82 s\n",
      "2025-05-17 03:40:02.380181: \n",
      "2025-05-17 03:40:02.380609: Epoch 265\n",
      "2025-05-17 03:40:02.380677: Current learning rate: 0.00758\n",
      "2025-05-17 03:41:51.328688: train_loss -0.9566\n",
      "2025-05-17 03:41:51.328912: val_loss -0.939\n",
      "2025-05-17 03:41:51.328951: Pseudo dice [np.float32(0.9699)]\n",
      "2025-05-17 03:41:51.328984: Epoch time: 108.95 s\n",
      "2025-05-17 03:41:51.836964: \n",
      "2025-05-17 03:41:51.837160: Epoch 266\n",
      "2025-05-17 03:41:51.837238: Current learning rate: 0.00757\n",
      "2025-05-17 03:43:40.611908: train_loss -0.956\n",
      "2025-05-17 03:43:40.612052: val_loss -0.9454\n",
      "2025-05-17 03:43:40.612127: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 03:43:40.612168: Epoch time: 108.78 s\n",
      "2025-05-17 03:43:41.119840: \n",
      "2025-05-17 03:43:41.119931: Epoch 267\n",
      "2025-05-17 03:43:41.119997: Current learning rate: 0.00756\n",
      "2025-05-17 03:45:30.134858: train_loss -0.956\n",
      "2025-05-17 03:45:30.134999: val_loss -0.9446\n",
      "2025-05-17 03:45:30.135032: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 03:45:30.135066: Epoch time: 109.02 s\n",
      "2025-05-17 03:45:30.647624: \n",
      "2025-05-17 03:45:30.647771: Epoch 268\n",
      "2025-05-17 03:45:30.647843: Current learning rate: 0.00755\n",
      "2025-05-17 03:47:19.653449: train_loss -0.956\n",
      "2025-05-17 03:47:19.653655: val_loss -0.9331\n",
      "2025-05-17 03:47:19.653700: Pseudo dice [np.float32(0.9697)]\n",
      "2025-05-17 03:47:19.653734: Epoch time: 109.01 s\n",
      "2025-05-17 03:47:20.165413: \n",
      "2025-05-17 03:47:20.165564: Epoch 269\n",
      "2025-05-17 03:47:20.165663: Current learning rate: 0.00754\n",
      "2025-05-17 03:49:09.062682: train_loss -0.9561\n",
      "2025-05-17 03:49:09.062805: val_loss -0.9431\n",
      "2025-05-17 03:49:09.062839: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-17 03:49:09.062871: Epoch time: 108.9 s\n",
      "2025-05-17 03:49:09.571930: \n",
      "2025-05-17 03:49:09.572052: Epoch 270\n",
      "2025-05-17 03:49:09.572137: Current learning rate: 0.00753\n",
      "2025-05-17 03:50:58.507709: train_loss -0.9557\n",
      "2025-05-17 03:50:58.507846: val_loss -0.9504\n",
      "2025-05-17 03:50:58.507880: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-17 03:50:58.507913: Epoch time: 108.94 s\n",
      "2025-05-17 03:50:59.018598: \n",
      "2025-05-17 03:50:59.018730: Epoch 271\n",
      "2025-05-17 03:50:59.018797: Current learning rate: 0.00752\n",
      "2025-05-17 03:52:48.032108: train_loss -0.9545\n",
      "2025-05-17 03:52:48.032465: val_loss -0.9425\n",
      "2025-05-17 03:52:48.032550: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-17 03:52:48.032607: Epoch time: 109.01 s\n",
      "2025-05-17 03:52:48.546811: \n",
      "2025-05-17 03:52:48.547033: Epoch 272\n",
      "2025-05-17 03:52:48.547107: Current learning rate: 0.00751\n",
      "2025-05-17 03:54:37.557116: train_loss -0.9549\n",
      "2025-05-17 03:54:37.557250: val_loss -0.9392\n",
      "2025-05-17 03:54:37.557285: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 03:54:37.557345: Epoch time: 109.01 s\n",
      "2025-05-17 03:54:38.265932: \n",
      "2025-05-17 03:54:38.266023: Epoch 273\n",
      "2025-05-17 03:54:38.266092: Current learning rate: 0.00751\n",
      "2025-05-17 03:56:27.239595: train_loss -0.9585\n",
      "2025-05-17 03:56:27.239801: val_loss -0.9352\n",
      "2025-05-17 03:56:27.239835: Pseudo dice [np.float32(0.9695)]\n",
      "2025-05-17 03:56:27.239867: Epoch time: 108.97 s\n",
      "2025-05-17 03:56:27.765258: \n",
      "2025-05-17 03:56:27.765352: Epoch 274\n",
      "2025-05-17 03:56:27.765417: Current learning rate: 0.0075\n",
      "2025-05-17 03:58:16.633336: train_loss -0.9524\n",
      "2025-05-17 03:58:16.633462: val_loss -0.9449\n",
      "2025-05-17 03:58:16.633500: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 03:58:16.633536: Epoch time: 108.87 s\n",
      "2025-05-17 03:58:17.143010: \n",
      "2025-05-17 03:58:17.143102: Epoch 275\n",
      "2025-05-17 03:58:17.143169: Current learning rate: 0.00749\n",
      "2025-05-17 04:00:05.960647: train_loss -0.9595\n",
      "2025-05-17 04:00:05.960836: val_loss -0.9429\n",
      "2025-05-17 04:00:05.960873: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 04:00:05.960909: Epoch time: 108.82 s\n",
      "2025-05-17 04:00:06.473446: \n",
      "2025-05-17 04:00:06.473758: Epoch 276\n",
      "2025-05-17 04:00:06.473862: Current learning rate: 0.00748\n",
      "2025-05-17 04:01:55.449894: train_loss -0.9564\n",
      "2025-05-17 04:01:55.450040: val_loss -0.9503\n",
      "2025-05-17 04:01:55.450075: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 04:01:55.450109: Epoch time: 108.98 s\n",
      "2025-05-17 04:01:55.965052: \n",
      "2025-05-17 04:01:55.965227: Epoch 277\n",
      "2025-05-17 04:01:55.965305: Current learning rate: 0.00747\n",
      "2025-05-17 04:03:44.942109: train_loss -0.9569\n",
      "2025-05-17 04:03:44.942342: val_loss -0.9461\n",
      "2025-05-17 04:03:44.942455: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 04:03:44.942530: Epoch time: 108.98 s\n",
      "2025-05-17 04:03:45.463873: \n",
      "2025-05-17 04:03:45.464230: Epoch 278\n",
      "2025-05-17 04:03:45.464316: Current learning rate: 0.00746\n",
      "2025-05-17 04:05:34.394871: train_loss -0.9574\n",
      "2025-05-17 04:05:34.395225: val_loss -0.948\n",
      "2025-05-17 04:05:34.395290: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 04:05:34.395332: Epoch time: 108.93 s\n",
      "2025-05-17 04:05:34.913568: \n",
      "2025-05-17 04:05:34.913734: Epoch 279\n",
      "2025-05-17 04:05:34.913803: Current learning rate: 0.00745\n",
      "2025-05-17 04:07:23.761076: train_loss -0.9561\n",
      "2025-05-17 04:07:23.761214: val_loss -0.9451\n",
      "2025-05-17 04:07:23.761306: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 04:07:23.761362: Epoch time: 108.85 s\n",
      "2025-05-17 04:07:24.287159: \n",
      "2025-05-17 04:07:24.287351: Epoch 280\n",
      "2025-05-17 04:07:24.287488: Current learning rate: 0.00744\n",
      "2025-05-17 04:09:13.197639: train_loss -0.9246\n",
      "2025-05-17 04:09:13.197798: val_loss -0.906\n",
      "2025-05-17 04:09:13.197893: Pseudo dice [np.float32(0.9589)]\n",
      "2025-05-17 04:09:13.197981: Epoch time: 108.91 s\n",
      "2025-05-17 04:09:13.711383: \n",
      "2025-05-17 04:09:13.711528: Epoch 281\n",
      "2025-05-17 04:09:13.711691: Current learning rate: 0.00743\n",
      "2025-05-17 04:11:02.647072: train_loss -0.93\n",
      "2025-05-17 04:11:02.647277: val_loss -0.9202\n",
      "2025-05-17 04:11:02.647353: Pseudo dice [np.float32(0.9668)]\n",
      "2025-05-17 04:11:02.647448: Epoch time: 108.94 s\n",
      "2025-05-17 04:11:03.157651: \n",
      "2025-05-17 04:11:03.157741: Epoch 282\n",
      "2025-05-17 04:11:03.157803: Current learning rate: 0.00742\n",
      "2025-05-17 04:12:52.040412: train_loss -0.9356\n",
      "2025-05-17 04:12:52.040559: val_loss -0.9341\n",
      "2025-05-17 04:12:52.040594: Pseudo dice [np.float32(0.9702)]\n",
      "2025-05-17 04:12:52.040629: Epoch time: 108.88 s\n",
      "2025-05-17 04:12:52.558233: \n",
      "2025-05-17 04:12:52.558400: Epoch 283\n",
      "2025-05-17 04:12:52.558481: Current learning rate: 0.00741\n",
      "2025-05-17 04:14:41.494193: train_loss -0.9493\n",
      "2025-05-17 04:14:41.494404: val_loss -0.9387\n",
      "2025-05-17 04:14:41.494494: Pseudo dice [np.float32(0.971)]\n",
      "2025-05-17 04:14:41.494601: Epoch time: 108.94 s\n",
      "2025-05-17 04:14:42.003684: \n",
      "2025-05-17 04:14:42.003812: Epoch 284\n",
      "2025-05-17 04:14:42.003878: Current learning rate: 0.0074\n",
      "2025-05-17 04:16:30.867880: train_loss -0.9539\n",
      "2025-05-17 04:16:30.868154: val_loss -0.9418\n",
      "2025-05-17 04:16:30.868199: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 04:16:30.868235: Epoch time: 108.86 s\n",
      "2025-05-17 04:16:31.377128: \n",
      "2025-05-17 04:16:31.377208: Epoch 285\n",
      "2025-05-17 04:16:31.377433: Current learning rate: 0.00739\n",
      "2025-05-17 04:18:20.321627: train_loss -0.9495\n",
      "2025-05-17 04:18:20.321798: val_loss -0.934\n",
      "2025-05-17 04:18:20.321832: Pseudo dice [np.float32(0.969)]\n",
      "2025-05-17 04:18:20.321864: Epoch time: 108.95 s\n",
      "2025-05-17 04:18:21.034183: \n",
      "2025-05-17 04:18:21.034270: Epoch 286\n",
      "2025-05-17 04:18:21.034338: Current learning rate: 0.00738\n",
      "2025-05-17 04:20:10.020381: train_loss -0.9514\n",
      "2025-05-17 04:20:10.020515: val_loss -0.9421\n",
      "2025-05-17 04:20:10.020552: Pseudo dice [np.float32(0.9708)]\n",
      "2025-05-17 04:20:10.020585: Epoch time: 108.99 s\n",
      "2025-05-17 04:20:10.536463: \n",
      "2025-05-17 04:20:10.536559: Epoch 287\n",
      "2025-05-17 04:20:10.536623: Current learning rate: 0.00738\n",
      "2025-05-17 04:21:59.422111: train_loss -0.9485\n",
      "2025-05-17 04:21:59.422255: val_loss -0.8737\n",
      "2025-05-17 04:21:59.422292: Pseudo dice [np.float32(0.9461)]\n",
      "2025-05-17 04:21:59.422325: Epoch time: 108.89 s\n",
      "2025-05-17 04:21:59.950113: \n",
      "2025-05-17 04:21:59.950261: Epoch 288\n",
      "2025-05-17 04:21:59.950329: Current learning rate: 0.00737\n",
      "2025-05-17 04:23:48.924682: train_loss -0.9428\n",
      "2025-05-17 04:23:48.924830: val_loss -0.9221\n",
      "2025-05-17 04:23:48.924866: Pseudo dice [np.float32(0.9657)]\n",
      "2025-05-17 04:23:48.924990: Epoch time: 108.98 s\n",
      "2025-05-17 04:23:49.449535: \n",
      "2025-05-17 04:23:49.449831: Epoch 289\n",
      "2025-05-17 04:23:49.449938: Current learning rate: 0.00736\n",
      "2025-05-17 04:25:38.418198: train_loss -0.931\n",
      "2025-05-17 04:25:38.418347: val_loss -0.9107\n",
      "2025-05-17 04:25:38.418494: Pseudo dice [np.float32(0.9604)]\n",
      "2025-05-17 04:25:38.418547: Epoch time: 108.97 s\n",
      "2025-05-17 04:25:38.951454: \n",
      "2025-05-17 04:25:38.951539: Epoch 290\n",
      "2025-05-17 04:25:38.951603: Current learning rate: 0.00735\n",
      "2025-05-17 04:27:27.870955: train_loss -0.9486\n",
      "2025-05-17 04:27:27.871127: val_loss -0.9357\n",
      "2025-05-17 04:27:27.871162: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-17 04:27:27.871194: Epoch time: 108.92 s\n",
      "2025-05-17 04:27:28.395605: \n",
      "2025-05-17 04:27:28.395707: Epoch 291\n",
      "2025-05-17 04:27:28.395771: Current learning rate: 0.00734\n",
      "2025-05-17 04:29:17.418119: train_loss -0.9516\n",
      "2025-05-17 04:29:17.418262: val_loss -0.9427\n",
      "2025-05-17 04:29:17.418347: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 04:29:17.418386: Epoch time: 109.02 s\n",
      "2025-05-17 04:29:17.941278: \n",
      "2025-05-17 04:29:17.941432: Epoch 292\n",
      "2025-05-17 04:29:17.941502: Current learning rate: 0.00733\n",
      "2025-05-17 04:31:06.931246: train_loss -0.9523\n",
      "2025-05-17 04:31:06.931418: val_loss -0.9423\n",
      "2025-05-17 04:31:06.931453: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-17 04:31:06.931487: Epoch time: 108.99 s\n",
      "2025-05-17 04:31:07.453806: \n",
      "2025-05-17 04:31:07.453943: Epoch 293\n",
      "2025-05-17 04:31:07.454056: Current learning rate: 0.00732\n",
      "2025-05-17 04:32:56.443451: train_loss -0.9541\n",
      "2025-05-17 04:32:56.443592: val_loss -0.94\n",
      "2025-05-17 04:32:56.443625: Pseudo dice [np.float32(0.9716)]\n",
      "2025-05-17 04:32:56.443658: Epoch time: 108.99 s\n",
      "2025-05-17 04:32:56.963348: \n",
      "2025-05-17 04:32:56.963511: Epoch 294\n",
      "2025-05-17 04:32:56.963579: Current learning rate: 0.00731\n",
      "2025-05-17 04:34:45.717202: train_loss -0.9461\n",
      "2025-05-17 04:34:45.717340: val_loss -0.9348\n",
      "2025-05-17 04:34:45.717373: Pseudo dice [np.float32(0.9705)]\n",
      "2025-05-17 04:34:45.717408: Epoch time: 108.75 s\n",
      "2025-05-17 04:34:46.240847: \n",
      "2025-05-17 04:34:46.240988: Epoch 295\n",
      "2025-05-17 04:34:46.241055: Current learning rate: 0.0073\n",
      "2025-05-17 04:36:35.230752: train_loss -0.9531\n",
      "2025-05-17 04:36:35.230878: val_loss -0.9441\n",
      "2025-05-17 04:36:35.230916: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-17 04:36:35.230949: Epoch time: 108.99 s\n",
      "2025-05-17 04:36:35.745706: \n",
      "2025-05-17 04:36:35.745875: Epoch 296\n",
      "2025-05-17 04:36:35.745992: Current learning rate: 0.00729\n",
      "2025-05-17 04:38:24.740431: train_loss -0.9547\n",
      "2025-05-17 04:38:24.740807: val_loss -0.9442\n",
      "2025-05-17 04:38:24.740848: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-17 04:38:24.741002: Epoch time: 109.0 s\n",
      "2025-05-17 04:38:25.282924: \n",
      "2025-05-17 04:38:25.283082: Epoch 297\n",
      "2025-05-17 04:38:25.283177: Current learning rate: 0.00728\n",
      "2025-05-17 04:40:14.267065: train_loss -0.9466\n",
      "2025-05-17 04:40:14.267185: val_loss -0.9367\n",
      "2025-05-17 04:40:14.267220: Pseudo dice [np.float32(0.9699)]\n",
      "2025-05-17 04:40:14.267252: Epoch time: 108.98 s\n",
      "2025-05-17 04:40:14.789088: \n",
      "2025-05-17 04:40:14.789223: Epoch 298\n",
      "2025-05-17 04:40:14.789366: Current learning rate: 0.00727\n",
      "2025-05-17 04:42:03.738924: train_loss -0.9534\n",
      "2025-05-17 04:42:03.739043: val_loss -0.9479\n",
      "2025-05-17 04:42:03.739076: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 04:42:03.739108: Epoch time: 108.95 s\n",
      "2025-05-17 04:42:04.460074: \n",
      "2025-05-17 04:42:04.460173: Epoch 299\n",
      "2025-05-17 04:42:04.460235: Current learning rate: 0.00726\n",
      "2025-05-17 04:43:53.316947: train_loss -0.9551\n",
      "2025-05-17 04:43:53.317101: val_loss -0.9424\n",
      "2025-05-17 04:43:53.317139: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 04:43:53.317172: Epoch time: 108.86 s\n",
      "2025-05-17 04:43:54.063457: \n",
      "2025-05-17 04:43:54.063585: Epoch 300\n",
      "2025-05-17 04:43:54.063650: Current learning rate: 0.00725\n",
      "2025-05-17 04:45:43.017435: train_loss -0.9546\n",
      "2025-05-17 04:45:43.017573: val_loss -0.941\n",
      "2025-05-17 04:45:43.017608: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-17 04:45:43.017642: Epoch time: 108.95 s\n",
      "2025-05-17 04:45:43.537700: \n",
      "2025-05-17 04:45:43.537861: Epoch 301\n",
      "2025-05-17 04:45:43.537993: Current learning rate: 0.00724\n",
      "2025-05-17 04:47:32.455249: train_loss -0.9572\n",
      "2025-05-17 04:47:32.455452: val_loss -0.9467\n",
      "2025-05-17 04:47:32.455487: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 04:47:32.455521: Epoch time: 108.92 s\n",
      "2025-05-17 04:47:32.974585: \n",
      "2025-05-17 04:47:32.974739: Epoch 302\n",
      "2025-05-17 04:47:32.974827: Current learning rate: 0.00724\n",
      "2025-05-17 04:49:21.941460: train_loss -0.9561\n",
      "2025-05-17 04:49:21.941583: val_loss -0.9371\n",
      "2025-05-17 04:49:21.941617: Pseudo dice [np.float32(0.9689)]\n",
      "2025-05-17 04:49:21.941650: Epoch time: 108.97 s\n",
      "2025-05-17 04:49:22.467580: \n",
      "2025-05-17 04:49:22.467772: Epoch 303\n",
      "2025-05-17 04:49:22.467839: Current learning rate: 0.00723\n",
      "2025-05-17 04:51:11.272577: train_loss -0.9584\n",
      "2025-05-17 04:51:11.272712: val_loss -0.945\n",
      "2025-05-17 04:51:11.272749: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-17 04:51:11.272784: Epoch time: 108.81 s\n",
      "2025-05-17 04:51:11.791902: \n",
      "2025-05-17 04:51:11.791994: Epoch 304\n",
      "2025-05-17 04:51:11.792056: Current learning rate: 0.00722\n",
      "2025-05-17 04:53:00.711625: train_loss -0.9571\n",
      "2025-05-17 04:53:00.711907: val_loss -0.9457\n",
      "2025-05-17 04:53:00.712009: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 04:53:00.712055: Epoch time: 108.92 s\n",
      "2025-05-17 04:53:01.231071: \n",
      "2025-05-17 04:53:01.231167: Epoch 305\n",
      "2025-05-17 04:53:01.231230: Current learning rate: 0.00721\n",
      "2025-05-17 04:54:50.214872: train_loss -0.9595\n",
      "2025-05-17 04:54:50.215004: val_loss -0.9457\n",
      "2025-05-17 04:54:50.215039: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 04:54:50.215071: Epoch time: 108.98 s\n",
      "2025-05-17 04:54:50.732895: \n",
      "2025-05-17 04:54:50.732991: Epoch 306\n",
      "2025-05-17 04:54:50.733055: Current learning rate: 0.0072\n",
      "2025-05-17 04:56:39.696208: train_loss -0.9573\n",
      "2025-05-17 04:56:39.696401: val_loss -0.9428\n",
      "2025-05-17 04:56:39.696446: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 04:56:39.696484: Epoch time: 108.96 s\n",
      "2025-05-17 04:56:40.220650: \n",
      "2025-05-17 04:56:40.220964: Epoch 307\n",
      "2025-05-17 04:56:40.221116: Current learning rate: 0.00719\n",
      "2025-05-17 04:58:28.966436: train_loss -0.9579\n",
      "2025-05-17 04:58:28.966628: val_loss -0.9481\n",
      "2025-05-17 04:58:28.966669: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 04:58:28.966733: Epoch time: 108.75 s\n",
      "2025-05-17 04:58:29.492884: \n",
      "2025-05-17 04:58:29.493105: Epoch 308\n",
      "2025-05-17 04:58:29.493261: Current learning rate: 0.00718\n",
      "2025-05-17 05:00:18.214104: train_loss -0.9552\n",
      "2025-05-17 05:00:18.214218: val_loss -0.9441\n",
      "2025-05-17 05:00:18.214252: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-17 05:00:18.214329: Epoch time: 108.72 s\n",
      "2025-05-17 05:00:18.725904: \n",
      "2025-05-17 05:00:18.726027: Epoch 309\n",
      "2025-05-17 05:00:18.726093: Current learning rate: 0.00717\n",
      "2025-05-17 05:02:07.699496: train_loss -0.9554\n",
      "2025-05-17 05:02:07.699633: val_loss -0.9467\n",
      "2025-05-17 05:02:07.699670: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 05:02:07.699704: Epoch time: 108.97 s\n",
      "2025-05-17 05:02:08.220582: \n",
      "2025-05-17 05:02:08.220895: Epoch 310\n",
      "2025-05-17 05:02:08.220987: Current learning rate: 0.00716\n",
      "2025-05-17 05:03:57.123585: train_loss -0.9539\n",
      "2025-05-17 05:03:57.123857: val_loss -0.9378\n",
      "2025-05-17 05:03:57.123952: Pseudo dice [np.float32(0.9709)]\n",
      "2025-05-17 05:03:57.124034: Epoch time: 108.9 s\n",
      "2025-05-17 05:03:57.839340: \n",
      "2025-05-17 05:03:57.839432: Epoch 311\n",
      "2025-05-17 05:03:57.839502: Current learning rate: 0.00715\n",
      "2025-05-17 05:05:46.812002: train_loss -0.958\n",
      "2025-05-17 05:05:46.812247: val_loss -0.947\n",
      "2025-05-17 05:05:46.812353: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 05:05:46.812494: Epoch time: 108.97 s\n",
      "2025-05-17 05:05:47.332601: \n",
      "2025-05-17 05:05:47.332850: Epoch 312\n",
      "2025-05-17 05:05:47.332945: Current learning rate: 0.00714\n",
      "2025-05-17 05:07:36.235828: train_loss -0.958\n",
      "2025-05-17 05:07:36.236017: val_loss -0.9461\n",
      "2025-05-17 05:07:36.236053: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-17 05:07:36.236195: Epoch time: 108.9 s\n",
      "2025-05-17 05:07:36.762013: \n",
      "2025-05-17 05:07:36.762263: Epoch 313\n",
      "2025-05-17 05:07:36.762344: Current learning rate: 0.00713\n",
      "2025-05-17 05:09:25.615896: train_loss -0.956\n",
      "2025-05-17 05:09:25.616074: val_loss -0.9476\n",
      "2025-05-17 05:09:25.616220: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 05:09:25.616421: Epoch time: 108.85 s\n",
      "2025-05-17 05:09:26.144637: \n",
      "2025-05-17 05:09:26.144730: Epoch 314\n",
      "2025-05-17 05:09:26.144792: Current learning rate: 0.00712\n",
      "2025-05-17 05:11:15.079456: train_loss -0.9583\n",
      "2025-05-17 05:11:15.079583: val_loss -0.944\n",
      "2025-05-17 05:11:15.079617: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 05:11:15.079651: Epoch time: 108.94 s\n",
      "2025-05-17 05:11:15.597011: \n",
      "2025-05-17 05:11:15.597106: Epoch 315\n",
      "2025-05-17 05:11:15.597170: Current learning rate: 0.00711\n",
      "2025-05-17 05:13:04.501600: train_loss -0.9564\n",
      "2025-05-17 05:13:04.501737: val_loss -0.9417\n",
      "2025-05-17 05:13:04.501775: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-17 05:13:04.501809: Epoch time: 108.91 s\n",
      "2025-05-17 05:13:05.029774: \n",
      "2025-05-17 05:13:05.029921: Epoch 316\n",
      "2025-05-17 05:13:05.029994: Current learning rate: 0.0071\n",
      "2025-05-17 05:14:53.769110: train_loss -0.9597\n",
      "2025-05-17 05:14:53.769241: val_loss -0.9431\n",
      "2025-05-17 05:14:53.769293: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-17 05:14:53.769328: Epoch time: 108.74 s\n",
      "2025-05-17 05:14:54.289346: \n",
      "2025-05-17 05:14:54.289500: Epoch 317\n",
      "2025-05-17 05:14:54.289569: Current learning rate: 0.0071\n",
      "2025-05-17 05:16:43.182706: train_loss -0.9588\n",
      "2025-05-17 05:16:43.182875: val_loss -0.9462\n",
      "2025-05-17 05:16:43.182907: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 05:16:43.182942: Epoch time: 108.89 s\n",
      "2025-05-17 05:16:43.711372: \n",
      "2025-05-17 05:16:43.711804: Epoch 318\n",
      "2025-05-17 05:16:43.712016: Current learning rate: 0.00709\n",
      "2025-05-17 05:18:32.644139: train_loss -0.9588\n",
      "2025-05-17 05:18:32.644275: val_loss -0.944\n",
      "2025-05-17 05:18:32.644317: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 05:18:32.644352: Epoch time: 108.93 s\n",
      "2025-05-17 05:18:33.156530: \n",
      "2025-05-17 05:18:33.156623: Epoch 319\n",
      "2025-05-17 05:18:33.156688: Current learning rate: 0.00708\n",
      "2025-05-17 05:20:21.902850: train_loss -0.9577\n",
      "2025-05-17 05:20:21.903033: val_loss -0.9456\n",
      "2025-05-17 05:20:21.903069: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 05:20:21.903104: Epoch time: 108.75 s\n",
      "2025-05-17 05:20:22.412559: \n",
      "2025-05-17 05:20:22.412725: Epoch 320\n",
      "2025-05-17 05:20:22.412825: Current learning rate: 0.00707\n",
      "2025-05-17 05:22:11.234876: train_loss -0.9585\n",
      "2025-05-17 05:22:11.235026: val_loss -0.9451\n",
      "2025-05-17 05:22:11.235062: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 05:22:11.235096: Epoch time: 108.82 s\n",
      "2025-05-17 05:22:11.757670: \n",
      "2025-05-17 05:22:11.757749: Epoch 321\n",
      "2025-05-17 05:22:11.757812: Current learning rate: 0.00706\n",
      "2025-05-17 05:24:00.519549: train_loss -0.9598\n",
      "2025-05-17 05:24:00.519808: val_loss -0.9453\n",
      "2025-05-17 05:24:00.519869: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 05:24:00.519981: Epoch time: 108.76 s\n",
      "2025-05-17 05:24:00.520031: Yayy! New best EMA pseudo Dice: 0.9732000231742859\n",
      "2025-05-17 05:24:01.261814: \n",
      "2025-05-17 05:24:01.261945: Epoch 322\n",
      "2025-05-17 05:24:01.262010: Current learning rate: 0.00705\n",
      "2025-05-17 05:25:50.233216: train_loss -0.9566\n",
      "2025-05-17 05:25:50.233398: val_loss -0.9449\n",
      "2025-05-17 05:25:50.233434: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 05:25:50.233503: Epoch time: 108.97 s\n",
      "2025-05-17 05:25:50.233632: Yayy! New best EMA pseudo Dice: 0.9733999967575073\n",
      "2025-05-17 05:25:51.173602: \n",
      "2025-05-17 05:25:51.173736: Epoch 323\n",
      "2025-05-17 05:25:51.173878: Current learning rate: 0.00704\n",
      "2025-05-17 05:27:40.120474: train_loss -0.9563\n",
      "2025-05-17 05:27:40.120655: val_loss -0.9333\n",
      "2025-05-17 05:27:40.120718: Pseudo dice [np.float32(0.9707)]\n",
      "2025-05-17 05:27:40.120768: Epoch time: 108.95 s\n",
      "2025-05-17 05:27:40.653984: \n",
      "2025-05-17 05:27:40.654120: Epoch 324\n",
      "2025-05-17 05:27:40.654185: Current learning rate: 0.00703\n",
      "2025-05-17 05:29:29.658422: train_loss -0.9577\n",
      "2025-05-17 05:29:29.658554: val_loss -0.9382\n",
      "2025-05-17 05:29:29.658588: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-17 05:29:29.658620: Epoch time: 109.0 s\n",
      "2025-05-17 05:29:30.178537: \n",
      "2025-05-17 05:29:30.178828: Epoch 325\n",
      "2025-05-17 05:29:30.178940: Current learning rate: 0.00702\n",
      "2025-05-17 05:31:19.082650: train_loss -0.9532\n",
      "2025-05-17 05:31:19.082785: val_loss -0.9453\n",
      "2025-05-17 05:31:19.082818: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 05:31:19.082852: Epoch time: 108.9 s\n",
      "2025-05-17 05:31:19.601249: \n",
      "2025-05-17 05:31:19.601403: Epoch 326\n",
      "2025-05-17 05:31:19.601479: Current learning rate: 0.00701\n",
      "2025-05-17 05:33:08.523641: train_loss -0.957\n",
      "2025-05-17 05:33:08.523854: val_loss -0.9447\n",
      "2025-05-17 05:33:08.524020: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 05:33:08.524088: Epoch time: 108.92 s\n",
      "2025-05-17 05:33:09.058133: \n",
      "2025-05-17 05:33:09.058385: Epoch 327\n",
      "2025-05-17 05:33:09.058644: Current learning rate: 0.007\n",
      "2025-05-17 05:34:57.985268: train_loss -0.9582\n",
      "2025-05-17 05:34:57.985448: val_loss -0.945\n",
      "2025-05-17 05:34:57.985482: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 05:34:57.985514: Epoch time: 108.93 s\n",
      "2025-05-17 05:34:58.516394: \n",
      "2025-05-17 05:34:58.516502: Epoch 328\n",
      "2025-05-17 05:34:58.516567: Current learning rate: 0.00699\n",
      "2025-05-17 05:36:47.351249: train_loss -0.9612\n",
      "2025-05-17 05:36:47.351377: val_loss -0.9447\n",
      "2025-05-17 05:36:47.351411: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 05:36:47.351444: Epoch time: 108.84 s\n",
      " 025-05-17 05:36:47.874331:\n",
      "2025-05-17 05:36:47.874589: Epoch 329\n",
      "2025-05-17 05:36:47.874832: Current learning rate: 0.00698\n",
      "2025-05-17 05:38:36.741875: train_loss -0.9582\n",
      "2025-05-17 05:38:36.742006: val_loss -0.9389\n",
      "2025-05-17 05:38:36.742041: Pseudo dice [np.float32(0.9689)]\n",
      "2025-05-17 05:38:36.742074: Epoch time: 108.87 s\n",
      "2025-05-17 05:38:37.264895: \n",
      "2025-05-17 05:38:37.265014: Epoch 330\n",
      "2025-05-17 05:38:37.265087: Current learning rate: 0.00697\n",
      "2025-05-17 05:40:26.189695: train_loss -0.9574\n",
      "2025-05-17 05:40:26.189833: val_loss -0.9402\n",
      "2025-05-17 05:40:26.189867: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-17 05:40:26.189901: Epoch time: 108.93 s\n",
      "2025-05-17 05:40:26.713989: \n",
      "2025-05-17 05:40:26.714287: Epoch 331\n",
      "2025-05-17 05:40:26.714358: Current learning rate: 0.00696\n",
      "2025-05-17 05:42:15.725106: train_loss -0.9587\n",
      "2025-05-17 05:42:15.725267: val_loss -0.9402\n",
      "2025-05-17 05:42:15.725316: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-17 05:42:15.725355: Epoch time: 109.01 s\n",
      "2025-05-17 05:42:16.251543: \n",
      "2025-05-17 05:42:16.251663: Epoch 332\n",
      "2025-05-17 05:42:16.251817: Current learning rate: 0.00696\n",
      "2025-05-17 05:44:05.036601: train_loss -0.9551\n",
      "2025-05-17 05:44:05.036778: val_loss -0.9465\n",
      "2025-05-17 05:44:05.036812: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-17 05:44:05.036865: Epoch time: 108.79 s\n",
      "2025-05-17 05:44:05.553924: \n",
      "2025-05-17 05:44:05.554119: Epoch 333\n",
      "2025-05-17 05:44:05.554187: Current learning rate: 0.00695\n",
      "2025-05-17 05:45:54.512939: train_loss -0.9589\n",
      "2025-05-17 05:45:54.513073: val_loss -0.9435\n",
      "2025-05-17 05:45:54.513108: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 05:45:54.513151: Epoch time: 108.96 s\n",
      "2025-05-17 05:45:55.026829: \n",
      "2025-05-17 05:45:55.026913: Epoch 334\n",
      "2025-05-17 05:45:55.026979: Current learning rate: 0.00694\n",
      "2025-05-17 05:47:43.934620: train_loss -0.9567\n",
      "2025-05-17 05:47:43.934736: val_loss -0.9406\n",
      "2025-05-17 05:47:43.934899: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 05:47:43.935019: Epoch time: 108.91 s\n",
      "2025-05-17 05:47:44.463107: \n",
      "2025-05-17 05:47:44.463183: Epoch 335\n",
      "2025-05-17 05:47:44.463271: Current learning rate: 0.00693\n",
      "2025-05-17 05:49:33.456368: train_loss -0.961\n",
      "2025-05-17 05:49:33.456495: val_loss -0.9485\n",
      "2025-05-17 05:49:33.456530: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 05:49:33.456564: Epoch time: 108.99 s\n",
      "2025-05-17 05:49:34.173946: \n",
      "2025-05-17 05:49:34.174257: Epoch 336\n",
      "2025-05-17 05:49:34.174405: Current learning rate: 0.00692\n",
      "2025-05-17 05:51:23.166173: train_loss -0.9596\n",
      "2025-05-17 05:51:23.166373: val_loss -0.9467\n",
      "2025-05-17 05:51:23.166407: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 05:51:23.166440: Epoch time: 108.99 s\n",
      "2025-05-17 05:51:23.698857: \n",
      "2025-05-17 05:51:23.698963: Epoch 337\n",
      "2025-05-17 05:51:23.699025: Current learning rate: 0.00691\n",
      "2025-05-17 05:53:12.726312: train_loss -0.9597\n",
      "2025-05-17 05:53:12.726446: val_loss -0.9441\n",
      "2025-05-17 05:53:12.726480: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 05:53:12.726513: Epoch time: 109.03 s\n",
      "2025-05-17 05:53:13.255828: \n",
      "2025-05-17 05:53:13.255997: Epoch 338\n",
      "2025-05-17 05:53:13.256130: Current learning rate: 0.0069\n",
      "2025-05-17 05:55:02.275938: train_loss -0.9603\n",
      "2025-05-17 05:55:02.276176: val_loss -0.9414\n",
      "2025-05-17 05:55:02.276217: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-17 05:55:02.276251: Epoch time: 109.02 s\n",
      "2025-05-17 05:55:02.806671: \n",
      "2025-05-17 05:55:02.806817: Epoch 339\n",
      "2025-05-17 05:55:02.806884: Current learning rate: 0.00689\n",
      "2025-05-17 05:56:51.580319: train_loss -0.9591\n",
      "2025-05-17 05:56:51.580501: val_loss -0.9495\n",
      "2025-05-17 05:56:51.580604: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 05:56:51.580647: Epoch time: 108.77 s\n",
      "2025-05-17 05:56:51.580671: Yayy! New best EMA pseudo Dice: 0.9735999703407288\n",
      "2025-05-17 05:56:52.324640: \n",
      "2025-05-17 05:56:52.324749: Epoch 340\n",
      "2025-05-17 05:56:52.325012: Current learning rate: 0.00688\n",
      "2025-05-17 05:58:41.159548: train_loss -0.9587\n",
      "2025-05-17 05:58:41.159671: val_loss -0.9462\n",
      "2025-05-17 05:58:41.159706: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 05:58:41.159739: Epoch time: 108.84 s\n",
      "2025-05-17 05:58:41.159759: Yayy! New best EMA pseudo Dice: 0.973800003528595\n",
      "2025-05-17 05:58:41.915288: \n",
      "2025-05-17 05:58:41.915495: Epoch 341\n",
      "2025-05-17 05:58:41.915717: Current learning rate: 0.00687\n",
      "2025-05-17 06:00:30.890426: train_loss -0.9613\n",
      "2025-05-17 06:00:30.890553: val_loss -0.9467\n",
      "2025-05-17 06:00:30.890587: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-17 06:00:30.890618: Epoch time: 108.98 s\n",
      "2025-05-17 06:00:30.890638: Yayy! New best EMA pseudo Dice: 0.9739999771118164\n",
      "2025-05-17 06:00:31.637485: \n",
      "2025-05-17 06:00:31.637598: Epoch 342\n",
      "2025-05-17 06:00:31.637732: Current learning rate: 0.00686\n",
      "2025-05-17 06:02:20.635002: train_loss -0.9604\n",
      "2025-05-17 06:02:20.635178: val_loss -0.9438\n",
      "2025-05-17 06:02:20.635212: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 06:02:20.635246: Epoch time: 109.0 s\n",
      "2025-05-17 06:02:21.166889: \n",
      "2025-05-17 06:02:21.167040: Epoch 343\n",
      "2025-05-17 06:02:21.167115: Current learning rate: 0.00685\n",
      "2025-05-17 06:04:10.118907: train_loss -0.9603\n",
      "2025-05-17 06:04:10.119050: val_loss -0.9449\n",
      "2025-05-17 06:04:10.119153: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-17 06:04:10.119194: Epoch time: 108.95 s\n",
      "2025-05-17 06:04:10.645670: \n",
      "2025-05-17 06:04:10.645964: Epoch 344\n",
      "2025-05-17 06:04:10.646083: Current learning rate: 0.00684\n",
      "2025-05-17 06:05:59.633197: train_loss -0.9606\n",
      "2025-05-17 06:05:59.633327: val_loss -0.9452\n",
      "2025-05-17 06:05:59.633361: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-17 06:05:59.633395: Epoch time: 108.99 s\n",
      "2025-05-17 06:06:00.170786: \n",
      "2025-05-17 06:06:00.170876: Epoch 345\n",
      "2025-05-17 06:06:00.171003: Current learning rate: 0.00683\n",
      "2025-05-17 06:07:49.182753: train_loss -0.959\n",
      "2025-05-17 06:07:49.182911: val_loss -0.9484\n",
      "2025-05-17 06:07:49.182947: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 06:07:49.182982: Epoch time: 109.01 s\n",
      "2025-05-17 06:07:49.713607: \n",
      "2025-05-17 06:07:49.713715: Epoch 346\n",
      "2025-05-17 06:07:49.713793: Current learning rate: 0.00682\n",
      "2025-05-17 06:09:38.658656: train_loss -0.961\n",
      "2025-05-17 06:09:38.658781: val_loss -0.9432\n",
      "2025-05-17 06:09:38.658815: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 06:09:38.658849: Epoch time: 108.95 s\n",
      "2025-05-17 06:09:38.658872: Yayy! New best EMA pseudo Dice: 0.9739999771118164\n",
      "2025-05-17 06:09:39.401046: \n",
      "2025-05-17 06:09:39.401139: Epoch 347\n",
      "2025-05-17 06:09:39.401203: Current learning rate: 0.00681\n",
      "2025-05-17 06:11:28.216001: train_loss -0.9604\n",
      "2025-05-17 06:11:28.216239: val_loss -0.9468\n",
      "2025-05-17 06:11:28.216283: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 06:11:28.216320: Epoch time: 108.82 s\n",
      "2025-05-17 06:11:28.934559: \n",
      "2025-05-17 06:11:28.934673: Epoch 348\n",
      "2025-05-17 06:11:28.934762: Current learning rate: 0.0068\n",
      "2025-05-17 06:13:17.957313: train_loss -0.9607\n",
      "2025-05-17 06:13:17.957434: val_loss -0.942\n",
      "2025-05-17 06:13:17.957473: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 06:13:17.957507: Epoch time: 109.02 s\n",
      "2025-05-17 06:13:18.491086: \n",
      "2025-05-17 06:13:18.491183: Epoch 349\n",
      "2025-05-17 06:13:18.491252: Current learning rate: 0.0068\n",
      "2025-05-17 06:15:07.266262: train_loss -0.961\n",
      "2025-05-17 06:15:07.266389: val_loss -0.939\n",
      "2025-05-17 06:15:07.266423: Pseudo dice [np.float32(0.9716)]\n",
      "2025-05-17 06:15:07.266456: Epoch time: 108.78 s\n",
      "2025-05-17 06:15:08.012952: \n",
      "2025-05-17 06:15:08.013107: Epoch 350\n",
      "2025-05-17 06:15:08.013178: Current learning rate: 0.00679\n",
      "2025-05-17 06:16:56.920001: train_loss -0.9591\n",
      "2025-05-17 06:16:56.920128: val_loss -0.9384\n",
      "2025-05-17 06:16:56.920161: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-17 06:16:56.920193: Epoch time: 108.91 s\n",
      "2025-05-17 06:16:57.448977: \n",
      "2025-05-17 06:16:57.449207: Epoch 351\n",
      "2025-05-17 06:16:57.449337: Current learning rate: 0.00678\n",
      "2025-05-17 06:18:46.402138: train_loss -0.9626\n",
      "2025-05-17 06:18:46.402421: val_loss -0.9487\n",
      "2025-05-17 06:18:46.402470: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 06:18:46.402506: Epoch time: 108.95 s\n",
      "2025-05-17 06:18:46.937662: \n",
      "2025-05-17 06:18:46.937744: Epoch 352\n",
      "2025-05-17 06:18:46.937808: Current learning rate: 0.00677\n",
      "2025-05-17 06:20:35.889572: train_loss -0.9617\n",
      "2025-05-17 06:20:35.889740: val_loss -0.9461\n",
      "2025-05-17 06:20:35.889910: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 06:20:35.889996: Epoch time: 108.95 s\n",
      "2025-05-17 06:20:36.418054: \n",
      "2025-05-17 06:20:36.418150: Epoch 353\n",
      "2025-05-17 06:20:36.418282: Current learning rate: 0.00676\n",
      "2025-05-17 06:22:25.203831: train_loss -0.9591\n",
      "2025-05-17 06:22:25.203964: val_loss -0.9444\n",
      "2025-05-17 06:22:25.203997: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 06:22:25.204031: Epoch time: 108.79 s\n",
      "2025-05-17 06:22:25.728979: \n",
      "2025-05-17 06:22:25.729380: Epoch 354\n",
      "2025-05-17 06:22:25.729494: Current learning rate: 0.00675\n",
      "2025-05-17 06:24:14.697562: train_loss -0.9619\n",
      "2025-05-17 06:24:14.697685: val_loss -0.9503\n",
      "2025-05-17 06:24:14.697719: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 06:24:14.697752: Epoch time: 108.97 s\n",
      "2025-05-17 06:24:14.697772: Yayy! New best EMA pseudo Dice: 0.9739999771118164\n",
      "2025-05-17 06:24:15.442933: \n",
      "2025-05-17 06:24:15.443099: Epoch 355\n",
      "2025-05-17 06:24:15.443175: Current learning rate: 0.00674\n",
      "2025-05-17 06:26:04.457833: train_loss -0.9605\n",
      "2025-05-17 06:26:04.458023: val_loss -0.9468\n",
      "2025-05-17 06:26:04.458059: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 06:26:04.458094: Epoch time: 109.02 s\n",
      "2025-05-17 06:26:04.458115: Yayy! New best EMA pseudo Dice: 0.9740999937057495\n",
      "2025-05-17 06:26:05.206800: \n",
      "2025-05-17 06:26:05.206953: Epoch 356\n",
      "2025-05-17 06:26:05.207026: Current learning rate: 0.00673\n",
      "2025-05-17 06:27:54.182212: train_loss -0.9599\n",
      "2025-05-17 06:27:54.182338: val_loss -0.9461\n",
      "2025-05-17 06:27:54.182375: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 06:27:54.182408: Epoch time: 108.98 s\n",
      "2025-05-17 06:27:54.182429: Yayy! New best EMA pseudo Dice: 0.9740999937057495\n",
      "2025-05-17 06:27:54.925770: \n",
      "2025-05-17 06:27:54.926027: Epoch 357\n",
      "2025-05-17 06:27:54.926098: Current learning rate: 0.00672\n",
      "2025-05-17 06:29:43.870883: train_loss -0.9603\n",
      "2025-05-17 06:29:43.871072: val_loss -0.9396\n",
      "2025-05-17 06:29:43.871105: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 06:29:43.871140: Epoch time: 108.95 s\n",
      "2025-05-17 06:29:44.397888: \n",
      "2025-05-17 06:29:44.397967: Epoch 358\n",
      "2025-05-17 06:29:44.398030: Current learning rate: 0.00671\n",
      "2025-05-17 06:31:33.351038: train_loss -0.9593\n",
      "2025-05-17 06:31:33.351181: val_loss -0.9402\n",
      "2025-05-17 06:31:33.351216: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 06:31:33.351249: Epoch time: 108.95 s\n",
      "2025-05-17 06:31:34.068143: \n",
      "2025-05-17 06:31:34.068421: Epoch 359\n",
      "2025-05-17 06:31:34.068502: Current learning rate: 0.0067\n",
      "2025-05-17 06:33:23.044450: train_loss -0.9614\n",
      "2025-05-17 06:33:23.044596: val_loss -0.9498\n",
      "2025-05-17 06:33:23.044634: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 06:33:23.044668: Epoch time: 108.98 s\n",
      "2025-05-17 06:33:23.570639: \n",
      "2025-05-17 06:33:23.570729: Epoch 360\n",
      "2025-05-17 06:33:23.570794: Current learning rate: 0.00669\n",
      "2025-05-17 06:35:12.453110: train_loss -0.959\n",
      "2025-05-17 06:35:12.453249: val_loss -0.9415\n",
      "2025-05-17 06:35:12.453284: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 06:35:12.453318: Epoch time: 108.88 s\n",
      "2025-05-17 06:35:12.987391: \n",
      "2025-05-17 06:35:12.987565: Epoch 361\n",
      "2025-05-17 06:35:12.987801: Current learning rate: 0.00668\n",
      "2025-05-17 06:37:02.014606: train_loss -0.9593\n",
      "2025-05-17 06:37:02.014859: val_loss -0.9489\n",
      "2025-05-17 06:37:02.015034: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 06:37:02.015177: Epoch time: 109.03 s\n",
      "2025-05-17 06:37:02.542901: \n",
      "2025-05-17 06:37:02.543029: Epoch 362\n",
      "2025-05-17 06:37:02.543095: Current learning rate: 0.00667\n",
      "2025-05-17 06:38:51.282525: train_loss -0.9611\n",
      "2025-05-17 06:38:51.282652: val_loss -0.9441\n",
      "2025-05-17 06:38:51.282690: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 06:38:51.282724: Epoch time: 108.74 s\n",
      "2025-05-17 06:38:51.810508: \n",
      "2025-05-17 06:38:51.810692: Epoch 363\n",
      "2025-05-17 06:38:51.810788: Current learning rate: 0.00666\n",
      "2025-05-17 06:40:40.591935: train_loss -0.9614\n",
      "2025-05-17 06:40:40.592062: val_loss -0.9515\n",
      "2025-05-17 06:40:40.592095: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 06:40:40.592228: Epoch time: 108.78 s\n",
      "2025-05-17 06:40:40.592343: Yayy! New best EMA pseudo Dice: 0.9740999937057495\n",
      "2025-05-17 06:40:41.334697: \n",
      "2025-05-17 06:40:41.334864: Epoch 364\n",
      "2025-05-17 06:40:41.334927: Current learning rate: 0.00665\n",
      "2025-05-17 06:42:30.174078: train_loss -0.9594\n",
      "2025-05-17 06:42:30.174208: val_loss -0.9461\n",
      "2025-05-17 06:42:30.174408: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 06:42:30.174458: Epoch time: 108.84 s\n",
      "2025-05-17 06:42:30.174481: Yayy! New best EMA pseudo Dice: 0.9743000268936157\n",
      "2025-05-17 06:42:30.919487: \n",
      "2025-05-17 06:42:30.919630: Epoch 365\n",
      "2025-05-17 06:42:30.919699: Current learning rate: 0.00665\n",
      "2025-05-17 06:44:19.872902: train_loss -0.9616\n",
      "2025-05-17 06:44:19.873040: val_loss -0.9437\n",
      "2025-05-17 06:44:19.873074: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-17 06:44:19.873107: Epoch time: 108.95 s\n",
      "2025-05-17 06:44:20.402292: \n",
      "2025-05-17 06:44:20.402459: Epoch 366\n",
      "2025-05-17 06:44:20.402537: Current learning rate: 0.00664\n",
      "2025-05-17 06:46:09.263411: train_loss -0.9599\n",
      "2025-05-17 06:46:09.263619: val_loss -0.9427\n",
      "2025-05-17 06:46:09.263658: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-17 06:46:09.263691: Epoch time: 108.86 s\n",
      "2025-05-17 06:46:09.790558: \n",
      "2025-05-17 06:46:09.790640: Epoch 367\n",
      "2025-05-17 06:46:09.790701: Current learning rate: 0.00663\n",
      "2025-05-17 06:47:58.695005: train_loss -0.9611\n",
      "2025-05-17 06:47:58.695140: val_loss -0.945\n",
      "2025-05-17 06:47:58.695177: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 06:47:58.695212: Epoch time: 108.9 s\n",
      "2025-05-17 06:47:59.218486: \n",
      "2025-05-17 06:47:59.218660: Epoch 368\n",
      "2025-05-17 06:47:59.218733: Current learning rate: 0.00662\n",
      "2025-05-17 06:49:48.227017: train_loss -0.9596\n",
      "2025-05-17 06:49:48.227141: val_loss -0.9366\n",
      "2025-05-17 06:49:48.227199: Pseudo dice [np.float32(0.9704)]\n",
      "2025-05-17 06:49:48.227238: Epoch time: 109.01 s\n",
      "2025-05-17 06:49:48.756697: \n",
      "2025-05-17 06:49:48.756970: Epoch 369\n",
      "2025-05-17 06:49:48.757050: Current learning rate: 0.00661\n",
      "2025-05-17 06:51:37.649819: train_loss -0.9527\n",
      "2025-05-17 06:51:37.649947: val_loss -0.9417\n",
      "2025-05-17 06:51:37.649980: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-17 06:51:37.650028: Epoch time: 108.89 s\n",
      "2025-05-17 06:51:38.171264: \n",
      "2025-05-17 06:51:38.171348: Epoch 370\n",
      "2025-05-17 06:51:38.171412: Current learning rate: 0.0066\n",
      "2025-05-17 06:53:27.180090: train_loss -0.9568\n",
      "2025-05-17 06:53:27.180214: val_loss -0.9476\n",
      "2025-05-17 06:53:27.180246: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 06:53:27.180279: Epoch time: 109.01 s\n",
      "2025-05-17 06:53:27.906919: \n",
      "2025-05-17 06:53:27.907048: Epoch 371\n",
      "2025-05-17 06:53:27.907154: Current learning rate: 0.00659\n",
      "2025-05-17 06:55:16.747628: train_loss -0.9587\n",
      "2025-05-17 06:55:16.747876: val_loss -0.9505\n",
      "2025-05-17 06:55:16.747958: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 06:55:16.747999: Epoch time: 108.84 s\n",
      "2025-05-17 06:55:17.280798: \n",
      "2025-05-17 06:55:17.281130: Epoch 372\n",
      "2025-05-17 06:55:17.281272: Current learning rate: 0.00658\n",
      "2025-05-17 06:57:06.179005: train_loss -0.9604\n",
      "2025-05-17 06:57:06.179198: val_loss -0.9443\n",
      "2025-05-17 06:57:06.179233: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-17 06:57:06.179267: Epoch time: 108.9 s\n",
      "2025-05-17 06:57:06.707014: \n",
      "2025-05-17 06:57:06.707156: Epoch 373\n",
      "2025-05-17 06:57:06.707223: Current learning rate: 0.00657\n",
      "2025-05-17 06:58:55.673803: train_loss -0.9597\n",
      "2025-05-17 06:58:55.673921: val_loss -0.9451\n",
      "2025-05-17 06:58:55.673954: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 06:58:55.673985: Epoch time: 108.97 s\n",
      "2025-05-17 06:58:56.200085: \n",
      "2025-05-17 06:58:56.200441: Epoch 374\n",
      "2025-05-17 06:58:56.200526: Current learning rate: 0.00656\n",
      "2025-05-17 07:00:45.032313: train_loss -0.9612\n",
      "2025-05-17 07:00:45.032444: val_loss -0.9407\n",
      "2025-05-17 07:00:45.032479: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-17 07:00:45.032511: Epoch time: 108.83 s\n",
      "2025-05-17 07:00:45.563124: \n",
      "2025-05-17 07:00:45.563263: Epoch 375\n",
      "2025-05-17 07:00:45.563329: Current learning rate: 0.00655\n",
      "2025-05-17 07:02:34.472091: train_loss -0.9589\n",
      "2025-05-17 07:02:34.472224: val_loss -0.9443\n",
      "2025-05-17 07:02:34.472259: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 07:02:34.472293: Epoch time: 108.91 s\n",
      "2025-05-17 07:02:34.999893: \n",
      "2025-05-17 07:02:35.000100: Epoch 376\n",
      "2025-05-17 07:02:35.000175: Current learning rate: 0.00654\n",
      "2025-05-17 07:04:23.934065: train_loss -0.96\n",
      "2025-05-17 07:04:23.934204: val_loss -0.9476\n",
      "2025-05-17 07:04:23.934266: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-17 07:04:23.934304: Epoch time: 108.93 s\n",
      "2025-05-17 07:04:24.474331: \n",
      "2025-05-17 07:04:24.474483: Epoch 377\n",
      "2025-05-17 07:04:24.474602: Current learning rate: 0.00653\n",
      "2025-05-17 07:06:13.433804: train_loss -0.9577\n",
      "2025-05-17 07:06:13.434058: val_loss -0.9451\n",
      "2025-05-17 07:06:13.434178: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 07:06:13.434267: Epoch time: 108.96 s\n",
      "2025-05-17 07:06:13.964392: \n",
      "2025-05-17 07:06:13.964673: Epoch 378\n",
      "2025-05-17 07:06:13.965056: Current learning rate: 0.00652\n",
      "2025-05-17 07:08:02.911426: train_loss -0.9583\n",
      "2025-05-17 07:08:02.911550: val_loss -0.9463\n",
      "2025-05-17 07:08:02.911585: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 07:08:02.911620: Epoch time: 108.95 s\n",
      "2025-05-17 07:08:03.438575: \n",
      "2025-05-17 07:08:03.438791: Epoch 379\n",
      "2025-05-17 07:08:03.438867: Current learning rate: 0.00651\n",
      "2025-05-17 07:09:52.356149: train_loss -0.9606\n",
      "2025-05-17 07:09:52.356289: val_loss -0.939\n",
      "2025-05-17 07:09:52.356325: Pseudo dice [np.float32(0.9716)]\n",
      "2025-05-17 07:09:52.356360: Epoch time: 108.92 s\n",
      "2025-05-17 07:09:52.886410: \n",
      "2025-05-17 07:09:52.886680: Epoch 380\n",
      "2025-05-17 07:09:52.886770: Current learning rate: 0.0065\n",
      "2025-05-17 07:11:41.838670: train_loss -0.9616\n",
      "2025-05-17 07:11:41.838794: val_loss -0.9467\n",
      "2025-05-17 07:11:41.838829: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 07:11:41.838860: Epoch time: 108.95 s\n",
      "2025-05-17 07:11:42.383437: \n",
      "2025-05-17 07:11:42.383525: Epoch 381\n",
      "2025-05-17 07:11:42.383590: Current learning rate: 0.00649\n",
      "2025-05-17 07:13:31.381264: train_loss -0.9612\n",
      "2025-05-17 07:13:31.381467: val_loss -0.9467\n",
      "2025-05-17 07:13:31.381514: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 07:13:31.381547: Epoch time: 109.0 s\n",
      "2025-05-17 07:13:31.909047: \n",
      "2025-05-17 07:13:31.909226: Epoch 382\n",
      "2025-05-17 07:13:31.909304: Current learning rate: 0.00648\n",
      "2025-05-17 07:15:20.649590: train_loss -0.9603\n",
      "2025-05-17 07:15:20.649856: val_loss -0.9393\n",
      "2025-05-17 07:15:20.649913: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-17 07:15:20.649961: Epoch time: 108.74 s\n",
      "2025-05-17 07:15:21.391140: \n",
      "2025-05-17 07:15:21.391328: Epoch 383\n",
      "2025-05-17 07:15:21.391407: Current learning rate: 0.00648\n",
      "2025-05-17 07:17:10.318642: train_loss -0.9624\n",
      "2025-05-17 07:17:10.318783: val_loss -0.9432\n",
      "2025-05-17 07:17:10.318815: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 07:17:10.318849: Epoch time: 108.93 s\n",
      "2025-05-17 07:17:10.849467: \n",
      "2025-05-17 07:17:10.849643: Epoch 384\n",
      "2025-05-17 07:17:10.849727: Current learning rate: 0.00647\n",
      "2025-05-17 07:18:59.620995: train_loss -0.9605\n",
      "2025-05-17 07:18:59.621136: val_loss -0.9456\n",
      "2025-05-17 07:18:59.621215: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 07:18:59.621263: Epoch time: 108.77 s\n",
      "2025-05-17 07:19:00.157853: \n",
      "2025-05-17 07:19:00.158112: Epoch 385\n",
      "2025-05-17 07:19:00.158200: Current learning rate: 0.00646\n",
      "2025-05-17 07:20:49.042498: train_loss -0.9629\n",
      "2025-05-17 07:20:49.042627: val_loss -0.9457\n",
      "2025-05-17 07:20:49.042662: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 07:20:49.042696: Epoch time: 108.89 s\n",
      "2025-05-17 07:20:49.581574: \n",
      "2025-05-17 07:20:49.581851: Epoch 386\n",
      "2025-05-17 07:20:49.581951: Current learning rate: 0.00645\n",
      "2025-05-17 07:22:38.464270: train_loss -0.9616\n",
      "2025-05-17 07:22:38.464457: val_loss -0.9503\n",
      "2025-05-17 07:22:38.464492: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 07:22:38.464525: Epoch time: 108.88 s\n",
      "2025-05-17 07:22:39.003256: \n",
      "2025-05-17 07:22:39.003468: Epoch 387\n",
      "2025-05-17 07:22:39.003584: Current learning rate: 0.00644\n",
      "2025-05-17 07:24:27.899906: train_loss -0.9619\n",
      "2025-05-17 07:24:27.900033: val_loss -0.9389\n",
      "2025-05-17 07:24:27.900064: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 07:24:27.900097: Epoch time: 108.9 s\n",
      "2025-05-17 07:24:28.433064: \n",
      "2025-05-17 07:24:28.433537: Epoch 388\n",
      "2025-05-17 07:24:28.433610: Current learning rate: 0.00643\n",
      "2025-05-17 07:26:17.370935: train_loss -0.9604\n",
      "2025-05-17 07:26:17.371056: val_loss -0.9489\n",
      "2025-05-17 07:26:17.371090: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 07:26:17.371125: Epoch time: 108.94 s\n",
      "2025-05-17 07:26:17.910796: \n",
      "2025-05-17 07:26:17.911129: Epoch 389\n",
      "2025-05-17 07:26:17.911212: Current learning rate: 0.00642\n",
      "2025-05-17 07:28:06.886409: train_loss -0.9611\n",
      "2025-05-17 07:28:06.886538: val_loss -0.9492\n",
      "2025-05-17 07:28:06.886570: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 07:28:06.886605: Epoch time: 108.98 s\n",
      "2025-05-17 07:28:07.421028: \n",
      "2025-05-17 07:28:07.421115: Epoch 390\n",
      "2025-05-17 07:28:07.421179: Current learning rate: 0.00641\n",
      "2025-05-17 07:29:56.405339: train_loss -0.9623\n",
      "2025-05-17 07:29:56.405544: val_loss -0.942\n",
      "2025-05-17 07:29:56.405578: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-17 07:29:56.405610: Epoch time: 108.98 s\n",
      "2025-05-17 07:29:56.937426: \n",
      "2025-05-17 07:29:56.937508: Epoch 391\n",
      "2025-05-17 07:29:56.937572: Current learning rate: 0.0064\n",
      "2025-05-17 07:31:45.807256: train_loss -0.9613\n",
      "2025-05-17 07:31:45.807383: val_loss -0.9455\n",
      "2025-05-17 07:31:45.807417: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 07:31:45.807449: Epoch time: 108.87 s\n",
      "2025-05-17 07:31:46.344635: \n",
      "2025-05-17 07:31:46.344766: Epoch 392\n",
      "2025-05-17 07:31:46.344835: Current learning rate: 0.00639\n",
      "2025-05-17 07:33:35.253607: train_loss -0.9609\n",
      "2025-05-17 07:33:35.253724: val_loss -0.9505\n",
      "2025-05-17 07:33:35.253754: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 07:33:35.253783: Epoch time: 108.91 s\n",
      "2025-05-17 07:33:35.783467: \n",
      "2025-05-17 07:33:35.783640: Epoch 393\n",
      "2025-05-17 07:33:35.783705: Current learning rate: 0.00638\n",
      "2025-05-17 07:35:24.704636: train_loss -0.9483\n",
      "2025-05-17 07:35:24.704764: val_loss -0.9304\n",
      "2025-05-17 07:35:24.704800: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-17 07:35:24.704833: Epoch time: 108.92 s\n",
      "2025-05-17 07:35:25.232468: \n",
      "2025-05-17 07:35:25.232886: Epoch 394\n",
      "2025-05-17 07:35:25.232957: Current learning rate: 0.00637\n",
      "2025-05-17 07:37:14.164806: train_loss -0.9042\n",
      "2025-05-17 07:37:14.164930: val_loss -0.9153\n",
      "2025-05-17 07:37:14.164962: Pseudo dice [np.float32(0.9634)]\n",
      "2025-05-17 07:37:14.164994: Epoch time: 108.93 s\n",
      "2025-05-17 07:37:14.900423: \n",
      "2025-05-17 07:37:14.900527: Epoch 395\n",
      "2025-05-17 07:37:14.900603: Current learning rate: 0.00636\n",
      "2025-05-17 07:39:03.896631: train_loss -0.9317\n",
      "2025-05-17 07:39:03.896772: val_loss -0.9224\n",
      "2025-05-17 07:39:03.896806: Pseudo dice [np.float32(0.9655)]\n",
      "2025-05-17 07:39:03.896836: Epoch time: 109.0 s\n",
      "2025-05-17 07:39:04.427468: \n",
      "2025-05-17 07:39:04.427628: Epoch 396\n",
      "2025-05-17 07:39:04.427705: Current learning rate: 0.00635\n",
      "2025-05-17 07:40:53.239561: train_loss -0.9419\n",
      "2025-05-17 07:40:53.239799: val_loss -0.929\n",
      "2025-05-17 07:40:53.239941: Pseudo dice [np.float32(0.9675)]\n",
      "2025-05-17 07:40:53.240107: Epoch time: 108.81 s\n",
      "2025-05-17 07:40:53.787750: \n",
      "2025-05-17 07:40:53.787901: Epoch 397\n",
      "2025-05-17 07:40:53.787967: Current learning rate: 0.00634\n",
      "2025-05-17 07:42:42.760387: train_loss -0.9488\n",
      "2025-05-17 07:42:42.760522: val_loss -0.9402\n",
      "2025-05-17 07:42:42.760689: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 07:42:42.760743: Epoch time: 108.97 s\n",
      "2025-05-17 07:42:43.287129: \n",
      "2025-05-17 07:42:43.287220: Epoch 398\n",
      "2025-05-17 07:42:43.287283: Current learning rate: 0.00633\n",
      "2025-05-17 07:44:32.170184: train_loss -0.9262\n",
      "2025-05-17 07:44:32.170313: val_loss -0.9012\n",
      "2025-05-17 07:44:32.170360: Pseudo dice [np.float32(0.9543)]\n",
      "2025-05-17 07:44:32.170400: Epoch time: 108.88 s\n",
      "2025-05-17 07:44:32.709432: \n",
      "2025-05-17 07:44:32.709558: Epoch 399\n",
      "2025-05-17 07:44:32.709696: Current learning rate: 0.00632\n",
      "2025-05-17 07:46:21.603202: train_loss -0.9289\n",
      "2025-05-17 07:46:21.603423: val_loss -0.9269\n",
      "2025-05-17 07:46:21.603492: Pseudo dice [np.float32(0.965)]\n",
      "2025-05-17 07:46:21.603533: Epoch time: 108.89 s\n",
      "2025-05-17 07:46:22.349036: \n",
      "2025-05-17 07:46:22.349144: Epoch 400\n",
      "2025-05-17 07:46:22.349210: Current learning rate: 0.00631\n",
      "2025-05-17 07:48:11.275543: train_loss -0.9455\n",
      "2025-05-17 07:48:11.275660: val_loss -0.9422\n",
      "2025-05-17 07:48:11.275694: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 07:48:11.275729: Epoch time: 108.93 s\n",
      "2025-05-17 07:48:11.811073: \n",
      "2025-05-17 07:48:11.811446: Epoch 401\n",
      "2025-05-17 07:48:11.811557: Current learning rate: 0.0063\n",
      "2025-05-17 07:50:00.664001: train_loss -0.9502\n",
      "2025-05-17 07:50:00.664187: val_loss -0.94\n",
      "2025-05-17 07:50:00.664219: Pseudo dice [np.float32(0.9711)]\n",
      "2025-05-17 07:50:00.664251: Epoch time: 108.85 s\n",
      "2025-05-17 07:50:01.201088: \n",
      "2025-05-17 07:50:01.201173: Epoch 402\n",
      "2025-05-17 07:50:01.201237: Current learning rate: 0.0063\n",
      "2025-05-17 07:51:50.084772: train_loss -0.9536\n",
      "2025-05-17 07:51:50.084940: val_loss -0.9298\n",
      "2025-05-17 07:51:50.084975: Pseudo dice [np.float32(0.9691)]\n",
      "2025-05-17 07:51:50.085010: Epoch time: 108.88 s\n",
      "2025-05-17 07:51:50.624891: \n",
      "2025-05-17 07:51:50.624983: Epoch 403\n",
      "2025-05-17 07:51:50.625048: Current learning rate: 0.00629\n",
      "2025-05-17 07:53:39.367677: train_loss -0.9516\n",
      "2025-05-17 07:53:39.367834: val_loss -0.9356\n",
      "2025-05-17 07:53:39.367961: Pseudo dice [np.float32(0.9694)]\n",
      "2025-05-17 07:53:39.368084: Epoch time: 108.74 s\n",
      "2025-05-17 07:53:39.899647: \n",
      "2025-05-17 07:53:39.899853: Epoch 404\n",
      "2025-05-17 07:53:39.899927: Current learning rate: 0.00628\n",
      "2025-05-17 07:55:28.806481: train_loss -0.9508\n",
      "2025-05-17 07:55:28.806620: val_loss -0.9374\n",
      "2025-05-17 07:55:28.806658: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-17 07:55:28.806692: Epoch time: 108.91 s\n",
      "2025-05-17 07:55:29.336785: \n",
      "2025-05-17 07:55:29.337149: Epoch 405\n",
      "2025-05-17 07:55:29.337242: Current learning rate: 0.00627\n",
      "2025-05-17 07:57:18.246960: train_loss -0.932\n",
      "2025-05-17 07:57:18.247082: val_loss -0.9306\n",
      "2025-05-17 07:57:18.247116: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-17 07:57:18.247149: Epoch time: 108.91 s\n",
      "2025-05-17 07:57:18.777587: \n",
      "2025-05-17 07:57:18.777673: Epoch 406\n",
      "2025-05-17 07:57:18.777749: Current learning rate: 0.00626\n",
      "2025-05-17 07:59:07.786147: train_loss -0.9376\n",
      "2025-05-17 07:59:07.786280: val_loss -0.9379\n",
      "2025-05-17 07:59:07.786312: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 07:59:07.786360: Epoch time: 109.01 s\n",
      "2025-05-17 07:59:08.519652: \n",
      "2025-05-17 07:59:08.519784: Epoch 407\n",
      "2025-05-17 07:59:08.519856: Current learning rate: 0.00625\n",
      "2025-05-17 08:00:57.390683: train_loss -0.9466\n",
      "2025-05-17 08:00:57.390811: val_loss -0.9391\n",
      "2025-05-17 08:00:57.390845: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 08:00:57.391094: Epoch time: 108.87 s\n",
      "2025-05-17 08:00:57.927278: \n",
      "2025-05-17 08:00:57.927543: Epoch 408\n",
      "2025-05-17 08:00:57.927678: Current learning rate: 0.00624\n",
      "2025-05-17 08:02:46.868921: train_loss -0.9474\n",
      "2025-05-17 08:02:46.869114: val_loss -0.9393\n",
      "2025-05-17 08:02:46.869154: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 08:02:46.869190: Epoch time: 108.94 s\n",
      "2025-05-17 08:02:47.407367: \n",
      "2025-05-17 08:02:47.407586: Epoch 409\n",
      "2025-05-17 08:02:47.407670: Current learning rate: 0.00623\n",
      "2025-05-17 08:04:36.389220: train_loss -0.9546\n",
      "2025-05-17 08:04:36.389344: val_loss -0.938\n",
      "2025-05-17 08:04:36.389378: Pseudo dice [np.float32(0.9713)]\n",
      "2025-05-17 08:04:36.389412: Epoch time: 108.98 s\n",
      "2025-05-17 08:04:36.925478: \n",
      "2025-05-17 08:04:36.925708: Epoch 410\n",
      "2025-05-17 08:04:36.925870: Current learning rate: 0.00622\n",
      "2025-05-17 08:06:25.901006: train_loss -0.948\n",
      "2025-05-17 08:06:25.901234: val_loss -0.9411\n",
      "2025-05-17 08:06:25.901325: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-17 08:06:25.901550: Epoch time: 108.98 s\n",
      "2025-05-17 08:06:26.423328: \n",
      "2025-05-17 08:06:26.423438: Epoch 411\n",
      "2025-05-17 08:06:26.423501: Current learning rate: 0.00621\n",
      "2025-05-17 08:08:15.371669: train_loss -0.944\n",
      "2025-05-17 08:08:15.371841: val_loss -0.9084\n",
      "2025-05-17 08:08:15.371898: Pseudo dice [np.float32(0.9578)]\n",
      "2025-05-17 08:08:15.371954: Epoch time: 108.95 s\n",
      "2025-05-17 08:08:15.888478: \n",
      "2025-05-17 08:08:15.888576: Epoch 412\n",
      "2025-05-17 08:08:15.888637: Current learning rate: 0.0062\n",
      "2025-05-17 08:10:04.857139: train_loss -0.9419\n",
      "2025-05-17 08:10:04.857325: val_loss -0.9417\n",
      "2025-05-17 08:10:04.857408: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-17 08:10:04.857496: Epoch time: 108.97 s\n",
      "2025-05-17 08:10:05.379390: \n",
      "2025-05-17 08:10:05.379488: Epoch 413\n",
      "2025-05-17 08:10:05.379552: Current learning rate: 0.00619\n",
      "2025-05-17 08:11:54.232317: train_loss -0.9448\n",
      "2025-05-17 08:11:54.232450: val_loss -0.9332\n",
      "2025-05-17 08:11:54.232496: Pseudo dice [np.float32(0.9699)]\n",
      "2025-05-17 08:11:54.232532: Epoch time: 108.85 s\n",
      "2025-05-17 08:11:54.750054: \n",
      "2025-05-17 08:11:54.750293: Epoch 414\n",
      "2025-05-17 08:11:54.750382: Current learning rate: 0.00618\n",
      "2025-05-17 08:13:43.720580: train_loss -0.9376\n",
      "2025-05-17 08:13:43.720784: val_loss -0.927\n",
      "2025-05-17 08:13:43.720816: Pseudo dice [np.float32(0.9654)]\n",
      "2025-05-17 08:13:43.720848: Epoch time: 108.97 s\n",
      "2025-05-17 08:13:44.240399: \n",
      "2025-05-17 08:13:44.240490: Epoch 415\n",
      "2025-05-17 08:13:44.240552: Current learning rate: 0.00617\n",
      "2025-05-17 08:15:33.244524: train_loss -0.9507\n",
      "2025-05-17 08:15:33.244662: val_loss -0.9308\n",
      "2025-05-17 08:15:33.244699: Pseudo dice [np.float32(0.9688)]\n",
      "2025-05-17 08:15:33.244731: Epoch time: 109.0 s\n",
      "2025-05-17 08:15:33.761419: \n",
      "2025-05-17 08:15:33.761739: Epoch 416\n",
      "2025-05-17 08:15:33.761899: Current learning rate: 0.00616\n",
      "2025-05-17 08:17:22.715373: train_loss -0.9513\n",
      "2025-05-17 08:17:22.715553: val_loss -0.9436\n",
      "2025-05-17 08:17:22.715586: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 08:17:22.715618: Epoch time: 108.95 s\n",
      "2025-05-17 08:17:23.231506: \n",
      "2025-05-17 08:17:23.231587: Epoch 417\n",
      "2025-05-17 08:17:23.231659: Current learning rate: 0.00615\n",
      "2025-05-17 08:19:12.279345: train_loss -0.9513\n",
      "2025-05-17 08:19:12.279594: val_loss -0.9399\n",
      "2025-05-17 08:19:12.279667: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 08:19:12.279706: Epoch time: 109.05 s\n",
      "2025-05-17 08:19:12.793293: \n",
      "2025-05-17 08:19:12.793424: Epoch 418\n",
      "2025-05-17 08:19:12.793516: Current learning rate: 0.00614\n",
      "2025-05-17 08:21:01.699179: train_loss -0.9502\n",
      "2025-05-17 08:21:01.699312: val_loss -0.9432\n",
      "2025-05-17 08:21:01.699349: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 08:21:01.699382: Epoch time: 108.91 s\n",
      "2025-05-17 08:21:02.418241: \n",
      "2025-05-17 08:21:02.418586: Epoch 419\n",
      "2025-05-17 08:21:02.418661: Current learning rate: 0.00613\n",
      "2025-05-17 08:22:51.382417: train_loss -0.9566\n",
      "2025-05-17 08:22:51.382669: val_loss -0.9425\n",
      "2025-05-17 08:22:51.382842: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-17 08:22:51.382887: Epoch time: 108.96 s\n",
      "2025-05-17 08:22:51.921664: \n",
      "2025-05-17 08:22:51.921824: Epoch 420\n",
      "2025-05-17 08:22:51.921889: Current learning rate: 0.00612\n",
      "2025-05-17 08:24:40.829974: train_loss -0.959\n",
      "2025-05-17 08:24:40.830313: val_loss -0.9347\n",
      "2025-05-17 08:24:40.830369: Pseudo dice [np.float32(0.9688)]\n",
      "2025-05-17 08:24:40.830454: Epoch time: 108.91 s\n",
      "2025-05-17 08:24:41.348722: \n",
      "2025-05-17 08:24:41.348964: Epoch 421\n",
      "2025-05-17 08:24:41.349035: Current learning rate: 0.00612\n",
      "2025-05-17 08:26:30.277296: train_loss -0.9585\n",
      "2025-05-17 08:26:30.277460: val_loss -0.9492\n",
      "2025-05-17 08:26:30.277496: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-17 08:26:30.277545: Epoch time: 108.93 s\n",
      "2025-05-17 08:26:30.800976: \n",
      "2025-05-17 08:26:30.801077: Epoch 422\n",
      "2025-05-17 08:26:30.801143: Current learning rate: 0.00611\n",
      "2025-05-17 08:28:19.735891: train_loss -0.9586\n",
      "2025-05-17 08:28:19.736022: val_loss -0.9461\n",
      "2025-05-17 08:28:19.736057: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-17 08:28:19.736090: Epoch time: 108.94 s\n",
      "2025-05-17 08:28:20.258707: \n",
      "2025-05-17 08:28:20.258923: Epoch 423\n",
      "2025-05-17 08:28:20.259043: Current learning rate: 0.0061\n",
      "2025-05-17 08:30:09.124020: train_loss -0.9582\n",
      "2025-05-17 08:30:09.124252: val_loss -0.9446\n",
      "2025-05-17 08:30:09.124304: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 08:30:09.124344: Epoch time: 108.87 s\n",
      "2025-05-17 08:30:09.636213: \n",
      "2025-05-17 08:30:09.636601: Epoch 424\n",
      "2025-05-17 08:30:09.636674: Current learning rate: 0.00609\n",
      "2025-05-17 08:31:58.546309: train_loss -0.9597\n",
      "2025-05-17 08:31:58.546513: val_loss -0.9372\n",
      "2025-05-17 08:31:58.546548: Pseudo dice [np.float32(0.9697)]\n",
      "2025-05-17 08:31:58.546581: Epoch time: 108.91 s\n",
      "2025-05-17 08:31:59.066481: \n",
      "2025-05-17 08:31:59.066576: Epoch 425\n",
      "2025-05-17 08:31:59.066639: Current learning rate: 0.00608\n",
      "2025-05-17 08:33:47.948661: train_loss -0.957\n",
      "2025-05-17 08:33:47.948875: val_loss -0.9421\n",
      "2025-05-17 08:33:47.949209: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-17 08:33:47.949287: Epoch time: 108.88 s\n",
      "2025-05-17 08:33:48.477794: \n",
      "2025-05-17 08:33:48.478128: Epoch 426\n",
      "2025-05-17 08:33:48.478204: Current learning rate: 0.00607\n",
      "2025-05-17 08:35:37.355023: train_loss -0.9584\n",
      "2025-05-17 08:35:37.355148: val_loss -0.9464\n",
      "2025-05-17 08:35:37.355183: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 08:35:37.355215: Epoch time: 108.88 s\n",
      "2025-05-17 08:35:37.871000: \n",
      "2025-05-17 08:35:37.871093: Epoch 427\n",
      "2025-05-17 08:35:37.871153: Current learning rate: 0.00606\n",
      "2025-05-17 08:37:26.800911: train_loss -0.9586\n",
      "2025-05-17 08:37:26.801036: val_loss -0.9449\n",
      "2025-05-17 08:37:26.801071: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 08:37:26.801105: Epoch time: 108.93 s\n",
      "2025-05-17 08:37:27.323518: \n",
      "2025-05-17 08:37:27.323683: Epoch 428\n",
      "2025-05-17 08:37:27.323757: Current learning rate: 0.00605\n",
      "2025-05-17 08:39:16.258529: train_loss -0.9599\n",
      "2025-05-17 08:39:16.258646: val_loss -0.9409\n",
      "2025-05-17 08:39:16.258677: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 08:39:16.258710: Epoch time: 108.94 s\n",
      "2025-05-17 08:39:16.776379: \n",
      "2025-05-17 08:39:16.776459: Epoch 429\n",
      "2025-05-17 08:39:16.776526: Current learning rate: 0.00604\n",
      "2025-05-17 08:41:05.732692: train_loss -0.9598\n",
      "2025-05-17 08:41:05.732825: val_loss -0.9471\n",
      "2025-05-17 08:41:05.732860: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 08:41:05.732893: Epoch time: 108.96 s\n",
      "2025-05-17 08:41:06.249129: \n",
      "2025-05-17 08:41:06.249306: Epoch 430\n",
      "2025-05-17 08:41:06.249379: Current learning rate: 0.00603\n",
      "2025-05-17 08:42:55.193503: train_loss -0.9611\n",
      "2025-05-17 08:42:55.193633: val_loss -0.944\n",
      "2025-05-17 08:42:55.193669: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-17 08:42:55.193702: Epoch time: 108.94 s\n",
      "2025-05-17 08:42:55.717147: \n",
      "2025-05-17 08:42:55.717304: Epoch 431\n",
      "2025-05-17 08:42:55.717377: Current learning rate: 0.00602\n",
      "2025-05-17 08:44:44.521827: train_loss -0.9593\n",
      "2025-05-17 08:44:44.521954: val_loss -0.9478\n",
      "2025-05-17 08:44:44.521988: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-17 08:44:44.522022: Epoch time: 108.81 s\n",
      "2025-05-17 08:44:45.244272: \n",
      "2025-05-17 08:44:45.244585: Epoch 432\n",
      "2025-05-17 08:44:45.244686: Current learning rate: 0.00601\n",
      "2025-05-17 08:46:34.172718: train_loss -0.96\n",
      "2025-05-17 08:46:34.172905: val_loss -0.9481\n",
      "2025-05-17 08:46:34.172940: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-17 08:46:34.172974: Epoch time: 108.93 s\n",
      "2025-05-17 08:46:34.684530: \n",
      "2025-05-17 08:46:34.684705: Epoch 433\n",
      "2025-05-17 08:46:34.684782: Current learning rate: 0.006\n",
      "2025-05-17 08:48:23.674361: train_loss -0.9616\n",
      "2025-05-17 08:48:23.674486: val_loss -0.9421\n",
      "2025-05-17 08:48:23.674524: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-17 08:48:23.674559: Epoch time: 108.99 s\n",
      "2025-05-17 08:48:24.185476: \n",
      "2025-05-17 08:48:24.185562: Epoch 434\n",
      "2025-05-17 08:48:24.185626: Current learning rate: 0.00599\n",
      "2025-05-17 08:50:13.130008: train_loss -0.9603\n",
      "2025-05-17 08:50:13.130135: val_loss -0.9408\n",
      "2025-05-17 08:50:13.130167: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 08:50:13.130202: Epoch time: 108.95 s\n",
      "2025-05-17 08:50:13.654263: \n",
      "2025-05-17 08:50:13.654417: Epoch 435\n",
      "2025-05-17 08:50:13.654553: Current learning rate: 0.00598\n",
      "2025-05-17 08:52:02.554777: train_loss -0.9587\n",
      "2025-05-17 08:52:02.554907: val_loss -0.9419\n",
      "2025-05-17 08:52:02.554940: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 08:52:02.554974: Epoch time: 108.9 s\n",
      "2025-05-17 08:52:03.076879: \n",
      "2025-05-17 08:52:03.077201: Epoch 436\n",
      "2025-05-17 08:52:03.077306: Current learning rate: 0.00597\n",
      "2025-05-17 08:53:52.016770: train_loss -0.9596\n",
      "2025-05-17 08:53:52.016896: val_loss -0.9463\n",
      "2025-05-17 08:53:52.016932: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 08:53:52.016965: Epoch time: 108.94 s\n",
      "2025-05-17 08:53:52.531232: \n",
      "2025-05-17 08:53:52.531432: Epoch 437\n",
      "2025-05-17 08:53:52.531511: Current learning rate: 0.00596\n",
      "2025-05-17 08:55:41.583689: train_loss -0.9603\n",
      "2025-05-17 08:55:41.583914: val_loss -0.947\n",
      "2025-05-17 08:55:41.584063: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 08:55:41.584127: Epoch time: 109.05 s\n",
      "2025-05-17 08:55:42.110863: \n",
      "2025-05-17 08:55:42.111076: Epoch 438\n",
      "2025-05-17 08:55:42.111160: Current learning rate: 0.00595\n",
      "2025-05-17 08:57:31.039535: train_loss -0.9619\n",
      "2025-05-17 08:57:31.039757: val_loss -0.9377\n",
      "2025-05-17 08:57:31.039829: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 08:57:31.039888: Epoch time: 108.93 s\n",
      "2025-05-17 08:57:31.552330: \n",
      "2025-05-17 08:57:31.552466: Epoch 439\n",
      "2025-05-17 08:57:31.552533: Current learning rate: 0.00594\n",
      "2025-05-17 08:59:20.533140: train_loss -0.9604\n",
      "2025-05-17 08:59:20.533282: val_loss -0.9459\n",
      "2025-05-17 08:59:20.533478: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-17 08:59:20.533562: Epoch time: 108.98 s\n",
      "2025-05-17 08:59:21.053145: \n",
      "2025-05-17 08:59:21.053293: Epoch 440\n",
      "2025-05-17 08:59:21.053374: Current learning rate: 0.00593\n",
      "2025-05-17 09:01:10.059309: train_loss -0.9619\n",
      "2025-05-17 09:01:10.059519: val_loss -0.9482\n",
      "2025-05-17 09:01:10.059554: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-17 09:01:10.059588: Epoch time: 109.01 s\n",
      "2025-05-17 09:01:10.570017: \n",
      "2025-05-17 09:01:10.570238: Epoch 441\n",
      "2025-05-17 09:01:10.570310: Current learning rate: 0.00592\n",
      "2025-05-17 09:02:59.434599: train_loss -0.9592\n",
      "2025-05-17 09:02:59.434720: val_loss -0.9407\n",
      "2025-05-17 09:02:59.434758: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-17 09:02:59.434819: Epoch time: 108.87 s\n",
      "2025-05-17 09:02:59.982087: \n",
      "2025-05-17 09:02:59.982183: Epoch 442\n",
      "2025-05-17 09:02:59.982252: Current learning rate: 0.00592\n",
      "2025-05-17 09:04:48.837333: train_loss -0.9582\n",
      "2025-05-17 09:04:48.837498: val_loss -0.9435\n",
      "2025-05-17 09:04:48.837534: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 09:04:48.837569: Epoch time: 108.86 s\n",
      "2025-05-17 09:04:49.351360: \n",
      "2025-05-17 09:04:49.351623: Epoch 443\n",
      "2025-05-17 09:04:49.351734: Current learning rate: 0.00591\n",
      "2025-05-17 09:06:38.297408: train_loss -0.9564\n",
      "2025-05-17 09:06:38.297540: val_loss -0.9416\n",
      "2025-05-17 09:06:38.297639: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-17 09:06:38.297702: Epoch time: 108.95 s\n",
      "2025-05-17 09:06:38.802637: \n",
      "2025-05-17 09:06:38.802811: Epoch 444\n",
      "2025-05-17 09:06:38.802883: Current learning rate: 0.0059\n",
      "2025-05-17 09:08:27.712928: train_loss -0.9554\n",
      "2025-05-17 09:08:27.713089: val_loss -0.94\n",
      "2025-05-17 09:08:27.713120: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 09:08:27.713153: Epoch time: 108.91 s\n",
      "2025-05-17 09:08:28.417332: \n",
      "2025-05-17 09:08:28.417432: Epoch 445\n",
      "2025-05-17 09:08:28.417495: Current learning rate: 0.00589\n",
      "2025-05-17 09:10:17.433778: train_loss -0.954\n",
      "2025-05-17 09:10:17.434025: val_loss -0.9435\n",
      "2025-05-17 09:10:17.434148: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 09:10:17.434244: Epoch time: 109.02 s\n",
      "2025-05-17 09:10:17.950781: \n",
      "2025-05-17 09:10:17.950883: Epoch 446\n",
      "2025-05-17 09:10:17.950947: Current learning rate: 0.00588\n",
      "2025-05-17 09:12:06.924778: train_loss -0.9582\n",
      "2025-05-17 09:12:06.924947: val_loss -0.9447\n",
      "2025-05-17 09:12:06.924989: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 09:12:06.925023: Epoch time: 108.97 s\n",
      "2025-05-17 09:12:07.430445: \n",
      "2025-05-17 09:12:07.430780: Epoch 447\n",
      "2025-05-17 09:12:07.431057: Current learning rate: 0.00587\n",
      "2025-05-17 09:13:56.359982: train_loss -0.9604\n",
      "2025-05-17 09:13:56.360118: val_loss -0.9447\n",
      "2025-05-17 09:13:56.360152: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 09:13:56.360187: Epoch time: 108.93 s\n",
      "2025-05-17 09:13:56.875311: \n",
      "2025-05-17 09:13:56.875655: Epoch 448\n",
      "2025-05-17 09:13:56.875760: Current learning rate: 0.00586\n",
      "2025-05-17 09:15:45.780957: train_loss -0.9594\n",
      "2025-05-17 09:15:45.781087: val_loss -0.9391\n",
      "2025-05-17 09:15:45.781120: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 09:15:45.781154: Epoch time: 108.91 s\n",
      "2025-05-17 09:15:46.283903: \n",
      "2025-05-17 09:15:46.284002: Epoch 449\n",
      "2025-05-17 09:15:46.284063: Current learning rate: 0.00585\n",
      "2025-05-17 09:17:35.261416: train_loss -0.9574\n",
      "2025-05-17 09:17:35.261540: val_loss -0.9456\n",
      "2025-05-17 09:17:35.261574: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 09:17:35.261776: Epoch time: 108.98 s\n",
      "2025-05-17 09:17:35.994558: \n",
      "2025-05-17 09:17:35.994664: Epoch 450\n",
      "2025-05-17 09:17:35.994732: Current learning rate: 0.00584\n",
      "2025-05-17 09:19:25.060460: train_loss -0.9597\n",
      "2025-05-17 09:19:25.060623: val_loss -0.9459\n",
      "2025-05-17 09:19:25.060678: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 09:19:25.060714: Epoch time: 109.07 s\n",
      "2025-05-17 09:19:25.573495: \n",
      "2025-05-17 09:19:25.573595: Epoch 451\n",
      "2025-05-17 09:19:25.573662: Current learning rate: 0.00583\n",
      "2025-05-17 09:21:14.583841: train_loss -0.959\n",
      "2025-05-17 09:21:14.583970: val_loss -0.9455\n",
      "2025-05-17 09:21:14.584007: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 09:21:14.584120: Epoch time: 109.01 s\n",
      "2025-05-17 09:21:15.094129: \n",
      "2025-05-17 09:21:15.094297: Epoch 452\n",
      "2025-05-17 09:21:15.094391: Current learning rate: 0.00582\n",
      "2025-05-17 09:23:04.037879: train_loss -0.9582\n",
      "2025-05-17 09:23:04.038010: val_loss -0.9436\n",
      "2025-05-17 09:23:04.038046: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 09:23:04.038078: Epoch time: 108.94 s\n",
      "2025-05-17 09:23:04.543460: \n",
      "2025-05-17 09:23:04.543788: Epoch 453\n",
      "2025-05-17 09:23:04.543920: Current learning rate: 0.00581\n",
      "2025-05-17 09:24:53.522470: train_loss -0.9611\n",
      "2025-05-17 09:24:53.522692: val_loss -0.9452\n",
      "2025-05-17 09:24:53.522736: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 09:24:53.522771: Epoch time: 108.98 s\n",
      "2025-05-17 09:24:54.029319: \n",
      "2025-05-17 09:24:54.029452: Epoch 454\n",
      "2025-05-17 09:24:54.029579: Current learning rate: 0.0058\n",
      "2025-05-17 09:26:42.964210: train_loss -0.9527\n",
      "2025-05-17 09:26:42.964339: val_loss -0.9354\n",
      "2025-05-17 09:26:42.964373: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-17 09:26:42.964406: Epoch time: 108.94 s\n",
      "2025-05-17 09:26:43.469398: \n",
      "2025-05-17 09:26:43.469483: Epoch 455\n",
      "2025-05-17 09:26:43.469548: Current learning rate: 0.00579\n",
      "2025-05-17 09:28:32.430658: train_loss -0.9484\n",
      "2025-05-17 09:28:32.430781: val_loss -0.9351\n",
      "2025-05-17 09:28:32.430817: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-17 09:28:32.430851: Epoch time: 108.96 s\n",
      "2025-05-17 09:28:32.936990: \n",
      "2025-05-17 09:28:32.937147: Epoch 456\n",
      "2025-05-17 09:28:32.937212: Current learning rate: 0.00578\n",
      "2025-05-17 09:30:21.932868: train_loss -0.9499\n",
      "2025-05-17 09:30:21.932987: val_loss -0.9439\n",
      "2025-05-17 09:30:21.933021: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-17 09:30:21.933053: Epoch time: 109.0 s\n",
      "2025-05-17 09:30:22.442512: \n",
      "2025-05-17 09:30:22.442685: Epoch 457\n",
      "2025-05-17 09:30:22.442842: Current learning rate: 0.00577\n",
      "2025-05-17 09:32:11.339300: train_loss -0.9523\n",
      "2025-05-17 09:32:11.339473: val_loss -0.947\n",
      "2025-05-17 09:32:11.339510: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-17 09:32:11.339543: Epoch time: 108.9 s\n",
      "2025-05-17 09:32:12.046074: \n",
      "2025-05-17 09:32:12.046306: Epoch 458\n",
      "2025-05-17 09:32:12.046412: Current learning rate: 0.00576\n",
      "2025-05-17 09:34:01.040992: train_loss -0.9576\n",
      "2025-05-17 09:34:01.041182: val_loss -0.9471\n",
      "2025-05-17 09:34:01.041216: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 09:34:01.041252: Epoch time: 109.0 s\n",
      "2025-05-17 09:34:01.550317: \n",
      "2025-05-17 09:34:01.550685: Epoch 459\n",
      "2025-05-17 09:34:01.550763: Current learning rate: 0.00575\n",
      "2025-05-17 09:35:50.446334: train_loss -0.9605\n",
      "2025-05-17 09:35:50.446465: val_loss -0.9457\n",
      "2025-05-17 09:35:50.446499: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 09:35:50.446534: Epoch time: 108.9 s\n",
      "2025-05-17 09:35:50.950903: \n",
      "2025-05-17 09:35:50.951085: Epoch 460\n",
      "2025-05-17 09:35:50.951159: Current learning rate: 0.00574\n",
      "2025-05-17 09:37:39.936584: train_loss -0.9576\n",
      "2025-05-17 09:37:39.936772: val_loss -0.9448\n",
      "2025-05-17 09:37:39.936814: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 09:37:39.936847: Epoch time: 108.99 s\n",
      "2025-05-17 09:37:40.449143: \n",
      "2025-05-17 09:37:40.449284: Epoch 461\n",
      "2025-05-17 09:37:40.449406: Current learning rate: 0.00573\n",
      "2025-05-17 09:39:29.388203: train_loss -0.9584\n",
      "2025-05-17 09:39:29.388362: val_loss -0.9457\n",
      "2025-05-17 09:39:29.388408: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 09:39:29.388446: Epoch time: 108.94 s\n",
      "2025-05-17 09:39:29.895558: \n",
      "2025-05-17 09:39:29.895660: Epoch 462\n",
      "2025-05-17 09:39:29.895786: Current learning rate: 0.00572\n",
      "2025-05-17 09:41:18.777627: train_loss -0.9607\n",
      "2025-05-17 09:41:18.777766: val_loss -0.9476\n",
      "2025-05-17 09:41:18.777799: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 09:41:18.777830: Epoch time: 108.88 s\n",
      "2025-05-17 09:41:19.284687: \n",
      "2025-05-17 09:41:19.284889: Epoch 463\n",
      "2025-05-17 09:41:19.284964: Current learning rate: 0.00571\n",
      "2025-05-17 09:43:08.216344: train_loss -0.9501\n",
      "2025-05-17 09:43:08.216484: val_loss -0.9406\n",
      "2025-05-17 09:43:08.216521: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 09:43:08.216554: Epoch time: 108.93 s\n",
      "2025-05-17 09:43:08.728647: \n",
      "2025-05-17 09:43:08.728945: Epoch 464\n",
      "2025-05-17 09:43:08.729059: Current learning rate: 0.0057\n",
      "2025-05-17 09:44:57.653160: train_loss -0.9598\n",
      "2025-05-17 09:44:57.653354: val_loss -0.9469\n",
      "2025-05-17 09:44:57.653401: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 09:44:57.653436: Epoch time: 108.93 s\n",
      "2025-05-17 09:44:58.174575: \n",
      "2025-05-17 09:44:58.174660: Epoch 465\n",
      "2025-05-17 09:44:58.174728: Current learning rate: 0.0057\n",
      "2025-05-17 09:46:46.913226: train_loss -0.959\n",
      "2025-05-17 09:46:46.913364: val_loss -0.9465\n",
      "2025-05-17 09:46:46.913399: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 09:46:46.913507: Epoch time: 108.74 s\n",
      "2025-05-17 09:46:47.421711: \n",
      "2025-05-17 09:46:47.421867: Epoch 466\n",
      "2025-05-17 09:46:47.421946: Current learning rate: 0.00569\n",
      "2025-05-17 09:48:36.363070: train_loss -0.9627\n",
      "2025-05-17 09:48:36.363245: val_loss -0.9445\n",
      "2025-05-17 09:48:36.363279: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 09:48:36.363312: Epoch time: 108.94 s\n",
      "2025-05-17 09:48:36.866863: \n",
      "2025-05-17 09:48:36.866957: Epoch 467\n",
      "2025-05-17 09:48:36.867023: Current learning rate: 0.00568\n",
      "2025-05-17 09:50:25.856316: train_loss -0.96\n",
      "2025-05-17 09:50:25.856554: val_loss -0.9418\n",
      "2025-05-17 09:50:25.856597: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-17 09:50:25.856632: Epoch time: 108.99 s\n",
      "2025-05-17 09:50:26.367006: \n",
      "2025-05-17 09:50:26.367088: Epoch 468\n",
      "2025-05-17 09:50:26.367153: Current learning rate: 0.00567\n",
      "2025-05-17 09:52:15.350436: train_loss -0.9619\n",
      "2025-05-17 09:52:15.350562: val_loss -0.9472\n",
      "2025-05-17 09:52:15.350598: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 09:52:15.350631: Epoch time: 108.98 s\n",
      "2025-05-17 09:52:15.858338: \n",
      "2025-05-17 09:52:15.858486: Epoch 469\n",
      "2025-05-17 09:52:15.858613: Current learning rate: 0.00566\n",
      "2025-05-17 09:54:04.706128: train_loss -0.9609\n",
      "2025-05-17 09:54:04.706270: val_loss -0.9428\n",
      "2025-05-17 09:54:04.706329: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 09:54:04.706443: Epoch time: 108.85 s\n",
      "2025-05-17 09:54:05.210226: \n",
      "2025-05-17 09:54:05.210311: Epoch 470\n",
      "2025-05-17 09:54:05.210374: Current learning rate: 0.00565\n",
      "2025-05-17 09:55:54.110760: train_loss -0.9492\n",
      "2025-05-17 09:55:54.110934: val_loss -0.9427\n",
      "2025-05-17 09:55:54.110969: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 09:55:54.111003: Epoch time: 108.9 s\n",
      "2025-05-17 09:55:54.617115: \n",
      "2025-05-17 09:55:54.617379: Epoch 471\n",
      "2025-05-17 09:55:54.617474: Current learning rate: 0.00564\n",
      "2025-05-17 09:57:43.523813: train_loss -0.9461\n",
      "2025-05-17 09:57:43.523956: val_loss -0.9352\n",
      "2025-05-17 09:57:43.523998: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-17 09:57:43.524031: Epoch time: 108.91 s\n",
      "2025-05-17 09:57:44.219047: \n",
      "2025-05-17 09:57:44.219141: Epoch 472\n",
      "2025-05-17 09:57:44.219209: Current learning rate: 0.00563\n",
      "2025-05-17 09:59:33.251985: train_loss -0.9537\n",
      "2025-05-17 09:59:33.252108: val_loss -0.9421\n",
      "2025-05-17 09:59:33.252142: Pseudo dice [np.float32(0.9717)]\n",
      "2025-05-17 09:59:33.252203: Epoch time: 109.03 s\n",
      "2025-05-17 09:59:33.759094: \n",
      "2025-05-17 09:59:33.759468: Epoch 473\n",
      "2025-05-17 09:59:33.759593: Current learning rate: 0.00562\n",
      "2025-05-17 10:01:22.826715: train_loss -0.9516\n",
      "2025-05-17 10:01:22.826860: val_loss -0.9417\n",
      "2025-05-17 10:01:22.826894: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-17 10:01:22.826928: Epoch time: 109.07 s\n",
      "2025-05-17 10:01:23.343503: \n",
      "2025-05-17 10:01:23.343600: Epoch 474\n",
      "2025-05-17 10:01:23.343664: Current learning rate: 0.00561\n",
      "2025-05-17 10:03:12.306798: train_loss -0.9589\n",
      "2025-05-17 10:03:12.306940: val_loss -0.9423\n",
      "2025-05-17 10:03:12.306977: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 10:03:12.307011: Epoch time: 108.96 s\n",
      "2025-05-17 10:03:12.823432: \n",
      "2025-05-17 10:03:12.823725: Epoch 475\n",
      "2025-05-17 10:03:12.823798: Current learning rate: 0.0056\n",
      "2025-05-17 10:05:01.832224: train_loss -0.9602\n",
      "2025-05-17 10:05:01.832391: val_loss -0.9458\n",
      "2025-05-17 10:05:01.832439: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 10:05:01.832485: Epoch time: 109.01 s\n",
      "2025-05-17 10:05:02.352041: \n",
      "2025-05-17 10:05:02.352136: Epoch 476\n",
      "2025-05-17 10:05:02.352201: Current learning rate: 0.00559\n",
      "2025-05-17 10:06:51.374390: train_loss -0.9591\n",
      "2025-05-17 10:06:51.374613: val_loss -0.9482\n",
      "2025-05-17 10:06:51.374650: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-17 10:06:51.374686: Epoch time: 109.02 s\n",
      "2025-05-17 10:06:51.883382: \n",
      "2025-05-17 10:06:51.883656: Epoch 477\n",
      "2025-05-17 10:06:51.883731: Current learning rate: 0.00558\n",
      "2025-05-17 10:08:40.819121: train_loss -0.961\n",
      "2025-05-17 10:08:40.819241: val_loss -0.9474\n",
      "2025-05-17 10:08:40.819344: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 10:08:40.819388: Epoch time: 108.94 s\n",
      "2025-05-17 10:08:41.335750: \n",
      "2025-05-17 10:08:41.335992: Epoch 478\n",
      "2025-05-17 10:08:41.336068: Current learning rate: 0.00557\n",
      "2025-05-17 10:10:30.170160: train_loss -0.9626\n",
      "2025-05-17 10:10:30.170279: val_loss -0.9415\n",
      "2025-05-17 10:10:30.170311: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-17 10:10:30.170345: Epoch time: 108.84 s\n",
      "2025-05-17 10:10:30.685638: \n",
      "2025-05-17 10:10:30.685935: Epoch 479\n",
      "2025-05-17 10:10:30.686176: Current learning rate: 0.00556\n",
      "2025-05-17 10:12:19.620286: train_loss -0.9601\n",
      "2025-05-17 10:12:19.620416: val_loss -0.951\n",
      "2025-05-17 10:12:19.620453: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 10:12:19.620485: Epoch time: 108.94 s\n",
      "2025-05-17 10:12:20.128074: \n",
      "2025-05-17 10:12:20.128211: Epoch 480\n",
      "2025-05-17 10:12:20.128297: Current learning rate: 0.00555\n",
      "2025-05-17 10:14:09.091362: train_loss -0.9516\n",
      "2025-05-17 10:14:09.091570: val_loss -0.917\n",
      "2025-05-17 10:14:09.091607: Pseudo dice [np.float32(0.9627)]\n",
      "2025-05-17 10:14:09.091715: Epoch time: 108.96 s\n",
      "2025-05-17 10:14:09.606472: \n",
      "2025-05-17 10:14:09.606627: Epoch 481\n",
      "2025-05-17 10:14:09.606694: Current learning rate: 0.00554\n",
      "2025-05-17 10:15:58.511451: train_loss -0.9433\n",
      "2025-05-17 10:15:58.511608: val_loss -0.9418\n",
      "2025-05-17 10:15:58.511643: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 10:15:58.511679: Epoch time: 108.91 s\n",
      "2025-05-17 10:15:59.024192: \n",
      "2025-05-17 10:15:59.024508: Epoch 482\n",
      "2025-05-17 10:15:59.024685: Current learning rate: 0.00553\n",
      "2025-05-17 10:17:47.907602: train_loss -0.9538\n",
      "2025-05-17 10:17:47.907768: val_loss -0.9419\n",
      "2025-05-17 10:17:47.907802: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-17 10:17:47.907836: Epoch time: 108.88 s\n",
      "2025-05-17 10:17:48.419715: \n",
      "2025-05-17 10:17:48.419794: Epoch 483\n",
      "2025-05-17 10:17:48.419859: Current learning rate: 0.00552\n",
      "2025-05-17 10:19:37.372274: train_loss -0.9567\n",
      "2025-05-17 10:19:37.372582: val_loss -0.949\n",
      "2025-05-17 10:19:37.374850: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-17 10:19:37.374928: Epoch time: 108.95 s\n",
      "2025-05-17 10:19:37.889215: \n",
      "2025-05-17 10:19:37.889308: Epoch 484\n",
      "2025-05-17 10:19:37.889372: Current learning rate: 0.00551\n",
      "2025-05-17 10:21:26.874893: train_loss -0.9559\n",
      "2025-05-17 10:21:26.875020: val_loss -0.9446\n",
      "2025-05-17 10:21:26.875054: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 10:21:26.875087: Epoch time: 108.99 s\n",
      "2025-05-17 10:21:27.586853: \n",
      "2025-05-17 10:21:27.587035: Epoch 485\n",
      "2025-05-17 10:21:27.587111: Current learning rate: 0.0055\n",
      "2025-05-17 10:23:16.532838: train_loss -0.9598\n",
      "2025-05-17 10:23:16.532965: val_loss -0.9445\n",
      "2025-05-17 10:23:16.533000: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 10:23:16.533032: Epoch time: 108.95 s\n",
      "2025-05-17 10:23:17.049505: \n",
      "2025-05-17 10:23:17.049670: Epoch 486\n",
      "2025-05-17 10:23:17.049800: Current learning rate: 0.00549\n",
      "2025-05-17 10:25:06.062704: train_loss -0.9585\n",
      "2025-05-17 10:25:06.062991: val_loss -0.9495\n",
      "2025-05-17 10:25:06.063098: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 10:25:06.063188: Epoch time: 109.01 s\n",
      "2025-05-17 10:25:06.582557: \n",
      "2025-05-17 10:25:06.582656: Epoch 487\n",
      "2025-05-17 10:25:06.582721: Current learning rate: 0.00548\n",
      "2025-05-17 10:26:55.585639: train_loss -0.9592\n",
      "2025-05-17 10:26:55.585802: val_loss -0.9519\n",
      "2025-05-17 10:26:55.585839: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-17 10:26:55.585872: Epoch time: 109.0 s\n",
      "2025-05-17 10:26:56.098748: \n",
      "2025-05-17 10:26:56.098924: Epoch 488\n",
      "2025-05-17 10:26:56.099009: Current learning rate: 0.00547\n",
      "2025-05-17 10:28:45.108693: train_loss -0.9587\n",
      "2025-05-17 10:28:45.108840: val_loss -0.9508\n",
      "2025-05-17 10:28:45.108881: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 10:28:45.108915: Epoch time: 109.01 s\n",
      "2025-05-17 10:28:45.622212: \n",
      "2025-05-17 10:28:45.622430: Epoch 489\n",
      "2025-05-17 10:28:45.622501: Current learning rate: 0.00546\n",
      "2025-05-17 10:30:34.608710: train_loss -0.9615\n",
      "2025-05-17 10:30:34.608835: val_loss -0.9483\n",
      "2025-05-17 10:30:34.608869: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 10:30:34.608901: Epoch time: 108.99 s\n",
      "2025-05-17 10:30:35.125134: \n",
      "2025-05-17 10:30:35.125307: Epoch 490\n",
      "2025-05-17 10:30:35.125415: Current learning rate: 0.00546\n",
      "2025-05-17 10:32:24.072880: train_loss -0.9583\n",
      "2025-05-17 10:32:24.073027: val_loss -0.9481\n",
      "2025-05-17 10:32:24.073063: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-17 10:32:24.073096: Epoch time: 108.95 s\n",
      "2025-05-17 10:32:24.594524: \n",
      "2025-05-17 10:32:24.594635: Epoch 491\n",
      "2025-05-17 10:32:24.594801: Current learning rate: 0.00545\n",
      "2025-05-17 10:34:13.490058: train_loss -0.9623\n",
      "2025-05-17 10:34:13.490248: val_loss -0.9461\n",
      "2025-05-17 10:34:13.490282: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 10:34:13.490314: Epoch time: 108.9 s\n",
      "2025-05-17 10:34:14.009627: \n",
      "2025-05-17 10:34:14.009822: Epoch 492\n",
      "2025-05-17 10:34:14.009896: Current learning rate: 0.00544\n",
      "2025-05-17 10:36:02.957358: train_loss -0.9624\n",
      "2025-05-17 10:36:02.957544: val_loss -0.9491\n",
      "2025-05-17 10:36:02.957578: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 10:36:02.957611: Epoch time: 108.95 s\n",
      "2025-05-17 10:36:02.957632: Yayy! New best EMA pseudo Dice: 0.9743000268936157\n",
      "2025-05-17 10:36:03.697068: \n",
      "2025-05-17 10:36:03.697397: Epoch 493\n",
      "2025-05-17 10:36:03.697557: Current learning rate: 0.00543\n",
      "2025-05-17 10:37:52.584491: train_loss -0.9625\n",
      "2025-05-17 10:37:52.584631: val_loss -0.944\n",
      "2025-05-17 10:37:52.584667: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 10:37:52.584701: Epoch time: 108.89 s\n",
      "2025-05-17 10:37:53.094847: \n",
      "2025-05-17 10:37:53.095226: Epoch 494\n",
      "2025-05-17 10:37:53.095502: Current learning rate: 0.00542\n",
      "2025-05-17 10:39:42.067566: train_loss -0.9624\n",
      "2025-05-17 10:39:42.067736: val_loss -0.9485\n",
      "2025-05-17 10:39:42.067797: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-17 10:39:42.067847: Epoch time: 108.97 s\n",
      "2025-05-17 10:39:42.067929: Yayy! New best EMA pseudo Dice: 0.974399983882904\n",
      "2025-05-17 10:39:42.805809: \n",
      "2025-05-17 10:39:42.805943: Epoch 495\n",
      "2025-05-17 10:39:42.806137: Current learning rate: 0.00541\n",
      "2025-05-17 10:41:31.765374: train_loss -0.9638\n",
      "2025-05-17 10:41:31.765504: val_loss -0.9481\n",
      "2025-05-17 10:41:31.765537: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-17 10:41:31.765588: Epoch time: 108.96 s\n",
      "2025-05-17 10:41:31.765616: Yayy! New best EMA pseudo Dice: 0.9745000004768372\n",
      "2025-05-17 10:41:32.502316: \n",
      "2025-05-17 10:41:32.502445: Epoch 496\n",
      "2025-05-17 10:41:32.502517: Current learning rate: 0.0054\n",
      "2025-05-17 10:43:21.518871: train_loss -0.9626\n",
      "2025-05-17 10:43:21.519003: val_loss -0.9423\n",
      "2025-05-17 10:43:21.519038: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 10:43:21.519072: Epoch time: 109.02 s\n",
      "2025-05-17 10:43:22.039566: \n",
      "2025-05-17 10:43:22.039729: Epoch 497\n",
      "2025-05-17 10:43:22.039798: Current learning rate: 0.00539\n",
      "2025-05-17 10:45:10.993701: train_loss -0.9611\n",
      "2025-05-17 10:45:10.993956: val_loss -0.9458\n",
      "2025-05-17 10:45:10.994068: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 10:45:10.994112: Epoch time: 108.95 s\n",
      "2025-05-17 10:45:11.698367: \n",
      "2025-05-17 10:45:11.698466: Epoch 498\n",
      "2025-05-17 10:45:11.698548: Current learning rate: 0.00538\n",
      "2025-05-17 10:47:00.687058: train_loss -0.9604\n",
      "2025-05-17 10:47:00.687290: val_loss -0.951\n",
      "2025-05-17 10:47:00.687437: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-17 10:47:00.687497: Epoch time: 108.99 s\n",
      "2025-05-17 10:47:00.687530: Yayy! New best EMA pseudo Dice: 0.9745000004768372\n",
      "2025-05-17 10:47:01.415645: \n",
      "2025-05-17 10:47:01.416064: Epoch 499\n",
      "2025-05-17 10:47:01.416250: Current learning rate: 0.00537\n",
      "2025-05-17 10:48:50.450389: train_loss -0.9626\n",
      "2025-05-17 10:48:50.450526: val_loss -0.9482\n",
      "2025-05-17 10:48:50.450561: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-17 10:48:50.450595: Epoch time: 109.04 s\n",
      "2025-05-17 10:48:50.658766: Yayy! New best EMA pseudo Dice: 0.9746000170707703\n",
      "2025-05-17 10:48:51.397445: \n",
      "2025-05-17 10:48:51.397600: Epoch 500\n",
      "2025-05-17 10:48:51.397680: Current learning rate: 0.00536\n",
      "2025-05-17 10:50:40.355127: train_loss -0.9607\n",
      "2025-05-17 10:50:40.355323: val_loss -0.9437\n",
      "2025-05-17 10:50:40.355369: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-17 10:50:40.355465: Epoch time: 108.96 s\n",
      "2025-05-17 10:50:40.876206: \n",
      "2025-05-17 10:50:40.876383: Epoch 501\n",
      "2025-05-17 10:50:40.876455: Current learning rate: 0.00535\n",
      "2025-05-17 10:52:29.859543: train_loss -0.9611\n",
      "2025-05-17 10:52:29.859671: val_loss -0.9456\n",
      "2025-05-17 10:52:29.859703: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-17 10:52:29.859738: Epoch time: 108.98 s\n",
      "2025-05-17 10:52:30.374543: \n",
      "2025-05-17 10:52:30.374892: Epoch 502\n",
      "2025-05-17 10:52:30.374964: Current learning rate: 0.00534\n",
      "2025-05-17 10:54:19.353096: train_loss -0.9634\n",
      "2025-05-17 10:54:19.353215: val_loss -0.9499\n",
      "2025-05-17 10:54:19.353246: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-17 10:54:19.353279: Epoch time: 108.98 s\n",
      "2025-05-17 10:54:19.353299: Yayy! New best EMA pseudo Dice: 0.9746999740600586\n",
      "2025-05-17 10:54:20.081595: \n",
      "2025-05-17 10:54:20.081767: Epoch 503\n",
      "2025-05-17 10:54:20.081873: Current learning rate: 0.00533\n",
      "2025-05-17 10:56:09.044501: train_loss -0.9603\n",
      "2025-05-17 10:56:09.044689: val_loss -0.9492\n",
      "2025-05-17 10:56:09.044724: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 10:56:09.044758: Epoch time: 108.96 s\n",
      "2025-05-17 10:56:09.044779: Yayy! New best EMA pseudo Dice: 0.9747999906539917\n",
      "2025-05-17 10:56:09.777923: \n",
      "2025-05-17 10:56:09.778233: Epoch 504\n",
      "2025-05-17 10:56:09.778373: Current learning rate: 0.00532\n",
      "2025-05-17 10:57:58.710573: train_loss -0.9631\n",
      "2025-05-17 10:57:58.710767: val_loss -0.9458\n",
      "2025-05-17 10:57:58.710857: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 10:57:58.710984: Epoch time: 108.93 s\n",
      "2025-05-17 10:57:59.239203: \n",
      "2025-05-17 10:57:59.239337: Epoch 505\n",
      "2025-05-17 10:57:59.239406: Current learning rate: 0.00531\n",
      "2025-05-17 10:59:48.133875: train_loss -0.9646\n",
      "2025-05-17 10:59:48.134073: val_loss -0.9462\n",
      "2025-05-17 10:59:48.134110: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 10:59:48.134143: Epoch time: 108.9 s\n",
      "2025-05-17 10:59:48.134164: Yayy! New best EMA pseudo Dice: 0.9749000072479248\n",
      "2025-05-17 10:59:48.867710: \n",
      "2025-05-17 10:59:48.867798: Epoch 506\n",
      "2025-05-17 10:59:48.867862: Current learning rate: 0.0053\n",
      "2025-05-17 11:01:37.930680: train_loss -0.9631\n",
      "2025-05-17 11:01:37.930810: val_loss -0.941\n",
      "2025-05-17 11:01:37.930846: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-17 11:01:37.930879: Epoch time: 109.06 s\n",
      "2025-05-17 11:01:38.447816: \n",
      "2025-05-17 11:01:38.447917: Epoch 507\n",
      "2025-05-17 11:01:38.447983: Current learning rate: 0.00529\n",
      "2025-05-17 11:03:27.398270: train_loss -0.962\n",
      "2025-05-17 11:03:27.398412: val_loss -0.9492\n",
      "2025-05-17 11:03:27.398445: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-17 11:03:27.398479: Epoch time: 108.95 s\n",
      "2025-05-17 11:03:27.936858: \n",
      "2025-05-17 11:03:27.937001: Epoch 508\n",
      "2025-05-17 11:03:27.937074: Current learning rate: 0.00528\n",
      "2025-05-17 11:05:16.874433: train_loss -0.9632\n",
      "2025-05-17 11:05:16.874556: val_loss -0.9494\n",
      "2025-05-17 11:05:16.874590: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 11:05:16.874624: Epoch time: 108.94 s\n",
      "2025-05-17 11:05:17.391186: \n",
      "2025-05-17 11:05:17.391279: Epoch 509\n",
      "2025-05-17 11:05:17.391343: Current learning rate: 0.00527\n",
      "2025-05-17 11:07:06.409996: train_loss -0.9621\n",
      "2025-05-17 11:07:06.410132: val_loss -0.9427\n",
      "2025-05-17 11:07:06.410165: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 11:07:06.410196: Epoch time: 109.02 s\n",
      "2025-05-17 11:07:07.106022: \n",
      "2025-05-17 11:07:07.106125: Epoch 510\n",
      "2025-05-17 11:07:07.106192: Current learning rate: 0.00526\n",
      "2025-05-17 11:08:56.046938: train_loss -0.9642\n",
      "2025-05-17 11:08:56.047065: val_loss -0.9437\n",
      "2025-05-17 11:08:56.047111: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 11:08:56.047148: Epoch time: 108.94 s\n",
      "2025-05-17 11:08:56.564532: \n",
      "2025-05-17 11:08:56.564724: Epoch 511\n",
      "2025-05-17 11:08:56.564796: Current learning rate: 0.00525\n",
      "2025-05-17 11:10:45.529459: train_loss -0.963\n",
      "2025-05-17 11:10:45.529594: val_loss -0.948\n",
      "2025-05-17 11:10:45.529629: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-17 11:10:45.529662: Epoch time: 108.97 s\n",
      "2025-05-17 11:10:46.049810: \n",
      "2025-05-17 11:10:46.049890: Epoch 512\n",
      "2025-05-17 11:10:46.049955: Current learning rate: 0.00524\n",
      "2025-05-17 11:12:35.020144: train_loss -0.9627\n",
      "2025-05-17 11:12:35.020488: val_loss -0.9455\n",
      "2025-05-17 11:12:35.020529: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 11:12:35.020563: Epoch time: 108.97 s\n",
      "2025-05-17 11:12:35.534286: \n",
      "2025-05-17 11:12:35.534385: Epoch 513\n",
      "2025-05-17 11:12:35.534451: Current learning rate: 0.00523\n",
      "2025-05-17 11:14:24.529794: train_loss -0.962\n",
      "2025-05-17 11:14:24.529921: val_loss -0.9482\n",
      "2025-05-17 11:14:24.529957: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 11:14:24.529991: Epoch time: 109.0 s\n",
      "2025-05-17 11:14:25.044629: \n",
      "2025-05-17 11:14:25.044955: Epoch 514\n",
      "2025-05-17 11:14:25.045043: Current learning rate: 0.00522\n",
      "2025-05-17 11:16:14.019909: train_loss -0.9635\n",
      "2025-05-17 11:16:14.020047: val_loss -0.9458\n",
      "2025-05-17 11:16:14.020082: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 11:16:14.020115: Epoch time: 108.98 s\n",
      "2025-05-17 11:16:14.536785: \n",
      "2025-05-17 11:16:14.536944: Epoch 515\n",
      "2025-05-17 11:16:14.537011: Current learning rate: 0.00521\n",
      "2025-05-17 11:18:03.516189: train_loss -0.9647\n",
      "2025-05-17 11:18:03.516314: val_loss -0.9488\n",
      "2025-05-17 11:18:03.516351: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 11:18:03.516385: Epoch time: 108.98 s\n",
      "2025-05-17 11:18:03.516407: Yayy! New best EMA pseudo Dice: 0.9749000072479248\n",
      "2025-05-17 11:18:04.252562: \n",
      "2025-05-17 11:18:04.252692: Epoch 516\n",
      "2025-05-17 11:18:04.252757: Current learning rate: 0.0052\n",
      "2025-05-17 11:19:53.180265: train_loss -0.9643\n",
      "2025-05-17 11:19:53.180434: val_loss -0.9442\n",
      "2025-05-17 11:19:53.180468: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 11:19:53.180507: Epoch time: 108.93 s\n",
      "2025-05-17 11:19:53.695129: \n",
      "2025-05-17 11:19:53.695246: Epoch 517\n",
      "2025-05-17 11:19:53.695520: Current learning rate: 0.00519\n",
      "2025-05-17 11:21:42.579202: train_loss -0.9638\n",
      "2025-05-17 11:21:42.579325: val_loss -0.9515\n",
      "2025-05-17 11:21:42.579360: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 11:21:42.579393: Epoch time: 108.88 s\n",
      "2025-05-17 11:21:43.101018: \n",
      "2025-05-17 11:21:43.101218: Epoch 518\n",
      "2025-05-17 11:21:43.101307: Current learning rate: 0.00518\n",
      "2025-05-17 11:23:32.067489: train_loss -0.9647\n",
      "2025-05-17 11:23:32.067614: val_loss -0.9464\n",
      "2025-05-17 11:23:32.067647: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-17 11:23:32.067680: Epoch time: 108.97 s\n",
      "2025-05-17 11:23:32.606384: \n",
      "2025-05-17 11:23:32.606549: Epoch 519\n",
      "2025-05-17 11:23:32.606618: Current learning rate: 0.00518\n",
      "2025-05-17 11:25:21.647699: train_loss -0.9643\n",
      "2025-05-17 11:25:21.647827: val_loss -0.9464\n",
      "2025-05-17 11:25:21.647860: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-17 11:25:21.647893: Epoch time: 109.04 s\n",
      "2025-05-17 11:25:22.167202: \n",
      "2025-05-17 11:25:22.167286: Epoch 520\n",
      "2025-05-17 11:25:22.167350: Current learning rate: 0.00517\n",
      "2025-05-17 11:27:11.103803: train_loss -0.9638\n",
      "2025-05-17 11:27:11.103931: val_loss -0.935\n",
      "2025-05-17 11:27:11.103971: Pseudo dice [np.float32(0.9704)]\n",
      "2025-05-17 11:27:11.104013: Epoch time: 108.94 s\n",
      "2025-05-17 11:27:11.618534: \n",
      "2025-05-17 11:27:11.618677: Epoch 521\n",
      "2025-05-17 11:27:11.618742: Current learning rate: 0.00516\n",
      "2025-05-17 11:29:00.587433: train_loss -0.9602\n",
      "2025-05-17 11:29:00.587565: val_loss -0.9481\n",
      "2025-05-17 11:29:00.587864: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-17 11:29:00.587986: Epoch time: 108.97 s\n",
      "2025-05-17 11:29:01.304169: \n",
      "2025-05-17 11:29:01.304360: Epoch 522\n",
      "2025-05-17 11:29:01.304438: Current learning rate: 0.00515\n",
      "2025-05-17 11:30:50.361617: train_loss -0.9637\n",
      "2025-05-17 11:30:50.361779: val_loss -0.9478\n",
      "2025-05-17 11:30:50.361883: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-17 11:30:50.362044: Epoch time: 109.06 s\n",
      "2025-05-17 11:30:50.875252: \n",
      "2025-05-17 11:30:50.875443: Epoch 523\n",
      "2025-05-17 11:30:50.875637: Current learning rate: 0.00514\n",
      "2025-05-17 11:32:39.805069: train_loss -0.9631\n",
      "2025-05-17 11:32:39.805201: val_loss -0.9468\n",
      "2025-05-17 11:32:39.805235: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 11:32:39.805268: Epoch time: 108.93 s\n",
      "2025-05-17 11:32:40.326049: \n",
      "2025-05-17 11:32:40.326146: Epoch 524\n",
      "2025-05-17 11:32:40.326209: Current learning rate: 0.00513\n",
      "2025-05-17 11:34:29.287702: train_loss -0.9657\n",
      "2025-05-17 11:34:29.287834: val_loss -0.9479\n",
      "2025-05-17 11:34:29.287869: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-17 11:34:29.287977: Epoch time: 108.96 s\n",
      "2025-05-17 11:34:29.804227: \n",
      "2025-05-17 11:34:29.804315: Epoch 525\n",
      "2025-05-17 11:34:29.804378: Current learning rate: 0.00512\n",
      "2025-05-17 11:36:18.750160: train_loss -0.9648\n",
      "2025-05-17 11:36:18.750300: val_loss -0.951\n",
      "2025-05-17 11:36:18.750339: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 11:36:18.750375: Epoch time: 108.95 s\n",
      "2025-05-17 11:36:19.271011: \n",
      "2025-05-17 11:36:19.271167: Epoch 526\n",
      "2025-05-17 11:36:19.271231: Current learning rate: 0.00511\n",
      "2025-05-17 11:38:08.315358: train_loss -0.96\n",
      "2025-05-17 11:38:08.315635: val_loss -0.9412\n",
      "2025-05-17 11:38:08.315674: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 11:38:08.315707: Epoch time: 109.05 s\n",
      "2025-05-17 11:38:08.840810: \n",
      "2025-05-17 11:38:08.841141: Epoch 527\n",
      "2025-05-17 11:38:08.841276: Current learning rate: 0.0051\n",
      "2025-05-17 11:39:57.820210: train_loss -0.9398\n",
      "2025-05-17 11:39:57.820338: val_loss -0.9349\n",
      "2025-05-17 11:39:57.820372: Pseudo dice [np.float32(0.9695)]\n",
      "2025-05-17 11:39:57.820406: Epoch time: 108.98 s\n",
      "2025-05-17 11:39:58.339803: \n",
      "2025-05-17 11:39:58.340050: Epoch 528\n",
      "2025-05-17 11:39:58.340152: Current learning rate: 0.00509\n",
      "2025-05-17 11:41:47.295310: train_loss -0.9562\n",
      "2025-05-17 11:41:47.295455: val_loss -0.9446\n",
      "2025-05-17 11:41:47.295488: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 11:41:47.295522: Epoch time: 108.96 s\n",
      "2025-05-17 11:41:47.821931: \n",
      "2025-05-17 11:41:47.822033: Epoch 529\n",
      "2025-05-17 11:41:47.822097: Current learning rate: 0.00508\n",
      "2025-05-17 11:43:36.690708: train_loss -0.959\n",
      "2025-05-17 11:43:36.690871: val_loss -0.9444\n",
      "2025-05-17 11:43:36.691016: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 11:43:36.691109: Epoch time: 108.87 s\n",
      "2025-05-17 11:43:37.212899: \n",
      "2025-05-17 11:43:37.212994: Epoch 530\n",
      "2025-05-17 11:43:37.213058: Current learning rate: 0.00507\n",
      "2025-05-17 11:45:26.044988: train_loss -0.9614\n",
      "2025-05-17 11:45:26.045116: val_loss -0.9432\n",
      "2025-05-17 11:45:26.045152: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 11:45:26.045186: Epoch time: 108.83 s\n",
      "2025-05-17 11:45:26.566270: \n",
      "2025-05-17 11:45:26.566363: Epoch 531\n",
      "2025-05-17 11:45:26.566480: Current learning rate: 0.00506\n",
      "2025-05-17 11:47:15.495952: train_loss -0.9624\n",
      "2025-05-17 11:47:15.496156: val_loss -0.9431\n",
      "2025-05-17 11:47:15.496263: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-17 11:47:15.496345: Epoch time: 108.93 s\n",
      "2025-05-17 11:47:16.013605: \n",
      "2025-05-17 11:47:16.013693: Epoch 532\n",
      "2025-05-17 11:47:16.013874: Current learning rate: 0.00505\n",
      "2025-05-17 11:49:05.008476: train_loss -0.961\n",
      "2025-05-17 11:49:05.008601: val_loss -0.9426\n",
      "2025-05-17 11:49:05.008637: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 11:49:05.008669: Epoch time: 109.0 s\n",
      "2025-05-17 11:49:05.522702: \n",
      "2025-05-17 11:49:05.522831: Epoch 533\n",
      "2025-05-17 11:49:05.522902: Current learning rate: 0.00504\n",
      "2025-05-17 11:50:54.432431: train_loss -0.9602\n",
      "2025-05-17 11:50:54.432744: val_loss -0.9478\n",
      "2025-05-17 11:50:54.432863: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-17 11:50:54.432916: Epoch time: 108.91 s\n",
      "2025-05-17 11:50:54.944868: \n",
      "2025-05-17 11:50:54.944996: Epoch 534\n",
      "2025-05-17 11:50:54.945060: Current learning rate: 0.00503\n",
      "2025-05-17 11:52:43.846964: train_loss -0.963\n",
      "2025-05-17 11:52:43.847090: val_loss -0.9492\n",
      "2025-05-17 11:52:43.847124: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-17 11:52:43.847156: Epoch time: 108.9 s\n",
      "2025-05-17 11:52:44.572222: \n",
      "2025-05-17 11:52:44.572332: Epoch 535\n",
      "2025-05-17 11:52:44.572400: Current learning rate: 0.00502\n",
      "2025-05-17 11:54:33.527131: train_loss -0.9619\n",
      "2025-05-17 11:54:33.527331: val_loss -0.9513\n",
      "2025-05-17 11:54:33.527366: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-17 11:54:33.527399: Epoch time: 108.96 s\n",
      "2025-05-17 11:54:34.047598: \n",
      "2025-05-17 11:54:34.047808: Epoch 536\n",
      "2025-05-17 11:54:34.047911: Current learning rate: 0.00501\n",
      "2025-05-17 11:56:23.016594: train_loss -0.9644\n",
      "2025-05-17 11:56:23.016759: val_loss -0.9472\n",
      "2025-05-17 11:56:23.017002: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 11:56:23.017114: Epoch time: 108.97 s\n",
      "2025-05-17 11:56:23.534654: \n",
      "2025-05-17 11:56:23.534783: Epoch 537\n",
      "2025-05-17 11:56:23.534900: Current learning rate: 0.005\n",
      "2025-05-17 11:58:12.403960: train_loss -0.9649\n",
      "2025-05-17 11:58:12.404171: val_loss -0.9467\n",
      "2025-05-17 11:58:12.404295: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-17 11:58:12.404338: Epoch time: 108.87 s\n",
      "2025-05-17 11:58:12.935976: \n",
      "2025-05-17 11:58:12.936075: Epoch 538\n",
      "2025-05-17 11:58:12.936142: Current learning rate: 0.00499\n",
      "2025-05-17 12:00:01.862625: train_loss -0.9644\n",
      "2025-05-17 12:00:01.862748: val_loss -0.9461\n",
      "2025-05-17 12:00:01.862782: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-17 12:00:01.862815: Epoch time: 108.93 s\n",
      "2025-05-17 12:00:02.385605: \n",
      "2025-05-17 12:00:02.385700: Epoch 539\n",
      "2025-05-17 12:00:02.385761: Current learning rate: 0.00498\n",
      "2025-05-17 12:01:51.343308: train_loss -0.9635\n",
      "2025-05-17 12:01:51.343448: val_loss -0.948\n",
      "2025-05-17 12:01:51.343483: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 12:01:51.343515: Epoch time: 108.96 s\n",
      "2025-05-17 12:01:51.343535: Yayy! New best EMA pseudo Dice: 0.9750000238418579\n",
      "2025-05-17 12:01:52.082223: \n",
      "2025-05-17 12:01:52.082426: Epoch 540\n",
      "2025-05-17 12:01:52.082640: Current learning rate: 0.00497\n",
      "2025-05-17 12:03:41.024114: train_loss -0.9652\n",
      "2025-05-17 12:03:41.024301: val_loss -0.9449\n",
      "2025-05-17 12:03:41.024333: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 12:03:41.024366: Epoch time: 108.94 s\n",
      "2025-05-17 12:03:41.556922: \n",
      "2025-05-17 12:03:41.557085: Epoch 541\n",
      "2025-05-17 12:03:41.557152: Current learning rate: 0.00496\n",
      "2025-05-17 12:05:30.544461: train_loss -0.9639\n",
      "2025-05-17 12:05:30.544609: val_loss -0.9488\n",
      "2025-05-17 12:05:30.544642: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 12:05:30.544676: Epoch time: 108.99 s\n",
      "2025-05-17 12:05:30.544696: Yayy! New best EMA pseudo Dice: 0.9750999808311462\n",
      "2025-05-17 12:05:31.289544: \n",
      "2025-05-17 12:05:31.289871: Epoch 542\n",
      "2025-05-17 12:05:31.289980: Current learning rate: 0.00495\n",
      "2025-05-17 12:07:20.293093: train_loss -0.9647\n",
      "2025-05-17 12:07:20.293283: val_loss -0.9456\n",
      "2025-05-17 12:07:20.293476: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 12:07:20.293611: Epoch time: 109.0 s\n",
      "2025-05-17 12:07:20.813962: \n",
      "2025-05-17 12:07:20.814166: Epoch 543\n",
      "2025-05-17 12:07:20.814266: Current learning rate: 0.00494\n",
      "2025-05-17 12:09:09.752401: train_loss -0.9659\n",
      "2025-05-17 12:09:09.752693: val_loss -0.9504\n",
      "2025-05-17 12:09:09.752729: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 12:09:09.752761: Epoch time: 108.94 s\n",
      "2025-05-17 12:09:10.276641: \n",
      "2025-05-17 12:09:10.276777: Epoch 544\n",
      "2025-05-17 12:09:10.276854: Current learning rate: 0.00493\n",
      "2025-05-17 12:10:59.269381: train_loss -0.9634\n",
      "2025-05-17 12:10:59.269634: val_loss -0.9523\n",
      "2025-05-17 12:10:59.269748: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 12:10:59.269802: Epoch time: 108.99 s\n",
      "2025-05-17 12:10:59.269826: Yayy! New best EMA pseudo Dice: 0.9751999974250793\n",
      "2025-05-17 12:11:00.012034: \n",
      "2025-05-17 12:11:00.012126: Epoch 545\n",
      "2025-05-17 12:11:00.012203: Current learning rate: 0.00492\n",
      "2025-05-17 12:12:48.967100: train_loss -0.9654\n",
      "2025-05-17 12:12:48.967288: val_loss -0.9495\n",
      "2025-05-17 12:12:48.967332: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-17 12:12:48.967372: Epoch time: 108.96 s\n",
      "2025-05-17 12:12:48.967398: Yayy! New best EMA pseudo Dice: 0.9753999710083008\n",
      "2025-05-17 12:12:49.696275: \n",
      "2025-05-17 12:12:49.696440: Epoch 546\n",
      "2025-05-17 12:12:49.696507: Current learning rate: 0.00491\n",
      "2025-05-17 12:14:38.649085: train_loss -0.9646\n",
      "2025-05-17 12:14:38.649289: val_loss -0.9484\n",
      "2025-05-17 12:14:38.649378: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 12:14:38.649433: Epoch time: 108.95 s\n",
      "2025-05-17 12:14:38.649492: Yayy! New best EMA pseudo Dice: 0.9753999710083008\n",
      "2025-05-17 12:14:39.381997: \n",
      "2025-05-17 12:14:39.382145: Epoch 547\n",
      "2025-05-17 12:14:39.382220: Current learning rate: 0.0049\n",
      "2025-05-17 12:16:28.328751: train_loss -0.9657\n",
      "2025-05-17 12:16:28.328958: val_loss -0.9447\n",
      "2025-05-17 12:16:28.329114: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 12:16:28.329234: Epoch time: 108.95 s\n",
      "2025-05-17 12:16:29.053495: \n",
      "2025-05-17 12:16:29.053673: Epoch 548\n",
      "2025-05-17 12:16:29.053742: Current learning rate: 0.00489\n",
      "train_loss -0.96538.032101: \n",
      "2025-05-17 12:18:18.032526: val_loss -0.951\n",
      "2025-05-17 12:18:18.032612: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 12:18:18.032656: Epoch time: 108.98 s\n",
      "2025-05-17 12:18:18.550327: \n",
      "2025-05-17 12:18:18.550428: Epoch 549\n",
      "2025-05-17 12:18:18.550491: Current learning rate: 0.00488\n",
      "2025-05-17 12:20:07.540913: train_loss -0.963\n",
      "2025-05-17 12:20:07.541070: val_loss -0.9429\n",
      "2025-05-17 12:20:07.541180: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-17 12:20:07.541232: Epoch time: 108.99 s\n",
      "2025-05-17 12:20:08.277617: \n",
      "2025-05-17 12:20:08.277797: Epoch 550\n",
      "2025-05-17 12:20:08.277865: Current learning rate: 0.00487\n",
      "2025-05-17 12:21:57.272289: train_loss -0.9649\n",
      "2025-05-17 12:21:57.272643: val_loss -0.9496\n",
      "2025-05-17 12:21:57.272765: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 12:21:57.272829: Epoch time: 109.0 s\n",
      "2025-05-17 12:21:57.793846: \n",
      "2025-05-17 12:21:57.793952: Epoch 551\n",
      "2025-05-17 12:21:57.794038: Current learning rate: 0.00486\n",
      "2025-05-17 12:23:46.762993: train_loss -0.9627\n",
      "2025-05-17 12:23:46.763171: val_loss -0.9422\n",
      "2025-05-17 12:23:46.763205: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 12:23:46.763239: Epoch time: 108.97 s\n",
      "2025-05-17 12:23:47.281221: \n",
      "2025-05-17 12:23:47.281420: Epoch 552\n",
      "2025-05-17 12:23:47.281490: Current learning rate: 0.00485\n",
      "2025-05-17 12:25:36.301152: train_loss -0.9637\n",
      "2025-05-17 12:25:36.301341: val_loss -0.9447\n",
      "2025-05-17 12:25:36.301374: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 12:25:36.301407: Epoch time: 109.02 s\n",
      "2025-05-17 12:25:36.825248: \n",
      "2025-05-17 12:25:36.825588: Epoch 553\n",
      "2025-05-17 12:25:36.825692: Current learning rate: 0.00484\n",
      "2025-05-17 12:27:25.802806: train_loss -0.9647\n",
      "2025-05-17 12:27:25.802933: val_loss -0.9452\n",
      "2025-05-17 12:27:25.802967: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 12:27:25.803000: Epoch time: 108.98 s\n",
      "2025-05-17 12:27:26.320426: \n",
      "2025-05-17 12:27:26.320728: Epoch 554\n",
      "2025-05-17 12:27:26.320832: Current learning rate: 0.00484\n",
      "2025-05-17 12:29:15.301262: train_loss -0.9653\n",
      "2025-05-17 12:29:15.301404: val_loss -0.951\n",
      "2025-05-17 12:29:15.301436: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-17 12:29:15.301469: Epoch time: 108.98 s\n",
      "2025-05-17 12:29:15.815078: \n",
      "2025-05-17 12:29:15.815386: Epoch 555\n",
      "2025-05-17 12:29:15.815458: Current learning rate: 0.00483\n",
      "2025-05-17 12:31:04.855606: train_loss -0.9659\n",
      "2025-05-17 12:31:04.855793: val_loss -0.9509\n",
      "2025-05-17 12:31:04.855826: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 12:31:04.855860: Epoch time: 109.04 s\n",
      "2025-05-17 12:31:05.378023: \n",
      "2025-05-17 12:31:05.378376: Epoch 556\n",
      "2025-05-17 12:31:05.378478: Current learning rate: 0.00482\n",
      "2025-05-17 12:32:54.288611: train_loss -0.9634\n",
      "2025-05-17 12:32:54.288822: val_loss -0.9447\n",
      "2025-05-17 12:32:54.288972: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 12:32:54.289072: Epoch time: 108.91 s\n",
      "2025-05-17 12:32:54.813851: \n",
      "2025-05-17 12:32:54.814126: Epoch 557\n",
      "2025-05-17 12:32:54.814295: Current learning rate: 0.00481\n",
      "2025-05-17 12:34:43.816261: train_loss -0.9662\n",
      "2025-05-17 12:34:43.816390: val_loss -0.9497\n",
      "2025-05-17 12:34:43.816423: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-17 12:34:43.816454: Epoch time: 109.0 s\n",
      "2025-05-17 12:34:44.334781: \n",
      "2025-05-17 12:34:44.334921: Epoch 558\n",
      "2025-05-17 12:34:44.334987: Current learning rate: 0.0048\n",
      "2025-05-17 12:36:33.306751: train_loss -0.9659\n",
      "2025-05-17 12:36:33.306894: val_loss -0.9477\n",
      "2025-05-17 12:36:33.306927: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 12:36:33.306960: Epoch time: 108.97 s\n",
      "2025-05-17 12:36:33.823783: \n",
      "2025-05-17 12:36:33.823869: Epoch 559\n",
      "2025-05-17 12:36:33.823933: Current learning rate: 0.00479\n",
      "2025-05-17 12:38:22.778446: train_loss -0.9651\n",
      "2025-05-17 12:38:22.778563: val_loss -0.9488\n",
      "2025-05-17 12:38:22.778595: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 12:38:22.778627: Epoch time: 108.96 s\n",
      "2025-05-17 12:38:23.483001: \n",
      "2025-05-17 12:38:23.483186: Epoch 560\n",
      "2025-05-17 12:38:23.483268: Current learning rate: 0.00478\n",
      "2025-05-17 12:40:12.508182: train_loss -0.9645\n",
      "2025-05-17 12:40:12.508311: val_loss -0.9475\n",
      "2025-05-17 12:40:12.508347: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 12:40:12.508379: Epoch time: 109.03 s\n",
      "2025-05-17 12:40:13.022158: \n",
      "2025-05-17 12:40:13.022325: Epoch 561\n",
      "2025-05-17 12:40:13.022404: Current learning rate: 0.00477\n",
      "2025-05-17 12:42:02.031812: train_loss -0.9658\n",
      "2025-05-17 12:42:02.031931: val_loss -0.9448\n",
      "2025-05-17 12:42:02.031962: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 12:42:02.031996: Epoch time: 109.01 s\n",
      "2025-05-17 12:42:02.543662: \n",
      "2025-05-17 12:42:02.543760: Epoch 562\n",
      "2025-05-17 12:42:02.543833: Current learning rate: 0.00476\n",
      "2025-05-17 12:43:51.605711: train_loss -0.9671\n",
      "2025-05-17 12:43:51.605885: val_loss -0.9434\n",
      "2025-05-17 12:43:51.605920: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-17 12:43:51.605963: Epoch time: 109.06 s\n",
      "2025-05-17 12:43:52.128994: \n",
      "2025-05-17 12:43:52.129165: Epoch 563\n",
      "2025-05-17 12:43:52.129275: Current learning rate: 0.00475\n",
      "2025-05-17 12:45:41.070887: train_loss -0.9631\n",
      "2025-05-17 12:45:41.071036: val_loss -0.9468\n",
      "2025-05-17 12:45:41.071206: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 12:45:41.071332: Epoch time: 108.94 s\n",
      "2025-05-17 12:45:41.584735: \n",
      "2025-05-17 12:45:41.584867: Epoch 564\n",
      "2025-05-17 12:45:41.584934: Current learning rate: 0.00474\n",
      "2025-05-17 12:47:30.608557: train_loss -0.9646\n",
      "2025-05-17 12:47:30.608729: val_loss -0.9494\n",
      "2025-05-17 12:47:30.608761: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 12:47:30.608829: Epoch time: 109.02 s\n",
      "2025-05-17 12:47:31.125344: \n",
      "2025-05-17 12:47:31.125531: Epoch 565\n",
      "2025-05-17 12:47:31.125608: Current learning rate: 0.00473\n",
      "2025-05-17 12:49:20.137869: train_loss -0.9654\n",
      "2025-05-17 12:49:20.137995: val_loss -0.9406\n",
      "2025-05-17 12:49:20.138029: Pseudo dice [np.float32(0.9707)]\n",
      "2025-05-17 12:49:20.138063: Epoch time: 109.01 s\n",
      "2025-05-17 12:49:20.654287: \n",
      "2025-05-17 12:49:20.654374: Epoch 566\n",
      "2025-05-17 12:49:20.654436: Current learning rate: 0.00472\n",
      "2025-05-17 12:51:09.568384: train_loss -0.9572\n",
      "2025-05-17 12:51:09.568563: val_loss -0.9436\n",
      "2025-05-17 12:51:09.568596: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-17 12:51:09.568629: Epoch time: 108.91 s\n",
      "2025-05-17 12:51:10.089699: \n",
      "2025-05-17 12:51:10.089821: Epoch 567\n",
      "2025-05-17 12:51:10.089891: Current learning rate: 0.00471\n",
      "2025-05-17 12:52:59.078197: train_loss -0.9569\n",
      "2025-05-17 12:52:59.078468: val_loss -0.943\n",
      "2025-05-17 12:52:59.078516: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 12:52:59.078552: Epoch time: 108.99 s\n",
      "2025-05-17 12:52:59.596349: \n",
      "2025-05-17 12:52:59.596452: Epoch 568\n",
      "2025-05-17 12:52:59.596571: Current learning rate: 0.0047\n",
      "2025-05-17 12:54:48.555196: train_loss -0.9586\n",
      "2025-05-17 12:54:48.555530: val_loss -0.9422\n",
      "2025-05-17 12:54:48.555632: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-17 12:54:48.555678: Epoch time: 108.96 s\n",
      "2025-05-17 12:54:49.077890: \n",
      "2025-05-17 12:54:49.077973: Epoch 569\n",
      "2025-05-17 12:54:49.078038: Current learning rate: 0.00469\n",
      "2025-05-17 12:56:38.017914: train_loss -0.9596\n",
      "2025-05-17 12:56:38.018046: val_loss -0.9479\n",
      "2025-05-17 12:56:38.018081: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-17 12:56:38.018115: Epoch time: 108.94 s\n",
      "2025-05-17 12:56:38.533244: \n",
      "2025-05-17 12:56:38.533354: Epoch 570\n",
      "2025-05-17 12:56:38.533457: Current learning rate: 0.00468\n",
      "2025-05-17 12:58:27.437373: train_loss -0.9637\n",
      "2025-05-17 12:58:27.437594: val_loss -0.9447\n",
      "2025-05-17 12:58:27.437660: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 12:58:27.437701: Epoch time: 108.9 s\n",
      "2025-05-17 12:58:27.963266: \n",
      "2025-05-17 12:58:27.963565: Epoch 571\n",
      "2025-05-17 12:58:27.963645: Current learning rate: 0.00467\n",
      "2025-05-17 13:00:16.925285: train_loss -0.9599\n",
      "2025-05-17 13:00:16.925412: val_loss -0.9502\n",
      "2025-05-17 13:00:16.925505: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 13:00:16.925570: Epoch time: 108.96 s\n",
      "2025-05-17 13:00:17.450245: \n",
      "2025-05-17 13:00:17.450454: Epoch 572\n",
      "2025-05-17 13:00:17.450736: Current learning rate: 0.00466\n",
      "2025-05-17 13:02:06.388804: train_loss -0.9641\n",
      "2025-05-17 13:02:06.388938: val_loss -0.947\n",
      "2025-05-17 13:02:06.389455: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 13:02:06.389743: Epoch time: 108.94 s\n",
      "2025-05-17 13:02:07.118849: \n",
      "2025-05-17 13:02:07.119015: Epoch 573\n",
      "2025-05-17 13:02:07.119153: Current learning rate: 0.00465\n",
      "train_loss -0.96456.079555: \n",
      "2025-05-17 13:03:56.079839: val_loss -0.9504\n",
      "2025-05-17 13:03:56.080042: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 13:03:56.080134: Epoch time: 108.96 s\n",
      "2025-05-17 13:03:56.603045: \n",
      "2025-05-17 13:03:56.603188: Epoch 574\n",
      "2025-05-17 13:03:56.603253: Current learning rate: 0.00464\n",
      "2025-05-17 13:05:45.593454: train_loss -0.963\n",
      "2025-05-17 13:05:45.593582: val_loss -0.9483\n",
      "2025-05-17 13:05:45.593615: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 13:05:45.593649: Epoch time: 108.99 s\n",
      "2025-05-17 13:05:46.120974: \n",
      "2025-05-17 13:05:46.121088: Epoch 575\n",
      "2025-05-17 13:05:46.121221: Current learning rate: 0.00463\n",
      "2025-05-17 13:07:35.114273: train_loss -0.9608\n",
      "2025-05-17 13:07:35.114477: val_loss -0.9456\n",
      "2025-05-17 13:07:35.114582: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 13:07:35.114671: Epoch time: 108.99 s\n",
      "2025-05-17 13:07:35.640952: \n",
      "2025-05-17 13:07:35.641061: Epoch 576\n",
      "2025-05-17 13:07:35.641125: Current learning rate: 0.00462\n",
      "2025-05-17 13:09:24.556037: train_loss -0.9625\n",
      "2025-05-17 13:09:24.556235: val_loss -0.9509\n",
      "2025-05-17 13:09:24.556272: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-17 13:09:24.556313: Epoch time: 108.92 s\n",
      "2025-05-17 13:09:25.088943: \n",
      "2025-05-17 13:09:25.089407: Epoch 577\n",
      "2025-05-17 13:09:25.089483: Current learning rate: 0.00461\n",
      "2025-05-17 13:11:14.062968: train_loss -0.9629\n",
      "2025-05-17 13:11:14.063095: val_loss -0.9513\n",
      "2025-05-17 13:11:14.063129: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 13:11:14.063163: Epoch time: 108.97 s\n",
      "2025-05-17 13:11:14.587738: \n",
      "2025-05-17 13:11:14.587883: Epoch 578\n",
      "2025-05-17 13:11:14.587977: Current learning rate: 0.0046\n",
      "2025-05-17 13:13:03.651219: train_loss -0.9641\n",
      "2025-05-17 13:13:03.651454: val_loss -0.9551\n",
      "2025-05-17 13:13:03.651575: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-17 13:13:03.651627: Epoch time: 109.06 s\n",
      "2025-05-17 13:13:03.651656: Yayy! New best EMA pseudo Dice: 0.975600004196167\n",
      "2025-05-17 13:13:04.403806: \n",
      "2025-05-17 13:13:04.403903: Epoch 579\n",
      "2025-05-17 13:13:04.403969: Current learning rate: 0.00459\n",
      "2025-05-17 13:14:53.326976: train_loss -0.9622\n",
      "2025-05-17 13:14:53.327101: val_loss -0.949\n",
      "2025-05-17 13:14:53.327134: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 13:14:53.327167: Epoch time: 108.92 s\n",
      "2025-05-17 13:14:53.327189: Yayy! New best EMA pseudo Dice: 0.9757000207901001\n",
      "2025-05-17 13:14:54.089138: \n",
      "2025-05-17 13:14:54.089286: Epoch 580\n",
      "2025-05-17 13:14:54.089361: Current learning rate: 0.00458\n",
      "2025-05-17 13:16:43.092997: train_loss -0.9464\n",
      "2025-05-17 13:16:43.093166: val_loss -0.7747\n",
      "2025-05-17 13:16:43.093201: Pseudo dice [np.float32(0.9085)]\n",
      "2025-05-17 13:16:43.093232: Epoch time: 109.0 s\n",
      "2025-05-17 13:16:43.615328: \n",
      "2025-05-17 13:16:43.615483: Epoch 581\n",
      "2025-05-17 13:16:43.615548: Current learning rate: 0.00457\n",
      "2025-05-17 13:18:32.534993: train_loss -0.8855\n",
      "2025-05-17 13:18:32.535154: val_loss -0.8762\n",
      "2025-05-17 13:18:32.535259: Pseudo dice [np.float32(0.9478)]\n",
      "2025-05-17 13:18:32.535312: Epoch time: 108.92 s\n",
      "2025-05-17 13:18:33.068580: \n",
      "2025-05-17 13:18:33.068731: Epoch 582\n",
      "2025-05-17 13:18:33.069034: Current learning rate: 0.00456\n",
      "2025-05-17 13:20:22.023250: train_loss -0.9075\n",
      "2025-05-17 13:20:22.023412: val_loss -0.9072\n",
      "2025-05-17 13:20:22.023448: Pseudo dice [np.float32(0.9626)]\n",
      "2025-05-17 13:20:22.023481: Epoch time: 108.96 s\n",
      "2025-05-17 13:20:22.544088: \n",
      "2025-05-17 13:20:22.544247: Epoch 583\n",
      "2025-05-17 13:20:22.544316: Current learning rate: 0.00455\n",
      "2025-05-17 13:22:11.420278: train_loss -0.9249\n",
      "2025-05-17 13:22:11.420412: val_loss -0.9319\n",
      "2025-05-17 13:22:11.420448: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-17 13:22:11.420482: Epoch time: 108.88 s\n",
      "2025-05-17 13:22:11.947053: \n",
      "2025-05-17 13:22:11.947209: Epoch 584\n",
      "2025-05-17 13:22:11.947290: Current learning rate: 0.00454\n",
      "2025-05-17 13:24:00.812064: train_loss -0.9453\n",
      "2025-05-17 13:24:00.812194: val_loss -0.9341\n",
      "2025-05-17 13:24:00.812321: Pseudo dice [np.float32(0.9693)]\n",
      "2025-05-17 13:24:00.812369: Epoch time: 108.87 s\n",
      "2025-05-17 13:24:01.539675: \n",
      "2025-05-17 13:24:01.539811: Epoch 585\n",
      "2025-05-17 13:24:01.539881: Current learning rate: 0.00453\n",
      "2025-05-17 13:25:50.516362: train_loss -0.9314\n",
      "2025-05-17 13:25:50.516499: val_loss -0.9073\n",
      "2025-05-17 13:25:50.516536: Pseudo dice [np.float32(0.9597)]\n",
      "2025-05-17 13:25:50.516569: Epoch time: 108.98 s\n",
      "2025-05-17 13:25:51.044922: \n",
      "2025-05-17 13:25:51.045263: Epoch 586\n",
      "2025-05-17 13:25:51.045412: Current learning rate: 0.00452\n",
      "2025-05-17 13:27:39.924238: train_loss -0.9029\n",
      "2025-05-17 13:27:39.924372: val_loss -0.9256\n",
      "2025-05-17 13:27:39.924408: Pseudo dice [np.float32(0.9681)]\n",
      "2025-05-17 13:27:39.924443: Epoch time: 108.88 s\n",
      "2025-05-17 13:27:40.452738: \n",
      "2025-05-17 13:27:40.452952: Epoch 587\n",
      "2025-05-17 13:27:40.453109: Current learning rate: 0.00451\n",
      "2025-05-17 13:29:29.394622: train_loss -0.9381\n",
      "2025-05-17 13:29:29.394764: val_loss -0.9297\n",
      "2025-05-17 13:29:29.394800: Pseudo dice [np.float32(0.9682)]\n",
      "2025-05-17 13:29:29.394835: Epoch time: 108.94 s\n",
      "2025-05-17 13:29:29.918332: \n",
      "2025-05-17 13:29:29.918492: Epoch 588\n",
      "2025-05-17 13:29:29.918564: Current learning rate: 0.0045\n",
      "2025-05-17 13:31:18.796009: train_loss -0.948\n",
      "2025-05-17 13:31:18.796152: val_loss -0.9326\n",
      "2025-05-17 13:31:18.796189: Pseudo dice [np.float32(0.9695)]\n",
      "2025-05-17 13:31:18.796223: Epoch time: 108.88 s\n",
      "2025-05-17 13:31:19.321826: \n",
      "2025-05-17 13:31:19.322118: Epoch 589\n",
      "2025-05-17 13:31:19.322261: Current learning rate: 0.00449\n",
      "2025-05-17 13:33:08.230030: train_loss -0.9115\n",
      "2025-05-17 13:33:08.230234: val_loss -0.9197\n",
      "2025-05-17 13:33:08.230279: Pseudo dice [np.float32(0.965)]\n",
      "2025-05-17 13:33:08.230315: Epoch time: 108.91 s\n",
      "2025-05-17 13:33:08.754594: \n",
      "2025-05-17 13:33:08.754853: Epoch 590\n",
      "2025-05-17 13:33:08.755009: Current learning rate: 0.00448\n",
      "2025-05-17 13:34:57.669466: train_loss -0.9297\n",
      "2025-05-17 13:34:57.669625: val_loss -0.9192\n",
      "2025-05-17 13:34:57.669806: Pseudo dice [np.float32(0.968)]\n",
      "2025-05-17 13:34:57.669915: Epoch time: 108.92 s\n",
      "2025-05-17 13:34:58.200033: \n",
      "2025-05-17 13:34:58.200122: Epoch 591\n",
      "2025-05-17 13:34:58.200194: Current learning rate: 0.00447\n",
      "2025-05-17 13:36:47.188302: train_loss -0.9311\n",
      "2025-05-17 13:36:47.188434: val_loss -0.9231\n",
      "2025-05-17 13:36:47.188466: Pseudo dice [np.float32(0.9658)]\n",
      "2025-05-17 13:36:47.188500: Epoch time: 108.99 s\n",
      "2025-05-17 13:36:47.715886: \n",
      "2025-05-17 13:36:47.716069: Epoch 592\n",
      "2025-05-17 13:36:47.716151: Current learning rate: 0.00446\n",
      "2025-05-17 13:38:36.610967: train_loss -0.9432\n",
      "2025-05-17 13:38:36.611080: val_loss -0.94\n",
      "2025-05-17 13:38:36.611113: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-17 13:38:36.611145: Epoch time: 108.9 s\n",
      "2025-05-17 13:38:37.138128: \n",
      "2025-05-17 13:38:37.138400: Epoch 593\n",
      "2025-05-17 13:38:37.138479: Current learning rate: 0.00445\n",
      "2025-05-17 13:40:26.002614: train_loss -0.9463\n",
      "2025-05-17 13:40:26.002757: val_loss -0.9337\n",
      "2025-05-17 13:40:26.002797: Pseudo dice [np.float32(0.9685)]\n",
      "2025-05-17 13:40:26.002913: Epoch time: 108.87 s\n",
      "2025-05-17 13:40:26.526118: \n",
      "2025-05-17 13:40:26.526253: Epoch 594\n",
      "2025-05-17 13:40:26.526320: Current learning rate: 0.00444\n",
      "2025-05-17 13:42:15.458123: train_loss -0.9521\n",
      "2025-05-17 13:42:15.458344: val_loss -0.9437\n",
      "2025-05-17 13:42:15.458444: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 13:42:15.458523: Epoch time: 108.93 s\n",
      "2025-05-17 13:42:15.979960: \n",
      "2025-05-17 13:42:15.980125: Epoch 595\n",
      "2025-05-17 13:42:15.980211: Current learning rate: 0.00443\n",
      "2025-05-17 13:44:04.930282: train_loss -0.9526\n",
      "2025-05-17 13:44:04.930417: val_loss -0.9418\n",
      "2025-05-17 13:44:04.930453: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 13:44:04.930488: Epoch time: 108.95 s\n",
      "2025-05-17 13:44:05.455647: \n",
      "2025-05-17 13:44:05.455894: Epoch 596\n",
      "2025-05-17 13:44:05.456198: Current learning rate: 0.00442\n",
      "2025-05-17 13:45:54.339811: train_loss -0.9541\n",
      "2025-05-17 13:45:54.339935: val_loss -0.9353\n",
      "2025-05-17 13:45:54.339969: Pseudo dice [np.float32(0.9687)]\n",
      "2025-05-17 13:45:54.340003: Epoch time: 108.88 s\n",
      "2025-05-17 13:45:54.860351: \n",
      "2025-05-17 13:45:54.860450: Epoch 597\n",
      "2025-05-17 13:45:54.860521: Current learning rate: 0.00441\n",
      "2025-05-17 13:47:43.679082: train_loss -0.954\n",
      "2025-05-17 13:47:43.679199: val_loss -0.9405\n",
      "2025-05-17 13:47:43.679234: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-17 13:47:43.679265: Epoch time: 108.82 s\n",
      "2025-05-17 13:47:44.401342: \n",
      "2025-05-17 13:47:44.401741: Epoch 598\n",
      "2025-05-17 13:47:44.401835: Current learning rate: 0.0044\n",
      "2025-05-17 13:49:33.411407: train_loss -0.9536\n",
      "2025-05-17 13:49:33.411539: val_loss -0.9386\n",
      "2025-05-17 13:49:33.411579: Pseudo dice [np.float32(0.9709)]\n",
      "2025-05-17 13:49:33.411622: Epoch time: 109.01 s\n",
      "2025-05-17 13:49:33.941613: \n",
      "2025-05-17 13:49:33.941917: Epoch 599\n",
      "2025-05-17 13:49:33.941995: Current learning rate: 0.00439\n",
      "2025-05-17 13:51:22.813300: train_loss -0.9526\n",
      "2025-05-17 13:51:22.813609: val_loss -0.9442\n",
      "2025-05-17 13:51:22.813663: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-17 13:51:22.813704: Epoch time: 108.87 s\n",
      "2025-05-17 13:51:23.555989: \n",
      "2025-05-17 13:51:23.556149: Epoch 600\n",
      "2025-05-17 13:51:23.556237: Current learning rate: 0.00438\n",
      "2025-05-17 13:53:12.510017: train_loss -0.9552\n",
      "2025-05-17 13:53:12.510265: val_loss -0.9395\n",
      "2025-05-17 13:53:12.510339: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-17 13:53:12.510388: Epoch time: 108.95 s\n",
      "2025-05-17 13:53:13.031302: \n",
      "2025-05-17 13:53:13.031608: Epoch 601\n",
      "2025-05-17 13:53:13.031691: Current learning rate: 0.00437\n",
      "2025-05-17 13:55:01.970727: train_loss -0.949\n",
      "2025-05-17 13:55:01.970907: val_loss -0.9342\n",
      "2025-05-17 13:55:01.971069: Pseudo dice [np.float32(0.9681)]\n",
      "2025-05-17 13:55:01.971142: Epoch time: 108.94 s\n",
      "2025-05-17 13:55:02.502786: \n",
      "2025-05-17 13:55:02.503111: Epoch 602\n",
      "2025-05-17 13:55:02.503331: Current learning rate: 0.00436\n",
      "2025-05-17 13:56:51.386696: train_loss -0.9508\n",
      "2025-05-17 13:56:51.386838: val_loss -0.9425\n",
      "2025-05-17 13:56:51.386872: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 13:56:51.386906: Epoch time: 108.88 s\n",
      "2025-05-17 13:56:51.920428: \n",
      "2025-05-17 13:56:51.920639: Epoch 603\n",
      "2025-05-17 13:56:51.920745: Current learning rate: 0.00435\n",
      "2025-05-17 13:58:40.850421: train_loss -0.9529\n",
      "2025-05-17 13:58:40.850556: val_loss -0.9443\n",
      "2025-05-17 13:58:40.850592: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-17 13:58:40.850703: Epoch time: 108.93 s\n",
      "2025-05-17 13:58:41.379926: \n",
      "2025-05-17 13:58:41.380081: Epoch 604\n",
      "2025-05-17 13:58:41.380147: Current learning rate: 0.00434\n",
      "2025-05-17 14:00:30.343392: train_loss -0.9541\n",
      "2025-05-17 14:00:30.343631: val_loss -0.9441\n",
      "2025-05-17 14:00:30.343731: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 14:00:30.343806: Epoch time: 108.96 s\n",
      "2025-05-17 14:00:30.868166: \n",
      "2025-05-17 14:00:30.868793: Epoch 605\n",
      "2025-05-17 14:00:30.868871: Current learning rate: 0.00433\n",
      "2025-05-17 14:02:19.860245: train_loss -0.9532\n",
      "2025-05-17 14:02:19.860374: val_loss -0.9417\n",
      "2025-05-17 14:02:19.860409: Pseudo dice [np.float32(0.9714)]\n",
      "2025-05-17 14:02:19.860442: Epoch time: 108.99 s\n",
      "2025-05-17 14:02:20.392277: \n",
      "2025-05-17 14:02:20.392359: Epoch 606\n",
      "2025-05-17 14:02:20.392422: Current learning rate: 0.00432\n",
      "2025-05-17 14:04:09.320932: train_loss -0.9581\n",
      "2025-05-17 14:04:09.321100: val_loss -0.9487\n",
      "2025-05-17 14:04:09.321149: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 14:04:09.321184: Epoch time: 108.93 s\n",
      "2025-05-17 14:04:09.851959: \n",
      "2025-05-17 14:04:09.852055: Epoch 607\n",
      "2025-05-17 14:04:09.852119: Current learning rate: 0.00431\n",
      "2025-05-17 14:05:58.783415: train_loss -0.9547\n",
      "2025-05-17 14:05:58.783542: val_loss -0.9451\n",
      "2025-05-17 14:05:58.783576: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 14:05:58.783609: Epoch time: 108.93 s\n",
      "2025-05-17 14:05:59.303451: \n",
      "2025-05-17 14:05:59.303560: Epoch 608\n",
      "2025-05-17 14:05:59.303725: Current learning rate: 0.0043\n",
      "2025-05-17 14:07:48.258500: train_loss -0.9593\n",
      "2025-05-17 14:07:48.258628: val_loss -0.9477\n",
      "2025-05-17 14:07:48.258662: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-17 14:07:48.258696: Epoch time: 108.96 s\n",
      "2025-05-17 14:07:48.785515: \n",
      "2025-05-17 14:07:48.785600: Epoch 609\n",
      "2025-05-17 14:07:48.785665: Current learning rate: 0.00429\n",
      "2025-05-17 14:09:37.691328: train_loss -0.9547\n",
      "2025-05-17 14:09:37.691527: val_loss -0.948\n",
      "2025-05-17 14:09:37.691563: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-17 14:09:37.691596: Epoch time: 108.91 s\n",
      "2025-05-17 14:09:38.419862: \n",
      "2025-05-17 14:09:38.420004: Epoch 610\n",
      "2025-05-17 14:09:38.420082: Current learning rate: 0.00429\n",
      "2025-05-17 14:11:27.398034: train_loss -0.9606\n",
      "2025-05-17 14:11:27.398170: val_loss -0.9476\n",
      "2025-05-17 14:11:27.398204: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-17 14:11:27.398296: Epoch time: 108.98 s\n",
      "2025-05-17 14:11:27.956118: \n",
      "2025-05-17 14:11:27.956224: Epoch 611\n",
      "2025-05-17 14:11:27.956290: Current learning rate: 0.00428\n",
      "2025-05-17 14:13:16.982381: train_loss -0.9541\n",
      "2025-05-17 14:13:16.982508: val_loss -0.932\n",
      "2025-05-17 14:13:16.982542: Pseudo dice [np.float32(0.9692)]\n",
      "2025-05-17 14:13:16.982618: Epoch time: 109.03 s\n",
      "2025-05-17 14:13:17.512153: \n",
      "2025-05-17 14:13:17.512277: Epoch 612\n",
      "2025-05-17 14:13:17.512343: Current learning rate: 0.00427\n",
      "2025-05-17 14:15:06.415105: train_loss -0.9529\n",
      "2025-05-17 14:15:06.415242: val_loss -0.9425\n",
      "2025-05-17 14:15:06.415293: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 14:15:06.415330: Epoch time: 108.9 s\n",
      "2025-05-17 14:15:06.954152: \n",
      "2025-05-17 14:15:06.954526: Epoch 613\n",
      "2025-05-17 14:15:06.954625: Current learning rate: 0.00426\n",
      "2025-05-17 14:16:55.901873: train_loss -0.955\n",
      "2025-05-17 14:16:55.901993: val_loss -0.9429\n",
      "2025-05-17 14:16:55.902132: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-17 14:16:55.902238: Epoch time: 108.95 s\n",
      "2025-05-17 14:16:56.427694: \n",
      "2025-05-17 14:16:56.427784: Epoch 614\n",
      "2025-05-17 14:16:56.427845: Current learning rate: 0.00425\n",
      "2025-05-17 14:18:45.397959: train_loss -0.96\n",
      "2025-05-17 14:18:45.398081: val_loss -0.9462\n",
      "2025-05-17 14:18:45.398115: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 14:18:45.398148: Epoch time: 108.97 s\n",
      "2025-05-17 14:18:45.928824: \n",
      "2025-05-17 14:18:45.929198: Epoch 615\n",
      "2025-05-17 14:18:45.929316: Current learning rate: 0.00424\n",
      "2025-05-17 14:20:34.834611: train_loss -0.9571\n",
      "2025-05-17 14:20:34.834740: val_loss -0.9463\n",
      "2025-05-17 14:20:34.834788: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 14:20:34.834907: Epoch time: 108.91 s\n",
      "2025-05-17 14:20:35.364818: \n",
      "2025-05-17 14:20:35.364985: Epoch 616\n",
      "2025-05-17 14:20:35.365080: Current learning rate: 0.00423\n",
      "2025-05-17 14:22:24.328676: train_loss -0.96\n",
      "2025-05-17 14:22:24.328835: val_loss -0.9509\n",
      "2025-05-17 14:22:24.328869: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 14:22:24.328901: Epoch time: 108.96 s\n",
      "2025-05-17 14:22:24.861440: \n",
      "2025-05-17 14:22:24.861535: Epoch 617\n",
      "2025-05-17 14:22:24.861598: Current learning rate: 0.00422\n",
      "2025-05-17 14:24:13.787942: train_loss -0.9609\n",
      "2025-05-17 14:24:13.788263: val_loss -0.9446\n",
      "2025-05-17 14:24:13.788460: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 14:24:13.788541: Epoch time: 108.93 s\n",
      "2025-05-17 14:24:14.311515: \n",
      "2025-05-17 14:24:14.311611: Epoch 618\n",
      "2025-05-17 14:24:14.311685: Current learning rate: 0.00421\n",
      "2025-05-17 14:26:03.358121: train_loss -0.9572\n",
      "2025-05-17 14:26:03.358246: val_loss -0.9452\n",
      "2025-05-17 14:26:03.358301: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 14:26:03.358344: Epoch time: 109.05 s\n",
      "2025-05-17 14:26:03.896969: \n",
      "2025-05-17 14:26:03.897054: Epoch 619\n",
      "2025-05-17 14:26:03.897120: Current learning rate: 0.0042\n",
      "2025-05-17 14:27:52.841334: train_loss -0.9584\n",
      "2025-05-17 14:27:52.841475: val_loss -0.944\n",
      "2025-05-17 14:27:52.841510: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 14:27:52.841545: Epoch time: 108.94 s\n",
      "2025-05-17 14:27:53.371099: \n",
      "2025-05-17 14:27:53.371190: Epoch 620\n",
      "2025-05-17 14:27:53.371254: Current learning rate: 0.00419\n",
      "2025-05-17 14:29:42.328763: train_loss -0.9606\n",
      "2025-05-17 14:29:42.328890: val_loss -0.9478\n",
      "2025-05-17 14:29:42.328924: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-17 14:29:42.328956: Epoch time: 108.96 s\n",
      "2025-05-17 14:29:42.858528: \n",
      "2025-05-17 14:29:42.858626: Epoch 621\n",
      "2025-05-17 14:29:42.858707: Current learning rate: 0.00418\n",
      "2025-05-17 14:31:31.832290: train_loss -0.9626\n",
      "2025-05-17 14:31:31.832612: val_loss -0.9457\n",
      "2025-05-17 14:31:31.832659: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 14:31:31.832696: Epoch time: 108.97 s\n",
      "2025-05-17 14:31:32.363032: \n",
      "2025-05-17 14:31:32.363156: Epoch 622\n",
      "2025-05-17 14:31:32.363224: Current learning rate: 0.00417\n",
      "2025-05-17 14:33:21.297889: train_loss -0.961\n",
      "2025-05-17 14:33:21.298086: val_loss -0.9502\n",
      "2025-05-17 14:33:21.298149: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 14:33:21.298187: Epoch time: 108.94 s\n",
      "2025-05-17 14:33:22.021316: \n",
      "2025-05-17 14:33:22.021485: Epoch 623\n",
      "2025-05-17 14:33:22.021585: Current learning rate: 0.00416\n",
      "2025-05-17 14:35:11.009086: train_loss -0.9609\n",
      "2025-05-17 14:35:11.009216: val_loss -0.9465\n",
      "2025-05-17 14:35:11.009250: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 14:35:11.009285: Epoch time: 108.99 s\n",
      "2025-05-17 14:35:11.540881: \n",
      "2025-05-17 14:35:11.541019: Epoch 624\n",
      "2025-05-17 14:35:11.541096: Current learning rate: 0.00415\n",
      "2025-05-17 14:37:00.529479: train_loss -0.9607\n",
      "2025-05-17 14:37:00.529605: val_loss -0.949\n",
      "2025-05-17 14:37:00.529639: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 14:37:00.529671: Epoch time: 108.99 s\n",
      "2025-05-17 14:37:01.057811: \n",
      "2025-05-17 14:37:01.057955: Epoch 625\n",
      "2025-05-17 14:37:01.058067: Current learning rate: 0.00414\n",
      "2025-05-17 14:38:50.029226: train_loss -0.9632\n",
      "2025-05-17 14:38:50.029385: val_loss -0.9491\n",
      "2025-05-17 14:38:50.029453: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 14:38:50.029530: Epoch time: 108.97 s\n",
      "2025-05-17 14:38:50.555620: \n",
      "2025-05-17 14:38:50.555717: Epoch 626\n",
      "2025-05-17 14:38:50.555782: Current learning rate: 0.00413\n",
      "2025-05-17 14:40:39.530025: train_loss -0.9625\n",
      "2025-05-17 14:40:39.530150: val_loss -0.9518\n",
      "2025-05-17 14:40:39.530184: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 14:40:39.530219: Epoch time: 108.97 s\n",
      "2025-05-17 14:40:40.063658: \n",
      "2025-05-17 14:40:40.063997: Epoch 627\n",
      "2025-05-17 14:40:40.064094: Current learning rate: 0.00412\n",
      "2025-05-17 14:42:29.037458: train_loss -0.9632\n",
      "2025-05-17 14:42:29.037607: val_loss -0.9464\n",
      "2025-05-17 14:42:29.037644: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 14:42:29.037678: Epoch time: 108.97 s\n",
      "2025-05-17 14:42:29.561364: \n",
      "2025-05-17 14:42:29.561495: Epoch 628\n",
      "2025-05-17 14:42:29.561690: Current learning rate: 0.00411\n",
      "2025-05-17 14:44:18.534825: train_loss -0.9606\n",
      "2025-05-17 14:44:18.535042: val_loss -0.9474\n",
      "2025-05-17 14:44:18.535110: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 14:44:18.535259: Epoch time: 108.97 s\n",
      "2025-05-17 14:44:19.066392: \n",
      "2025-05-17 14:44:19.066623: Epoch 629\n",
      "2025-05-17 14:44:19.066761: Current learning rate: 0.0041\n",
      "2025-05-17 14:46:08.039413: train_loss -0.9632\n",
      "2025-05-17 14:46:08.039549: val_loss -0.9486\n",
      "2025-05-17 14:46:08.039583: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-17 14:46:08.039617: Epoch time: 108.97 s\n",
      "2025-05-17 14:46:08.563332: \n",
      "2025-05-17 14:46:08.563471: Epoch 630\n",
      "2025-05-17 14:46:08.563545: Current learning rate: 0.00409\n",
      "2025-05-17 14:47:57.568235: train_loss -0.9637\n",
      "2025-05-17 14:47:57.568477: val_loss -0.9491\n",
      "2025-05-17 14:47:57.568675: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 14:47:57.568782: Epoch time: 109.01 s\n",
      "2025-05-17 14:47:58.107080: \n",
      "2025-05-17 14:47:58.107166: Epoch 631\n",
      "2025-05-17 14:47:58.107240: Current learning rate: 0.00408\n",
      "2025-05-17 14:49:47.138493: train_loss -0.9644\n",
      "2025-05-17 14:49:47.138695: val_loss -0.9472\n",
      "2025-05-17 14:49:47.138803: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-17 14:49:47.138946: Epoch time: 109.03 s\n",
      "2025-05-17 14:49:47.675346: \n",
      "2025-05-17 14:49:47.675440: Epoch 632\n",
      "2025-05-17 14:49:47.675508: Current learning rate: 0.00407\n",
      "2025-05-17 14:51:36.598237: train_loss -0.9623\n",
      "2025-05-17 14:51:36.598474: val_loss -0.9503\n",
      "2025-05-17 14:51:36.598538: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 14:51:36.598581: Epoch time: 108.92 s\n",
      "2025-05-17 14:51:37.139454: \n",
      "2025-05-17 14:51:37.139560: Epoch 633\n",
      "2025-05-17 14:51:37.139705: Current learning rate: 0.00406\n",
      "2025-05-17 14:53:26.118634: train_loss -0.9638\n",
      "2025-05-17 14:53:26.118962: val_loss -0.9509\n",
      "2025-05-17 14:53:26.119008: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 14:53:26.119044: Epoch time: 108.98 s\n",
      "2025-05-17 14:53:26.649532: \n",
      "2025-05-17 14:53:26.649666: Epoch 634\n",
      "2025-05-17 14:53:26.649832: Current learning rate: 0.00405\n",
      "2025-05-17 14:55:15.659147: train_loss -0.9618\n",
      "2025-05-17 14:55:15.659269: val_loss -0.9491\n",
      "2025-05-17 14:55:15.659305: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 14:55:15.659338: Epoch time: 109.01 s\n",
      "2025-05-17 14:55:16.382726: \n",
      "2025-05-17 14:55:16.382945: Epoch 635\n",
      "2025-05-17 14:55:16.383020: Current learning rate: 0.00404\n",
      "2025-05-17 14:57:05.347547: train_loss -0.9635\n",
      "2025-05-17 14:57:05.347674: val_loss -0.9512\n",
      "2025-05-17 14:57:05.347708: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-17 14:57:05.347741: Epoch time: 108.97 s\n",
      "2025-05-17 14:57:05.879533: \n",
      "2025-05-17 14:57:05.879635: Epoch 636\n",
      "2025-05-17 14:57:05.879726: Current learning rate: 0.00403\n",
      "2025-05-17 14:58:54.899083: train_loss -0.963\n",
      "2025-05-17 14:58:54.899223: val_loss -0.9426\n",
      "2025-05-17 14:58:54.899261: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-17 14:58:54.899296: Epoch time: 109.02 s\n",
      "2025-05-17 14:58:55.430861: \n",
      "2025-05-17 14:58:55.431108: Epoch 637\n",
      "2025-05-17 14:58:55.431422: Current learning rate: 0.00402\n",
      "2025-05-17 15:00:44.418712: train_loss -0.9633\n",
      "2025-05-17 15:00:44.418857: val_loss -0.9513\n",
      "2025-05-17 15:00:44.418894: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-17 15:00:44.418929: Epoch time: 108.99 s\n",
      "2025-05-17 15:00:44.950689: \n",
      "2025-05-17 15:00:44.950791: Epoch 638\n",
      "2025-05-17 15:00:44.950883: Current learning rate: 0.00401\n",
      "2025-05-17 15:02:33.883914: train_loss -0.9641\n",
      "2025-05-17 15:02:33.884115: val_loss -0.9437\n",
      "2025-05-17 15:02:33.884219: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 15:02:33.884455: Epoch time: 108.93 s\n",
      "2025-05-17 15:02:34.416100: \n",
      "2025-05-17 15:02:34.416432: Epoch 639\n",
      "2025-05-17 15:02:34.416503: Current learning rate: 0.004\n",
      "2025-05-17 15:04:23.393772: train_loss -0.9651\n",
      "2025-05-17 15:04:23.394041: val_loss -0.947\n",
      "2025-05-17 15:04:23.394089: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-17 15:04:23.394129: Epoch time: 108.98 s\n",
      "2025-05-17 15:04:23.931056: \n",
      "2025-05-17 15:04:23.931268: Epoch 640\n",
      "2025-05-17 15:04:23.931337: Current learning rate: 0.00399\n",
      "2025-05-17 15:06:12.915462: train_loss -0.964\n",
      "2025-05-17 15:06:12.915643: val_loss -0.9489\n",
      "2025-05-17 15:06:12.915711: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 15:06:12.915751: Epoch time: 108.99 s\n",
      "2025-05-17 15:06:13.437388: \n",
      "2025-05-17 15:06:13.437541: Epoch 641\n",
      "2025-05-17 15:06:13.437612: Current learning rate: 0.00398\n",
      "2025-05-17 15:08:02.467328: train_loss -0.9622\n",
      "2025-05-17 15:08:02.467467: val_loss -0.9459\n",
      "2025-05-17 15:08:02.467500: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-17 15:08:02.467532: Epoch time: 109.03 s\n",
      "2025-05-17 15:08:03.002025: \n",
      "2025-05-17 15:08:03.002214: Epoch 642\n",
      "2025-05-17 15:08:03.002320: Current learning rate: 0.00397\n",
      "2025-05-17 15:09:52.018265: train_loss -0.9641\n",
      "2025-05-17 15:09:52.018419: val_loss -0.9461\n",
      "2025-05-17 15:09:52.018454: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 15:09:52.018487: Epoch time: 109.02 s\n",
      "2025-05-17 15:09:52.547349: \n",
      "2025-05-17 15:09:52.547504: Epoch 643\n",
      "2025-05-17 15:09:52.547578: Current learning rate: 0.00396\n",
      "2025-05-17 15:11:41.526550: train_loss -0.9653\n",
      "2025-05-17 15:11:41.526741: val_loss -0.9496\n",
      "2025-05-17 15:11:41.526779: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 15:11:41.526811: Epoch time: 108.98 s\n",
      "2025-05-17 15:11:42.054900: \n",
      "2025-05-17 15:11:42.055243: Epoch 644\n",
      "2025-05-17 15:11:42.055328: Current learning rate: 0.00395\n",
      "2025-05-17 15:13:31.052578: train_loss -0.9655\n",
      "2025-05-17 15:13:31.052859: val_loss -0.948\n",
      "2025-05-17 15:13:31.052900: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 15:13:31.052934: Epoch time: 109.0 s\n",
      "2025-05-17 15:13:31.586290: \n",
      "2025-05-17 15:13:31.586462: Epoch 645\n",
      "2025-05-17 15:13:31.586544: Current learning rate: 0.00394\n",
      "2025-05-17 15:15:20.517080: train_loss -0.9655\n",
      "2025-05-17 15:15:20.517204: val_loss -0.95\n",
      "2025-05-17 15:15:20.517241: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 15:15:20.517288: Epoch time: 108.93 s\n",
      "2025-05-17 15:15:21.040340: \n",
      "2025-05-17 15:15:21.040503: Epoch 646\n",
      "2025-05-17 15:15:21.040571: Current learning rate: 0.00393\n",
      "2025-05-17 15:17:09.984510: train_loss -0.9633\n",
      "2025-05-17 15:17:09.984632: val_loss -0.9482\n",
      "2025-05-17 15:17:09.984663: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 15:17:09.984695: Epoch time: 108.94 s\n",
      "2025-05-17 15:17:10.714784: \n",
      "2025-05-17 15:17:10.714961: Epoch 647\n",
      "2025-05-17 15:17:10.715039: Current learning rate: 0.00392\n",
      "2025-05-17 15:18:59.739241: train_loss -0.9654\n",
      "2025-05-17 15:18:59.739407: val_loss -0.9424\n",
      "2025-05-17 15:18:59.739441: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 15:18:59.739473: Epoch time: 109.02 s\n",
      "2025-05-17 15:19:00.272039: \n",
      "2025-05-17 15:19:00.272240: Epoch 648\n",
      "2025-05-17 15:19:00.272347: Current learning rate: 0.00391\n",
      "2025-05-17 15:20:49.217323: train_loss -0.9632\n",
      "2025-05-17 15:20:49.217451: val_loss -0.9473\n",
      "2025-05-17 15:20:49.217492: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-17 15:20:49.217544: Epoch time: 108.95 s\n",
      "2025-05-17 15:20:49.750880: \n",
      "2025-05-17 15:20:49.751078: Epoch 649\n",
      "2025-05-17 15:20:49.751150: Current learning rate: 0.0039\n",
      "2025-05-17 15:22:38.761793: train_loss -0.9647\n",
      "2025-05-17 15:22:38.762040: val_loss -0.9498\n",
      "2025-05-17 15:22:38.762188: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 15:22:38.762290: Epoch time: 109.01 s\n",
      "2025-05-17 15:22:39.509634: \n",
      "2025-05-17 15:22:39.509733: Epoch 650\n",
      "2025-05-17 15:22:39.509795: Current learning rate: 0.00389\n",
      "2025-05-17 15:24:28.532777: train_loss -0.9661\n",
      "2025-05-17 15:24:28.532967: val_loss -0.9542\n",
      "2025-05-17 15:24:28.533001: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-17 15:24:28.533037: Epoch time: 109.02 s\n",
      "2025-05-17 15:24:29.054459: \n",
      "2025-05-17 15:24:29.054650: Epoch 651\n",
      "2025-05-17 15:24:29.054719: Current learning rate: 0.00388\n",
      "2025-05-17 15:26:18.025270: train_loss -0.9644\n",
      "2025-05-17 15:26:18.025454: val_loss -0.9494\n",
      "2025-05-17 15:26:18.025488: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 15:26:18.025522: Epoch time: 108.97 s\n",
      "2025-05-17 15:26:18.556820: \n",
      "2025-05-17 15:26:18.557091: Epoch 652\n",
      "2025-05-17 15:26:18.557226: Current learning rate: 0.00387\n",
      "2025-05-17 15:28:07.488816: train_loss -0.9656\n",
      "2025-05-17 15:28:07.488993: val_loss -0.949\n",
      "2025-05-17 15:28:07.489026: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 15:28:07.489058: Epoch time: 108.93 s\n",
      "2025-05-17 15:28:08.025147: \n",
      "2025-05-17 15:28:08.025350: Epoch 653\n",
      "2025-05-17 15:28:08.025423: Current learning rate: 0.00386\n",
      "2025-05-17 15:29:57.029564: train_loss -0.9657\n",
      "2025-05-17 15:29:57.029762: val_loss -0.9461\n",
      "2025-05-17 15:29:57.029806: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 15:29:57.029840: Epoch time: 109.0 s\n",
      "2025-05-17 15:29:57.570352: \n",
      "2025-05-17 15:29:57.570451: Epoch 654\n",
      "2025-05-17 15:29:57.570516: Current learning rate: 0.00385\n",
      "2025-05-17 15:31:46.551740: train_loss -0.9637\n",
      "2025-05-17 15:31:46.551877: val_loss -0.9488\n",
      "2025-05-17 15:31:46.551913: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 15:31:46.551948: Epoch time: 108.98 s\n",
      "2025-05-17 15:31:47.089978: \n",
      "2025-05-17 15:31:47.090122: Epoch 655\n",
      "2025-05-17 15:31:47.090188: Current learning rate: 0.00384\n",
      "2025-05-17 15:33:36.074198: train_loss -0.9635\n",
      "2025-05-17 15:33:36.074404: val_loss -0.9453\n",
      "2025-05-17 15:33:36.074486: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-17 15:33:36.074667: Epoch time: 108.98 s\n",
      "2025-05-17 15:33:36.603815: \n",
      "2025-05-17 15:33:36.603977: Epoch 656\n",
      "2025-05-17 15:33:36.604062: Current learning rate: 0.00383\n",
      "2025-05-17 15:35:25.582832: train_loss -0.9662\n",
      "2025-05-17 15:35:25.582953: val_loss -0.9484\n",
      "2025-05-17 15:35:25.583003: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 15:35:25.583037: Epoch time: 108.98 s\n",
      "2025-05-17 15:35:26.110395: \n",
      "2025-05-17 15:35:26.110550: Epoch 657\n",
      "2025-05-17 15:35:26.110714: Current learning rate: 0.00382\n",
      "2025-05-17 15:37:15.119020: train_loss -0.9654\n",
      "2025-05-17 15:37:15.119162: val_loss -0.9462\n",
      "2025-05-17 15:37:15.119197: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 15:37:15.119229: Epoch time: 109.01 s\n",
      "2025-05-17 15:37:15.649026: \n",
      "2025-05-17 15:37:15.649111: Epoch 658\n",
      "2025-05-17 15:37:15.649205: Current learning rate: 0.00381\n",
      "2025-05-17 15:39:04.675163: train_loss -0.9661\n",
      "2025-05-17 15:39:04.675282: val_loss -0.9504\n",
      "2025-05-17 15:39:04.675315: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 15:39:04.675348: Epoch time: 109.03 s\n",
      "2025-05-17 15:39:05.202518: \n",
      "2025-05-17 15:39:05.202655: Epoch 659\n",
      "2025-05-17 15:39:05.202727: Current learning rate: 0.0038\n",
      "2025-05-17 15:40:54.220996: train_loss -0.9643\n",
      "2025-05-17 15:40:54.221168: val_loss -0.9472\n",
      "2025-05-17 15:40:54.221224: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 15:40:54.221265: Epoch time: 109.02 s\n",
      "2025-05-17 15:40:54.968155: \n",
      "2025-05-17 15:40:54.968247: Epoch 660\n",
      "2025-05-17 15:40:54.968440: Current learning rate: 0.00379\n",
      "2025-05-17 15:42:44.019245: train_loss -0.9629\n",
      "2025-05-17 15:42:44.019357: val_loss -0.9435\n",
      "2025-05-17 15:42:44.019485: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 15:42:44.019537: Epoch time: 109.05 s\n",
      "2025-05-17 15:42:44.542309: \n",
      "2025-05-17 15:42:44.542580: Epoch 661\n",
      "2025-05-17 15:42:44.542750: Current learning rate: 0.00378\n",
      "2025-05-17 15:44:33.496387: train_loss -0.9654\n",
      "2025-05-17 15:44:33.496518: val_loss -0.9499\n",
      "2025-05-17 15:44:33.496551: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-17 15:44:33.496586: Epoch time: 108.95 s\n",
      "2025-05-17 15:44:34.033210: \n",
      "2025-05-17 15:44:34.033496: Epoch 662\n",
      "2025-05-17 15:44:34.033597: Current learning rate: 0.00377\n",
      "2025-05-17 15:46:23.009664: train_loss -0.9631\n",
      "2025-05-17 15:46:23.009854: val_loss -0.9435\n",
      "2025-05-17 15:46:23.009887: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-17 15:46:23.009921: Epoch time: 108.98 s\n",
      "2025-05-17 15:46:23.536680: \n",
      "2025-05-17 15:46:23.536777: Epoch 663\n",
      "2025-05-17 15:46:23.536877: Current learning rate: 0.00376\n",
      "2025-05-17 15:48:12.560385: train_loss -0.9639\n",
      "2025-05-17 15:48:12.560672: val_loss -0.9512\n",
      "2025-05-17 15:48:12.560726: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-17 15:48:12.560764: Epoch time: 109.02 s\n",
      "2025-05-17 15:48:13.089638: \n",
      "2025-05-17 15:48:13.089737: Epoch 664\n",
      "2025-05-17 15:48:13.089801: Current learning rate: 0.00375\n",
      "2025-05-17 15:50:02.112566: train_loss -0.9659\n",
      "2025-05-17 15:50:02.112785: val_loss -0.9497\n",
      "2025-05-17 15:50:02.112878: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 15:50:02.112927: Epoch time: 109.02 s\n",
      "2025-05-17 15:50:02.646651: \n",
      "2025-05-17 15:50:02.646749: Epoch 665\n",
      "2025-05-17 15:50:02.646813: Current learning rate: 0.00374\n",
      "2025-05-17 15:51:51.650231: train_loss -0.9649\n",
      "2025-05-17 15:51:51.650375: val_loss -0.9474\n",
      "2025-05-17 15:51:51.650407: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 15:51:51.650440: Epoch time: 109.0 s\n",
      "2025-05-17 15:51:52.194863: \n",
      "2025-05-17 15:51:52.195234: Epoch 666\n",
      "2025-05-17 15:51:52.195382: Current learning rate: 0.00373\n",
      "2025-05-17 15:53:41.210847: train_loss -0.9654\n",
      "2025-05-17 15:53:41.210977: val_loss -0.9471\n",
      "2025-05-17 15:53:41.211011: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 15:53:41.211057: Epoch time: 109.02 s\n",
      "2025-05-17 15:53:41.741282: \n",
      "2025-05-17 15:53:41.741589: Epoch 667\n",
      "2025-05-17 15:53:41.741661: Current learning rate: 0.00372\n",
      "2025-05-17 15:55:30.748453: train_loss -0.9632\n",
      "2025-05-17 15:55:30.748592: val_loss -0.9494\n",
      "2025-05-17 15:55:30.748634: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 15:55:30.748694: Epoch time: 109.01 s\n",
      "2025-05-17 15:55:31.283893: \n",
      "2025-05-17 15:55:31.284080: Epoch 668\n",
      "2025-05-17 15:55:31.284181: Current learning rate: 0.00371\n",
      "2025-05-17 15:57:20.243360: train_loss -0.966\n",
      "2025-05-17 15:57:20.243582: val_loss -0.9423\n",
      "2025-05-17 15:57:20.243696: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-17 15:57:20.243778: Epoch time: 108.96 s\n",
      "2025-05-17 15:57:20.779786: \n",
      "2025-05-17 15:57:20.779969: Epoch 669\n",
      "2025-05-17 15:57:20.780064: Current learning rate: 0.0037\n",
      "2025-05-17 15:59:09.821222: train_loss -0.9659\n",
      "2025-05-17 15:59:09.821355: val_loss -0.9458\n",
      "2025-05-17 15:59:09.821390: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-17 15:59:09.821423: Epoch time: 109.04 s\n",
      "2025-05-17 15:59:10.348687: \n",
      "2025-05-17 15:59:10.348767: Epoch 670\n",
      "2025-05-17 15:59:10.348830: Current learning rate: 0.00369\n",
      "2025-05-17 16:00:59.336619: train_loss -0.9662\n",
      "2025-05-17 16:00:59.336754: val_loss -0.9467\n",
      "2025-05-17 16:00:59.336792: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 16:00:59.336825: Epoch time: 108.99 s\n",
      "2025-05-17 16:00:59.867366: \n",
      "2025-05-17 16:00:59.867519: Epoch 671\n",
      "2025-05-17 16:00:59.867593: Current learning rate: 0.00368\n",
      "2025-05-17 16:02:48.807670: train_loss -0.9645\n",
      "2025-05-17 16:02:48.807807: val_loss -0.9439\n",
      "2025-05-17 16:02:48.807842: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 16:02:48.807874: Epoch time: 108.94 s\n",
      "2025-05-17 16:02:49.540252: \n",
      "2025-05-17 16:02:49.540392: Epoch 672\n",
      "2025-05-17 16:02:49.540472: Current learning rate: 0.00367\n",
      "2025-05-17 16:04:38.499854: train_loss -0.9655\n",
      "2025-05-17 16:04:38.500040: val_loss -0.9553\n",
      "2025-05-17 16:04:38.500081: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-17 16:04:38.500114: Epoch time: 108.96 s\n",
      "2025-05-17 16:04:39.038844: \n",
      "2025-05-17 16:04:39.039008: Epoch 673\n",
      "2025-05-17 16:04:39.039093: Current learning rate: 0.00366\n",
      "2025-05-17 16:06:28.080354: train_loss -0.9647\n",
      "2025-05-17 16:06:28.080479: val_loss -0.9489\n",
      "2025-05-17 16:06:28.080512: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 16:06:28.080612: Epoch time: 109.04 s\n",
      "2025-05-17 16:06:28.617434: \n",
      "2025-05-17 16:06:28.617529: Epoch 674\n",
      "2025-05-17 16:06:28.617594: Current learning rate: 0.00365\n",
      "2025-05-17 16:08:17.600562: train_loss -0.964\n",
      "2025-05-17 16:08:17.600767: val_loss -0.9458\n",
      "2025-05-17 16:08:17.600803: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-17 16:08:17.600837: Epoch time: 108.98 s\n",
      "2025-05-17 16:08:18.141087: \n",
      "2025-05-17 16:08:18.141246: Epoch 675\n",
      "2025-05-17 16:08:18.141320: Current learning rate: 0.00364\n",
      "2025-05-17 16:10:07.157455: train_loss -0.9639\n",
      "2025-05-17 16:10:07.157579: val_loss -0.9491\n",
      "2025-05-17 16:10:07.157612: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-17 16:10:07.157725: Epoch time: 109.02 s\n",
      "2025-05-17 16:10:07.694916: \n",
      "2025-05-17 16:10:07.695076: Epoch 676\n",
      "2025-05-17 16:10:07.695147: Current learning rate: 0.00363\n",
      "2025-05-17 16:11:56.688186: train_loss -0.9662\n",
      "2025-05-17 16:11:56.688350: val_loss -0.951\n",
      "2025-05-17 16:11:56.688388: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-17 16:11:56.688422: Epoch time: 108.99 s\n",
      "2025-05-17 16:11:57.221310: \n",
      "2025-05-17 16:11:57.221493: Epoch 677\n",
      "2025-05-17 16:11:57.221560: Current learning rate: 0.00362\n",
      "2025-05-17 16:13:46.312879: train_loss -0.9637\n",
      "2025-05-17 16:13:46.313002: val_loss -0.9462\n",
      "2025-05-17 16:13:46.313035: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 16:13:46.313067: Epoch time: 109.09 s\n",
      "2025-05-17 16:13:46.848621: \n",
      "2025-05-17 16:13:46.848791: Epoch 678\n",
      "2025-05-17 16:13:46.848868: Current learning rate: 0.00361\n",
      "2025-05-17 16:15:35.865753: train_loss -0.9647\n",
      "2025-05-17 16:15:35.865881: val_loss -0.944\n",
      "2025-05-17 16:15:35.866057: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-17 16:15:35.866139: Epoch time: 109.02 s\n",
      "2025-05-17 16:15:36.405534: \n",
      "2025-05-17 16:15:36.405709: Epoch 679\n",
      "2025-05-17 16:15:36.405823: Current learning rate: 0.0036\n",
      "2025-05-17 16:17:25.351178: train_loss -0.9656\n",
      "2025-05-17 16:17:25.351343: val_loss -0.9466\n",
      "2025-05-17 16:17:25.351391: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 16:17:25.351426: Epoch time: 108.95 s\n",
      "2025-05-17 16:17:25.892384: \n",
      "2025-05-17 16:17:25.892822: Epoch 680\n",
      "2025-05-17 16:17:25.892971: Current learning rate: 0.00359\n",
      "2025-05-17 16:19:14.894438: train_loss -0.9652\n",
      "2025-05-17 16:19:14.894607: val_loss -0.9475\n",
      "2025-05-17 16:19:14.894677: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-17 16:19:14.894716: Epoch time: 109.0 s\n",
      "2025-05-17 16:19:15.425350: \n",
      "2025-05-17 16:19:15.425533: Epoch 681\n",
      "2025-05-17 16:19:15.425603: Current learning rate: 0.00358\n",
      "2025-05-17 16:21:04.406040: train_loss -0.964\n",
      "2025-05-17 16:21:04.406163: val_loss -0.9467\n",
      "2025-05-17 16:21:04.406197: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 16:21:04.406230: Epoch time: 108.98 s\n",
      "2025-05-17 16:21:04.935872: \n",
      "2025-05-17 16:21:04.935956: Epoch 682\n",
      "2025-05-17 16:21:04.936022: Current learning rate: 0.00357\n",
      "2025-05-17 16:22:53.970295: train_loss -0.965\n",
      "2025-05-17 16:22:53.970510: val_loss -0.9477\n",
      "2025-05-17 16:22:53.970612: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 16:22:53.970689: Epoch time: 109.03 s\n",
      "2025-05-17 16:22:54.500604: \n",
      "2025-05-17 16:22:54.500834: Epoch 683\n",
      "2025-05-17 16:22:54.500918: Current learning rate: 0.00356\n",
      "2025-05-17 16:24:43.533488: train_loss -0.9659\n",
      "2025-05-17 16:24:43.533649: val_loss -0.9508\n",
      "2025-05-17 16:24:43.533688: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 16:24:43.533720: Epoch time: 109.03 s\n",
      "2025-05-17 16:24:44.273890: \n",
      "2025-05-17 16:24:44.274184: Epoch 684\n",
      "2025-05-17 16:24:44.274276: Current learning rate: 0.00355\n",
      "2025-05-17 16:26:33.320134: train_loss -0.9661\n",
      "2025-05-17 16:26:33.320321: val_loss -0.9544\n",
      "2025-05-17 16:26:33.320359: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-17 16:26:33.320405: Epoch time: 109.05 s\n",
      "2025-05-17 16:26:33.843409: \n",
      "2025-05-17 16:26:33.843759: Epoch 685\n",
      "2025-05-17 16:26:33.843863: Current learning rate: 0.00354\n",
      "2025-05-17 16:28:22.859769: train_loss -0.9675\n",
      "2025-05-17 16:28:22.859925: val_loss -0.9431\n",
      "2025-05-17 16:28:22.859961: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 16:28:22.859995: Epoch time: 109.02 s\n",
      "2025-05-17 16:28:23.388521: \n",
      "2025-05-17 16:28:23.388806: Epoch 686\n",
      "2025-05-17 16:28:23.388919: Current learning rate: 0.00353\n",
      "2025-05-17 16:30:12.384169: train_loss -0.9668\n",
      "2025-05-17 16:30:12.384295: val_loss -0.9484\n",
      "2025-05-17 16:30:12.384331: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 16:30:12.384364: Epoch time: 109.0 s\n",
      "2025-05-17 16:30:12.924473: \n",
      "2025-05-17 16:30:12.924675: Epoch 687\n",
      "2025-05-17 16:30:12.924743: Current learning rate: 0.00352\n",
      "2025-05-17 16:32:01.893473: train_loss -0.9673\n",
      "2025-05-17 16:32:01.893802: val_loss -0.9458\n",
      "2025-05-17 16:32:01.893840: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 16:32:01.893873: Epoch time: 108.97 s\n",
      "2025-05-17 16:32:02.428394: \n",
      "2025-05-17 16:32:02.428840: Epoch 688\n",
      "2025-05-17 16:32:02.428927: Current learning rate: 0.00351\n",
      "2025-05-17 16:33:52.283949: train_loss -0.9676\n",
      "2025-05-17 16:33:52.284088: val_loss -0.9511\n",
      "2025-05-17 16:33:52.284124: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 16:33:52.284159: Epoch time: 109.86 s\n",
      "2025-05-17 16:33:52.817309: \n",
      "2025-05-17 16:33:52.817513: Epoch 689\n",
      "2025-05-17 16:33:52.817625: Current learning rate: 0.0035\n",
      "2025-05-17 16:35:42.250874: train_loss -0.9647\n",
      "2025-05-17 16:35:42.251012: val_loss -0.949\n",
      "2025-05-17 16:35:42.251046: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 16:35:42.251080: Epoch time: 109.43 s\n",
      "2025-05-17 16:35:42.251103: Yayy! New best EMA pseudo Dice: 0.9757000207901001\n",
      "2025-05-17 16:35:43.009636: \n",
      "2025-05-17 16:35:43.009737: Epoch 690\n",
      "2025-05-17 16:35:43.009825: Current learning rate: 0.00349\n",
      "2025-05-17 16:37:32.309859: train_loss -0.9661\n",
      "2025-05-17 16:37:32.310065: val_loss -0.9429\n",
      "2025-05-17 16:37:32.310113: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 16:37:32.310147: Epoch time: 109.3 s\n",
      "2025-05-17 16:37:32.866799: \n",
      "2025-05-17 16:37:32.866900: Epoch 691\n",
      "2025-05-17 16:37:32.866965: Current learning rate: 0.00348\n",
      "2025-05-17 16:39:22.145182: train_loss -0.9661\n",
      "2025-05-17 16:39:22.145370: val_loss -0.9415\n",
      "2025-05-17 16:39:22.145496: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 16:39:22.145541: Epoch time: 109.28 s\n",
      "2025-05-17 16:39:22.689595: \n",
      "2025-05-17 16:39:22.689689: Epoch 692\n",
      "2025-05-17 16:39:22.689753: Current learning rate: 0.00346\n",
      "2025-05-17 16:41:12.037323: train_loss -0.9653\n",
      "2025-05-17 16:41:12.037560: val_loss -0.9487\n",
      "2025-05-17 16:41:12.037644: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-17 16:41:12.037736: Epoch time: 109.35 s\n",
      "2025-05-17 16:41:12.577961: \n",
      "2025-05-17 16:41:12.578045: Epoch 693\n",
      "2025-05-17 16:41:12.578108: Current learning rate: 0.00345\n",
      "2025-05-17 16:43:01.911192: train_loss -0.9686\n",
      "2025-05-17 16:43:01.911321: val_loss -0.9495\n",
      "2025-05-17 16:43:01.911355: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 16:43:01.911388: Epoch time: 109.33 s\n",
      "2025-05-17 16:43:02.450763: \n",
      "2025-05-17 16:43:02.450991: Epoch 694\n",
      "2025-05-17 16:43:02.451085: Current learning rate: 0.00344\n",
      "2025-05-17 16:44:51.745692: train_loss -0.9681\n",
      "2025-05-17 16:44:51.745887: val_loss -0.9466\n",
      "2025-05-17 16:44:51.745921: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 16:44:51.745954: Epoch time: 109.3 s\n",
      "2025-05-17 16:44:52.278934: \n",
      "2025-05-17 16:44:52.279010: Epoch 695\n",
      "2025-05-17 16:44:52.279072: Current learning rate: 0.00343\n",
      "2025-05-17 16:46:41.619143: train_loss -0.9673\n",
      "2025-05-17 16:46:41.619277: val_loss -0.9444\n",
      "2025-05-17 16:46:41.619316: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-17 16:46:41.619352: Epoch time: 109.34 s\n",
      "2025-05-17 16:46:42.367865: \n",
      "2025-05-17 16:46:42.368037: Epoch 696\n",
      "2025-05-17 16:46:42.368205: Current learning rate: 0.00342\n",
      "2025-05-17 16:48:31.713141: train_loss -0.9663\n",
      "2025-05-17 16:48:31.713272: val_loss -0.9463\n",
      "2025-05-17 16:48:31.713304: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 16:48:31.713336: Epoch time: 109.35 s\n",
      "2025-05-17 16:48:32.245723: \n",
      "2025-05-17 16:48:32.245921: Epoch 697\n",
      "2025-05-17 16:48:32.246243: Current learning rate: 0.00341\n",
      "2025-05-17 16:50:21.591183: train_loss -0.9661\n",
      "2025-05-17 16:50:21.591318: val_loss -0.9505\n",
      "Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 16:50:21.591465: Epoch time: 109.35 s\n",
      "2025-05-17 16:50:22.136811: \n",
      "2025-05-17 16:50:22.137047: Epoch 698\n",
      "2025-05-17 16:50:22.137126: Current learning rate: 0.0034\n",
      "2025-05-17 16:52:11.466603: train_loss -0.967\n",
      "2025-05-17 16:52:11.466791: val_loss -0.9509\n",
      "2025-05-17 16:52:11.466824: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 16:52:11.466856: Epoch time: 109.33 s\n",
      "2025-05-17 16:52:11.466880: Yayy! New best EMA pseudo Dice: 0.9757999777793884\n",
      "2025-05-17 16:52:12.242946: \n",
      "2025-05-17 16:52:12.243366: Epoch 699\n",
      "2025-05-17 16:52:12.243468: Current learning rate: 0.00339\n",
      "2025-05-17 16:54:01.605472: train_loss -0.9675\n",
      "2025-05-17 16:54:01.605600: val_loss -0.9462\n",
      "2025-05-17 16:54:01.605754: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 16:54:01.605833: Epoch time: 109.36 s\n",
      "2025-05-17 16:54:02.374260: \n",
      "2025-05-17 16:54:02.374418: Epoch 700\n",
      "2025-05-17 16:54:02.374489: Current learning rate: 0.00338\n",
      "2025-05-17 16:55:51.745622: train_loss -0.9677\n",
      "2025-05-17 16:55:51.745758: val_loss -0.9479\n",
      "2025-05-17 16:55:51.745793: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 16:55:51.745826: Epoch time: 109.37 s\n",
      "2025-05-17 16:55:52.281214: \n",
      "2025-05-17 16:55:52.281309: Epoch 701\n",
      "2025-05-17 16:55:52.281373: Current learning rate: 0.00337\n",
      "2025-05-17 16:57:41.603356: train_loss -0.9683\n",
      "2025-05-17 16:57:41.603556: val_loss -0.9491\n",
      "2025-05-17 16:57:41.603783: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 16:57:41.603865: Epoch time: 109.32 s\n",
      "2025-05-17 16:57:42.147359: \n",
      "2025-05-17 16:57:42.147472: Epoch 702\n",
      "2025-05-17 16:57:42.147651: Current learning rate: 0.00336\n",
      "2025-05-17 16:59:31.488651: train_loss -0.968\n",
      "2025-05-17 16:59:31.488821: val_loss -0.9511\n",
      "2025-05-17 16:59:31.488852: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-17 16:59:31.488885: Epoch time: 109.34 s\n",
      "2025-05-17 16:59:32.024595: \n",
      "2025-05-17 16:59:32.024713: Epoch 703\n",
      "2025-05-17 16:59:32.024779: Current learning rate: 0.00335\n",
      "2025-05-17 17:01:21.409704: train_loss -0.9673\n",
      "2025-05-17 17:01:21.409873: val_loss -0.9485\n",
      "2025-05-17 17:01:21.409906: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-17 17:01:21.409937: Epoch time: 109.39 s\n",
      "2025-05-17 17:01:21.410047: Yayy! New best EMA pseudo Dice: 0.9758999943733215\n",
      "2025-05-17 17:01:22.164763: \n",
      "2025-05-17 17:01:22.164972: Epoch 704\n",
      "2025-05-17 17:01:22.165105: Current learning rate: 0.00334\n",
      "2025-05-17 17:03:11.482341: train_loss -0.9671\n",
      "2025-05-17 17:03:11.482477: val_loss -0.9471\n",
      "2025-05-17 17:03:11.482513: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 17:03:11.482555: Epoch time: 109.32 s\n",
      "2025-05-17 17:03:12.014716: \n",
      "2025-05-17 17:03:12.014796: Epoch 705\n",
      "2025-05-17 17:03:12.014942: Current learning rate: 0.00333\n",
      "2025-05-17 17:05:01.348902: train_loss -0.9663\n",
      "2025-05-17 17:05:01.349024: val_loss -0.9483\n",
      "2025-05-17 17:05:01.349056: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 17:05:01.349087: Epoch time: 109.33 s\n",
      "2025-05-17 17:05:01.880019: \n",
      "2025-05-17 17:05:01.880198: Epoch 706\n",
      "2025-05-17 17:05:01.880311: Current learning rate: 0.00332\n",
      "2025-05-17 17:06:51.242031: train_loss -0.9677\n",
      "2025-05-17 17:06:51.242155: val_loss -0.9506\n",
      "2025-05-17 17:06:51.242190: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 17:06:51.242223: Epoch time: 109.36 s\n",
      "2025-05-17 17:06:51.242244: Yayy! New best EMA pseudo Dice: 0.9758999943733215\n",
      "2025-05-17 17:06:51.999943: \n",
      "2025-05-17 17:06:52.000081: Epoch 707\n",
      "2025-05-17 17:06:52.000173: Current learning rate: 0.00331\n",
      "2025-05-17 17:08:41.261127: train_loss -0.9668\n",
      "2025-05-17 17:08:41.261303: val_loss -0.9519\n",
      "2025-05-17 17:08:41.261338: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-17 17:08:41.261372: Epoch time: 109.26 s\n",
      "2025-05-17 17:08:41.261392: Yayy! New best EMA pseudo Dice: 0.9760000109672546\n",
      "2025-05-17 17:08:42.205770: \n",
      "2025-05-17 17:08:42.205919: Epoch 708\n",
      "2025-05-17 17:08:42.205991: Current learning rate: 0.0033\n",
      "2025-05-17 17:10:31.551681: train_loss -0.9665\n",
      "2025-05-17 17:10:31.551818: val_loss -0.9471\n",
      "2025-05-17 17:10:31.551850: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 17:10:31.551895: Epoch time: 109.35 s\n",
      "2025-05-17 17:10:32.082258: \n",
      "2025-05-17 17:10:32.082353: Epoch 709\n",
      "2025-05-17 17:10:32.082486: Current learning rate: 0.00329\n",
      "2025-05-17 17:12:21.474236: train_loss -0.9676\n",
      "2025-05-17 17:12:21.474425: val_loss -0.9453\n",
      "2025-05-17 17:12:21.474465: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 17:12:21.474504: Epoch time: 109.39 s\n",
      "2025-05-17 17:12:22.016487: \n",
      "2025-05-17 17:12:22.016671: Epoch 710\n",
      "2025-05-17 17:12:22.016877: Current learning rate: 0.00328\n",
      "2025-05-17 17:14:10.972992: train_loss -0.9683\n",
      "2025-05-17 17:14:10.973121: val_loss -0.9492\n",
      "2025-05-17 17:14:10.973157: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 17:14:10.973191: Epoch time: 108.96 s\n",
      "2025-05-17 17:14:11.509355: \n",
      "2025-05-17 17:14:11.509491: Epoch 711\n",
      "2025-05-17 17:14:11.509556: Current learning rate: 0.00327\n",
      "2025-05-17 17:16:00.559882: train_loss -0.9657\n",
      "2025-05-17 17:16:00.560006: val_loss -0.9489\n",
      "2025-05-17 17:16:00.560040: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-17 17:16:00.560072: Epoch time: 109.05 s\n",
      "2025-05-17 17:16:01.103635: \n",
      "2025-05-17 17:16:01.103805: Epoch 712\n",
      "2025-05-17 17:16:01.103873: Current learning rate: 0.00326\n",
      "2025-05-17 17:17:50.092853: train_loss -0.967\n",
      "2025-05-17 17:17:50.093037: val_loss -0.948\n",
      "2025-05-17 17:17:50.093074: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 17:17:50.093107: Epoch time: 108.99 s\n",
      "2025-05-17 17:17:50.624848: \n",
      "2025-05-17 17:17:50.624946: Epoch 713\n",
      "2025-05-17 17:17:50.625009: Current learning rate: 0.00325\n",
      "2025-05-17 17:19:39.622468: train_loss -0.9689\n",
      "2025-05-17 17:19:39.622605: val_loss -0.9477\n",
      "2025-05-17 17:19:39.622642: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 17:19:39.622674: Epoch time: 109.0 s\n",
      "2025-05-17 17:19:40.150275: \n",
      "2025-05-17 17:19:40.150468: Epoch 714\n",
      "2025-05-17 17:19:40.150594: Current learning rate: 0.00324\n",
      "2025-05-17 17:21:29.150213: train_loss -0.9684\n",
      "2025-05-17 17:21:29.150375: val_loss -0.9517\n",
      "2025-05-17 17:21:29.150408: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-17 17:21:29.150441: Epoch time: 109.0 s\n",
      "2025-05-17 17:21:29.150473: Yayy! New best EMA pseudo Dice: 0.9760000109672546\n",
      "2025-05-17 17:21:29.908423: \n",
      "2025-05-17 17:21:29.908692: Epoch 715\n",
      "2025-05-17 17:21:29.908798: Current learning rate: 0.00323\n",
      "2025-05-17 17:23:18.927299: train_loss -0.9675\n",
      "2025-05-17 17:23:18.927423: val_loss -0.9527\n",
      "2025-05-17 17:23:18.927455: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-17 17:23:18.927487: Epoch time: 109.02 s\n",
      "2025-05-17 17:23:18.927507: Yayy! New best EMA pseudo Dice: 0.9761999845504761\n",
      "2025-05-17 17:23:19.685167: \n",
      "2025-05-17 17:23:19.685252: Epoch 716\n",
      "2025-05-17 17:23:19.685318: Current learning rate: 0.00322\n",
      "2025-05-17 17:25:08.715407: train_loss -0.9669\n",
      "2025-05-17 17:25:08.715528: val_loss -0.9459\n",
      "2025-05-17 17:25:08.715562: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 17:25:08.715596: Epoch time: 109.03 s\n",
      "2025-05-17 17:25:09.249483: \n",
      "2025-05-17 17:25:09.249575: Epoch 717\n",
      "2025-05-17 17:25:09.249641: Current learning rate: 0.00321\n",
      "2025-05-17 17:26:58.249938: train_loss -0.9665\n",
      "2025-05-17 17:26:58.250104: val_loss -0.9451\n",
      "2025-05-17 17:26:58.250139: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 17:26:58.250171: Epoch time: 109.0 s\n",
      "2025-05-17 17:26:58.785594: \n",
      "2025-05-17 17:26:58.785671: Epoch 718\n",
      "2025-05-17 17:26:58.785733: Current learning rate: 0.0032\n",
      "2025-05-17 17:28:47.782274: train_loss -0.9672\n",
      "2025-05-17 17:28:47.782398: val_loss -0.9495\n",
      "2025-05-17 17:28:47.782431: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 17:28:47.782464: Epoch time: 109.0 s\n",
      "2025-05-17 17:28:48.326599: \n",
      "2025-05-17 17:28:48.326733: Epoch 719\n",
      "2025-05-17 17:28:48.326797: Current learning rate: 0.00319\n",
      "2025-05-17 17:30:37.331944: train_loss -0.9694\n",
      "2025-05-17 17:30:37.332084: val_loss -0.9488\n",
      "2025-05-17 17:30:37.332116: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-17 17:30:37.332148: Epoch time: 109.01 s\n",
      "2025-05-17 17:30:38.058227: \n",
      "2025-05-17 17:30:38.058380: Epoch 720\n",
      "2025-05-17 17:30:38.058499: Current learning rate: 0.00318\n",
      "2025-05-17 17:32:27.045121: train_loss -0.9681\n",
      "2025-05-17 17:32:27.045259: val_loss -0.9526\n",
      "2025-05-17 17:32:27.045294: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-17 17:32:27.045325: Epoch time: 108.99 s\n",
      "2025-05-17 17:32:27.045345: Yayy! New best EMA pseudo Dice: 0.9761999845504761\n",
      " 025-05-17 17:32:27.796318:\n",
      "2025-05-17 17:32:27.796708: Epoch 721\n",
      "2025-05-17 17:32:27.796813: Current learning rate: 0.00317\n",
      "2025-05-17 17:34:16.757179: train_loss -0.9677\n",
      "2025-05-17 17:34:16.757362: val_loss -0.9516\n",
      "2025-05-17 17:34:16.757396: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 17:34:16.757429: Epoch time: 108.96 s\n",
      "2025-05-17 17:34:16.757450: Yayy! New best EMA pseudo Dice: 0.9763000011444092\n",
      "2025-05-17 17:34:17.512158: \n",
      "2025-05-17 17:34:17.512282: Epoch 722\n",
      "2025-05-17 17:34:17.512355: Current learning rate: 0.00316\n",
      "2025-05-17 17:36:06.475363: train_loss -0.9673\n",
      "2025-05-17 17:36:06.475489: val_loss -0.9524\n",
      "2025-05-17 17:36:06.475523: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 17:36:06.475555: Epoch time: 108.96 s\n",
      "2025-05-17 17:36:06.475576: Yayy! New best EMA pseudo Dice: 0.9764000177383423\n",
      "2025-05-17 17:36:07.237521: \n",
      "2025-05-17 17:36:07.237728: Epoch 723\n",
      "2025-05-17 17:36:07.237800: Current learning rate: 0.00315\n",
      "2025-05-17 17:37:56.254639: train_loss -0.9682\n",
      "2025-05-17 17:37:56.254842: val_loss -0.9491\n",
      "2025-05-17 17:37:56.254879: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 17:37:56.254913: Epoch time: 109.02 s\n",
      "2025-05-17 17:37:56.254936: Yayy! New best EMA pseudo Dice: 0.9764000177383423\n",
      "2025-05-17 17:37:57.009830: \n",
      "2025-05-17 17:37:57.010090: Epoch 724\n",
      "2025-05-17 17:37:57.010204: Current learning rate: 0.00314\n",
      "2025-05-17 17:39:46.540179: train_loss -0.9685\n",
      "2025-05-17 17:39:46.540308: val_loss -0.9448\n",
      "2025-05-17 17:39:46.540344: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 17:39:46.540378: Epoch time: 109.53 s\n",
      "2025-05-17 17:39:47.080291: \n",
      "2025-05-17 17:39:47.080464: Epoch 725\n",
      "2025-05-17 17:39:47.080543: Current learning rate: 0.00313\n",
      "2025-05-17 17:41:36.483641: train_loss -0.9682\n",
      "2025-05-17 17:41:36.483771: val_loss -0.9431\n",
      "2025-05-17 17:41:36.483803: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 17:41:36.483837: Epoch time: 109.4 s\n",
      "2025-05-17 17:41:37.037115: \n",
      "2025-05-17 17:41:37.037377: Epoch 726\n",
      "2025-05-17 17:41:37.037549: Current learning rate: 0.00312\n",
      "2025-05-17 17:43:26.379231: train_loss -0.9681\n",
      "2025-05-17 17:43:26.379400: val_loss -0.9511\n",
      "2025-05-17 17:43:26.379444: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-17 17:43:26.379482: Epoch time: 109.34 s\n",
      "2025-05-17 17:43:26.912273: \n",
      "2025-05-17 17:43:26.912358: Epoch 727\n",
      "2025-05-17 17:43:26.912421: Current learning rate: 0.00311\n",
      "2025-05-17 17:45:16.240018: train_loss -0.9681\n",
      "2025-05-17 17:45:16.240265: val_loss -0.9475\n",
      "2025-05-17 17:45:16.240307: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 17:45:16.240419: Epoch time: 109.33 s\n",
      "2025-05-17 17:45:16.782213: \n",
      "2025-05-17 17:45:16.782513: Epoch 728\n",
      "2025-05-17 17:45:16.782753: Current learning rate: 0.0031\n",
      "2025-05-17 17:47:06.154543: train_loss -0.9685\n",
      "2025-05-17 17:47:06.154680: val_loss -0.9457\n",
      "2025-05-17 17:47:06.154718: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 17:47:06.154751: Epoch time: 109.37 s\n",
      "2025-05-17 17:47:06.700457: \n",
      "2025-05-17 17:47:06.700594: Epoch 729\n",
      "2025-05-17 17:47:06.700661: Current learning rate: 0.00309\n",
      "2025-05-17 17:48:56.036766: train_loss -0.9684\n",
      "2025-05-17 17:48:56.036945: val_loss -0.9514\n",
      "2025-05-17 17:48:56.037090: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-17 17:48:56.037231: Epoch time: 109.34 s\n",
      "2025-05-17 17:48:56.578645: \n",
      "2025-05-17 17:48:56.578774: Epoch 730\n",
      "2025-05-17 17:48:56.578843: Current learning rate: 0.00308\n",
      "2025-05-17 17:50:45.925455: train_loss -0.9676\n",
      "2025-05-17 17:50:45.925646: val_loss -0.946\n",
      "2025-05-17 17:50:45.925683: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 17:50:45.925716: Epoch time: 109.35 s\n",
      "2025-05-17 17:50:46.455794: \n",
      "2025-05-17 17:50:46.455887: Epoch 731\n",
      "2025-05-17 17:50:46.456065: Current learning rate: 0.00307\n",
      "2025-05-17 17:52:35.824629: train_loss -0.9672\n",
      "2025-05-17 17:52:35.824783: val_loss -0.9507\n",
      "2025-05-17 17:52:35.824965: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-17 17:52:35.825013: Epoch time: 109.37 s\n",
      "2025-05-17 17:52:36.564358: \n",
      "2025-05-17 17:52:36.564460: Epoch 732\n",
      "2025-05-17 17:52:36.564587: Current learning rate: 0.00306\n",
      "2025-05-17 17:54:25.936805: train_loss -0.9683\n",
      "2025-05-17 17:54:25.936929: val_loss -0.9509\n",
      "2025-05-17 17:54:25.936964: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-17 17:54:25.937005: Epoch time: 109.37 s\n",
      "2025-05-17 17:54:26.481588: \n",
      "2025-05-17 17:54:26.481886: Epoch 733\n",
      "2025-05-17 17:54:26.481959: Current learning rate: 0.00305\n",
      "2025-05-17 17:56:15.847310: train_loss -0.9679\n",
      "2025-05-17 17:56:15.847433: val_loss -0.9467\n",
      "2025-05-17 17:56:15.847589: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 17:56:15.847844: Epoch time: 109.37 s\n",
      "2025-05-17 17:56:16.389211: \n",
      "2025-05-17 17:56:16.389412: Epoch 734\n",
      "2025-05-17 17:56:16.389706: Current learning rate: 0.00304\n",
      "2025-05-17 17:58:05.795513: train_loss -0.9676\n",
      "2025-05-17 17:58:05.795652: val_loss -0.9478\n",
      "2025-05-17 17:58:05.795692: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-17 17:58:05.795725: Epoch time: 109.41 s\n",
      "2025-05-17 17:58:06.331683: \n",
      "2025-05-17 17:58:06.331849: Epoch 735\n",
      "2025-05-17 17:58:06.331917: Current learning rate: 0.00303\n",
      "2025-05-17 17:59:55.720845: train_loss -0.969\n",
      "2025-05-17 17:59:55.721028: val_loss -0.9516\n",
      "2025-05-17 17:59:55.721196: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-17 17:59:55.721285: Epoch time: 109.39 s\n",
      "2025-05-17 17:59:56.265311: \n",
      "2025-05-17 17:59:56.265460: Epoch 736\n",
      "2025-05-17 17:59:56.265525: Current learning rate: 0.00302\n",
      "2025-05-17 18:01:45.653715: train_loss -0.969\n",
      "2025-05-17 18:01:45.653836: val_loss -0.9514\n",
      "2025-05-17 18:01:45.653868: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 18:01:45.654019: Epoch time: 109.39 s\n",
      "2025-05-17 18:01:46.207359: \n",
      "2025-05-17 18:01:46.207690: Epoch 737\n",
      "2025-05-17 18:01:46.207777: Current learning rate: 0.00301\n",
      "2025-05-17 18:03:35.550518: train_loss -0.9688\n",
      "2025-05-17 18:03:35.550714: val_loss -0.9484\n",
      "2025-05-17 18:03:35.550831: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-17 18:03:35.550873: Epoch time: 109.34 s\n",
      "2025-05-17 18:03:36.080907: \n",
      "2025-05-17 18:03:36.081108: Epoch 738\n",
      "2025-05-17 18:03:36.081180: Current learning rate: 0.003\n",
      "2025-05-17 18:05:25.426780: train_loss -0.9693\n",
      "2025-05-17 18:05:25.426983: val_loss -0.9461\n",
      "2025-05-17 18:05:25.427033: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-17 18:05:25.427066: Epoch time: 109.35 s\n",
      "2025-05-17 18:05:25.960034: \n",
      "2025-05-17 18:05:25.960202: Epoch 739\n",
      "2025-05-17 18:05:25.960329: Current learning rate: 0.00299\n",
      "2025-05-17 18:07:15.325545: train_loss -0.9686\n",
      "2025-05-17 18:07:15.325678: val_loss -0.9519\n",
      "2025-05-17 18:07:15.325786: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-17 18:07:15.325830: Epoch time: 109.37 s\n",
      "2025-05-17 18:07:15.862367: \n",
      "2025-05-17 18:07:15.862453: Epoch 740\n",
      "2025-05-17 18:07:15.862519: Current learning rate: 0.00297\n",
      "2025-05-17 18:09:05.157233: train_loss -0.9682\n",
      "2025-05-17 18:09:05.157362: val_loss -0.9483\n",
      "2025-05-17 18:09:05.157558: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 18:09:05.157650: Epoch time: 109.3 s\n",
      "2025-05-17 18:09:05.693618: \n",
      "2025-05-17 18:09:05.693701: Epoch 741\n",
      "2025-05-17 18:09:05.693801: Current learning rate: 0.00296\n",
      "2025-05-17 18:10:55.007009: train_loss -0.9679\n",
      "2025-05-17 18:10:55.007173: val_loss -0.9487\n",
      "2025-05-17 18:10:55.007207: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-17 18:10:55.007242: Epoch time: 109.31 s\n",
      "2025-05-17 18:10:55.007262: Yayy! New best EMA pseudo Dice: 0.9764000177383423\n",
      "2025-05-17 18:10:55.764801: \n",
      "2025-05-17 18:10:55.764880: Epoch 742\n",
      "2025-05-17 18:10:55.764944: Current learning rate: 0.00295\n",
      "2025-05-17 18:12:45.059527: train_loss -0.9682\n",
      "2025-05-17 18:12:45.059649: val_loss -0.954\n",
      "2025-05-17 18:12:45.059683: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-17 18:12:45.059717: Epoch time: 109.3 s\n",
      "2025-05-17 18:12:45.059739: Yayy! New best EMA pseudo Dice: 0.9767000079154968\n",
      "2025-05-17 18:12:45.996814: \n",
      "2025-05-17 18:12:45.996908: Epoch 743\n",
      "2025-05-17 18:12:45.996977: Current learning rate: 0.00294\n",
      "2025-05-17 18:14:34.971029: train_loss -0.969\n",
      "2025-05-17 18:14:34.971212: val_loss -0.9414\n",
      "2025-05-17 18:14:34.971362: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-17 18:14:34.971430: Epoch time: 108.97 s\n",
      "2025-05-17 18:14:35.508594: \n",
      "2025-05-17 18:14:35.508844: Epoch 744\n",
      "2025-05-17 18:14:35.508921: Current learning rate: 0.00293\n",
      "2025-05-17 18:16:24.496283: train_loss -0.9653\n",
      "2025-05-17 18:16:24.496541: val_loss -0.9451\n",
      "2025-05-17 18:16:24.496700: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-17 18:16:24.496771: Epoch time: 108.99 s\n",
      "2025-05-17 18:16:25.036249: \n",
      "2025-05-17 18:16:25.036524: Epoch 745\n",
      "2025-05-17 18:16:25.036681: Current learning rate: 0.00292\n",
      "2025-05-17 18:18:14.039724: train_loss -0.9668\n",
      "2025-05-17 18:18:14.039854: val_loss -0.9518\n",
      "2025-05-17 18:18:14.039889: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 18:18:14.039922: Epoch time: 109.0 s\n",
      "2025-05-17 18:18:14.577098: \n",
      "2025-05-17 18:18:14.577364: Epoch 746\n",
      "2025-05-17 18:18:14.577641: Current learning rate: 0.00291\n",
      "2025-05-17 18:20:03.597713: train_loss -0.9682\n",
      "2025-05-17 18:20:03.597844: val_loss -0.9547\n",
      "2025-05-17 18:20:03.597879: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-17 18:20:03.597912: Epoch time: 109.02 s\n",
      "2025-05-17 18:20:04.131210: \n",
      "2025-05-17 18:20:04.131467: Epoch 747\n",
      "2025-05-17 18:20:04.131595: Current learning rate: 0.0029\n",
      "2025-05-17 18:21:53.083582: train_loss -0.9693\n",
      "2025-05-17 18:21:53.083841: val_loss -0.95\n",
      "2025-05-17 18:21:53.084056: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 18:21:53.084120: Epoch time: 108.95 s\n",
      "2025-05-17 18:21:53.634591: \n",
      "2025-05-17 18:21:53.634840: Epoch 748\n",
      "2025-05-17 18:21:53.634925: Current learning rate: 0.00289\n",
      "2025-05-17 18:23:42.656932: train_loss -0.9699\n",
      "2025-05-17 18:23:42.657091: val_loss -0.9499\n",
      "2025-05-17 18:23:42.657845: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-17 18:23:42.657999: Epoch time: 109.02 s\n",
      "2025-05-17 18:23:43.194478: \n",
      "2025-05-17 18:23:43.194573: Epoch 749\n",
      "2025-05-17 18:23:43.194638: Current learning rate: 0.00288\n",
      "2025-05-17 18:25:32.213832: train_loss -0.9679\n",
      "2025-05-17 18:25:32.214017: val_loss -0.9458\n",
      "2025-05-17 18:25:32.214050: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 18:25:32.214083: Epoch time: 109.02 s\n",
      "2025-05-17 18:25:32.971513: \n",
      "2025-05-17 18:25:32.971610: Epoch 750\n",
      "2025-05-17 18:25:32.971683: Current learning rate: 0.00287\n",
      "2025-05-17 18:27:21.939746: train_loss -0.9689\n",
      "2025-05-17 18:27:21.939896: val_loss -0.9453\n",
      "2025-05-17 18:27:21.940010: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 18:27:21.940065: Epoch time: 108.97 s\n",
      "2025-05-17 18:27:22.476044: \n",
      "2025-05-17 18:27:22.476199: Epoch 751\n",
      "2025-05-17 18:27:22.476276: Current learning rate: 0.00286\n",
      "2025-05-17 18:29:11.485870: train_loss -0.9699\n",
      "2025-05-17 18:29:11.485994: val_loss -0.9519\n",
      "2025-05-17 18:29:11.486031: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-17 18:29:11.486066: Epoch time: 109.01 s\n",
      "2025-05-17 18:29:12.024838: \n",
      "2025-05-17 18:29:12.024965: Epoch 752\n",
      "2025-05-17 18:29:12.025068: Current learning rate: 0.00285\n",
      "2025-05-17 18:31:01.033014: train_loss -0.9695\n",
      "2025-05-17 18:31:01.033131: val_loss -0.9414\n",
      "2025-05-17 18:31:01.033163: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-17 18:31:01.033196: Epoch time: 109.01 s\n",
      "2025-05-17 18:31:01.569629: \n",
      "2025-05-17 18:31:01.569776: Epoch 753\n",
      "2025-05-17 18:31:01.569852: Current learning rate: 0.00284\n",
      "2025-05-17 18:32:50.492481: train_loss -0.9702\n",
      "2025-05-17 18:32:50.492606: val_loss -0.9511\n",
      "2025-05-17 18:32:50.492640: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-17 18:32:50.492674: Epoch time: 108.92 s\n",
      "2025-05-17 18:32:51.020343: \n",
      "2025-05-17 18:32:51.020421: Epoch 754\n",
      "2025-05-17 18:32:51.020481: Current learning rate: 0.00283\n",
      "2025-05-17 18:34:40.011497: train_loss -0.9701\n",
      "2025-05-17 18:34:40.011621: val_loss -0.9522\n",
      "2025-05-17 18:34:40.011654: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 18:34:40.011760: Epoch time: 108.99 s\n",
      "2025-05-17 18:34:40.550317: \n",
      "2025-05-17 18:34:40.550625: Epoch 755\n",
      "2025-05-17 18:34:40.550750: Current learning rate: 0.00282\n",
      "2025-05-17 18:36:29.636353: train_loss -0.9681\n",
      "2025-05-17 18:36:29.636480: val_loss -0.9418\n",
      "2025-05-17 18:36:29.636513: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 18:36:29.636545: Epoch time: 109.09 s\n",
      "2025-05-17 18:36:30.168833: \n",
      "2025-05-17 18:36:30.169147: Epoch 756\n",
      "2025-05-17 18:36:30.169250: Current learning rate: 0.00281\n",
      "2025-05-17 18:38:19.147189: train_loss -0.9676\n",
      "2025-05-17 18:38:19.147367: val_loss -0.9475\n",
      "2025-05-17 18:38:19.147404: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-17 18:38:19.147447: Epoch time: 108.98 s\n",
      "2025-05-17 18:38:19.691090: \n",
      "2025-05-17 18:38:19.691187: Epoch 757\n",
      "2025-05-17 18:38:19.691249: Current learning rate: 0.0028\n",
      "2025-05-17 18:40:08.649925: train_loss -0.9693\n",
      "2025-05-17 18:40:08.650081: val_loss -0.9495\n",
      "2025-05-17 18:40:08.650117: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 18:40:08.650151: Epoch time: 108.96 s\n",
      "2025-05-17 18:40:09.188517: \n",
      "2025-05-17 18:40:09.188628: Epoch 758\n",
      "2025-05-17 18:40:09.188696: Current learning rate: 0.00279\n",
      "2025-05-17 18:41:58.166178: train_loss -0.9692\n",
      "2025-05-17 18:41:58.166302: val_loss -0.9475\n",
      "2025-05-17 18:41:58.166337: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 18:41:58.166460: Epoch time: 108.98 s\n",
      "2025-05-17 18:41:58.705921: \n",
      "2025-05-17 18:41:58.706337: Epoch 759\n",
      "2025-05-17 18:41:58.706408: Current learning rate: 0.00278\n",
      "2025-05-17 18:43:47.725414: train_loss -0.9679\n",
      "2025-05-17 18:43:47.725981: val_loss -0.9514\n",
      "2025-05-17 18:43:47.726041: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-17 18:43:47.726081: Epoch time: 109.02 s\n",
      "2025-05-17 18:43:48.264048: \n",
      "2025-05-17 18:43:48.264266: Epoch 760\n",
      "2025-05-17 18:43:48.264372: Current learning rate: 0.00277\n",
      "2025-05-17 18:45:37.248338: train_loss -0.97\n",
      "2025-05-17 18:45:37.248537: val_loss -0.9505\n",
      "2025-05-17 18:45:37.248585: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-17 18:45:37.248621: Epoch time: 108.98 s\n",
      "2025-05-17 18:45:37.786513: \n",
      "2025-05-17 18:45:37.786618: Epoch 761\n",
      "2025-05-17 18:45:37.786706: Current learning rate: 0.00276\n",
      "2025-05-17 18:47:26.802398: train_loss -0.9684\n",
      "2025-05-17 18:47:26.802613: val_loss -0.9492\n",
      "2025-05-17 18:47:26.802683: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 18:47:26.802720: Epoch time: 109.02 s\n",
      "2025-05-17 18:47:27.335613: \n",
      "2025-05-17 18:47:27.335766: Epoch 762\n",
      "2025-05-17 18:47:27.335880: Current learning rate: 0.00275\n",
      "2025-05-17 18:49:16.348960: train_loss -0.9707\n",
      "2025-05-17 18:49:16.349169: val_loss -0.9487\n",
      "2025-05-17 18:49:16.349210: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 18:49:16.349245: Epoch time: 109.01 s\n",
      "2025-05-17 18:49:16.880711: \n",
      "2025-05-17 18:49:16.880812: Epoch 763\n",
      "2025-05-17 18:49:16.880883: Current learning rate: 0.00274\n",
      "2025-05-17 18:51:05.827490: train_loss -0.968\n",
      "2025-05-17 18:51:05.827902: val_loss -0.9505\n",
      "2025-05-17 18:51:05.827969: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 18:51:05.828042: Epoch time: 108.95 s\n",
      "2025-05-17 18:51:06.374782: \n",
      "2025-05-17 18:51:06.374865: Epoch 764\n",
      "2025-05-17 18:51:06.374929: Current learning rate: 0.00273\n",
      "2025-05-17 18:52:55.390138: train_loss -0.9707\n",
      "2025-05-17 18:52:55.390310: val_loss -0.9517\n",
      "2025-05-17 18:52:55.390343: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-17 18:52:55.390377: Epoch time: 109.02 s\n",
      "2025-05-17 18:52:55.926374: \n",
      "2025-05-17 18:52:55.926529: Epoch 765\n",
      "2025-05-17 18:52:55.926917: Current learning rate: 0.00272\n",
      "2025-05-17 18:54:44.951087: train_loss -0.9683\n",
      "2025-05-17 18:54:44.951236: val_loss -0.9472\n",
      "2025-05-17 18:54:44.951331: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 18:54:44.951377: Epoch time: 109.03 s\n",
      "2025-05-17 18:54:45.492027: \n",
      "2025-05-17 18:54:45.492105: Epoch 766\n",
      "2025-05-17 18:54:45.492167: Current learning rate: 0.00271\n",
      "2025-05-17 18:56:34.455322: train_loss -0.969\n",
      "2025-05-17 18:56:34.455478: val_loss -0.9525\n",
      "2025-05-17 18:56:34.455533: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-17 18:56:34.455573: Epoch time: 108.96 s\n",
      "2025-05-17 18:56:35.195318: \n",
      "2025-05-17 18:56:35.195471: Epoch 767\n",
      "2025-05-17 18:56:35.195549: Current learning rate: 0.0027\n",
      "2025-05-17 18:58:24.200525: train_loss -0.9694\n",
      "2025-05-17 18:58:24.200820: val_loss -0.9521\n",
      "2025-05-17 18:58:24.201005: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 18:58:24.201133: Epoch time: 109.01 s\n",
      "2025-05-17 18:58:24.748177: \n",
      "2025-05-17 18:58:24.748481: Epoch 768\n",
      "2025-05-17 18:58:24.748558: Current learning rate: 0.00268\n",
      "2025-05-17 19:00:13.765020: train_loss -0.9691\n",
      "2025-05-17 19:00:13.765162: val_loss -0.9491\n",
      "2025-05-17 19:00:13.765194: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-17 19:00:13.765228: Epoch time: 109.02 s\n",
      "2025-05-17 19:00:14.307475: \n",
      "2025-05-17 19:00:14.307635: Epoch 769\n",
      "2025-05-17 19:00:14.307706: Current learning rate: 0.00267\n",
      "2025-05-17 19:02:03.338433: train_loss -0.9708\n",
      "2025-05-17 19:02:03.338603: val_loss -0.9482\n",
      "2025-05-17 19:02:03.338636: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-17 19:02:03.338668: Epoch time: 109.03 s\n",
      "2025-05-17 19:02:03.892000: \n",
      "2025-05-17 19:02:03.892152: Epoch 770\n",
      "2025-05-17 19:02:03.892239: Current learning rate: 0.00266\n",
      "2025-05-17 19:03:52.897384: train_loss -0.9702\n",
      "2025-05-17 19:03:52.897510: val_loss -0.9464\n",
      "2025-05-17 19:03:52.897542: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-17 19:03:52.897575: Epoch time: 109.01 s\n",
      "2025-05-17 19:03:53.441552: \n",
      "2025-05-17 19:03:53.441713: Epoch 771\n",
      "2025-05-17 19:03:53.441981: Current learning rate: 0.00265\n",
      "2025-05-17 19:05:42.472300: train_loss -0.9673\n",
      "2025-05-17 19:05:42.472423: val_loss -0.9419\n",
      "2025-05-17 19:05:42.472457: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-17 19:05:42.472491: Epoch time: 109.03 s\n",
      "2025-05-17 19:05:43.029268: \n",
      "2025-05-17 19:05:43.029645: Epoch 772\n",
      "2025-05-17 19:05:43.029752: Current learning rate: 0.00264\n",
      "2025-05-17 19:07:32.064908: train_loss -0.9699\n",
      "2025-05-17 19:07:32.065035: val_loss -0.9494\n",
      "2025-05-17 19:07:32.065071: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-17 19:07:32.065106: Epoch time: 109.04 s\n",
      "2025-05-17 19:07:32.607629: \n",
      "2025-05-17 19:07:32.607720: Epoch 773\n",
      "2025-05-17 19:07:32.607782: Current learning rate: 0.00263\n",
      "2025-05-17 19:09:21.600347: train_loss -0.9697\n",
      "2025-05-17 19:09:21.600554: val_loss -0.9495\n",
      "2025-05-17 19:09:21.600604: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 19:09:21.600643: Epoch time: 108.99 s\n",
      "2025-05-17 19:09:22.135339: \n",
      "2025-05-17 19:09:22.135543: Epoch 774\n",
      "2025-05-17 19:09:22.135696: Current learning rate: 0.00262\n",
      "2025-05-17 19:11:11.135816: train_loss -0.9687\n",
      "2025-05-17 19:11:11.135941: val_loss -0.9485\n",
      "2025-05-17 19:11:11.135974: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 19:11:11.136008: Epoch time: 109.0 s\n",
      "2025-05-17 19:11:11.681989: \n",
      "2025-05-17 19:11:11.682385: Epoch 775\n",
      "2025-05-17 19:11:11.682476: Current learning rate: 0.00261\n",
      "2025-05-17 19:13:00.706583: train_loss -0.9701\n",
      "2025-05-17 19:13:00.706727: val_loss -0.948\n",
      "2025-05-17 19:13:00.706761: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-17 19:13:00.706795: Epoch time: 109.03 s\n",
      "2025-05-17 19:13:01.242062: \n",
      "2025-05-17 19:13:01.242145: Epoch 776\n",
      "2025-05-17 19:13:01.242209: Current learning rate: 0.0026\n",
      "2025-05-17 19:14:50.243870: train_loss -0.9684\n",
      "2025-05-17 19:14:50.244009: val_loss -0.9457\n",
      "2025-05-17 19:14:50.244050: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 19:14:50.244084: Epoch time: 109.0 s\n",
      "2025-05-17 19:14:50.806975: \n",
      "2025-05-17 19:14:50.807151: Epoch 777\n",
      "2025-05-17 19:14:50.807237: Current learning rate: 0.00259\n",
      "2025-05-17 19:16:39.762209: train_loss -0.9692\n",
      "2025-05-17 19:16:39.762402: val_loss -0.9496\n",
      "2025-05-17 19:16:39.762436: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-17 19:16:39.762470: Epoch time: 108.96 s\n",
      "2025-05-17 19:16:40.300708: \n",
      "2025-05-17 19:16:40.300782: Epoch 778\n",
      "2025-05-17 19:16:40.300843: Current learning rate: 0.00258\n",
      "2025-05-17 19:18:29.289882: train_loss -0.969\n",
      "2025-05-17 19:18:29.290008: val_loss -0.9486\n",
      "2025-05-17 19:18:29.290044: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 19:18:29.290079: Epoch time: 108.99 s\n",
      "2025-05-17 19:18:30.035361: \n",
      "2025-05-17 19:18:30.035450: Epoch 779\n",
      "2025-05-17 19:18:30.035529: Current learning rate: 0.00257\n",
      "2025-05-17 19:20:19.035037: train_loss -0.9692\n",
      "2025-05-17 19:20:19.035223: val_loss -0.9516\n",
      "2025-05-17 19:20:19.035258: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-17 19:20:19.035291: Epoch time: 109.0 s\n",
      "2025-05-17 19:20:19.571786: \n",
      "2025-05-17 19:20:19.571882: Epoch 780\n",
      "2025-05-17 19:20:19.571944: Current learning rate: 0.00256\n",
      "2025-05-17 19:22:08.537389: train_loss -0.9696\n",
      "2025-05-17 19:22:08.537527: val_loss -0.9476\n",
      "2025-05-17 19:22:08.537563: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 19:22:08.537595: Epoch time: 108.97 s\n",
      "2025-05-17 19:22:09.079847: \n",
      "2025-05-17 19:22:09.080130: Epoch 781\n",
      "2025-05-17 19:22:09.080251: Current learning rate: 0.00255\n",
      "2025-05-17 19:23:58.114765: train_loss -0.9693\n",
      "2025-05-17 19:23:58.114897: val_loss -0.9483\n",
      "2025-05-17 19:23:58.114930: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 19:23:58.114964: Epoch time: 109.04 s\n",
      "2025-05-17 19:23:58.655573: \n",
      "2025-05-17 19:23:58.655884: Epoch 782\n",
      "2025-05-17 19:23:58.655961: Current learning rate: 0.00254\n",
      "2025-05-17 19:25:47.660110: train_loss -0.9695\n",
      "2025-05-17 19:25:47.660225: val_loss -0.9481\n",
      "2025-05-17 19:25:47.660255: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 19:25:47.660287: Epoch time: 109.01 s\n",
      "2025-05-17 19:25:48.207656: \n",
      "2025-05-17 19:25:48.207826: Epoch 783\n",
      "2025-05-17 19:25:48.207960: Current learning rate: 0.00253\n",
      "2025-05-17 19:27:37.144473: train_loss -0.9686\n",
      "2025-05-17 19:27:37.144604: val_loss -0.9432\n",
      "2025-05-17 19:27:37.144637: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 19:27:37.144670: Epoch time: 108.94 s\n",
      "2025-05-17 19:27:37.687089: \n",
      "2025-05-17 19:27:37.687339: Epoch 784\n",
      "2025-05-17 19:27:37.687525: Current learning rate: 0.00252\n",
      "2025-05-17 19:29:26.675017: train_loss -0.962\n",
      "2025-05-17 19:29:26.675242: val_loss -0.9426\n",
      "2025-05-17 19:29:26.675379: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-17 19:29:26.675425: Epoch time: 108.99 s\n",
      "2025-05-17 19:29:27.212980: \n",
      "2025-05-17 19:29:27.213077: Epoch 785\n",
      "2025-05-17 19:29:27.213197: Current learning rate: 0.00251\n",
      "2025-05-17 19:31:16.207625: train_loss -0.9598\n",
      "2025-05-17 19:31:16.207899: val_loss -0.9429\n",
      "2025-05-17 19:31:16.207983: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-17 19:31:16.208052: Epoch time: 109.0 s\n",
      "2025-05-17 19:31:16.753634: \n",
      "2025-05-17 19:31:16.753950: Epoch 786\n",
      "2025-05-17 19:31:16.754049: Current learning rate: 0.0025\n",
      "2025-05-17 19:33:05.771863: train_loss -0.9616\n",
      "2025-05-17 19:33:05.771991: val_loss -0.943\n",
      "2025-05-17 19:33:05.772026: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 19:33:05.772060: Epoch time: 109.02 s\n",
      "2025-05-17 19:33:06.309887: \n",
      "2025-05-17 19:33:06.310018: Epoch 787\n",
      "2025-05-17 19:33:06.310086: Current learning rate: 0.00249\n",
      "2025-05-17 19:34:55.351839: train_loss -0.966\n",
      "2025-05-17 19:34:55.352032: val_loss -0.9497\n",
      "2025-05-17 19:34:55.352074: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 19:34:55.352107: Epoch time: 109.04 s\n",
      "2025-05-17 19:34:55.884348: \n",
      "2025-05-17 19:34:55.884493: Epoch 788\n",
      "2025-05-17 19:34:55.884563: Current learning rate: 0.00248\n",
      "2025-05-17 19:36:44.908700: train_loss -0.967\n",
      "2025-05-17 19:36:44.908820: val_loss -0.9514\n",
      "2025-05-17 19:36:44.908854: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-17 19:36:44.908887: Epoch time: 109.02 s\n",
      "2025-05-17 19:36:45.445790: \n",
      "2025-05-17 19:36:45.446149: Epoch 789\n",
      "2025-05-17 19:36:45.446250: Current learning rate: 0.00247\n",
      "2025-05-17 19:38:34.413994: train_loss -0.9635\n",
      "2025-05-17 19:38:34.414167: val_loss -0.9477\n",
      "2025-05-17 19:38:34.414200: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 19:38:34.414232: Epoch time: 108.97 s\n",
      "2025-05-17 19:38:34.955285: \n",
      "2025-05-17 19:38:34.955558: Epoch 790\n",
      "2025-05-17 19:38:34.955634: Current learning rate: 0.00245\n",
      "2025-05-17 19:40:23.937512: train_loss -0.966\n",
      "2025-05-17 19:40:23.937647: val_loss -0.9443\n",
      "2025-05-17 19:40:23.937686: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 19:40:23.937719: Epoch time: 108.98 s\n",
      "2025-05-17 19:40:24.689749: \n",
      "2025-05-17 19:40:24.689858: Epoch 791\n",
      "2025-05-17 19:40:24.689989: Current learning rate: 0.00244\n",
      "2025-05-17 19:42:13.706391: train_loss -0.9658\n",
      "2025-05-17 19:42:13.706528: val_loss -0.9499\n",
      "2025-05-17 19:42:13.706562: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-17 19:42:13.706596: Epoch time: 109.02 s\n",
      "2025-05-17 19:42:14.251833: \n",
      "2025-05-17 19:42:14.252089: Epoch 792\n",
      "2025-05-17 19:42:14.252229: Current learning rate: 0.00243\n",
      "2025-05-17 19:44:03.285687: train_loss -0.9683\n",
      "2025-05-17 19:44:03.285922: val_loss -0.943\n",
      "2025-05-17 19:44:03.286087: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-17 19:44:03.286139: Epoch time: 109.03 s\n",
      "2025-05-17 19:44:03.830334: \n",
      "2025-05-17 19:44:03.830566: Epoch 793\n",
      "2025-05-17 19:44:03.830733: Current learning rate: 0.00242\n",
      "2025-05-17 19:45:52.805512: train_loss -0.9681\n",
      "2025-05-17 19:45:52.805636: val_loss -0.9444\n",
      "2025-05-17 19:45:52.805675: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 19:45:52.805711: Epoch time: 108.98 s\n",
      "2025-05-17 19:45:53.340866: \n",
      "2025-05-17 19:45:53.341038: Epoch 794\n",
      "2025-05-17 19:45:53.341156: Current learning rate: 0.00241\n",
      "2025-05-17 19:47:42.337147: train_loss -0.9691\n",
      "2025-05-17 19:47:42.337346: val_loss -0.9464\n",
      "2025-05-17 19:47:42.337394: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-17 19:47:42.337430: Epoch time: 109.0 s\n",
      "2025-05-17 19:47:42.897306: \n",
      "2025-05-17 19:47:42.897785: Epoch 795\n",
      "2025-05-17 19:47:42.897883: Current learning rate: 0.0024\n",
      "2025-05-17 19:49:31.921810: train_loss -0.9696\n",
      "2025-05-17 19:49:31.921982: val_loss -0.9524\n",
      "2025-05-17 19:49:31.922015: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-17 19:49:31.922048: Epoch time: 109.03 s\n",
      "2025-05-17 19:49:32.462720: \n",
      "2025-05-17 19:49:32.462817: Epoch 796\n",
      "2025-05-17 19:49:32.462879: Current learning rate: 0.00239\n",
      "2025-05-17 19:51:21.464683: train_loss -0.9705\n",
      "2025-05-17 19:51:21.464833: val_loss -0.9525\n",
      "2025-05-17 19:51:21.464983: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 19:51:21.465043: Epoch time: 109.0 s\n",
      "2025-05-17 19:51:22.001973: \n",
      "2025-05-17 19:51:22.002267: Epoch 797\n",
      "2025-05-17 19:51:22.002341: Current learning rate: 0.00238\n",
      "2025-05-17 19:53:11.075901: train_loss -0.9691\n",
      "2025-05-17 19:53:11.076020: val_loss -0.9501\n",
      "2025-05-17 19:53:11.076055: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-17 19:53:11.076090: Epoch time: 109.07 s\n",
      "2025-05-17 19:53:11.627357: \n",
      "2025-05-17 19:53:11.627494: Epoch 798\n",
      "2025-05-17 19:53:11.627570: Current learning rate: 0.00237\n",
      "2025-05-17 19:55:00.694452: train_loss -0.9692\n",
      "2025-05-17 19:55:00.694574: val_loss -0.9483\n",
      "2025-05-17 19:55:00.694608: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 19:55:00.694642: Epoch time: 109.07 s\n",
      "2025-05-17 19:55:01.237804: \n",
      "2025-05-17 19:55:01.237928: Epoch 799\n",
      "2025-05-17 19:55:01.238001: Current learning rate: 0.00236\n",
      "2025-05-17 19:56:50.263345: train_loss -0.9679\n",
      "2025-05-17 19:56:50.263465: val_loss -0.9508\n",
      "2025-05-17 19:56:50.263498: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 19:56:50.263532: Epoch time: 109.03 s\n",
      "2025-05-17 19:56:51.030566: \n",
      "2025-05-17 19:56:51.030808: Epoch 800\n",
      "2025-05-17 19:56:51.030887: Current learning rate: 0.00235\n",
      "2025-05-17 19:58:40.024086: train_loss -0.9695\n",
      "2025-05-17 19:58:40.024225: val_loss -0.9519\n",
      "2025-05-17 19:58:40.024259: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 19:58:40.024295: Epoch time: 108.99 s\n",
      "2025-05-17 19:58:40.560884: \n",
      "2025-05-17 19:58:40.560968: Epoch 801\n",
      "2025-05-17 19:58:40.561033: Current learning rate: 0.00234\n",
      "2025-05-17 20:00:29.535925: train_loss -0.9688\n",
      "2025-05-17 20:00:29.536059: val_loss -0.9489\n",
      "2025-05-17 20:00:29.536093: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-17 20:00:29.536127: Epoch time: 108.98 s\n",
      "2025-05-17 20:00:30.071274: \n",
      "2025-05-17 20:00:30.071404: Epoch 802\n",
      "2025-05-17 20:00:30.071481: Current learning rate: 0.00233\n",
      "2025-05-17 20:02:19.072296: train_loss -0.9685\n",
      "2025-05-17 20:02:19.072437: val_loss -0.9481\n",
      "2025-05-17 20:02:19.072479: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 20:02:19.072517: Epoch time: 109.0 s\n",
      "2025-05-17 20:02:19.806363: \n",
      "2025-05-17 20:02:19.806467: Epoch 803\n",
      "2025-05-17 20:02:19.806530: Current learning rate: 0.00232\n",
      "2025-05-17 20:04:08.767525: train_loss -0.9696\n",
      "2025-05-17 20:04:08.767666: val_loss -0.9538\n",
      "2025-05-17 20:04:08.767703: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-17 20:04:08.767736: Epoch time: 108.96 s\n",
      "2025-05-17 20:04:09.325175: \n",
      "2025-05-17 20:04:09.325272: Epoch 804\n",
      "2025-05-17 20:04:09.325341: Current learning rate: 0.00231\n",
      "2025-05-17 20:05:58.322966: train_loss -0.9696\n",
      "2025-05-17 20:05:58.323091: val_loss -0.9474\n",
      "2025-05-17 20:05:58.323124: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-17 20:05:58.323157: Epoch time: 109.0 s\n",
      "2025-05-17 20:05:58.870006: \n",
      "2025-05-17 20:05:58.870362: Epoch 805\n",
      "2025-05-17 20:05:58.870451: Current learning rate: 0.0023\n",
      "2025-05-17 20:07:47.946106: train_loss -0.97\n",
      "2025-05-17 20:07:47.946210: val_loss -0.9469\n",
      "2025-05-17 20:07:47.946238: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 20:07:47.946278: Epoch time: 109.08 s\n",
      "2025-05-17 20:07:48.494853: \n",
      "2025-05-17 20:07:48.494958: Epoch 806\n",
      "2025-05-17 20:07:48.495023: Current learning rate: 0.00229\n",
      "2025-05-17 20:09:37.475192: train_loss -0.9721\n",
      "2025-05-17 20:09:37.475340: val_loss -0.9483\n",
      "2025-05-17 20:09:37.475382: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 20:09:37.475416: Epoch time: 108.98 s\n",
      "2025-05-17 20:09:38.022285: \n",
      "2025-05-17 20:09:38.022517: Epoch 807\n",
      "2025-05-17 20:09:38.022580: Current learning rate: 0.00228\n",
      "2025-05-17 20:11:27.023305: train_loss -0.9713\n",
      "2025-05-17 20:11:27.023428: val_loss -0.9473\n",
      "2025-05-17 20:11:27.023461: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-17 20:11:27.023492: Epoch time: 109.0 s\n",
      "2025-05-17 20:11:27.572064: \n",
      "2025-05-17 20:11:27.572160: Epoch 808\n",
      "2025-05-17 20:11:27.572229: Current learning rate: 0.00226\n",
      "2025-05-17 20:13:16.573525: train_loss -0.9716\n",
      "2025-05-17 20:13:16.573726: val_loss -0.9516\n",
      "2025-05-17 20:13:16.573758: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-17 20:13:16.573807: Epoch time: 109.0 s\n",
      "2025-05-17 20:13:17.116381: \n",
      "2025-05-17 20:13:17.116578: Epoch 809\n",
      "2025-05-17 20:13:17.116668: Current learning rate: 0.00225\n",
      "2025-05-17 20:15:06.072863: train_loss -0.9705\n",
      "2025-05-17 20:15:06.072994: val_loss -0.9506\n",
      "2025-05-17 20:15:06.073030: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-17 20:15:06.073062: Epoch time: 108.96 s\n",
      "2025-05-17 20:15:06.619164: \n",
      "2025-05-17 20:15:06.619251: Epoch 810\n",
      "2025-05-17 20:15:06.619316: Current learning rate: 0.00224\n",
      "2025-05-17 20:16:55.618973: train_loss -0.9694\n",
      "2025-05-17 20:16:55.619098: val_loss -0.9539\n",
      "2025-05-17 20:16:55.619132: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-17 20:16:55.619164: Epoch time: 109.0 s\n",
      "2025-05-17 20:16:55.619187: Yayy! New best EMA pseudo Dice: 0.9767000079154968\n",
      "2025-05-17 20:16:56.379585: \n",
      "2025-05-17 20:16:56.379731: Epoch 811\n",
      "2025-05-17 20:16:56.379812: Current learning rate: 0.00223\n",
      "2025-05-17 20:18:45.426163: train_loss -0.9701\n",
      "2025-05-17 20:18:45.426296: val_loss -0.9503\n",
      "2025-05-17 20:18:45.426331: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-17 20:18:45.426364: Epoch time: 109.05 s\n",
      "2025-05-17 20:18:45.426385: Yayy! New best EMA pseudo Dice: 0.9768000245094299\n",
      "2025-05-17 20:18:46.190382: \n",
      "2025-05-17 20:18:46.190527: Epoch 812\n",
      "2025-05-17 20:18:46.190599: Current learning rate: 0.00222\n",
      "2025-05-17 20:20:35.187983: train_loss -0.9707\n",
      "2025-05-17 20:20:35.188112: val_loss -0.9501\n",
      "2025-05-17 20:20:35.188146: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-17 20:20:35.188179: Epoch time: 109.0 s\n",
      "2025-05-17 20:20:35.188201: Yayy! New best EMA pseudo Dice: 0.9768000245094299\n",
      "2025-05-17 20:20:35.958457: \n",
      "2025-05-17 20:20:35.958611: Epoch 813\n",
      "2025-05-17 20:20:35.958687: Current learning rate: 0.00221\n",
      "2025-05-17 20:22:24.921561: train_loss -0.9698\n",
      "2025-05-17 20:22:24.921692: val_loss -0.9544\n",
      "2025-05-17 20:22:24.921726: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-17 20:22:24.921759: Epoch time: 108.96 s\n",
      "2025-05-17 20:22:24.921780: Yayy! New best EMA pseudo Dice: 0.9769999980926514\n",
      "2025-05-17 20:22:25.873022: \n",
      "2025-05-17 20:22:25.873338: Epoch 814\n",
      "2025-05-17 20:22:25.873424: Current learning rate: 0.0022\n",
      "2025-05-17 20:24:14.873431: train_loss -0.9709\n",
      "2025-05-17 20:24:14.873637: val_loss -0.9484\n",
      "2025-05-17 20:24:14.873680: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-17 20:24:14.873713: Epoch time: 109.0 s\n",
      "2025-05-17 20:24:15.424094: \n",
      "2025-05-17 20:24:15.424190: Epoch 815\n",
      "2025-05-17 20:24:15.424253: Current learning rate: 0.00219\n",
      "2025-05-17 20:26:04.412422: train_loss -0.9707\n",
      "2025-05-17 20:26:04.412548: val_loss -0.9468\n",
      "2025-05-17 20:26:04.412581: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 20:26:04.412615: Epoch time: 108.99 s\n",
      "2025-05-17 20:26:04.964728: \n",
      "2025-05-17 20:26:04.964838: Epoch 816\n",
      "2025-05-17 20:26:04.964906: Current learning rate: 0.00218\n",
      "2025-05-17 20:27:53.934738: train_loss -0.971\n",
      "2025-05-17 20:27:53.934863: val_loss -0.9465\n",
      "2025-05-17 20:27:53.934896: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 20:27:53.934930: Epoch time: 108.97 s\n",
      "2025-05-17 20:27:54.482848: \n",
      "2025-05-17 20:27:54.482944: Epoch 817\n",
      "2025-05-17 20:27:54.483009: Current learning rate: 0.00217\n",
      "2025-05-17 20:29:43.469932: train_loss -0.9703\n",
      "2025-05-17 20:29:43.470085: val_loss -0.9489\n",
      "2025-05-17 20:29:43.470126: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-17 20:29:43.470161: Epoch time: 108.99 s\n",
      "2025-05-17 20:29:44.026380: \n",
      "2025-05-17 20:29:44.026571: Epoch 818\n",
      "2025-05-17 20:29:44.026640: Current learning rate: 0.00216\n",
      "2025-05-17 20:31:33.040487: train_loss -0.9715\n",
      "2025-05-17 20:31:33.040696: val_loss -0.9478\n",
      "2025-05-17 20:31:33.040865: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-17 20:31:33.040913: Epoch time: 109.01 s\n",
      "2025-05-17 20:31:33.586037: \n",
      "2025-05-17 20:31:33.586191: Epoch 819\n",
      "2025-05-17 20:31:33.586267: Current learning rate: 0.00215\n",
      "2025-05-17 20:33:22.580649: train_loss -0.969\n",
      "2025-05-17 20:33:22.580783: val_loss -0.9542\n",
      "2025-05-17 20:33:22.580816: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 20:33:22.580850: Epoch time: 109.0 s\n",
      "2025-05-17 20:33:23.108427: \n",
      "2025-05-17 20:33:23.108562: Epoch 820\n",
      "2025-05-17 20:33:23.108632: Current learning rate: 0.00214\n",
      "2025-05-17 20:35:12.090079: train_loss -0.9675\n",
      "2025-05-17 20:35:12.090237: val_loss -0.9462\n",
      "2025-05-17 20:35:12.090381: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-17 20:35:12.090445: Epoch time: 108.98 s\n",
      "2025-05-17 20:35:12.605215: \n",
      "2025-05-17 20:35:12.605358: Epoch 821\n",
      "2025-05-17 20:35:12.605422: Current learning rate: 0.00213\n",
      "2025-05-17 20:37:01.620791: train_loss -0.9692\n",
      "2025-05-17 20:37:01.621062: val_loss -0.9401\n",
      "2025-05-17 20:37:01.621103: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-17 20:37:01.621175: Epoch time: 109.02 s\n",
      "2025-05-17 20:37:02.146708: \n",
      "2025-05-17 20:37:02.146834: Epoch 822\n",
      "2025-05-17 20:37:02.146899: Current learning rate: 0.00212\n",
      "2025-05-17 20:38:51.144983: train_loss -0.97\n",
      "2025-05-17 20:38:51.145215: val_loss -0.9509\n",
      "2025-05-17 20:38:51.145260: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 20:38:51.145293: Epoch time: 109.0 s\n",
      "2025-05-17 20:38:51.670893: \n",
      "2025-05-17 20:38:51.670978: Epoch 823\n",
      "2025-05-17 20:38:51.671042: Current learning rate: 0.0021\n",
      "2025-05-17 20:40:40.628852: train_loss -0.971\n",
      "2025-05-17 20:40:40.628980: val_loss -0.9535\n",
      "2025-05-17 20:40:40.629013: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-17 20:40:40.629048: Epoch time: 108.96 s\n",
      "2025-05-17 20:40:41.145234: \n",
      "2025-05-17 20:40:41.145331: Epoch 824\n",
      "2025-05-17 20:40:41.145416: Current learning rate: 0.00209\n",
      "2025-05-17 20:42:30.107167: train_loss -0.9708\n",
      "2025-05-17 20:42:30.107304: val_loss -0.9487\n",
      "2025-05-17 20:42:30.107345: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 20:42:30.107385: Epoch time: 108.96 s\n",
      "2025-05-17 20:42:30.624976: \n",
      "2025-05-17 20:42:30.625052: Epoch 825\n",
      "2025-05-17 20:42:30.625114: Current learning rate: 0.00208\n",
      "2025-05-17 20:44:19.621403: train_loss -0.9705\n",
      "2025-05-17 20:44:19.621606: val_loss -0.9509\n",
      "2025-05-17 20:44:19.621642: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 20:44:19.621676: Epoch time: 109.0 s\n",
      "2025-05-17 20:44:20.344453: \n",
      "2025-05-17 20:44:20.344640: Epoch 826\n",
      "2025-05-17 20:44:20.344713: Current learning rate: 0.00207\n",
      "2025-05-17 20:46:09.381694: train_loss -0.9719\n",
      "2025-05-17 20:46:09.381902: val_loss -0.9462\n",
      "2025-05-17 20:46:09.381937: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 20:46:09.381975: Epoch time: 109.04 s\n",
      "2025-05-17 20:46:09.900682: \n",
      "2025-05-17 20:46:09.900944: Epoch 827\n",
      "2025-05-17 20:46:09.901020: Current learning rate: 0.00206\n",
      "2025-05-17 20:47:58.915217: train_loss -0.9703\n",
      "2025-05-17 20:47:58.915344: val_loss -0.9508\n",
      "2025-05-17 20:47:58.915380: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 20:47:58.915412: Epoch time: 109.02 s\n",
      "2025-05-17 20:47:59.428791: \n",
      "2025-05-17 20:47:59.429004: Epoch 828\n",
      "2025-05-17 20:47:59.429131: Current learning rate: 0.00205\n",
      "2025-05-17 20:49:48.445358: train_loss -0.9716\n",
      "2025-05-17 20:49:48.445477: val_loss -0.9511\n",
      "2025-05-17 20:49:48.445509: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-17 20:49:48.445543: Epoch time: 109.02 s\n",
      "2025-05-17 20:49:48.972670: \n",
      "2025-05-17 20:49:48.972767: Epoch 829\n",
      "2025-05-17 20:49:48.972831: Current learning rate: 0.00204\n",
      "2025-05-17 20:51:37.972577: train_loss -0.9705\n",
      "2025-05-17 20:51:37.972708: val_loss -0.9486\n",
      "2025-05-17 20:51:37.972742: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 20:51:37.972774: Epoch time: 109.0 s\n",
      "2025-05-17 20:51:38.488973: \n",
      "2025-05-17 20:51:38.489113: Epoch 830\n",
      "2025-05-17 20:51:38.489177: Current learning rate: 0.00203\n",
      "2025-05-17 20:53:27.506825: train_loss -0.9695\n",
      "2025-05-17 20:53:27.507206: val_loss -0.9451\n",
      "2025-05-17 20:53:27.507283: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 20:53:27.507325: Epoch time: 109.02 s\n",
      "2025-05-17 20:53:28.032559: \n",
      "2025-05-17 20:53:28.032681: Epoch 831\n",
      "2025-05-17 20:53:28.032832: Current learning rate: 0.00202\n",
      "2025-05-17 20:55:17.061647: train_loss -0.9705\n",
      "2025-05-17 20:55:17.061828: val_loss -0.947\n",
      "2025-05-17 20:55:17.062032: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-17 20:55:17.062261: Epoch time: 109.03 s\n",
      "2025-05-17 20:55:17.585225: \n",
      "2025-05-17 20:55:17.585580: Epoch 832\n",
      "2025-05-17 20:55:17.585814: Current learning rate: 0.00201\n",
      "2025-05-17 20:57:06.555426: train_loss -0.9719\n",
      "2025-05-17 20:57:06.555654: val_loss -0.9505\n",
      "2025-05-17 20:57:06.555770: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 20:57:06.555834: Epoch time: 108.97 s\n",
      "2025-05-17 20:57:07.083350: \n",
      "2025-05-17 20:57:07.083706: Epoch 833\n",
      "2025-05-17 20:57:07.083835: Current learning rate: 0.002\n",
      "2025-05-17 20:58:56.149047: train_loss -0.9713\n",
      "2025-05-17 20:58:56.149242: val_loss -0.9483\n",
      "2025-05-17 20:58:56.149392: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 20:58:56.149460: Epoch time: 109.07 s\n",
      "2025-05-17 20:58:56.668969: \n",
      "2025-05-17 20:58:56.669124: Epoch 834\n",
      "2025-05-17 20:58:56.669196: Current learning rate: 0.00199\n",
      "2025-05-17 21:00:45.717065: train_loss -0.9712\n",
      "2025-05-17 21:00:45.717268: val_loss -0.9502\n",
      "2025-05-17 21:00:45.717311: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-17 21:00:45.717345: Epoch time: 109.05 s\n",
      "2025-05-17 21:00:46.244038: \n",
      "2025-05-17 21:00:46.244210: Epoch 835\n",
      "2025-05-17 21:00:46.244287: Current learning rate: 0.00198\n",
      "2025-05-17 21:02:35.233157: train_loss -0.9722\n",
      "2025-05-17 21:02:35.233286: val_loss -0.951\n",
      "2025-05-17 21:02:35.233322: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 21:02:35.233356: Epoch time: 108.99 s\n",
      "2025-05-17 21:02:35.757699: \n",
      "2025-05-17 21:02:35.757894: Epoch 836\n",
      "2025-05-17 21:02:35.757984: Current learning rate: 0.00196\n",
      "2025-05-17 21:04:24.753129: train_loss -0.9713\n",
      "2025-05-17 21:04:24.753251: val_loss -0.9507\n",
      "2025-05-17 21:04:24.753284: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 21:04:24.753315: Epoch time: 109.0 s\n",
      "2025-05-17 21:04:25.261731: \n",
      "2025-05-17 21:04:25.261842: Epoch 837\n",
      "2025-05-17 21:04:25.261952: Current learning rate: 0.00195\n",
      "2025-05-17 21:06:14.204256: train_loss -0.972\n",
      "2025-05-17 21:06:14.204376: val_loss -0.9534\n",
      "2025-05-17 21:06:14.204411: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-17 21:06:14.204443: Epoch time: 108.94 s\n",
      "2025-05-17 21:06:14.721987: \n",
      "2025-05-17 21:06:14.722066: Epoch 838\n",
      "2025-05-17 21:06:14.722129: Current learning rate: 0.00194\n",
      "2025-05-17 21:08:03.773253: train_loss -0.9716\n",
      "2025-05-17 21:08:03.773432: val_loss -0.9507\n",
      "2025-05-17 21:08:03.773467: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-17 21:08:03.773499: Epoch time: 109.05 s\n",
      "2025-05-17 21:08:04.489208: \n",
      "2025-05-17 21:08:04.489609: Epoch 839\n",
      "2025-05-17 21:08:04.489712: Current learning rate: 0.00193\n",
      "2025-05-17 21:09:53.494252: train_loss -0.9704\n",
      "2025-05-17 21:09:53.494383: val_loss -0.9524\n",
      "2025-05-17 21:09:53.494416: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 21:09:53.494449: Epoch time: 109.01 s\n",
      "2025-05-17 21:09:54.037158: \n",
      "2025-05-17 21:09:54.037421: Epoch 840\n",
      "2025-05-17 21:09:54.037502: Current learning rate: 0.00192\n",
      "2025-05-17 21:11:43.093245: train_loss -0.9699\n",
      "2025-05-17 21:11:43.093393: val_loss -0.95\n",
      "2025-05-17 21:11:43.093432: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-17 21:11:43.093466: Epoch time: 109.06 s\n",
      "2025-05-17 21:11:43.614411: \n",
      "2025-05-17 21:11:43.614510: Epoch 841\n",
      "2025-05-17 21:11:43.614574: Current learning rate: 0.00191\n",
      "2025-05-17 21:13:32.690047: train_loss -0.9711\n",
      "2025-05-17 21:13:32.690213: val_loss -0.944\n",
      "2025-05-17 21:13:32.690349: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-17 21:13:32.690438: Epoch time: 109.08 s\n",
      "2025-05-17 21:13:33.219774: \n",
      "2025-05-17 21:13:33.220095: Epoch 842\n",
      "2025-05-17 21:13:33.220202: Current learning rate: 0.0019\n",
      "2025-05-17 21:15:22.232019: train_loss -0.9713\n",
      "2025-05-17 21:15:22.232143: val_loss -0.946\n",
      "2025-05-17 21:15:22.232178: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 21:15:22.232209: Epoch time: 109.01 s\n",
      "2025-05-17 21:15:22.752980: \n",
      "2025-05-17 21:15:22.753286: Epoch 843\n",
      "2025-05-17 21:15:22.753370: Current learning rate: 0.00189\n",
      "2025-05-17 21:17:11.791269: train_loss -0.9719\n",
      "2025-05-17 21:17:11.791533: val_loss -0.9513\n",
      "2025-05-17 21:17:11.791605: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-17 21:17:11.791646: Epoch time: 109.04 s\n",
      "2025-05-17 21:17:12.317338: \n",
      "2025-05-17 21:17:12.317428: Epoch 844\n",
      "2025-05-17 21:17:12.317497: Current learning rate: 0.00188\n",
      "2025-05-17 21:19:01.338894: train_loss -0.967\n",
      "2025-05-17 21:19:01.339015: val_loss -0.9398\n",
      "2025-05-17 21:19:01.339048: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-17 21:19:01.339081: Epoch time: 109.02 s\n",
      "2025-05-17 21:19:01.860240: \n",
      "2025-05-17 21:19:01.860434: Epoch 845\n",
      "2025-05-17 21:19:01.860511: Current learning rate: 0.00187\n",
      "2025-05-17 21:20:50.849272: train_loss -0.9617\n",
      "2025-05-17 21:20:50.849443: val_loss -0.9473\n",
      "2025-05-17 21:20:50.849476: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 21:20:50.849574: Epoch time: 108.99 s\n",
      "2025-05-17 21:20:51.371975: \n",
      "2025-05-17 21:20:51.372123: Epoch 846\n",
      "2025-05-17 21:20:51.372190: Current learning rate: 0.00186\n",
      "2025-05-17 21:22:40.374433: train_loss -0.9641\n",
      "2025-05-17 21:22:40.374554: val_loss -0.9454\n",
      "2025-05-17 21:22:40.374589: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-17 21:22:40.374622: Epoch time: 109.0 s\n",
      "2025-05-17 21:22:40.890542: \n",
      "2025-05-17 21:22:40.890685: Epoch 847\n",
      "2025-05-17 21:22:40.890751: Current learning rate: 0.00185\n",
      "2025-05-17 21:24:29.914406: train_loss -0.9669\n",
      "2025-05-17 21:24:29.914528: val_loss -0.953\n",
      "2025-05-17 21:24:29.914563: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-17 21:24:29.914597: Epoch time: 109.02 s\n",
      "2025-05-17 21:24:30.439403: \n",
      "2025-05-17 21:24:30.439541: Epoch 848\n",
      "2025-05-17 21:24:30.439606: Current learning rate: 0.00184\n",
      "2025-05-17 21:26:19.459201: train_loss -0.9687\n",
      "2025-05-17 21:26:19.459325: val_loss -0.9498\n",
      "2025-05-17 21:26:19.459358: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-17 21:26:19.459391: Epoch time: 109.02 s\n",
      "2025-05-17 21:26:19.985337: \n",
      "2025-05-17 21:26:19.985538: Epoch 849\n",
      "2025-05-17 21:26:19.985663: Current learning rate: 0.00182\n",
      "2025-05-17 21:28:08.948874: train_loss -0.9689\n",
      "2025-05-17 21:28:08.949032: val_loss -0.9432\n",
      "2025-05-17 21:28:08.949065: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-17 21:28:08.949097: Epoch time: 108.96 s\n",
      "2025-05-17 21:28:09.693973: \n",
      "2025-05-17 21:28:09.694207: Epoch 850\n",
      "2025-05-17 21:28:09.694327: Current learning rate: 0.00181\n",
      "2025-05-17 21:29:58.695442: train_loss -0.9706\n",
      "2025-05-17 21:29:58.695561: val_loss -0.9468\n",
      "2025-05-17 21:29:58.695596: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 21:29:58.695629: Epoch time: 109.0 s\n",
      "2025-05-17 21:29:59.217204: \n",
      "2025-05-17 21:29:59.217282: Epoch 851\n",
      "2025-05-17 21:29:59.217354: Current learning rate: 0.0018\n",
      "2025-05-17 21:31:48.219246: train_loss -0.9707\n",
      "2025-05-17 21:31:48.219431: val_loss -0.9482\n",
      "2025-05-17 21:31:48.219467: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-17 21:31:48.219502: Epoch time: 109.0 s\n",
      "2025-05-17 21:31:48.930889: \n",
      "2025-05-17 21:31:48.930994: Epoch 852\n",
      "2025-05-17 21:31:48.931073: Current learning rate: 0.00179\n",
      "2025-05-17 21:33:37.949035: train_loss -0.9695\n",
      "2025-05-17 21:33:37.949201: val_loss -0.9502\n",
      "2025-05-17 21:33:37.949235: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 21:33:37.949270: Epoch time: 109.02 s\n",
      "2025-05-17 21:33:38.470446: \n",
      "2025-05-17 21:33:38.470548: Epoch 853\n",
      "2025-05-17 21:33:38.470615: Current learning rate: 0.00178\n",
      "2025-05-17 21:35:27.509209: train_loss -0.9714\n",
      "2025-05-17 21:35:27.509339: val_loss -0.9515\n",
      "2025-05-17 21:35:27.509479: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-17 21:35:27.509556: Epoch time: 109.04 s\n",
      "2025-05-17 21:35:28.020266: \n",
      "2025-05-17 21:35:28.020431: Epoch 854\n",
      "2025-05-17 21:35:28.020512: Current learning rate: 0.00177\n",
      "2025-05-17 21:37:17.043192: train_loss -0.971\n",
      "2025-05-17 21:37:17.043314: val_loss -0.9509\n",
      "2025-05-17 21:37:17.043349: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-17 21:37:17.043381: Epoch time: 109.02 s\n",
      "2025-05-17 21:37:17.560519: \n",
      "2025-05-17 21:37:17.560672: Epoch 855\n",
      "2025-05-17 21:37:17.560741: Current learning rate: 0.00176\n",
      "2025-05-17 21:39:06.549973: train_loss -0.9704\n",
      "2025-05-17 21:39:06.550096: val_loss -0.9459\n",
      "2025-05-17 21:39:06.550141: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-17 21:39:06.550176: Epoch time: 108.99 s\n",
      "2025-05-17 21:39:07.074594: \n",
      "2025-05-17 21:39:07.074775: Epoch 856\n",
      "2025-05-17 21:39:07.074887: Current learning rate: 0.00175\n",
      "2025-05-17 21:40:56.107005: train_loss -0.9714\n",
      "2025-05-17 21:40:56.107228: val_loss -0.9502\n",
      "2025-05-17 21:40:56.107331: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 21:40:56.107426: Epoch time: 109.03 s\n",
      "2025-05-17 21:40:56.637881: \n",
      "2025-05-17 21:40:56.638211: Epoch 857\n",
      "2025-05-17 21:40:56.638370: Current learning rate: 0.00174\n",
      "2025-05-17 21:42:45.674744: train_loss -0.9704\n",
      "2025-05-17 21:42:45.674865: val_loss -0.9513\n",
      "2025-05-17 21:42:45.674896: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 21:42:45.674929: Epoch time: 109.04 s\n",
      "2025-05-17 21:42:46.184336: \n",
      "2025-05-17 21:42:46.184494: Epoch 858\n",
      "2025-05-17 21:42:46.184558: Current learning rate: 0.00173\n",
      "2025-05-17 21:44:35.141132: train_loss -0.9697\n",
      "2025-05-17 21:44:35.141262: val_loss -0.9518\n",
      "2025-05-17 21:44:35.141294: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-17 21:44:35.141325: Epoch time: 108.96 s\n",
      "2025-05-17 21:44:35.657009: \n",
      "2025-05-17 21:44:35.657186: Epoch 859\n",
      "2025-05-17 21:44:35.657299: Current learning rate: 0.00172\n",
      "2025-05-17 21:46:24.632940: train_loss -0.9702\n",
      "2025-05-17 21:46:24.633080: val_loss -0.9512\n",
      "2025-05-17 21:46:24.633113: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 21:46:24.633146: Epoch time: 108.98 s\n",
      "2025-05-17 21:46:25.152405: \n",
      "2025-05-17 21:46:25.152488: Epoch 860\n",
      "2025-05-17 21:46:25.152554: Current learning rate: 0.0017\n",
      "2025-05-17 21:48:14.094131: train_loss -0.9711\n",
      "2025-05-17 21:48:14.094261: val_loss -0.9443\n",
      "2025-05-17 21:48:14.094298: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 21:48:14.094332: Epoch time: 108.94 s\n",
      "2025-05-17 21:48:14.605358: \n",
      "2025-05-17 21:48:14.605450: Epoch 861\n",
      "2025-05-17 21:48:14.605516: Current learning rate: 0.00169\n",
      "2025-05-17 21:50:03.635693: train_loss -0.971\n",
      "2025-05-17 21:50:03.635850: val_loss -0.9491\n",
      "2025-05-17 21:50:03.635883: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 21:50:03.635915: Epoch time: 109.03 s\n",
      "2025-05-17 21:50:04.158586: \n",
      "2025-05-17 21:50:04.158827: Epoch 862\n",
      "2025-05-17 21:50:04.158966: Current learning rate: 0.00168\n",
      "2025-05-17 21:51:53.153776: train_loss -0.9705\n",
      "2025-05-17 21:51:53.153901: val_loss -0.9516\n",
      "2025-05-17 21:51:53.153934: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-17 21:51:53.153966: Epoch time: 109.0 s\n",
      "2025-05-17 21:51:53.696977: \n",
      "2025-05-17 21:51:53.697059: Epoch 863\n",
      "2025-05-17 21:51:53.697163: Current learning rate: 0.00167\n",
      "2025-05-17 21:53:42.676430: train_loss -0.9719\n",
      "2025-05-17 21:53:42.676556: val_loss -0.9494\n",
      "2025-05-17 21:53:42.676589: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-17 21:53:42.676624: Epoch time: 108.98 s\n",
      "2025-05-17 21:53:43.191301: \n",
      "2025-05-17 21:53:43.191555: Epoch 864\n",
      "2025-05-17 21:53:43.191632: Current learning rate: 0.00166\n",
      "2025-05-17 21:55:32.197417: train_loss -0.9721\n",
      "2025-05-17 21:55:32.197554: val_loss -0.9486\n",
      "2025-05-17 21:55:32.197588: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-17 21:55:32.197622: Epoch time: 109.01 s\n",
      "2025-05-17 21:55:32.917320: \n",
      "2025-05-17 21:55:32.917462: Epoch 865\n",
      "2025-05-17 21:55:32.917534: Current learning rate: 0.00165\n",
      "2025-05-17 21:57:21.932123: train_loss -0.9721\n",
      "2025-05-17 21:57:21.932316: val_loss -0.9457\n",
      "2025-05-17 21:57:21.932501: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-17 21:57:21.932561: Epoch time: 109.02 s\n",
      "2025-05-17 21:57:22.449958: \n",
      "2025-05-17 21:57:22.450143: Epoch 866\n",
      "2025-05-17 21:57:22.450296: Current learning rate: 0.00164\n",
      "2025-05-17 21:59:11.480241: train_loss -0.9711\n",
      "2025-05-17 21:59:11.480365: val_loss -0.953\n",
      "2025-05-17 21:59:11.480398: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-17 21:59:11.480431: Epoch time: 109.03 s\n",
      "2025-05-17 21:59:12.005763: \n",
      "2025-05-17 21:59:12.005863: Epoch 867\n",
      "2025-05-17 21:59:12.005937: Current learning rate: 0.00163\n",
      "2025-05-17 22:01:01.046593: train_loss -0.9715\n",
      "2025-05-17 22:01:01.046727: val_loss -0.9524\n",
      "2025-05-17 22:01:01.046796: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-17 22:01:01.046839: Epoch time: 109.04 s\n",
      "2025-05-17 22:01:01.564038: \n",
      "2025-05-17 22:01:01.564128: Epoch 868\n",
      "2025-05-17 22:01:01.564192: Current learning rate: 0.00162\n",
      "2025-05-17 22:02:50.534970: train_loss -0.9722\n",
      "2025-05-17 22:02:50.535136: val_loss -0.9485\n",
      "2025-05-17 22:02:50.535178: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 22:02:50.535213: Epoch time: 108.97 s\n",
      "2025-05-17 22:02:51.060332: \n",
      "2025-05-17 22:02:51.060418: Epoch 869\n",
      "2025-05-17 22:02:51.060486: Current learning rate: 0.00161\n",
      "2025-05-17 22:04:40.083130: train_loss -0.9725\n",
      "2025-05-17 22:04:40.083260: val_loss -0.9478\n",
      "2025-05-17 22:04:40.083297: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-17 22:04:40.083345: Epoch time: 109.02 s\n",
      "2025-05-17 22:04:40.598445: \n",
      "2025-05-17 22:04:40.598535: Epoch 870\n",
      "2025-05-17 22:04:40.598596: Current learning rate: 0.00159\n",
      "2025-05-17 22:06:29.597458: train_loss -0.9716\n",
      "2025-05-17 22:06:29.597646: val_loss -0.9498\n",
      "2025-05-17 22:06:29.597697: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 22:06:29.597746: Epoch time: 109.0 s\n",
      "2025-05-17 22:06:30.118878: \n",
      "2025-05-17 22:06:30.119045: Epoch 871\n",
      "2025-05-17 22:06:30.119157: Current learning rate: 0.00158\n",
      "2025-05-17 22:08:19.110229: train_loss -0.9723\n",
      "2025-05-17 22:08:19.110353: val_loss -0.9468\n",
      "2025-05-17 22:08:19.110542: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-17 22:08:19.110609: Epoch time: 108.99 s\n",
      "2025-05-17 22:08:19.628567: \n",
      "2025-05-17 22:08:19.628770: Epoch 872\n",
      "2025-05-17 22:08:19.628861: Current learning rate: 0.00157\n",
      "2025-05-17 22:10:08.643335: train_loss -0.9716\n",
      "2025-05-17 22:10:08.643481: val_loss -0.9529\n",
      "2025-05-17 22:10:08.643515: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-17 22:10:08.643548: Epoch time: 109.02 s\n",
      "2025-05-17 22:10:09.165784: \n",
      "2025-05-17 22:10:09.165977: Epoch 873\n",
      "2025-05-17 22:10:09.166050: Current learning rate: 0.00156\n",
      "2025-05-17 22:11:58.195657: train_loss -0.9716\n",
      "2025-05-17 22:11:58.195833: val_loss -0.9495\n",
      "2025-05-17 22:11:58.195867: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-17 22:11:58.195900: Epoch time: 109.03 s\n",
      "2025-05-17 22:11:58.711221: \n",
      "2025-05-17 22:11:58.711304: Epoch 874\n",
      "2025-05-17 22:11:58.711369: Current learning rate: 0.00155\n",
      "2025-05-17 22:13:47.788183: train_loss -0.9727\n",
      "2025-05-17 22:13:47.788328: val_loss -0.944\n",
      "2025-05-17 22:13:47.788367: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-17 22:13:47.788401: Epoch time: 109.08 s\n",
      "2025-05-17 22:13:48.308738: \n",
      "2025-05-17 22:13:48.308821: Epoch 875\n",
      "2025-05-17 22:13:48.308884: Current learning rate: 0.00154\n",
      "2025-05-17 22:15:37.285722: train_loss -0.9703\n",
      "2025-05-17 22:15:37.285842: val_loss -0.9506\n",
      "2025-05-17 22:15:37.285877: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-17 22:15:37.285911: Epoch time: 108.98 s\n",
      "2025-05-17 22:15:37.804345: \n",
      "2025-05-17 22:15:37.804426: Epoch 876\n",
      "2025-05-17 22:15:37.804498: Current learning rate: 0.00153\n",
      "2025-05-17 22:17:26.798532: train_loss -0.9712\n",
      "2025-05-17 22:17:26.798652: val_loss -0.947\n",
      "2025-05-17 22:17:26.798688: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 22:17:26.798725: Epoch time: 108.99 s\n",
      "2025-05-17 22:17:27.317634: \n",
      "2025-05-17 22:17:27.317794: Epoch 877\n",
      "2025-05-17 22:17:27.317860: Current learning rate: 0.00152\n",
      "2025-05-17 22:19:16.342736: train_loss -0.972\n",
      "2025-05-17 22:19:16.342907: val_loss -0.9479\n",
      "2025-05-17 22:19:16.342940: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-17 22:19:16.342973: Epoch time: 109.03 s\n",
      "2025-05-17 22:19:17.065862: \n",
      "2025-05-17 22:19:17.066023: Epoch 878\n",
      "2025-05-17 22:19:17.066095: Current learning rate: 0.00151\n",
      "2025-05-17 22:21:06.041845: train_loss -0.9717\n",
      "2025-05-17 22:21:06.041977: val_loss -0.9505\n",
      "2025-05-17 22:21:06.042010: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-17 22:21:06.042042: Epoch time: 108.98 s\n",
      "2025-05-17 22:21:06.558777: \n",
      "2025-05-17 22:21:06.558959: Epoch 879\n",
      "2025-05-17 22:21:06.559041: Current learning rate: 0.00149\n",
      "2025-05-17 22:22:55.650152: train_loss -0.972\n",
      "2025-05-17 22:22:55.650290: val_loss -0.9463\n",
      "2025-05-17 22:22:55.650328: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 22:22:55.650362: Epoch time: 109.09 s\n",
      "2025-05-17 22:22:56.169268: \n",
      "2025-05-17 22:22:56.169392: Epoch 880\n",
      "2025-05-17 22:22:56.169459: Current learning rate: 0.00148\n",
      "2025-05-17 22:24:45.227926: train_loss -0.9736\n",
      "2025-05-17 22:24:45.228048: val_loss -0.9508\n",
      "2025-05-17 22:24:45.228081: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-17 22:24:45.228115: Epoch time: 109.06 s\n",
      "2025-05-17 22:24:45.742828: \n",
      "2025-05-17 22:24:45.743051: Epoch 881\n",
      "2025-05-17 22:24:45.743135: Current learning rate: 0.00147\n",
      "2025-05-17 22:26:34.765856: train_loss -0.9721\n",
      "2025-05-17 22:26:34.766155: val_loss -0.9489\n",
      "2025-05-17 22:26:34.766246: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 22:26:34.766423: Epoch time: 109.02 s\n",
      "2025-05-17 22:26:35.297117: \n",
      "2025-05-17 22:26:35.297292: Epoch 882\n",
      "2025-05-17 22:26:35.297370: Current learning rate: 0.00146\n",
      "2025-05-17 22:28:24.298938: train_loss -0.9717\n",
      "2025-05-17 22:28:24.299073: val_loss -0.9427\n",
      "2025-05-17 22:28:24.299108: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-17 22:28:24.299141: Epoch time: 109.0 s\n",
      "2025-05-17 22:28:24.819277: \n",
      "2025-05-17 22:28:24.819467: Epoch 883\n",
      "2025-05-17 22:28:24.819540: Current learning rate: 0.00145\n",
      "2025-05-17 22:30:13.858974: train_loss -0.9723\n",
      "2025-05-17 22:30:13.859163: val_loss -0.9529\n",
      "2025-05-17 22:30:13.859199: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-17 22:30:13.859233: Epoch time: 109.04 s\n",
      "2025-05-17 22:30:14.375494: \n",
      "2025-05-17 22:30:14.375636: Epoch 884\n",
      "2025-05-17 22:30:14.375713: Current learning rate: 0.00144\n",
      "2025-05-17 22:32:03.424767: train_loss -0.9723\n",
      "2025-05-17 22:32:03.425024: val_loss -0.9461\n",
      "2025-05-17 22:32:03.425097: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 22:32:03.425133: Epoch time: 109.05 s\n",
      "2025-05-17 22:32:03.945567: \n",
      "2025-05-17 22:32:03.945653: Epoch 885\n",
      "2025-05-17 22:32:03.945719: Current learning rate: 0.00143\n",
      "2025-05-17 22:33:52.911948: train_loss -0.9721\n",
      "2025-05-17 22:33:52.912071: val_loss -0.9499\n",
      "2025-05-17 22:33:52.912106: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-17 22:33:52.912140: Epoch time: 108.97 s\n",
      "2025-05-17 22:33:53.427710: \n",
      "2025-05-17 22:33:53.427800: Epoch 886\n",
      "2025-05-17 22:33:53.427936: Current learning rate: 0.00142\n",
      "2025-05-17 22:35:42.371930: train_loss -0.9723\n",
      "2025-05-17 22:35:42.372091: val_loss -0.9487\n",
      "2025-05-17 22:35:42.372138: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-17 22:35:42.372263: Epoch time: 108.94 s\n",
      "2025-05-17 22:35:42.888840: \n",
      "2025-05-17 22:35:42.888988: Epoch 887\n",
      "2025-05-17 22:35:42.889054: Current learning rate: 0.00141\n",
      "2025-05-17 22:37:31.931632: train_loss -0.9722\n",
      "2025-05-17 22:37:31.931846: val_loss -0.9503\n",
      "2025-05-17 22:37:31.931882: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 22:37:31.931950: Epoch time: 109.04 s\n",
      "2025-05-17 22:37:32.457546: \n",
      "2025-05-17 22:37:32.457912: Epoch 888\n",
      "2025-05-17 22:37:32.458055: Current learning rate: 0.00139\n",
      "2025-05-17 22:39:21.444117: train_loss -0.9729\n",
      "2025-05-17 22:39:21.444264: val_loss -0.9528\n",
      "2025-05-17 22:39:21.444303: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-17 22:39:21.444338: Epoch time: 108.99 s\n",
      "2025-05-17 22:39:21.966002: \n",
      "2025-05-17 22:39:21.966518: Epoch 889\n",
      "2025-05-17 22:39:21.966770: Current learning rate: 0.00138\n",
      "2025-05-17 22:41:10.973088: train_loss -0.9722\n",
      "2025-05-17 22:41:10.973228: val_loss -0.9514\n",
      "2025-05-17 22:41:10.973276: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-17 22:41:10.973316: Epoch time: 109.01 s\n",
      "2025-05-17 22:41:11.500159: \n",
      "2025-05-17 22:41:11.500280: Epoch 890\n",
      "2025-05-17 22:41:11.500471: Current learning rate: 0.00137\n",
      "2025-05-17 22:43:00.514623: train_loss -0.9726\n",
      "2025-05-17 22:43:00.514839: val_loss -0.9485\n",
      "2025-05-17 22:43:00.514878: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-17 22:43:00.514911: Epoch time: 109.01 s\n",
      "2025-05-17 22:43:01.231076: \n",
      "2025-05-17 22:43:01.231290: Epoch 891\n",
      "2025-05-17 22:43:01.231387: Current learning rate: 0.00136\n",
      "2025-05-17 22:44:50.246172: train_loss -0.9731\n",
      "2025-05-17 22:44:50.246294: val_loss -0.9509\n",
      "2025-05-17 22:44:50.246328: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-17 22:44:50.246362: Epoch time: 109.02 s\n",
      "2025-05-17 22:44:50.770058: \n",
      "2025-05-17 22:44:50.770205: Epoch 892\n",
      "2025-05-17 22:44:50.770274: Current learning rate: 0.00135\n",
      "2025-05-17 22:46:39.827952: train_loss -0.9724\n",
      "2025-05-17 22:46:39.828133: val_loss -0.947\n",
      "2025-05-17 22:46:39.828171: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 22:46:39.828220: Epoch time: 109.06 s\n",
      "2025-05-17 22:46:40.343937: \n",
      "2025-05-17 22:46:40.344033: Epoch 893\n",
      "2025-05-17 22:46:40.344096: Current learning rate: 0.00134\n",
      "2025-05-17 22:48:29.356219: train_loss -0.9708\n",
      "2025-05-17 22:48:29.356414: val_loss -0.95\n",
      "2025-05-17 22:48:29.356448: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-17 22:48:29.356482: Epoch time: 109.01 s\n",
      "2025-05-17 22:48:29.356573: Yayy! New best EMA pseudo Dice: 0.9769999980926514\n",
      "2025-05-17 22:48:30.098097: \n",
      "2025-05-17 22:48:30.098541: Epoch 894\n",
      "2025-05-17 22:48:30.098637: Current learning rate: 0.00133\n",
      "2025-05-17 22:50:19.093578: train_loss -0.9723\n",
      "2025-05-17 22:50:19.093715: val_loss -0.9501\n",
      "2025-05-17 22:50:19.093748: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-17 22:50:19.093779: Epoch time: 109.0 s\n",
      "2025-05-17 22:50:19.093818: Yayy! New best EMA pseudo Dice: 0.9771000146865845\n",
      "2025-05-17 22:50:19.850292: \n",
      "2025-05-17 22:50:19.850558: Epoch 895\n",
      "2025-05-17 22:50:19.850803: Current learning rate: 0.00132\n",
      "2025-05-17 22:52:08.848057: train_loss -0.9721\n",
      "2025-05-17 22:52:08.848224: val_loss -0.9514\n",
      "2025-05-17 22:52:08.848256: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-17 22:52:08.848315: Epoch time: 109.0 s\n",
      "2025-05-17 22:52:09.370784: \n",
      "2025-05-17 22:52:09.370915: Epoch 896\n",
      "2025-05-17 22:52:09.370980: Current learning rate: 0.0013\n",
      "2025-05-17 22:53:58.367632: train_loss -0.9726\n",
      "2025-05-17 22:53:58.367770: val_loss -0.9519\n",
      "2025-05-17 22:53:58.367809: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-17 22:53:58.367842: Epoch time: 109.0 s\n",
      "2025-05-17 22:53:58.367868: Yayy! New best EMA pseudo Dice: 0.9771000146865845\n",
      "2025-05-17 22:53:59.111363: \n",
      "2025-05-17 22:53:59.111563: Epoch 897\n",
      "2025-05-17 22:53:59.111739: Current learning rate: 0.00129\n",
      "2025-05-17 22:55:48.118694: train_loss -0.9728\n",
      "2025-05-17 22:55:48.118822: val_loss -0.9478\n",
      "2025-05-17 22:55:48.118856: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 22:55:48.118890: Epoch time: 109.01 s\n",
      "2025-05-17 22:55:48.637918: \n",
      "2025-05-17 22:55:48.638060: Epoch 898\n",
      "2025-05-17 22:55:48.638135: Current learning rate: 0.00128\n",
      "2025-05-17 22:57:37.657686: train_loss -0.9724\n",
      "2025-05-17 22:57:37.657845: val_loss -0.9491\n",
      "2025-05-17 22:57:37.657877: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 22:57:37.657910: Epoch time: 109.02 s\n",
      "2025-05-17 22:57:38.176730: \n",
      "2025-05-17 22:57:38.176822: Epoch 899\n",
      "2025-05-17 22:57:38.176888: Current learning rate: 0.00127\n",
      "2025-05-17 22:59:27.223578: train_loss -0.9724\n",
      "2025-05-17 22:59:27.223766: val_loss -0.9509\n",
      "2025-05-17 22:59:27.223802: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-17 22:59:27.223837: Epoch time: 109.05 s\n",
      "2025-05-17 22:59:27.966618: \n",
      "2025-05-17 22:59:27.966784: Epoch 900\n",
      "2025-05-17 22:59:27.966856: Current learning rate: 0.00126\n",
      "2025-05-17 23:01:16.994140: train_loss -0.9728\n",
      "2025-05-17 23:01:16.994258: val_loss -0.9424\n",
      "2025-05-17 23:01:16.994290: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-17 23:01:16.994322: Epoch time: 109.03 s\n",
      "2025-05-17 23:01:17.510566: \n",
      "2025-05-17 23:01:17.510650: Epoch 901\n",
      "2025-05-17 23:01:17.510714: Current learning rate: 0.00125\n",
      "2025-05-17 23:03:06.511529: train_loss -0.9739\n",
      "2025-05-17 23:03:06.511658: val_loss -0.9438\n",
      "2025-05-17 23:03:06.511754: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-17 23:03:06.511823: Epoch time: 109.0 s\n",
      "2025-05-17 23:03:07.031229: \n",
      "2025-05-17 23:03:07.031465: Epoch 902\n",
      "2025-05-17 23:03:07.031560: Current learning rate: 0.00124\n",
      "2025-05-17 23:04:56.041672: train_loss -0.9736\n",
      "2025-05-17 23:04:56.041813: val_loss -0.9436\n",
      "2025-05-17 23:04:56.041854: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-17 23:04:56.041886: Epoch time: 109.01 s\n",
      "2025-05-17 23:04:56.558541: \n",
      "2025-05-17 23:04:56.558675: Epoch 903\n",
      "2025-05-17 23:04:56.558785: Current learning rate: 0.00122\n",
      "2025-05-17 23:06:45.642655: train_loss -0.9731\n",
      "2025-05-17 23:06:45.642792: val_loss -0.9525\n",
      "2025-05-17 23:06:45.642827: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-17 23:06:45.642874: Epoch time: 109.08 s\n",
      "2025-05-17 23:06:46.345022: \n",
      "2025-05-17 23:06:46.345237: Epoch 904\n",
      "2025-05-17 23:06:46.345384: Current learning rate: 0.00121\n",
      "2025-05-17 23:08:35.302315: train_loss -0.9723\n",
      "2025-05-17 23:08:35.302620: val_loss -0.952\n",
      "2025-05-17 23:08:35.302709: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-17 23:08:35.302804: Epoch time: 108.96 s\n",
      "2025-05-17 23:08:35.819666: \n",
      "2025-05-17 23:08:35.819807: Epoch 905\n",
      "2025-05-17 23:08:35.819871: Current learning rate: 0.0012\n",
      "2025-05-17 23:10:24.857004: train_loss -0.9725\n",
      "2025-05-17 23:10:24.857128: val_loss -0.9463\n",
      "2025-05-17 23:10:24.857162: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-17 23:10:24.857204: Epoch time: 109.04 s\n",
      "2025-05-17 23:10:25.383540: \n",
      "2025-05-17 23:10:25.383852: Epoch 906\n",
      "2025-05-17 23:10:25.383922: Current learning rate: 0.00119\n",
      "2025-05-17 23:12:14.379738: train_loss -0.9734\n",
      "2025-05-17 23:12:14.379874: val_loss -0.9488\n",
      "2025-05-17 23:12:14.379907: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 23:12:14.379992: Epoch time: 109.0 s\n",
      "2025-05-17 23:12:14.897049: \n",
      "2025-05-17 23:12:14.897216: Epoch 907\n",
      "2025-05-17 23:12:14.897284: Current learning rate: 0.00118\n",
      "2025-05-17 23:14:03.993870: train_loss -0.9724\n",
      "2025-05-17 23:14:03.994001: val_loss -0.9478\n",
      "2025-05-17 23:14:03.994036: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 23:14:03.994068: Epoch time: 109.1 s\n",
      "2025-05-17 23:14:04.519241: \n",
      "2025-05-17 23:14:04.519640: Epoch 908\n",
      "2025-05-17 23:14:04.519721: Current learning rate: 0.00117\n",
      "2025-05-17 23:15:53.554432: train_loss -0.9723\n",
      "2025-05-17 23:15:53.554631: val_loss -0.9478\n",
      "2025-05-17 23:15:53.554666: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 23:15:53.554699: Epoch time: 109.04 s\n",
      "2025-05-17 23:15:54.075025: \n",
      "2025-05-17 23:15:54.075118: Epoch 909\n",
      "2025-05-17 23:15:54.075321: Current learning rate: 0.00116\n",
      "2025-05-17 23:17:43.087010: train_loss -0.9734\n",
      "2025-05-17 23:17:43.087146: val_loss -0.9482\n",
      "2025-05-17 23:17:43.087186: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-17 23:17:43.087221: Epoch time: 109.01 s\n",
      "2025-05-17 23:17:43.596170: \n",
      "2025-05-17 23:17:43.596263: Epoch 910\n",
      "2025-05-17 23:17:43.596357: Current learning rate: 0.00115\n",
      "2025-05-17 23:19:32.654991: train_loss -0.9744\n",
      "2025-05-17 23:19:32.655208: val_loss -0.9522\n",
      "2025-05-17 23:19:32.655256: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-17 23:19:32.655288: Epoch time: 109.06 s\n",
      "2025-05-17 23:19:33.171355: \n",
      "2025-05-17 23:19:33.171509: Epoch 911\n",
      "2025-05-17 23:19:33.171674: Current learning rate: 0.00113\n",
      "2025-05-17 23:21:22.169360: train_loss -0.9734\n",
      "2025-05-17 23:21:22.169487: val_loss -0.9544\n",
      "2025-05-17 23:21:22.169555: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-17 23:21:22.169593: Epoch time: 109.0 s\n",
      "2025-05-17 23:21:22.693036: \n",
      "2025-05-17 23:21:22.693264: Epoch 912\n",
      "2025-05-17 23:21:22.693356: Current learning rate: 0.00112\n",
      "2025-05-17 23:23:11.685248: train_loss -0.972\n",
      "2025-05-17 23:23:11.685419: val_loss -0.9488\n",
      "2025-05-17 23:23:11.685467: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-17 23:23:11.685507: Epoch time: 108.99 s\n",
      "2025-05-17 23:23:12.205039: \n",
      "2025-05-17 23:23:12.205126: Epoch 913\n",
      "2025-05-17 23:23:12.205192: Current learning rate: 0.00111\n",
      "2025-05-17 23:25:01.215442: train_loss -0.9728\n",
      "2025-05-17 23:25:01.215584: val_loss -0.9513\n",
      "2025-05-17 23:25:01.215617: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-17 23:25:01.215658: Epoch time: 109.01 s\n",
      "2025-05-17 23:25:01.215697: Yayy! New best EMA pseudo Dice: 0.9771000146865845\n",
      "2025-05-17 23:25:01.953923: \n",
      "2025-05-17 23:25:01.954057: Epoch 914\n",
      "2025-05-17 23:25:01.954175: Current learning rate: 0.0011\n",
      "2025-05-17 23:26:50.966428: train_loss -0.9721\n",
      "2025-05-17 23:26:50.966555: val_loss -0.9471\n",
      "2025-05-17 23:26:50.966587: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 23:26:50.966620: Epoch time: 109.01 s\n",
      "2025-05-17 23:26:51.475507: \n",
      "2025-05-17 23:26:51.475634: Epoch 915\n",
      "2025-05-17 23:26:51.475700: Current learning rate: 0.00109\n",
      "2025-05-17 23:28:40.463138: train_loss -0.9724\n",
      "2025-05-17 23:28:40.463356: val_loss -0.9458\n",
      "2025-05-17 23:28:40.463424: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-17 23:28:40.463476: Epoch time: 108.99 s\n",
      "2025-05-17 23:28:40.977279: \n",
      "2025-05-17 23:28:40.977481: Epoch 916\n",
      "2025-05-17 23:28:40.977610: Current learning rate: 0.00108\n",
      "2025-05-17 23:30:29.951073: train_loss -0.9739\n",
      "2025-05-17 23:30:29.951218: val_loss -0.9463\n",
      "2025-05-17 23:30:29.951253: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 23:30:29.951285: Epoch time: 108.97 s\n",
      "2025-05-17 23:30:30.659209: \n",
      "2025-05-17 23:30:30.659359: Epoch 917\n",
      "2025-05-17 23:30:30.659428: Current learning rate: 0.00106\n",
      "2025-05-17 23:32:19.676298: train_loss -0.974\n",
      "2025-05-17 23:32:19.676420: val_loss -0.9481\n",
      "2025-05-17 23:32:19.676452: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 23:32:19.676487: Epoch time: 109.02 s\n",
      "2025-05-17 23:32:20.199795: \n",
      "2025-05-17 23:32:20.199888: Epoch 918\n",
      "2025-05-17 23:32:20.199950: Current learning rate: 0.00105\n",
      "2025-05-17 23:34:09.212772: train_loss -0.9727\n",
      "2025-05-17 23:34:09.212952: val_loss -0.9545\n",
      "2025-05-17 23:34:09.213049: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-17 23:34:09.213123: Epoch time: 109.01 s\n",
      "2025-05-17 23:34:09.725978: \n",
      "2025-05-17 23:34:09.726360: Epoch 919\n",
      "2025-05-17 23:34:09.726435: Current learning rate: 0.00104\n",
      "2025-05-17 23:35:58.744799: train_loss -0.9733\n",
      "2025-05-17 23:35:58.744919: val_loss -0.9506\n",
      "2025-05-17 23:35:58.744953: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-17 23:35:58.744987: Epoch time: 109.02 s\n",
      "2025-05-17 23:35:59.274708: \n",
      "2025-05-17 23:35:59.274932: Epoch 920\n",
      "2025-05-17 23:35:59.275061: Current learning rate: 0.00103\n",
      "2025-05-17 23:37:48.347219: train_loss -0.973\n",
      "2025-05-17 23:37:48.347435: val_loss -0.9507\n",
      "2025-05-17 23:37:48.347477: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-17 23:37:48.347510: Epoch time: 109.07 s\n",
      "2025-05-17 23:37:48.874344: \n",
      "2025-05-17 23:37:48.874525: Epoch 921\n",
      "2025-05-17 23:37:48.874600: Current learning rate: 0.00102\n",
      "2025-05-17 23:39:39.003337: train_loss -0.9744\n",
      "2025-05-17 23:39:39.003459: val_loss -0.9483\n",
      "2025-05-17 23:39:39.003495: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-17 23:39:39.003530: Epoch time: 110.13 s\n",
      "2025-05-17 23:39:39.563292: \n",
      "2025-05-17 23:39:39.563453: Epoch 922\n",
      "2025-05-17 23:39:39.563527: Current learning rate: 0.00101\n",
      "2025-05-17 23:41:29.020448: train_loss -0.9724\n",
      "2025-05-17 23:41:29.020691: val_loss -0.9447\n",
      "2025-05-17 23:41:29.020727: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-17 23:41:29.020762: Epoch time: 109.46 s\n",
      "2025-05-17 23:41:29.555374: \n",
      "2025-05-17 23:41:29.555608: Epoch 923\n",
      "2025-05-17 23:41:29.555707: Current learning rate: 0.001\n",
      "2025-05-17 23:43:18.963646: train_loss -0.9728\n",
      "2025-05-17 23:43:18.963785: val_loss -0.9473\n",
      "2025-05-17 23:43:18.963817: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-17 23:43:18.963852: Epoch time: 109.41 s\n",
      "2025-05-17 23:43:19.507267: \n",
      "2025-05-17 23:43:19.507397: Epoch 924\n",
      "2025-05-17 23:43:19.507479: Current learning rate: 0.00098\n",
      "2025-05-17 23:45:08.831608: train_loss -0.9726\n",
      "2025-05-17 23:45:08.831861: val_loss -0.951\n",
      "2025-05-17 23:45:08.832019: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-17 23:45:08.832106: Epoch time: 109.32 s\n",
      "2025-05-17 23:45:09.365534: \n",
      "2025-05-17 23:45:09.365636: Epoch 925\n",
      "2025-05-17 23:45:09.365704: Current learning rate: 0.00097\n",
      "2025-05-17 23:46:58.766991: train_loss -0.9732\n",
      "2025-05-17 23:46:58.767120: val_loss -0.9553\n",
      "2025-05-17 23:46:58.767156: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-17 23:46:58.767210: Epoch time: 109.4 s\n",
      "2025-05-17 23:46:58.767236: Yayy! New best EMA pseudo Dice: 0.9771000146865845\n",
      "2025-05-17 23:46:59.537331: \n",
      "2025-05-17 23:46:59.537472: Epoch 926\n",
      "2025-05-17 23:46:59.537551: Current learning rate: 0.00096\n",
      "2025-05-17 23:48:49.027025: train_loss -0.9726\n",
      "2025-05-17 23:48:49.027218: val_loss -0.9533\n",
      "2025-05-17 23:48:49.027269: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-17 23:48:49.027337: Epoch time: 109.49 s\n",
      "2025-05-17 23:48:49.027377: Yayy! New best EMA pseudo Dice: 0.9772999882698059\n",
      "2025-05-17 23:48:49.786513: \n",
      "2025-05-17 23:48:49.786700: Epoch 927\n",
      "2025-05-17 23:48:49.786782: Current learning rate: 0.00095\n",
      "2025-05-17 23:50:39.140387: train_loss -0.9728\n",
      "2025-05-17 23:50:39.140559: val_loss -0.9497\n",
      "2025-05-17 23:50:39.140593: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-17 23:50:39.140626: Epoch time: 109.35 s\n",
      "2025-05-17 23:50:39.140647: Yayy! New best EMA pseudo Dice: 0.9772999882698059\n",
      "2025-05-17 23:50:39.915238: \n",
      "2025-05-17 23:50:39.915329: Epoch 928\n",
      "2025-05-17 23:50:39.915396: Current learning rate: 0.00094\n",
      "2025-05-17 23:52:29.307856: train_loss -0.9733\n",
      "2025-05-17 23:52:29.307986: val_loss -0.948\n",
      "2025-05-17 23:52:29.308020: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-17 23:52:29.308053: Epoch time: 109.39 s\n",
      "2025-05-17 23:52:30.079593: \n",
      "2025-05-17 23:52:30.079862: Epoch 929\n",
      "2025-05-17 23:52:30.080019: Current learning rate: 0.00092\n",
      "2025-05-17 23:54:19.455587: train_loss -0.9729\n",
      "2025-05-17 23:54:19.455713: val_loss -0.9523\n",
      "2025-05-17 23:54:19.455747: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-17 23:54:19.455780: Epoch time: 109.38 s\n",
      "2025-05-17 23:54:19.455800: Yayy! New best EMA pseudo Dice: 0.977400004863739\n",
      "2025-05-17 23:54:20.232498: \n",
      "2025-05-17 23:54:20.232604: Epoch 930\n",
      "2025-05-17 23:54:20.232672: Current learning rate: 0.00091\n",
      "2025-05-17 23:56:09.677655: train_loss -0.9746\n",
      "2025-05-17 23:56:09.677803: val_loss -0.9493\n",
      "2025-05-17 23:56:09.677840: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-17 23:56:09.677884: Epoch time: 109.45 s\n",
      "2025-05-17 23:56:10.206330: \n",
      "2025-05-17 23:56:10.206499: Epoch 931\n",
      "2025-05-17 23:56:10.206579: Current learning rate: 0.0009\n",
      "2025-05-17 23:57:59.590392: train_loss -0.9742\n",
      "2025-05-17 23:57:59.590580: val_loss -0.9474\n",
      "2025-05-17 23:57:59.590656: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-17 23:57:59.590809: Epoch time: 109.38 s\n",
      "2025-05-17 23:58:00.122625: \n",
      "2025-05-17 23:58:00.122721: Epoch 932\n",
      "2025-05-17 23:58:00.122795: Current learning rate: 0.00089\n",
      "2025-05-17 23:59:49.534696: train_loss -0.9744\n",
      "2025-05-17 23:59:49.534892: val_loss -0.9534\n",
      "2025-05-17 23:59:49.534938: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-17 23:59:49.534975: Epoch time: 109.41 s\n",
      "2025-05-17 23:59:50.068840: \n",
      "2025-05-17 23:59:50.068939: Epoch 933\n",
      "2025-05-17 23:59:50.069008: Current learning rate: 0.00088\n",
      "2025-05-18 00:01:39.461090: train_loss -0.9726\n",
      "2025-05-18 00:01:39.461220: val_loss -0.951\n",
      "2025-05-18 00:01:39.461254: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-18 00:01:39.461287: Epoch time: 109.39 s\n",
      "2025-05-18 00:01:39.461308: Yayy! New best EMA pseudo Dice: 0.977400004863739\n",
      "2025-05-18 00:01:40.233972: \n",
      "2025-05-18 00:01:40.234355: Epoch 934\n",
      "2025-05-18 00:01:40.234596: Current learning rate: 0.00087\n",
      "2025-05-18 00:03:29.590839: train_loss -0.9741\n",
      "2025-05-18 00:03:29.590964: val_loss -0.9522\n",
      "2025-05-18 00:03:29.590997: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 00:03:29.591032: Epoch time: 109.36 s\n",
      "2025-05-18 00:03:29.591067: Yayy! New best EMA pseudo Dice: 0.9775000214576721\n",
      "2025-05-18 00:03:30.355190: \n",
      "2025-05-18 00:03:30.355370: Epoch 935\n",
      "2025-05-18 00:03:30.355447: Current learning rate: 0.00085\n",
      "2025-05-18 00:05:19.757145: train_loss -0.9732\n",
      "2025-05-18 00:05:19.757349: val_loss -0.9405\n",
      "2025-05-18 00:05:19.757381: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-18 00:05:19.757415: Epoch time: 109.4 s\n",
      "2025-05-18 00:05:20.286998: \n",
      "2025-05-18 00:05:20.287270: Epoch 936\n",
      "2025-05-18 00:05:20.287365: Current learning rate: 0.00084\n",
      "2025-05-18 00:07:09.732541: train_loss -0.9748\n",
      "2025-05-18 00:07:09.732682: val_loss -0.9522\n",
      "2025-05-18 00:07:09.732714: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-18 00:07:09.732748: Epoch time: 109.45 s\n",
      "2025-05-18 00:07:10.265201: \n",
      "2025-05-18 00:07:10.265299: Epoch 937\n",
      "2025-05-18 00:07:10.265367: Current learning rate: 0.00083\n",
      "2025-05-18 00:08:59.611979: train_loss -0.9739\n",
      "2025-05-18 00:08:59.612176: val_loss -0.9538\n",
      "2025-05-18 00:08:59.612208: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-18 00:08:59.612242: Epoch time: 109.35 s\n",
      "2025-05-18 00:09:00.139486: \n",
      "2025-05-18 00:09:00.139642: Epoch 938\n",
      "2025-05-18 00:09:00.139713: Current learning rate: 0.00082\n",
      "2025-05-18 00:10:49.560693: train_loss -0.9735\n",
      "2025-05-18 00:10:49.560825: val_loss -0.9511\n",
      "2025-05-18 00:10:49.560860: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 00:10:49.560893: Epoch time: 109.42 s\n",
      "2025-05-18 00:10:50.098967: \n",
      "2025-05-18 00:10:50.099152: Epoch 939\n",
      "2025-05-18 00:10:50.099231: Current learning rate: 0.00081\n",
      "2025-05-18 00:12:39.498388: train_loss -0.9745\n",
      "2025-05-18 00:12:39.498516: val_loss -0.9451\n",
      "2025-05-18 00:12:39.498552: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-18 00:12:39.498586: Epoch time: 109.4 s\n",
      "2025-05-18 00:12:40.023794: \n",
      "2025-05-18 00:12:40.023887: Epoch 940\n",
      "2025-05-18 00:12:40.024054: Current learning rate: 0.00079\n",
      "2025-05-18 00:14:29.350986: train_loss -0.972\n",
      "2025-05-18 00:14:29.351115: val_loss -0.949\n",
      "2025-05-18 00:14:29.351146: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-18 00:14:29.351225: Epoch time: 109.33 s\n",
      "2025-05-18 00:14:29.882337: \n",
      "2025-05-18 00:14:29.882583: Epoch 941\n",
      "2025-05-18 00:14:29.882677: Current learning rate: 0.00078\n",
      "2025-05-18 00:16:20.633462: train_loss -0.9735\n",
      "2025-05-18 00:16:20.633707: val_loss -0.948\n",
      "2025-05-18 00:16:20.633860: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 00:16:20.633978: Epoch time: 110.75 s\n",
      "2025-05-18 00:16:21.386214: \n",
      "2025-05-18 00:16:21.386322: Epoch 942\n",
      "2025-05-18 00:16:21.386393: Current learning rate: 0.00077\n",
      "2025-05-18 00:18:10.876151: train_loss -0.974\n",
      "2025-05-18 00:18:10.876280: val_loss -0.9525\n",
      "2025-05-18 00:18:10.876314: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-18 00:18:10.876349: Epoch time: 109.49 s\n",
      "2025-05-18 00:18:11.396422: \n",
      "2025-05-18 00:18:11.396651: Epoch 943\n",
      "2025-05-18 00:18:11.396736: Current learning rate: 0.00076\n",
      "2025-05-18 00:20:00.912044: train_loss -0.9748\n",
      "2025-05-18 00:20:00.912235: val_loss -0.9477\n",
      "2025-05-18 00:20:00.912271: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-18 00:20:00.912311: Epoch time: 109.52 s\n",
      "2025-05-18 00:20:01.433998: \n",
      "2025-05-18 00:20:01.434102: Epoch 944\n",
      "2025-05-18 00:20:01.434171: Current learning rate: 0.00075\n",
      "2025-05-18 00:21:50.872883: train_loss -0.9746\n",
      "2025-05-18 00:21:50.873026: val_loss -0.9461\n",
      "2025-05-18 00:21:50.873059: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-18 00:21:50.873094: Epoch time: 109.44 s\n",
      "2025-05-18 00:21:51.395267: \n",
      "2025-05-18 00:21:51.395463: Epoch 945\n",
      "2025-05-18 00:21:51.395590: Current learning rate: 0.00074\n",
      "2025-05-18 00:23:40.899870: train_loss -0.9743\n",
      "2025-05-18 00:23:40.900008: val_loss -0.948\n",
      "2025-05-18 00:23:40.900043: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 00:23:40.900079: Epoch time: 109.51 s\n",
      "2025-05-18 00:23:41.425264: \n",
      "2025-05-18 00:23:41.425421: Epoch 946\n",
      "2025-05-18 00:23:41.425508: Current learning rate: 0.00072\n",
      "2025-05-18 00:25:30.884896: train_loss -0.9753\n",
      "2025-05-18 00:25:30.885026: val_loss -0.9492\n",
      "2025-05-18 00:25:30.885071: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 00:25:30.885109: Epoch time: 109.46 s\n",
      "2025-05-18 00:25:31.408944: \n",
      "2025-05-18 00:25:31.409320: Epoch 947\n",
      "2025-05-18 00:25:31.409559: Current learning rate: 0.00071\n",
      "2025-05-18 00:27:20.875932: train_loss -0.9732\n",
      "2025-05-18 00:27:20.876109: val_loss -0.9534\n",
      "2025-05-18 00:27:20.876215: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 00:27:20.876260: Epoch time: 109.47 s\n",
      "2025-05-18 00:27:21.393611: \n",
      "2025-05-18 00:27:21.393790: Epoch 948\n",
      "2025-05-18 00:27:21.393862: Current learning rate: 0.0007\n",
      "2025-05-18 00:29:10.917983: train_loss -0.9735\n",
      "2025-05-18 00:29:10.918189: val_loss -0.9515\n",
      "2025-05-18 00:29:10.918236: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 00:29:10.918272: Epoch time: 109.52 s\n",
      "2025-05-18 00:29:11.436785: \n",
      "2025-05-18 00:29:11.436888: Epoch 949\n",
      "2025-05-18 00:29:11.436956: Current learning rate: 0.00069\n",
      "2025-05-18 00:31:00.866037: train_loss -0.9734\n",
      "2025-05-18 00:31:00.866263: val_loss -0.9503\n",
      "2025-05-18 00:31:00.866307: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 00:31:00.866342: Epoch time: 109.43 s\n",
      "2025-05-18 00:31:01.618800: \n",
      "2025-05-18 00:31:01.619019: Epoch 950\n",
      "2025-05-18 00:31:01.619117: Current learning rate: 0.00067\n",
      "2025-05-18 00:32:51.035614: train_loss -0.9738\n",
      "2025-05-18 00:32:51.035841: val_loss -0.9507\n",
      "2025-05-18 00:32:51.035879: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-18 00:32:51.035913: Epoch time: 109.42 s\n",
      "2025-05-18 00:32:51.549276: \n",
      "2025-05-18 00:32:51.549445: Epoch 951\n",
      "2025-05-18 00:32:51.549525: Current learning rate: 0.00066\n",
      "2025-05-18 00:34:41.008938: train_loss -0.9731\n",
      "2025-05-18 00:34:41.009067: val_loss -0.9488\n",
      "2025-05-18 00:34:41.009100: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 00:34:41.009130: Epoch time: 109.46 s\n",
      "2025-05-18 00:34:41.532460: \n",
      "2025-05-18 00:34:41.532552: Epoch 952\n",
      "2025-05-18 00:34:41.532628: Current learning rate: 0.00065\n",
      "2025-05-18 00:36:30.989314: train_loss -0.9749\n",
      "2025-05-18 00:36:30.989532: val_loss -0.9524\n",
      "2025-05-18 00:36:30.989578: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 00:36:30.989615: Epoch time: 109.46 s\n",
      "2025-05-18 00:36:31.509651: \n",
      "2025-05-18 00:36:31.509746: Epoch 953\n",
      "2025-05-18 00:36:31.509814: Current learning rate: 0.00064\n",
      "2025-05-18 00:38:20.938132: train_loss -0.974\n",
      "2025-05-18 00:38:20.938370: val_loss -0.9467\n",
      "2025-05-18 00:38:20.938432: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 00:38:20.938471: Epoch time: 109.43 s\n",
      "2025-05-18 00:38:21.466571: \n",
      "2025-05-18 00:38:21.466673: Epoch 954\n",
      "2025-05-18 00:38:21.466750: Current learning rate: 0.00063\n",
      "2025-05-18 00:40:10.880310: train_loss -0.9737\n",
      "2025-05-18 00:40:10.880441: val_loss -0.9508\n",
      "2025-05-18 00:40:10.880474: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 00:40:10.880506: Epoch time: 109.41 s\n",
      "2025-05-18 00:40:11.637307: \n",
      "2025-05-18 00:40:11.637681: Epoch 955\n",
      "2025-05-18 00:40:11.637798: Current learning rate: 0.00061\n",
      "2025-05-18 00:42:01.143597: train_loss -0.9744\n",
      "2025-05-18 00:42:01.143793: val_loss -0.9513\n",
      "2025-05-18 00:42:01.143842: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 00:42:01.143891: Epoch time: 109.51 s\n",
      "2025-05-18 00:42:01.143931: Yayy! New best EMA pseudo Dice: 0.9775000214576721\n",
      "2025-05-18 00:42:01.908945: \n",
      "2025-05-18 00:42:01.909261: Epoch 956\n",
      "2025-05-18 00:42:01.909332: Current learning rate: 0.0006\n",
      "2025-05-18 00:43:51.428041: train_loss -0.9736\n",
      "2025-05-18 00:43:51.428163: val_loss -0.9533\n",
      "2025-05-18 00:43:51.428200: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-18 00:43:51.428236: Epoch time: 109.52 s\n",
      "2025-05-18 00:43:51.428257: Yayy! New best EMA pseudo Dice: 0.9776999950408936\n",
      "2025-05-18 00:43:52.177055: \n",
      "2025-05-18 00:43:52.177167: Epoch 957\n",
      "2025-05-18 00:43:52.177235: Current learning rate: 0.00059\n",
      "2025-05-18 00:45:41.619470: train_loss -0.9738\n",
      "2025-05-18 00:45:41.619599: val_loss -0.9483\n",
      "2025-05-18 00:45:41.619633: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 00:45:41.619665: Epoch time: 109.44 s\n",
      "2025-05-18 00:45:42.146294: \n",
      "2025-05-18 00:45:42.146601: Epoch 958\n",
      "2025-05-18 00:45:42.146877: Current learning rate: 0.00058\n",
      "2025-05-18 00:47:31.613636: train_loss -0.9737\n",
      "2025-05-18 00:47:31.613837: val_loss -0.9489\n",
      "2025-05-18 00:47:31.613936: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 00:47:31.613978: Epoch time: 109.47 s\n",
      "2025-05-18 00:47:32.142674: \n",
      "2025-05-18 00:47:32.142899: Epoch 959\n",
      "2025-05-18 00:47:32.142991: Current learning rate: 0.00056\n",
      "2025-05-18 00:49:21.631562: train_loss -0.974\n",
      "2025-05-18 00:49:21.631717: val_loss -0.9456\n",
      "2025-05-18 00:49:21.631786: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 00:49:21.631826: Epoch time: 109.49 s\n",
      "2025-05-18 00:49:22.162150: \n",
      "2025-05-18 00:49:22.162493: Epoch 960\n",
      "2025-05-18 00:49:22.162608: Current learning rate: 0.00055\n",
      "2025-05-18 00:51:11.612115: train_loss -0.9733\n",
      "2025-05-18 00:51:11.612249: val_loss -0.9493\n",
      "2025-05-18 00:51:11.612282: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 00:51:11.612316: Epoch time: 109.45 s\n",
      "2025-05-18 00:51:12.142517: \n",
      "2025-05-18 00:51:12.142760: Epoch 961\n",
      "2025-05-18 00:51:12.142958: Current learning rate: 0.00054\n",
      "2025-05-18 00:53:01.616121: train_loss -0.9751\n",
      "2025-05-18 00:53:01.616261: val_loss -0.9499\n",
      "2025-05-18 00:53:01.616298: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 00:53:01.616333: Epoch time: 109.47 s\n",
      "2025-05-18 00:53:02.143411: \n",
      "2025-05-18 00:53:02.143528: Epoch 962\n",
      "2025-05-18 00:53:02.143599: Current learning rate: 0.00053\n",
      "2025-05-18 00:54:51.616490: train_loss -0.9739\n",
      "2025-05-18 00:54:51.616657: val_loss -0.9458\n",
      "2025-05-18 00:54:51.616688: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-18 00:54:51.616721: Epoch time: 109.47 s\n",
      "2025-05-18 00:54:52.140332: \n",
      "2025-05-18 00:54:52.140720: Epoch 963\n",
      "2025-05-18 00:54:52.140915: Current learning rate: 0.00051\n",
      "2025-05-18 00:56:41.526179: train_loss -0.9733\n",
      "2025-05-18 00:56:41.526387: val_loss -0.9498\n",
      "2025-05-18 00:56:41.526432: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 00:56:41.526467: Epoch time: 109.39 s\n",
      "2025-05-18 00:56:42.054999: \n",
      "2025-05-18 00:56:42.055106: Epoch 964\n",
      "2025-05-18 00:56:42.055175: Current learning rate: 0.0005\n",
      "2025-05-18 00:58:31.488559: train_loss -0.9748\n",
      "2025-05-18 00:58:31.488678: val_loss -0.942\n",
      "2025-05-18 00:58:31.488711: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-18 00:58:31.488743: Epoch time: 109.43 s\n",
      "2025-05-18 00:58:32.015079: \n",
      "2025-05-18 00:58:32.015173: Epoch 965\n",
      "2025-05-18 00:58:32.015240: Current learning rate: 0.00049\n",
      "2025-05-18 01:00:21.471364: train_loss -0.9744\n",
      "2025-05-18 01:00:21.471492: val_loss -0.9535\n",
      "2025-05-18 01:00:21.471527: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 01:00:21.471687: Epoch time: 109.46 s\n",
      "2025-05-18 01:00:22.003443: \n",
      "2025-05-18 01:00:22.003535: Epoch 966\n",
      "2025-05-18 01:00:22.003603: Current learning rate: 0.00048\n",
      "2025-05-18 01:02:11.379128: train_loss -0.975\n",
      "2025-05-18 01:02:11.379263: val_loss -0.9479\n",
      "2025-05-18 01:02:11.379298: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 01:02:11.379332: Epoch time: 109.38 s\n",
      "2025-05-18 01:02:11.905962: \n",
      "2025-05-18 01:02:11.906158: Epoch 967\n",
      "2025-05-18 01:02:11.906251: Current learning rate: 0.00046\n",
      "2025-05-18 01:04:01.261124: train_loss -0.975\n",
      "2025-05-18 01:04:01.261257: val_loss -0.9505\n",
      "2025-05-18 01:04:01.261299: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 01:04:01.261338: Epoch time: 109.36 s\n",
      "2025-05-18 01:04:01.983244: \n",
      "2025-05-18 01:04:01.983587: Epoch 968\n",
      "2025-05-18 01:04:01.983760: Current learning rate: 0.00045\n",
      "2025-05-18 01:05:51.350640: train_loss -0.9743\n",
      "2025-05-18 01:05:51.350774: val_loss -0.9517\n",
      "2025-05-18 01:05:51.350810: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 01:05:51.350845: Epoch time: 109.37 s\n",
      "2025-05-18 01:05:51.875296: \n",
      "2025-05-18 01:05:51.875409: Epoch 969\n",
      "2025-05-18 01:05:51.875478: Current learning rate: 0.00044\n",
      "2025-05-18 01:07:41.249276: train_loss -0.9724\n",
      "2025-05-18 01:07:41.249404: val_loss -0.9444\n",
      "2025-05-18 01:07:41.249439: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 01:07:41.249473: Epoch time: 109.37 s\n",
      "2025-05-18 01:07:41.781530: \n",
      "2025-05-18 01:07:41.781651: Epoch 970\n",
      "2025-05-18 01:07:41.781723: Current learning rate: 0.00043\n",
      "2025-05-18 01:09:31.155651: train_loss -0.9742\n",
      "2025-05-18 01:09:31.155785: val_loss -0.9493\n",
      "2025-05-18 01:09:31.155964: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 01:09:31.156042: Epoch time: 109.37 s\n",
      "2025-05-18 01:09:31.700462: \n",
      "2025-05-18 01:09:31.700596: Epoch 971\n",
      "2025-05-18 01:09:31.700695: Current learning rate: 0.00041\n",
      "2025-05-18 01:11:20.706336: train_loss -0.9738\n",
      "2025-05-18 01:11:20.706477: val_loss -0.953\n",
      "2025-05-18 01:11:20.706511: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-18 01:11:20.706547: Epoch time: 109.01 s\n",
      "2025-05-18 01:11:21.236598: \n",
      "2025-05-18 01:11:21.236717: Epoch 972\n",
      "2025-05-18 01:11:21.236789: Current learning rate: 0.0004\n",
      "2025-05-18 01:13:10.322623: train_loss -0.9746\n",
      "2025-05-18 01:13:10.322737: val_loss -0.9524\n",
      "2025-05-18 01:13:10.322958: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-18 01:13:10.323052: Epoch time: 109.09 s\n",
      "2025-05-18 01:13:10.855913: \n",
      "2025-05-18 01:13:10.856096: Epoch 973\n",
      "2025-05-18 01:13:10.856172: Current learning rate: 0.00039\n",
      "2025-05-18 01:14:59.833226: train_loss -0.9743\n",
      "2025-05-18 01:14:59.833349: val_loss -0.95\n",
      "2025-05-18 01:14:59.833384: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 01:14:59.833418: Epoch time: 108.98 s\n",
      "2025-05-18 01:15:00.361875: \n",
      "2025-05-18 01:15:00.362050: Epoch 974\n",
      "2025-05-18 01:15:00.362144: Current learning rate: 0.00037\n",
      "2025-05-18 01:16:49.386173: train_loss -0.9746\n",
      "2025-05-18 01:16:49.386397: val_loss -0.9487\n",
      "2025-05-18 01:16:49.386478: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-18 01:16:49.386520: Epoch time: 109.02 s\n",
      "2025-05-18 01:16:49.916778: \n",
      "2025-05-18 01:16:49.916876: Epoch 975\n",
      "2025-05-18 01:16:49.916944: Current learning rate: 0.00036\n",
      "2025-05-18 01:18:38.922397: train_loss -0.9751\n",
      "2025-05-18 01:18:38.922528: val_loss -0.9456\n",
      "2025-05-18 01:18:38.922563: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 01:18:38.922605: Epoch time: 109.01 s\n",
      "2025-05-18 01:18:39.478994: \n",
      "2025-05-18 01:18:39.479594: Epoch 976\n",
      "2025-05-18 01:18:39.479704: Current learning rate: 0.00035\n",
      "2025-05-18 01:20:28.436587: train_loss -0.975\n",
      "2025-05-18 01:20:28.436702: val_loss -0.9491\n",
      "2025-05-18 01:20:28.436735: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 01:20:28.436766: Epoch time: 108.96 s\n",
      "2025-05-18 01:20:28.966858: \n",
      "2025-05-18 01:20:28.966954: Epoch 977\n",
      "2025-05-18 01:20:28.967020: Current learning rate: 0.00034\n",
      "2025-05-18 01:22:17.922608: train_loss -0.9743\n",
      "2025-05-18 01:22:17.922780: val_loss -0.9475\n",
      "2025-05-18 01:22:17.922814: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 01:22:17.922846: Epoch time: 108.96 s\n",
      "2025-05-18 01:22:18.440025: \n",
      "2025-05-18 01:22:18.440285: Epoch 978\n",
      "2025-05-18 01:22:18.440444: Current learning rate: 0.00032\n",
      "2025-05-18 01:24:07.439460: train_loss -0.9745\n",
      "2025-05-18 01:24:07.439582: val_loss -0.9529\n",
      "2025-05-18 01:24:07.439617: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-18 01:24:07.439651: Epoch time: 109.0 s\n",
      "2025-05-18 01:24:07.962309: \n",
      "2025-05-18 01:24:07.962489: Epoch 979\n",
      "2025-05-18 01:24:07.962578: Current learning rate: 0.00031\n",
      "2025-05-18 01:25:56.998458: train_loss -0.9737\n",
      "2025-05-18 01:25:56.998904: val_loss -0.95\n",
      "2025-05-18 01:25:56.998984: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-18 01:25:56.999152: Epoch time: 109.04 s\n",
      "2025-05-18 01:25:57.727955: \n",
      "2025-05-18 01:25:57.728071: Epoch 980\n",
      "2025-05-18 01:25:57.728146: Current learning rate: 0.0003\n",
      "2025-05-18 01:27:46.768767: train_loss -0.9752\n",
      "2025-05-18 01:27:46.768897: val_loss -0.9529\n",
      "2025-05-18 01:27:46.768932: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-18 01:27:46.769023: Epoch time: 109.04 s\n",
      "2025-05-18 01:27:47.303807: \n",
      "2025-05-18 01:27:47.304049: Epoch 981\n",
      "2025-05-18 01:27:47.304125: Current learning rate: 0.00028\n",
      "2025-05-18 01:29:36.316852: train_loss -0.9745\n",
      "2025-05-18 01:29:36.316985: val_loss -0.9533\n",
      "2025-05-18 01:29:36.317018: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-18 01:29:36.317051: Epoch time: 109.01 s\n",
      "2025-05-18 01:29:36.317072: Yayy! New best EMA pseudo Dice: 0.9776999950408936\n",
      "2025-05-18 01:29:37.080588: \n",
      "2025-05-18 01:29:37.080697: Epoch 982\n",
      "2025-05-18 01:29:37.080768: Current learning rate: 0.00027\n",
      "2025-05-18 01:31:26.081524: train_loss -0.9744\n",
      "2025-05-18 01:31:26.081651: val_loss -0.9464\n",
      "2025-05-18 01:31:26.081687: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 01:31:26.081720: Epoch time: 109.0 s\n",
      "2025-05-18 01:31:26.613552: \n",
      "2025-05-18 01:31:26.613855: Epoch 983\n",
      "2025-05-18 01:31:26.613937: Current learning rate: 0.00026\n",
      "2025-05-18 01:33:15.609451: train_loss -0.9755\n",
      "2025-05-18 01:33:15.609636: val_loss -0.951\n",
      "2025-05-18 01:33:15.609671: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 01:33:15.609702: Epoch time: 109.0 s\n",
      "2025-05-18 01:33:16.135215: \n",
      "2025-05-18 01:33:16.135317: Epoch 984\n",
      "2025-05-18 01:33:16.135384: Current learning rate: 0.00024\n",
      "2025-05-18 01:35:05.179664: train_loss -0.9739\n",
      "2025-05-18 01:35:05.179792: val_loss -0.9515\n",
      "2025-05-18 01:35:05.179827: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 01:35:05.179860: Epoch time: 109.04 s\n",
      "2025-05-18 01:35:05.706261: \n",
      "2025-05-18 01:35:05.706372: Epoch 985\n",
      "2025-05-18 01:35:05.706440: Current learning rate: 0.00023\n",
      "2025-05-18 01:36:54.802586: train_loss -0.9751\n",
      "2025-05-18 01:36:54.802721: val_loss -0.9467\n",
      "2025-05-18 01:36:54.802759: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 01:36:54.802794: Epoch time: 109.1 s\n",
      "2025-05-18 01:36:55.336714: \n",
      "2025-05-18 01:36:55.336814: Epoch 986\n",
      "2025-05-18 01:36:55.336880: Current learning rate: 0.00021\n",
      "2025-05-18 01:38:44.361340: train_loss -0.9754\n",
      "2025-05-18 01:38:44.361498: val_loss -0.9554\n",
      "2025-05-18 01:38:44.361531: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-18 01:38:44.361565: Epoch time: 109.03 s\n",
      "2025-05-18 01:38:44.361587: Yayy! New best EMA pseudo Dice: 0.9776999950408936\n",
      "2025-05-18 01:38:45.110197: \n",
      "2025-05-18 01:38:45.110296: Epoch 987\n",
      "2025-05-18 01:38:45.110365: Current learning rate: 0.0002\n",
      "2025-05-18 01:40:34.104836: train_loss -0.9747\n",
      "2025-05-18 01:40:34.104953: val_loss -0.9483\n",
      "2025-05-18 01:40:34.104987: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 01:40:34.105020: Epoch time: 109.0 s\n",
      "2025-05-18 01:40:34.625457: \n",
      "2025-05-18 01:40:34.625561: Epoch 988\n",
      "2025-05-18 01:40:34.625627: Current learning rate: 0.00019\n",
      "2025-05-18 01:42:23.659797: train_loss -0.976\n",
      "2025-05-18 01:42:23.659920: val_loss -0.9478\n",
      "2025-05-18 01:42:23.659954: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 01:42:23.659988: Epoch time: 109.03 s\n",
      "2025-05-18 01:42:24.181958: \n",
      "2025-05-18 01:42:24.182055: Epoch 989\n",
      "2025-05-18 01:42:24.182121: Current learning rate: 0.00017\n",
      "2025-05-18 01:44:13.203606: train_loss -0.9739\n",
      "2025-05-18 01:44:13.204030: val_loss -0.9472\n",
      "2025-05-18 01:44:13.204074: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 01:44:13.204110: Epoch time: 109.02 s\n",
      "2025-05-18 01:44:13.732792: \n",
      "2025-05-18 01:44:13.733006: Epoch 990\n",
      "2025-05-18 01:44:13.733090: Current learning rate: 0.00016\n",
      "2025-05-18 01:46:02.671456: train_loss -0.9746\n",
      "2025-05-18 01:46:02.671663: val_loss -0.9478\n",
      "2025-05-18 01:46:02.671725: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-18 01:46:02.672250: Epoch time: 108.94 s\n",
      "2025-05-18 01:46:03.199955: \n",
      "2025-05-18 01:46:03.200106: Epoch 991\n",
      "2025-05-18 01:46:03.200196: Current learning rate: 0.00014\n",
      "2025-05-18 01:47:52.190971: train_loss -0.9738\n",
      "2025-05-18 01:47:52.191090: val_loss -0.9479\n",
      "2025-05-18 01:47:52.191123: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 01:47:52.191157: Epoch time: 108.99 s\n",
      "2025-05-18 01:47:52.715511: \n",
      "2025-05-18 01:47:52.715623: Epoch 992\n",
      "2025-05-18 01:47:52.715761: Current learning rate: 0.00013\n",
      "2025-05-18 01:49:41.743944: train_loss -0.9749\n",
      "2025-05-18 01:49:41.744092: val_loss -0.9519\n",
      "2025-05-18 01:49:41.744285: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 01:49:41.744400: Epoch time: 109.03 s\n",
      "2025-05-18 01:49:42.462263: \n",
      "2025-05-18 01:49:42.462385: Epoch 993\n",
      "2025-05-18 01:49:42.462453: Current learning rate: 0.00011\n",
      "2025-05-18 01:51:31.422808: train_loss -0.9749\n",
      "2025-05-18 01:51:31.422946: val_loss -0.9429\n",
      "2025-05-18 01:51:31.422986: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 01:51:31.423033: Epoch time: 108.96 s\n",
      "2025-05-18 01:51:31.950988: \n",
      "2025-05-18 01:51:31.951313: Epoch 994\n",
      "2025-05-18 01:51:31.951464: Current learning rate: 0.0001\n",
      "2025-05-18 01:53:20.963792: train_loss -0.9741\n",
      "2025-05-18 01:53:20.963999: val_loss -0.9448\n",
      "2025-05-18 01:53:20.964144: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-18 01:53:20.964313: Epoch time: 109.01 s\n",
      "2025-05-18 01:53:21.502211: \n",
      "2025-05-18 01:53:21.502324: Epoch 995\n",
      "2025-05-18 01:53:21.502393: Current learning rate: 8e-05\n",
      "2025-05-18 01:55:10.552572: train_loss -0.9746\n",
      "2025-05-18 01:55:10.552733: val_loss -0.9518\n",
      "2025-05-18 01:55:10.552769: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 01:55:10.552804: Epoch time: 109.05 s\n",
      "2025-05-18 01:55:11.080577: \n",
      "2025-05-18 01:55:11.080695: Epoch 996\n",
      "2025-05-18 01:55:11.080767: Current learning rate: 7e-05\n",
      "2025-05-18 01:57:00.042180: train_loss -0.9749\n",
      "2025-05-18 01:57:00.042370: val_loss -0.9472\n",
      "2025-05-18 01:57:00.042406: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-18 01:57:00.042530: Epoch time: 108.96 s\n",
      "2025-05-18 01:57:00.565186: \n",
      "2025-05-18 01:57:00.565556: Epoch 997\n",
      "2025-05-18 01:57:00.565662: Current learning rate: 5e-05\n",
      "2025-05-18 01:58:49.610056: train_loss -0.9746\n",
      "2025-05-18 01:58:49.610182: val_loss -0.9485\n",
      "2025-05-18 01:58:49.610222: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 01:58:49.610257: Epoch time: 109.05 s\n",
      "2025-05-18 01:58:50.140981: \n",
      "2025-05-18 01:58:50.141108: Epoch 998\n",
      "2025-05-18 01:58:50.141260: Current learning rate: 4e-05\n",
      "2025-05-18 02:00:39.176429: train_loss -0.9744\n",
      "2025-05-18 02:00:39.176546: val_loss -0.95\n",
      "2025-05-18 02:00:39.176579: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-18 02:00:39.176612: Epoch time: 109.04 s\n",
      "2025-05-18 02:00:39.703868: \n",
      "2025-05-18 02:00:39.703971: Epoch 999\n",
      "2025-05-18 02:00:39.704042: Current learning rate: 2e-05\n",
      "2025-05-18 02:02:28.733763: train_loss -0.9742\n",
      "2025-05-18 02:02:28.733886: val_loss -0.9483\n",
      "2025-05-18 02:02:28.733919: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 02:02:28.733952: Epoch time: 109.03 s\n",
      "2025-05-18 02:02:29.509223: Training done.\n",
      "2025-05-18 02:02:29.520042: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-18 02:02:29.520306: The split file contains 5 splits.\n",
      "2025-05-18 02:02:29.520391: Desired fold for training: 3\n",
      "2025-05-18 02:02:29.520440: This split has 29 training and 7 validation cases.\n",
      "2025-05-18 02:02:29.520569: predicting LCTSC-Train-S1-010\n",
      "2025-05-18 02:02:29.521571: LCTSC-Train-S1-010, shape torch.Size([1, 146, 512, 512]), rank 0\n",
      "2025-05-18 02:03:34.502717: predicting LCTSC-Train-S2-003\n",
      "2025-05-18 02:03:34.504689: LCTSC-Train-S2-003, shape torch.Size([1, 144, 512, 512]), rank 0\n",
      "2025-05-18 02:04:33.490401: predicting LCTSC-Train-S2-005\n",
      "2025-05-18 02:04:33.492636: LCTSC-Train-S2-005, shape torch.Size([1, 152, 512, 512]), rank 0\n",
      "2025-05-18 02:05:32.474530: predicting LCTSC-Train-S2-007\n",
      "2025-05-18 02:05:32.476631: LCTSC-Train-S2-007, shape torch.Size([1, 160, 512, 512]), rank 0\n",
      "2025-05-18 02:06:31.487285: predicting LCTSC-Train-S2-012\n",
      "2025-05-18 02:06:31.489535: LCTSC-Train-S2-012, shape torch.Size([1, 160, 512, 512]), rank 0\n",
      "2025-05-18 02:07:30.482110: predicting LCTSC-Train-S3-004\n",
      "2025-05-18 02:07:30.484369: LCTSC-Train-S3-004, shape torch.Size([1, 223, 717, 717]), rank 0\n",
      "2025-05-18 02:10:12.578014: predicting LCTSC-Train-S3-007\n",
      "2025-05-18 02:10:12.583026: LCTSC-Train-S3-007, shape torch.Size([1, 196, 716, 716]), rank 0\n",
      "2025-05-18 02:12:42.754270: Validation complete\n",
      "2025-05-18 02:12:42.754335: Mean Validation Dice:  0.9764240664019471\n",
      "🔁 Running Fold 4: nnUNetv2_train 3 3d_fullres 4 --c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "WARNING: Cannot continue training because there seems to be no checkpoint available to continue from. Starting a new training...\n",
      "2025-05-18 02:12:47.008605: do_dummy_2d_data_aug: True\n",
      "2025-05-18 02:12:47.008881: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-18 02:12:47.008972: The split file contains 5 splits.\n",
      "2025-05-18 02:12:47.009001: Desired fold for training: 4\n",
      "2025-05-18 02:12:47.009017: This split has 29 training and 7 validation cases.\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pin_memory on device 0\n",
      "2025-05-18 02:12:48.930384: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [56, 192, 192], 'median_image_size_in_voxels': [162.5, 512.0, 512.0], 'spacing': [2.5, 0.9765625, 0.9765625], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset003_Lung_only', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.9765625, 0.9765625], 'original_median_shape_after_transp': [156, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1910.0, 'mean': -742.67333984375, 'median': -794.0, 'min': -1023.0, 'percentile_00_5': -981.0, 'percentile_99_5': -51.0, 'std': 175.66612243652344}}} \n",
      "\n",
      "2025-05-18 02:12:49.509119: unpacking dataset...\n",
      "2025-05-18 02:12:53.414033: unpacking done...\n",
      "2025-05-18 02:12:53.414928: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-05-18 02:12:53.417599: \n",
      "2025-05-18 02:12:53.417646: Epoch 0\n",
      "2025-05-18 02:12:53.417715: Current learning rate: 0.01\n",
      "2025-05-18 02:15:01.047931: train_loss -0.4684\n",
      "2025-05-18 02:15:01.048181: val_loss -0.6631\n",
      "2025-05-18 02:15:01.048227: Pseudo dice [np.float32(0.8588)]\n",
      "2025-05-18 02:15:01.048262: Epoch time: 127.63 s\n",
      "2025-05-18 02:15:01.048323: Yayy! New best EMA pseudo Dice: 0.8587999939918518\n",
      "2025-05-18 02:15:01.675324: \n",
      "2025-05-18 02:15:01.675414: Epoch 1\n",
      "2025-05-18 02:15:01.675484: Current learning rate: 0.00999\n",
      "2025-05-18 02:16:50.599135: train_loss -0.7718\n",
      "2025-05-18 02:16:50.599255: val_loss -0.7933\n",
      "2025-05-18 02:16:50.599289: Pseudo dice [np.float32(0.9165)]\n",
      "2025-05-18 02:16:50.599343: Epoch time: 108.92 s\n",
      "2025-05-18 02:16:50.599370: Yayy! New best EMA pseudo Dice: 0.8646000027656555\n",
      "2025-05-18 02:16:51.439638: \n",
      "2025-05-18 02:16:51.439945: Epoch 2\n",
      "2025-05-18 02:16:51.440016: Current learning rate: 0.00998\n",
      "2025-05-18 02:18:40.413419: train_loss -0.8029\n",
      "2025-05-18 02:18:40.413620: val_loss -0.6726\n",
      "2025-05-18 02:18:40.413697: Pseudo dice [np.float32(0.8742)]\n",
      "2025-05-18 02:18:40.413740: Epoch time: 108.97 s\n",
      "2025-05-18 02:18:40.413763: Yayy! New best EMA pseudo Dice: 0.8654999732971191\n",
      "2025-05-18 02:18:41.126747: \n",
      "2025-05-18 02:18:41.126893: Epoch 3\n",
      "2025-05-18 02:18:41.126961: Current learning rate: 0.00997\n",
      "2025-05-18 02:20:30.035159: train_loss -0.8016\n",
      "2025-05-18 02:20:30.035289: val_loss -0.8216\n",
      "2025-05-18 02:20:30.035323: Pseudo dice [np.float32(0.9305)]\n",
      "2025-05-18 02:20:30.035358: Epoch time: 108.91 s\n",
      "2025-05-18 02:20:30.035379: Yayy! New best EMA pseudo Dice: 0.871999979019165\n",
      "2025-05-18 02:20:30.743322: \n",
      "2025-05-18 02:20:30.743404: Epoch 4\n",
      "2025-05-18 02:20:30.743467: Current learning rate: 0.00996\n",
      "2025-05-18 02:22:19.668634: train_loss -0.8146\n",
      "2025-05-18 02:22:19.668759: val_loss -0.708\n",
      "2025-05-18 02:22:19.668792: Pseudo dice [np.float32(0.8787)]\n",
      "2025-05-18 02:22:19.668825: Epoch time: 108.93 s\n",
      "2025-05-18 02:22:19.668844: Yayy! New best EMA pseudo Dice: 0.8726999759674072\n",
      "2025-05-18 02:22:20.397945: \n",
      "2025-05-18 02:22:20.398109: Epoch 5\n",
      "2025-05-18 02:22:20.398177: Current learning rate: 0.00995\n",
      "2025-05-18 02:24:09.363869: train_loss -0.8047\n",
      "2025-05-18 02:24:09.364049: val_loss -0.7836\n",
      "2025-05-18 02:24:09.364198: Pseudo dice [np.float32(0.9141)]\n",
      "2025-05-18 02:24:09.364281: Epoch time: 108.97 s\n",
      "2025-05-18 02:24:09.364311: Yayy! New best EMA pseudo Dice: 0.876800000667572\n",
      "2025-05-18 02:24:10.066467: \n",
      "2025-05-18 02:24:10.066650: Epoch 6\n",
      "2025-05-18 02:24:10.066721: Current learning rate: 0.00995\n",
      "2025-05-18 02:25:59.054727: train_loss -0.8551\n",
      "2025-05-18 02:25:59.054897: val_loss -0.8014\n",
      "2025-05-18 02:25:59.054931: Pseudo dice [np.float32(0.917)]\n",
      "2025-05-18 02:25:59.054965: Epoch time: 108.99 s\n",
      "2025-05-18 02:25:59.054986: Yayy! New best EMA pseudo Dice: 0.8809000253677368\n",
      "2025-05-18 02:25:59.763925: \n",
      "2025-05-18 02:25:59.764189: Epoch 7\n",
      "2025-05-18 02:25:59.764283: Current learning rate: 0.00994\n",
      "2025-05-18 02:27:48.643797: train_loss -0.8515\n",
      "2025-05-18 02:27:48.643922: val_loss -0.8161\n",
      "2025-05-18 02:27:48.643956: Pseudo dice [np.float32(0.9288)]\n",
      "2025-05-18 02:27:48.644004: Epoch time: 108.88 s\n",
      "2025-05-18 02:27:48.644025: Yayy! New best EMA pseudo Dice: 0.885699987411499\n",
      "2025-05-18 02:27:49.368525: \n",
      "2025-05-18 02:27:49.368602: Epoch 8\n",
      "2025-05-18 02:27:49.368661: Current learning rate: 0.00993\n",
      "2025-05-18 02:29:38.284096: train_loss -0.8232\n",
      "2025-05-18 02:29:38.284216: val_loss -0.8088\n",
      "2025-05-18 02:29:38.284252: Pseudo dice [np.float32(0.9218)]\n",
      "2025-05-18 02:29:38.284285: Epoch time: 108.92 s\n",
      "2025-05-18 02:29:38.284307: Yayy! New best EMA pseudo Dice: 0.8892999887466431\n",
      "2025-05-18 02:29:39.015567: \n",
      "2025-05-18 02:29:39.015861: Epoch 9\n",
      "2025-05-18 02:29:39.015942: Current learning rate: 0.00992\n",
      "2025-05-18 02:31:27.952146: train_loss -0.8772\n",
      "2025-05-18 02:31:27.952260: val_loss -0.8556\n",
      "2025-05-18 02:31:27.952303: Pseudo dice [np.float32(0.9463)]\n",
      "2025-05-18 02:31:27.952339: Epoch time: 108.94 s\n",
      "2025-05-18 02:31:27.952361: Yayy! New best EMA pseudo Dice: 0.8949999809265137\n",
      "2025-05-18 02:31:28.658183: \n",
      "2025-05-18 02:31:28.658272: Epoch 10\n",
      "2025-05-18 02:31:28.658365: Current learning rate: 0.00991\n",
      "2025-05-18 02:33:17.507962: train_loss -0.8548\n",
      "2025-05-18 02:33:17.508242: val_loss -0.875\n",
      "2025-05-18 02:33:17.508474: Pseudo dice [np.float32(0.9545)]\n",
      "2025-05-18 02:33:17.508545: Epoch time: 108.85 s\n",
      "2025-05-18 02:33:17.508577: Yayy! New best EMA pseudo Dice: 0.9009000062942505\n",
      "2025-05-18 02:33:18.218770: \n",
      "2025-05-18 02:33:18.218862: Epoch 11\n",
      "2025-05-18 02:33:18.218925: Current learning rate: 0.0099\n",
      "2025-05-18 02:35:07.091156: train_loss -0.892\n",
      "2025-05-18 02:35:07.091304: val_loss -0.8873\n",
      "2025-05-18 02:35:07.091487: Pseudo dice [np.float32(0.9554)]\n",
      "2025-05-18 02:35:07.091570: Epoch time: 108.87 s\n",
      "2025-05-18 02:35:07.091603: Yayy! New best EMA pseudo Dice: 0.9064000248908997\n",
      "2025-05-18 02:35:07.794040: \n",
      "2025-05-18 02:35:07.794118: Epoch 12\n",
      "2025-05-18 02:35:07.794360: Current learning rate: 0.00989\n",
      "2025-05-18 02:36:56.761224: train_loss -0.899\n",
      "2025-05-18 02:36:56.761420: val_loss -0.9096\n",
      "2025-05-18 02:36:56.761456: Pseudo dice [np.float32(0.9644)]\n",
      "2025-05-18 02:36:56.761489: Epoch time: 108.97 s\n",
      "2025-05-18 02:36:56.761511: Yayy! New best EMA pseudo Dice: 0.9121999740600586\n",
      "2025-05-18 02:36:57.479413: \n",
      "2025-05-18 02:36:57.479582: Epoch 13\n",
      "2025-05-18 02:36:57.479681: Current learning rate: 0.00988\n",
      "2025-05-18 02:38:46.404158: train_loss -0.871\n",
      "2025-05-18 02:38:46.404287: val_loss -0.8692\n",
      "2025-05-18 02:38:46.404324: Pseudo dice [np.float32(0.9446)]\n",
      "2025-05-18 02:38:46.404440: Epoch time: 108.93 s\n",
      "2025-05-18 02:38:46.404471: Yayy! New best EMA pseudo Dice: 0.9154000282287598\n",
      "2025-05-18 02:38:47.296971: \n",
      "2025-05-18 02:38:47.297139: Epoch 14\n",
      "2025-05-18 02:38:47.297207: Current learning rate: 0.00987\n",
      "2025-05-18 02:40:36.235388: train_loss -0.879\n",
      "2025-05-18 02:40:36.235581: val_loss -0.9069\n",
      "2025-05-18 02:40:36.235769: Pseudo dice [np.float32(0.9637)]\n",
      "2025-05-18 02:40:36.235815: Epoch time: 108.94 s\n",
      "2025-05-18 02:40:36.235837: Yayy! New best EMA pseudo Dice: 0.920199990272522\n",
      "2025-05-18 02:40:36.956573: \n",
      "2025-05-18 02:40:36.956659: Epoch 15\n",
      "2025-05-18 02:40:36.956720: Current learning rate: 0.00986\n",
      "2025-05-18 02:42:25.913891: train_loss -0.8845\n",
      "2025-05-18 02:42:25.914013: val_loss -0.8449\n",
      "2025-05-18 02:42:25.914048: Pseudo dice [np.float32(0.9362)]\n",
      "2025-05-18 02:42:25.914080: Epoch time: 108.96 s\n",
      "2025-05-18 02:42:25.914104: Yayy! New best EMA pseudo Dice: 0.9218000173568726\n",
      "2025-05-18 02:42:26.636950: \n",
      "2025-05-18 02:42:26.637039: Epoch 16\n",
      "2025-05-18 02:42:26.637101: Current learning rate: 0.00986\n",
      "2025-05-18 02:44:15.600631: train_loss -0.871\n",
      "2025-05-18 02:44:15.600753: val_loss -0.8697\n",
      "2025-05-18 02:44:15.600786: Pseudo dice [np.float32(0.9488)]\n",
      "2025-05-18 02:44:15.600817: Epoch time: 108.96 s\n",
      "2025-05-18 02:44:15.601000: Yayy! New best EMA pseudo Dice: 0.9244999885559082\n",
      "2025-05-18 02:44:16.321997: \n",
      "2025-05-18 02:44:16.322079: Epoch 17\n",
      "2025-05-18 02:44:16.322143: Current learning rate: 0.00985\n",
      "2025-05-18 02:46:05.243501: train_loss -0.8768\n",
      "2025-05-18 02:46:05.243714: val_loss -0.874\n",
      "2025-05-18 02:46:05.243828: Pseudo dice [np.float32(0.9545)]\n",
      "2025-05-18 02:46:05.243875: Epoch time: 108.92 s\n",
      "2025-05-18 02:46:05.243899: Yayy! New best EMA pseudo Dice: 0.9275000095367432\n",
      "2025-05-18 02:46:05.976231: \n",
      "2025-05-18 02:46:05.976457: Epoch 18\n",
      "2025-05-18 02:46:05.976529: Current learning rate: 0.00984\n",
      "2025-05-18 02:47:54.900133: train_loss -0.8812\n",
      "2025-05-18 02:47:54.900315: val_loss -0.8835\n",
      "2025-05-18 02:47:54.900357: Pseudo dice [np.float32(0.9533)]\n",
      "2025-05-18 02:47:54.900391: Epoch time: 108.92 s\n",
      "2025-05-18 02:47:54.900411: Yayy! New best EMA pseudo Dice: 0.9301000237464905\n",
      "2025-05-18 02:47:55.623497: \n",
      "2025-05-18 02:47:55.623577: Epoch 19\n",
      "2025-05-18 02:47:55.623639: Current learning rate: 0.00983\n",
      "2025-05-18 02:49:44.518090: train_loss -0.8973\n",
      "2025-05-18 02:49:44.518456: val_loss -0.9094\n",
      "2025-05-18 02:49:44.518529: Pseudo dice [np.float32(0.9607)]\n",
      "2025-05-18 02:49:44.518570: Epoch time: 108.9 s\n",
      "Yayy! New best EMA pseudo Dice: 0.9332000017166138\n",
      "2025-05-18 02:49:45.241330: \n",
      "2025-05-18 02:49:45.241414: Epoch 20\n",
      "2025-05-18 02:49:45.241477: Current learning rate: 0.00982\n",
      "2025-05-18 02:51:34.108042: train_loss -0.9153\n",
      "2025-05-18 02:51:34.108164: val_loss -0.9114\n",
      "2025-05-18 02:51:34.108199: Pseudo dice [np.float32(0.9649)]\n",
      "2025-05-18 02:51:34.108232: Epoch time: 108.87 s\n",
      "2025-05-18 02:51:34.108254: Yayy! New best EMA pseudo Dice: 0.9362999796867371\n",
      "2025-05-18 02:51:34.845014: \n",
      "2025-05-18 02:51:34.845168: Epoch 21\n",
      "2025-05-18 02:51:34.845311: Current learning rate: 0.00981\n",
      "2025-05-18 02:53:23.738787: train_loss -0.9251\n",
      "2025-05-18 02:53:23.738966: val_loss -0.9294\n",
      "2025-05-18 02:53:23.739015: Pseudo dice [np.float32(0.9717)]\n",
      "2025-05-18 02:53:23.739050: Epoch time: 108.89 s\n",
      "2025-05-18 02:53:23.739072: Yayy! New best EMA pseudo Dice: 0.9398999810218811\n",
      "2025-05-18 02:53:24.447850: \n",
      "2025-05-18 02:53:24.448244: Epoch 22\n",
      "2025-05-18 02:53:24.448535: Current learning rate: 0.0098\n",
      "2025-05-18 02:55:13.444333: train_loss -0.899\n",
      "2025-05-18 02:55:13.444515: val_loss -0.8864\n",
      "2025-05-18 02:55:13.444549: Pseudo dice [np.float32(0.953)]\n",
      "2025-05-18 02:55:13.444579: Epoch time: 109.0 s\n",
      "2025-05-18 02:55:13.444613: Yayy! New best EMA pseudo Dice: 0.9412000179290771\n",
      "2025-05-18 02:55:14.154243: \n",
      "2025-05-18 02:55:14.154373: Epoch 23\n",
      "2025-05-18 02:55:14.154439: Current learning rate: 0.00979\n",
      "2025-05-18 02:57:03.049019: train_loss -0.9101\n",
      "2025-05-18 02:57:03.049170: val_loss -0.9114\n",
      "2025-05-18 02:57:03.049207: Pseudo dice [np.float32(0.966)]\n",
      "2025-05-18 02:57:03.049239: Epoch time: 108.9 s\n",
      "2025-05-18 02:57:03.049391: Yayy! New best EMA pseudo Dice: 0.9437000155448914\n",
      "2025-05-18 02:57:03.756478: \n",
      "2025-05-18 02:57:03.756557: Epoch 24\n",
      "2025-05-18 02:57:03.756617: Current learning rate: 0.00978\n",
      "2025-05-18 02:58:52.713327: train_loss -0.9211\n",
      "2025-05-18 02:58:52.713632: val_loss -0.8947\n",
      "2025-05-18 02:58:52.713785: Pseudo dice [np.float32(0.9567)]\n",
      "2025-05-18 02:58:52.713840: Epoch time: 108.96 s\n",
      "2025-05-18 02:58:52.713864: Yayy! New best EMA pseudo Dice: 0.9449999928474426\n",
      "2025-05-18 02:58:53.424435: \n",
      "2025-05-18 02:58:53.424514: Epoch 25\n",
      "2025-05-18 02:58:53.424584: Current learning rate: 0.00977\n",
      "2025-05-18 03:00:42.353180: train_loss -0.9229\n",
      "2025-05-18 03:00:42.353305: val_loss -0.8981\n",
      "2025-05-18 03:00:42.353339: Pseudo dice [np.float32(0.9607)]\n",
      "2025-05-18 03:00:42.353372: Epoch time: 108.93 s\n",
      "2025-05-18 03:00:42.353392: Yayy! New best EMA pseudo Dice: 0.9465000033378601\n",
      "2025-05-18 03:00:43.056742: \n",
      "2025-05-18 03:00:43.056819: Epoch 26\n",
      "2025-05-18 03:00:43.056885: Current learning rate: 0.00977\n",
      "2025-05-18 03:02:31.969609: train_loss -0.8963\n",
      "2025-05-18 03:02:31.969791: val_loss -0.9013\n",
      "2025-05-18 03:02:31.969939: Pseudo dice [np.float32(0.9631)]\n",
      "2025-05-18 03:02:31.970012: Epoch time: 108.91 s\n",
      "2025-05-18 03:02:31.970042: Yayy! New best EMA pseudo Dice: 0.948199987411499\n",
      "2025-05-18 03:02:32.853150: \n",
      "2025-05-18 03:02:32.853254: Epoch 27\n",
      "2025-05-18 03:02:32.853327: Current learning rate: 0.00976\n",
      "2025-05-18 03:04:21.760900: train_loss -0.9077\n",
      "2025-05-18 03:04:21.761112: val_loss -0.8894\n",
      "2025-05-18 03:04:21.761186: Pseudo dice [np.float32(0.9553)]\n",
      "2025-05-18 03:04:21.761228: Epoch time: 108.91 s\n",
      "2025-05-18 03:04:21.761250: Yayy! New best EMA pseudo Dice: 0.9488999843597412\n",
      "2025-05-18 03:04:22.465708: \n",
      "2025-05-18 03:04:22.465807: Epoch 28\n",
      "2025-05-18 03:04:22.465872: Current learning rate: 0.00975\n",
      "2025-05-18 03:06:11.358488: train_loss -0.9092\n",
      "2025-05-18 03:06:11.358647: val_loss -0.8934\n",
      "2025-05-18 03:06:11.358729: Pseudo dice [np.float32(0.957)]\n",
      "2025-05-18 03:06:11.358771: Epoch time: 108.89 s\n",
      "2025-05-18 03:06:11.358794: Yayy! New best EMA pseudo Dice: 0.9496999979019165\n",
      "2025-05-18 03:06:12.061806: \n",
      "2025-05-18 03:06:12.062582: Epoch 29\n",
      "2025-05-18 03:06:12.062696: Current learning rate: 0.00974\n",
      "2025-05-18 03:08:01.052711: train_loss -0.9159\n",
      "2025-05-18 03:08:01.052836: val_loss -0.9219\n",
      "2025-05-18 03:08:01.052884: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-18 03:08:01.052919: Epoch time: 108.99 s\n",
      "2025-05-18 03:08:01.052939: Yayy! New best EMA pseudo Dice: 0.95169997215271\n",
      "2025-05-18 03:08:01.773475: \n",
      "2025-05-18 03:08:01.773640: Epoch 30\n",
      "2025-05-18 03:08:01.773769: Current learning rate: 0.00973\n",
      "2025-05-18 03:09:50.685003: train_loss -0.9097\n",
      "2025-05-18 03:09:50.685121: val_loss -0.9105\n",
      "2025-05-18 03:09:50.685154: Pseudo dice [np.float32(0.966)]\n",
      "2025-05-18 03:09:50.685199: Epoch time: 108.91 s\n",
      "2025-05-18 03:09:50.685225: Yayy! New best EMA pseudo Dice: 0.9531000256538391\n",
      "2025-05-18 03:09:51.396666: \n",
      "2025-05-18 03:09:51.396803: Epoch 31\n",
      "2025-05-18 03:09:51.396870: Current learning rate: 0.00972\n",
      "2025-05-18 03:11:40.325694: train_loss -0.9093\n",
      "2025-05-18 03:11:40.325818: val_loss -0.902\n",
      "2025-05-18 03:11:40.325853: Pseudo dice [np.float32(0.9599)]\n",
      "2025-05-18 03:11:40.325904: Epoch time: 108.93 s\n",
      "2025-05-18 03:11:40.325939: Yayy! New best EMA pseudo Dice: 0.9538000226020813\n",
      "2025-05-18 03:11:41.043107: \n",
      "2025-05-18 03:11:41.043194: Epoch 32\n",
      "2025-05-18 03:11:41.043258: Current learning rate: 0.00971\n",
      "2025-05-18 03:13:30.029660: train_loss -0.9157\n",
      "2025-05-18 03:13:30.029785: val_loss -0.9179\n",
      "2025-05-18 03:13:30.029820: Pseudo dice [np.float32(0.9673)]\n",
      "2025-05-18 03:13:30.029854: Epoch time: 108.99 s\n",
      "2025-05-18 03:13:30.029874: Yayy! New best EMA pseudo Dice: 0.9550999999046326\n",
      "2025-05-18 03:13:30.746185: \n",
      "2025-05-18 03:13:30.746273: Epoch 33\n",
      "2025-05-18 03:13:30.746337: Current learning rate: 0.0097\n",
      "2025-05-18 03:15:19.649068: train_loss -0.9172\n",
      "2025-05-18 03:15:19.649316: val_loss -0.9185\n",
      "2025-05-18 03:15:19.649355: Pseudo dice [np.float32(0.9684)]\n",
      "2025-05-18 03:15:19.649393: Epoch time: 108.9 s\n",
      "2025-05-18 03:15:19.649415: Yayy! New best EMA pseudo Dice: 0.9564999938011169\n",
      "2025-05-18 03:15:20.373556: \n",
      "2025-05-18 03:15:20.373641: Epoch 34\n",
      "2025-05-18 03:15:20.373702: Current learning rate: 0.00969\n",
      "2025-05-18 03:17:09.343651: train_loss -0.9263\n",
      "2025-05-18 03:17:09.343775: val_loss -0.9185\n",
      "2025-05-18 03:17:09.343809: Pseudo dice [np.float32(0.967)]\n",
      "2025-05-18 03:17:09.343842: Epoch time: 108.97 s\n",
      "2025-05-18 03:17:09.343860: Yayy! New best EMA pseudo Dice: 0.9574999809265137\n",
      "2025-05-18 03:17:10.065036: \n",
      "2025-05-18 03:17:10.065302: Epoch 35\n",
      "2025-05-18 03:17:10.065383: Current learning rate: 0.00968\n",
      "2025-05-18 03:18:59.027441: train_loss -0.9334\n",
      "2025-05-18 03:18:59.027739: val_loss -0.9179\n",
      "2025-05-18 03:18:59.027808: Pseudo dice [np.float32(0.9688)]\n",
      "2025-05-18 03:18:59.027847: Epoch time: 108.96 s\n",
      "2025-05-18 03:18:59.027870: Yayy! New best EMA pseudo Dice: 0.9587000012397766\n",
      "2025-05-18 03:18:59.747992: \n",
      "2025-05-18 03:18:59.748179: Epoch 36\n",
      "2025-05-18 03:18:59.748338: Current learning rate: 0.00968\n",
      "2025-05-18 03:20:48.642767: train_loss -0.9339\n",
      "2025-05-18 03:20:48.642901: val_loss -0.9172\n",
      "2025-05-18 03:20:48.642935: Pseudo dice [np.float32(0.9691)]\n",
      "2025-05-18 03:20:48.642968: Epoch time: 108.9 s\n",
      "2025-05-18 03:20:48.642988: Yayy! New best EMA pseudo Dice: 0.9596999883651733\n",
      "2025-05-18 03:20:49.356800: \n",
      "2025-05-18 03:20:49.357017: Epoch 37\n",
      "2025-05-18 03:20:49.357462: Current learning rate: 0.00967\n",
      "2025-05-18 03:22:38.300643: train_loss -0.8898\n",
      "2025-05-18 03:22:38.300766: val_loss -0.8967\n",
      "2025-05-18 03:22:38.300801: Pseudo dice [np.float32(0.9609)]\n",
      "2025-05-18 03:22:38.300834: Epoch time: 108.94 s\n",
      "2025-05-18 03:22:38.300854: Yayy! New best EMA pseudo Dice: 0.9598000049591064\n",
      "2025-05-18 03:22:39.023631: \n",
      "2025-05-18 03:22:39.023777: Epoch 38\n",
      "2025-05-18 03:22:39.023879: Current learning rate: 0.00966\n",
      "2025-05-18 03:24:27.940672: train_loss -0.9024\n",
      "2025-05-18 03:24:27.940795: val_loss -0.9157\n",
      "2025-05-18 03:24:27.940830: Pseudo dice [np.float32(0.9671)]\n",
      "2025-05-18 03:24:27.940863: Epoch time: 108.92 s\n",
      "2025-05-18 03:24:27.940890: Yayy! New best EMA pseudo Dice: 0.9606000185012817\n",
      "2025-05-18 03:24:28.838205: \n",
      "2025-05-18 03:24:28.838397: Epoch 39\n",
      "2025-05-18 03:24:28.838484: Current learning rate: 0.00965\n",
      "2025-05-18 03:26:17.788608: train_loss -0.9212\n",
      "2025-05-18 03:26:17.788880: val_loss -0.9131\n",
      "2025-05-18 03:26:17.789045: Pseudo dice [np.float32(0.9637)]\n",
      "2025-05-18 03:26:17.789130: Epoch time: 108.95 s\n",
      "2025-05-18 03:26:17.789169: Yayy! New best EMA pseudo Dice: 0.9609000086784363\n",
      "2025-05-18 03:26:18.516663: \n",
      "2025-05-18 03:26:18.516832: Epoch 40\n",
      "2025-05-18 03:26:18.517048: Current learning rate: 0.00964\n",
      "2025-05-18 03:28:07.408532: train_loss -0.9195\n",
      "2025-05-18 03:28:07.408788: val_loss -0.9095\n",
      "2025-05-18 03:28:07.408852: Pseudo dice [np.float32(0.9641)]\n",
      "2025-05-18 03:28:07.408892: Epoch time: 108.89 s\n",
      "2025-05-18 03:28:07.408915: Yayy! New best EMA pseudo Dice: 0.9611999988555908\n",
      "2025-05-18 03:28:08.142206: \n",
      "2025-05-18 03:28:08.142303: Epoch 41\n",
      "2025-05-18 03:28:08.142369: Current learning rate: 0.00963\n",
      "2025-05-18 03:29:57.081699: train_loss -0.9216\n",
      "2025-05-18 03:29:57.081870: val_loss -0.9136\n",
      "2025-05-18 03:29:57.081946: Pseudo dice [np.float32(0.9671)]\n",
      "2025-05-18 03:29:57.081995: Epoch time: 108.94 s\n",
      "2025-05-18 03:29:57.082027: Yayy! New best EMA pseudo Dice: 0.9617999792098999\n",
      "2025-05-18 03:29:57.790032: \n",
      "2025-05-18 03:29:57.790269: Epoch 42\n",
      "2025-05-18 03:29:57.790402: Current learning rate: 0.00962\n",
      "2025-05-18 03:31:46.753147: train_loss -0.9344\n",
      "2025-05-18 03:31:46.753422: val_loss -0.9254\n",
      "2025-05-18 03:31:46.753458: Pseudo dice [np.float32(0.9707)]\n",
      "2025-05-18 03:31:46.753492: Epoch time: 108.96 s\n",
      "2025-05-18 03:31:46.753512: Yayy! New best EMA pseudo Dice: 0.9627000093460083\n",
      "2025-05-18 03:31:47.468429: \n",
      "2025-05-18 03:31:47.468590: Epoch 43\n",
      "2025-05-18 03:31:47.468866: Current learning rate: 0.00961\n",
      "2025-05-18 03:33:36.348773: train_loss -0.8991\n",
      "2025-05-18 03:33:36.349074: val_loss -0.9034\n",
      "2025-05-18 03:33:36.349157: Pseudo dice [np.float32(0.9613)]\n",
      "2025-05-18 03:33:36.349230: Epoch time: 108.88 s\n",
      "2025-05-18 03:33:36.851956: \n",
      "2025-05-18 03:33:36.852064: Epoch 44\n",
      "2025-05-18 03:33:36.852137: Current learning rate: 0.0096\n",
      "2025-05-18 03:35:25.822964: train_loss -0.9189\n",
      "2025-05-18 03:35:25.823103: val_loss -0.9142\n",
      "2025-05-18 03:35:25.823141: Pseudo dice [np.float32(0.9666)]\n",
      "2025-05-18 03:35:25.823174: Epoch time: 108.97 s\n",
      "2025-05-18 03:35:25.823195: Yayy! New best EMA pseudo Dice: 0.9628999829292297\n",
      "2025-05-18 03:35:26.536247: \n",
      "2025-05-18 03:35:26.536500: Epoch 45\n",
      "2025-05-18 03:35:26.536570: Current learning rate: 0.00959\n",
      "2025-05-18 03:37:15.527231: train_loss -0.9303\n",
      "2025-05-18 03:37:15.527442: val_loss -0.927\n",
      "2025-05-18 03:37:15.527554: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-18 03:37:15.527657: Epoch time: 108.99 s\n",
      "2025-05-18 03:37:15.527711: Yayy! New best EMA pseudo Dice: 0.9638000130653381\n",
      "2025-05-18 03:37:16.237059: \n",
      "2025-05-18 03:37:16.237197: Epoch 46\n",
      "2025-05-18 03:37:16.237266: Current learning rate: 0.00959\n",
      "2025-05-18 03:39:05.135279: train_loss -0.9218\n",
      "2025-05-18 03:39:05.135611: val_loss -0.9244\n",
      "2025-05-18 03:39:05.135716: Pseudo dice [np.float32(0.9693)]\n",
      "2025-05-18 03:39:05.135818: Epoch time: 108.9 s\n",
      "2025-05-18 03:39:05.135904: Yayy! New best EMA pseudo Dice: 0.9642999768257141\n",
      "2025-05-18 03:39:05.844270: \n",
      "2025-05-18 03:39:05.844355: Epoch 47\n",
      "2025-05-18 03:39:05.844419: Current learning rate: 0.00958\n",
      "2025-05-18 03:40:54.822160: train_loss -0.9237\n",
      "2025-05-18 03:40:54.822287: val_loss -0.9101\n",
      "2025-05-18 03:40:54.822368: Pseudo dice [np.float32(0.9639)]\n",
      "2025-05-18 03:40:54.822511: Epoch time: 108.98 s\n",
      "2025-05-18 03:40:55.315427: \n",
      "2025-05-18 03:40:55.315507: Epoch 48\n",
      "2025-05-18 03:40:55.315570: Current learning rate: 0.00957\n",
      "2025-05-18 03:42:44.280752: train_loss -0.9291\n",
      "2025-05-18 03:42:44.280879: val_loss -0.9214\n",
      "2025-05-18 03:42:44.280915: Pseudo dice [np.float32(0.969)]\n",
      "2025-05-18 03:42:44.280954: Epoch time: 108.97 s\n",
      "2025-05-18 03:42:44.280980: Yayy! New best EMA pseudo Dice: 0.9648000001907349\n",
      "2025-05-18 03:42:44.985481: \n",
      "2025-05-18 03:42:44.985563: Epoch 49\n",
      "2025-05-18 03:42:44.985624: Current learning rate: 0.00956\n",
      "2025-05-18 03:44:33.910540: train_loss -0.9323\n",
      "2025-05-18 03:44:33.910655: val_loss -0.8973\n",
      "2025-05-18 03:44:33.910765: Pseudo dice [np.float32(0.957)]\n",
      "2025-05-18 03:44:33.910824: Epoch time: 108.93 s\n",
      "2025-05-18 03:44:34.573482: \n",
      "2025-05-18 03:44:34.573575: Epoch 50\n",
      "2025-05-18 03:44:34.573642: Current learning rate: 0.00955\n",
      "2025-05-18 03:46:23.515764: train_loss -0.9098\n",
      "2025-05-18 03:46:23.515890: val_loss -0.9045\n",
      "2025-05-18 03:46:23.515923: Pseudo dice [np.float32(0.9661)]\n",
      "2025-05-18 03:46:23.515956: Epoch time: 108.94 s\n",
      "2025-05-18 03:46:24.013006: \n",
      "2025-05-18 03:46:24.013229: Epoch 51\n",
      "2025-05-18 03:46:24.013325: Current learning rate: 0.00954\n",
      "2025-05-18 03:48:12.934067: train_loss -0.8819\n",
      "2025-05-18 03:48:12.934192: val_loss -0.8502\n",
      "2025-05-18 03:48:12.934225: Pseudo dice [np.float32(0.942)]\n",
      "2025-05-18 03:48:12.934257: Epoch time: 108.92 s\n",
      "2025-05-18 03:48:13.618895: \n",
      "2025-05-18 03:48:13.619049: Epoch 52\n",
      "2025-05-18 03:48:13.619137: Current learning rate: 0.00953\n",
      "2025-05-18 03:50:02.558077: train_loss -0.9014\n",
      "2025-05-18 03:50:02.558206: val_loss -0.9052\n",
      "2025-05-18 03:50:02.558242: Pseudo dice [np.float32(0.9637)]\n",
      "2025-05-18 03:50:02.558296: Epoch time: 108.94 s\n",
      "2025-05-18 03:50:03.059194: \n",
      "2025-05-18 03:50:03.059494: Epoch 53\n",
      "2025-05-18 03:50:03.059587: Current learning rate: 0.00952\n",
      "2025-05-18 03:51:51.926038: train_loss -0.9124\n",
      "2025-05-18 03:51:51.926186: val_loss -0.8919\n",
      "2025-05-18 03:51:51.926232: Pseudo dice [np.float32(0.9565)]\n",
      "2025-05-18 03:51:51.926266: Epoch time: 108.87 s\n",
      "2025-05-18 03:51:52.428906: \n",
      "2025-05-18 03:51:52.429070: Epoch 54\n",
      "2025-05-18 03:51:52.429149: Current learning rate: 0.00951\n",
      "2025-05-18 03:53:41.413798: train_loss -0.9156\n",
      "2025-05-18 03:53:41.413918: val_loss -0.9086\n",
      "2025-05-18 03:53:41.413950: Pseudo dice [np.float32(0.9647)]\n",
      "2025-05-18 03:53:41.414034: Epoch time: 108.99 s\n",
      "2025-05-18 03:53:41.921144: \n",
      "2025-05-18 03:53:41.921371: Epoch 55\n",
      "2025-05-18 03:53:41.921620: Current learning rate: 0.0095\n",
      "2025-05-18 03:55:30.849300: train_loss -0.9155\n",
      "2025-05-18 03:55:30.849505: val_loss -0.9134\n",
      "2025-05-18 03:55:30.849540: Pseudo dice [np.float32(0.9655)]\n",
      "2025-05-18 03:55:30.849574: Epoch time: 108.93 s\n",
      "2025-05-18 03:55:31.349807: \n",
      "2025-05-18 03:55:31.349894: Epoch 56\n",
      "2025-05-18 03:55:31.349957: Current learning rate: 0.00949\n",
      "2025-05-18 03:57:20.272541: train_loss -0.9349\n",
      "2025-05-18 03:57:20.272661: val_loss -0.9288\n",
      "2025-05-18 03:57:20.272696: Pseudo dice [np.float32(0.9713)]\n",
      "2025-05-18 03:57:20.272727: Epoch time: 108.92 s\n",
      "2025-05-18 03:57:20.771926: \n",
      "2025-05-18 03:57:20.772018: Epoch 57\n",
      "2025-05-18 03:57:20.772082: Current learning rate: 0.00949\n",
      "2025-05-18 03:59:09.757602: train_loss -0.9342\n",
      "2025-05-18 03:59:09.757927: val_loss -0.9086\n",
      "2025-05-18 03:59:09.757970: Pseudo dice [np.float32(0.9593)]\n",
      "2025-05-18 03:59:09.758003: Epoch time: 108.99 s\n",
      "2025-05-18 03:59:10.258204: \n",
      "2025-05-18 03:59:10.258411: Epoch 58\n",
      "2025-05-18 03:59:10.258504: Current learning rate: 0.00948\n",
      "2025-05-18 04:00:59.199214: train_loss -0.9285\n",
      "2025-05-18 04:00:59.199336: val_loss -0.9209\n",
      "2025-05-18 04:00:59.199368: Pseudo dice [np.float32(0.9684)]\n",
      "2025-05-18 04:00:59.199403: Epoch time: 108.94 s\n",
      "2025-05-18 04:00:59.710850: \n",
      "2025-05-18 04:00:59.711176: Epoch 59\n",
      "2025-05-18 04:00:59.711263: Current learning rate: 0.00947\n",
      "2025-05-18 04:02:48.665869: train_loss -0.91\n",
      "2025-05-18 04:02:48.666082: val_loss -0.8541\n",
      "2025-05-18 04:02:48.666118: Pseudo dice [np.float32(0.9386)]\n",
      "2025-05-18 04:02:48.666150: Epoch time: 108.96 s\n",
      "2025-05-18 04:02:49.176191: \n",
      "2025-05-18 04:02:49.176281: Epoch 60\n",
      "2025-05-18 04:02:49.176356: Current learning rate: 0.00946\n",
      "2025-05-18 04:04:38.140500: train_loss -0.9\n",
      "2025-05-18 04:04:38.140633: val_loss -0.9021\n",
      "2025-05-18 04:04:38.140689: Pseudo dice [np.float32(0.9625)]\n",
      "2025-05-18 04:04:38.140801: Epoch time: 108.96 s\n",
      "2025-05-18 04:04:38.649027: \n",
      "2025-05-18 04:04:38.649180: Epoch 61\n",
      "2025-05-18 04:04:38.649246: Current learning rate: 0.00945\n",
      "2025-05-18 04:06:27.561664: train_loss -0.9302\n",
      "2025-05-18 04:06:27.561792: val_loss -0.9242\n",
      "2025-05-18 04:06:27.561864: Pseudo dice [np.float32(0.9682)]\n",
      "2025-05-18 04:06:27.561967: Epoch time: 108.91 s\n",
      "2025-05-18 04:06:28.075285: \n",
      "2025-05-18 04:06:28.075479: Epoch 62\n",
      "2025-05-18 04:06:28.075551: Current learning rate: 0.00944\n",
      "2025-05-18 04:08:17.032112: train_loss -0.9327\n",
      "2025-05-18 04:08:17.032232: val_loss -0.9194\n",
      "2025-05-18 04:08:17.032266: Pseudo dice [np.float32(0.9679)]\n",
      "2025-05-18 04:08:17.032299: Epoch time: 108.96 s\n",
      "2025-05-18 04:08:17.541558: \n",
      "2025-05-18 04:08:17.541883: Epoch 63\n",
      "2025-05-18 04:08:17.541986: Current learning rate: 0.00943\n",
      "train_loss -0.93916.471373: \n",
      "2025-05-18 04:10:06.471873: val_loss -0.9168\n",
      "2025-05-18 04:10:06.471990: Pseudo dice [np.float32(0.9657)]\n",
      "2025-05-18 04:10:06.472151: Epoch time: 108.93 s\n",
      "2025-05-18 04:10:07.177470: \n",
      "2025-05-18 04:10:07.177642: Epoch 64\n",
      "2025-05-18 04:10:07.177721: Current learning rate: 0.00942\n",
      "2025-05-18 04:11:56.190673: train_loss -0.9347\n",
      "2025-05-18 04:11:56.190815: val_loss -0.9236\n",
      "2025-05-18 04:11:56.190858: Pseudo dice [np.float32(0.9669)]\n",
      "2025-05-18 04:11:56.190892: Epoch time: 109.01 s\n",
      "2025-05-18 04:11:56.702863: \n",
      "2025-05-18 04:11:56.702965: Epoch 65\n",
      "2025-05-18 04:11:56.703078: Current learning rate: 0.00941\n",
      "2025-05-18 04:13:45.697490: train_loss -0.941\n",
      "2025-05-18 04:13:45.697645: val_loss -0.9338\n",
      "2025-05-18 04:13:45.697684: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-18 04:13:45.697719: Epoch time: 109.0 s\n",
      "2025-05-18 04:13:46.206368: \n",
      "2025-05-18 04:13:46.206469: Epoch 66\n",
      "2025-05-18 04:13:46.206533: Current learning rate: 0.0094\n",
      "2025-05-18 04:15:35.168622: train_loss -0.9403\n",
      "2025-05-18 04:15:35.168803: val_loss -0.9353\n",
      "2025-05-18 04:15:35.168864: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-18 04:15:35.168905: Epoch time: 108.96 s\n",
      "2025-05-18 04:15:35.168944: Yayy! New best EMA pseudo Dice: 0.9649999737739563\n",
      "2025-05-18 04:15:35.896045: \n",
      "2025-05-18 04:15:35.896183: Epoch 67\n",
      "2025-05-18 04:15:35.896256: Current learning rate: 0.00939\n",
      "2025-05-18 04:17:24.861234: train_loss -0.9448\n",
      "2025-05-18 04:17:24.861351: val_loss -0.9353\n",
      "2025-05-18 04:17:24.861386: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-18 04:17:24.861429: Epoch time: 108.97 s\n",
      "2025-05-18 04:17:24.861456: Yayy! New best EMA pseudo Dice: 0.9657999873161316\n",
      "2025-05-18 04:17:25.582707: \n",
      "2025-05-18 04:17:25.582844: Epoch 68\n",
      "2025-05-18 04:17:25.582909: Current learning rate: 0.00939\n",
      "2025-05-18 04:19:14.609413: train_loss -0.9473\n",
      "2025-05-18 04:19:14.609537: val_loss -0.9398\n",
      "2025-05-18 04:19:14.609571: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 04:19:14.609604: Epoch time: 109.03 s\n",
      "2025-05-18 04:19:14.609678: Yayy! New best EMA pseudo Dice: 0.9666000008583069\n",
      "2025-05-18 04:19:15.339502: \n",
      "2025-05-18 04:19:15.339687: Epoch 69\n",
      "2025-05-18 04:19:15.339811: Current learning rate: 0.00938\n",
      "2025-05-18 04:21:04.298222: train_loss -0.947\n",
      "2025-05-18 04:21:04.298419: val_loss -0.9251\n",
      "2025-05-18 04:21:04.298460: Pseudo dice [np.float32(0.9684)]\n",
      "2025-05-18 04:21:04.298495: Epoch time: 108.96 s\n",
      "2025-05-18 04:21:04.298518: Yayy! New best EMA pseudo Dice: 0.9667999744415283\n",
      "2025-05-18 04:21:05.027389: \n",
      "2025-05-18 04:21:05.027610: Epoch 70\n",
      "2025-05-18 04:21:05.027678: Current learning rate: 0.00937\n",
      "2025-05-18 04:22:54.072703: train_loss -0.946\n",
      "2025-05-18 04:22:54.072843: val_loss -0.9175\n",
      "2025-05-18 04:22:54.072954: Pseudo dice [np.float32(0.9649)]\n",
      "2025-05-18 04:22:54.073006: Epoch time: 109.05 s\n",
      "2025-05-18 04:22:54.586323: \n",
      "2025-05-18 04:22:54.586498: Epoch 71\n",
      "2025-05-18 04:22:54.586568: Current learning rate: 0.00936\n",
      "2025-05-18 04:24:43.564432: train_loss -0.945\n",
      "2025-05-18 04:24:43.564656: val_loss -0.9321\n",
      "2025-05-18 04:24:43.564737: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-18 04:24:43.564849: Epoch time: 108.98 s\n",
      "2025-05-18 04:24:43.564912: Yayy! New best EMA pseudo Dice: 0.967199981212616\n",
      "2025-05-18 04:24:44.288231: \n",
      "2025-05-18 04:24:44.288324: Epoch 72\n",
      "2025-05-18 04:24:44.288389: Current learning rate: 0.00935\n",
      "2025-05-18 04:26:33.172559: train_loss -0.9455\n",
      "2025-05-18 04:26:33.172678: val_loss -0.9344\n",
      "2025-05-18 04:26:33.172711: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-18 04:26:33.172744: Epoch time: 108.88 s\n",
      "2025-05-18 04:26:33.172763: Yayy! New best EMA pseudo Dice: 0.9678000211715698\n",
      "2025-05-18 04:26:33.895665: \n",
      "2025-05-18 04:26:33.895749: Epoch 73\n",
      "2025-05-18 04:26:33.895811: Current learning rate: 0.00934\n",
      "2025-05-18 04:28:22.848727: train_loss -0.9488\n",
      "2025-05-18 04:28:22.848913: val_loss -0.9305\n",
      "2025-05-18 04:28:22.848946: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-18 04:28:22.848979: Epoch time: 108.95 s\n",
      "2025-05-18 04:28:22.848998: Yayy! New best EMA pseudo Dice: 0.9682999849319458\n",
      "2025-05-18 04:28:23.572815: \n",
      "2025-05-18 04:28:23.572905: Epoch 74\n",
      "2025-05-18 04:28:23.572971: Current learning rate: 0.00933\n",
      "2025-05-18 04:30:12.567226: train_loss -0.9483\n",
      "2025-05-18 04:30:12.567421: val_loss -0.937\n",
      "2025-05-18 04:30:12.567457: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-18 04:30:12.567492: Epoch time: 108.99 s\n",
      "2025-05-18 04:30:12.567512: Yayy! New best EMA pseudo Dice: 0.9689000248908997\n",
      "2025-05-18 04:30:13.298553: \n",
      "2025-05-18 04:30:13.298635: Epoch 75\n",
      "2025-05-18 04:30:13.298699: Current learning rate: 0.00932\n",
      "2025-05-18 04:32:02.288813: train_loss -0.9492\n",
      "2025-05-18 04:32:02.288989: val_loss -0.9356\n",
      "2025-05-18 04:32:02.291818: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-18 04:32:02.291900: Epoch time: 108.99 s\n",
      "2025-05-18 04:32:02.291934: Yayy! New best EMA pseudo Dice: 0.9692999720573425\n",
      "2025-05-18 04:32:03.008901: \n",
      "2025-05-18 04:32:03.009035: Epoch 76\n",
      "2025-05-18 04:32:03.009167: Current learning rate: 0.00931\n",
      "2025-05-18 04:33:51.960187: train_loss -0.944\n",
      "2025-05-18 04:33:51.960342: val_loss -0.9323\n",
      "2025-05-18 04:33:51.960387: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-18 04:33:51.960424: Epoch time: 108.95 s\n",
      "2025-05-18 04:33:51.960565: Yayy! New best EMA pseudo Dice: 0.9696000218391418\n",
      "2025-05-18 04:33:52.876993: \n",
      "2025-05-18 04:33:52.877104: Epoch 77\n",
      "2025-05-18 04:33:52.877169: Current learning rate: 0.0093\n",
      "2025-05-18 04:35:41.842802: train_loss -0.9477\n",
      "2025-05-18 04:35:41.842925: val_loss -0.9331\n",
      "2025-05-18 04:35:41.842961: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-18 04:35:41.842995: Epoch time: 108.97 s\n",
      "2025-05-18 04:35:41.843015: Yayy! New best EMA pseudo Dice: 0.9699000120162964\n",
      "2025-05-18 04:35:42.580693: \n",
      "2025-05-18 04:35:42.581012: Epoch 78\n",
      "2025-05-18 04:35:42.581082: Current learning rate: 0.0093\n",
      "2025-05-18 04:37:31.476015: train_loss -0.947\n",
      "2025-05-18 04:37:31.476139: val_loss -0.9253\n",
      "2025-05-18 04:37:31.476173: Pseudo dice [np.float32(0.9698)]\n",
      "2025-05-18 04:37:31.476205: Epoch time: 108.9 s\n",
      "2025-05-18 04:37:32.003859: \n",
      "2025-05-18 04:37:32.003955: Epoch 79\n",
      "2025-05-18 04:37:32.004019: Current learning rate: 0.00929\n",
      "2025-05-18 04:39:20.927602: train_loss -0.9495\n",
      "2025-05-18 04:39:20.927773: val_loss -0.9419\n",
      "2025-05-18 04:39:20.927853: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 04:39:20.927975: Epoch time: 108.92 s\n",
      "2025-05-18 04:39:20.928010: Yayy! New best EMA pseudo Dice: 0.9704999923706055\n",
      "2025-05-18 04:39:21.666034: \n",
      "2025-05-18 04:39:21.666131: Epoch 80\n",
      "2025-05-18 04:39:21.666194: Current learning rate: 0.00928\n",
      "2025-05-18 04:41:10.581640: train_loss -0.9515\n",
      "2025-05-18 04:41:10.581788: val_loss -0.9404\n",
      "2025-05-18 04:41:10.581825: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 04:41:10.581864: Epoch time: 108.92 s\n",
      "2025-05-18 04:41:10.581890: Yayy! New best EMA pseudo Dice: 0.9710000157356262\n",
      "2025-05-18 04:41:11.318257: \n",
      "2025-05-18 04:41:11.318387: Epoch 81\n",
      "2025-05-18 04:41:11.318463: Current learning rate: 0.00927\n",
      "2025-05-18 04:43:00.250662: train_loss -0.9485\n",
      "2025-05-18 04:43:00.250793: val_loss -0.9276\n",
      "2025-05-18 04:43:00.250829: Pseudo dice [np.float32(0.9704)]\n",
      "2025-05-18 04:43:00.250863: Epoch time: 108.93 s\n",
      "2025-05-18 04:43:00.771975: \n",
      "2025-05-18 04:43:00.772166: Epoch 82\n",
      "2025-05-18 04:43:00.772316: Current learning rate: 0.00926\n",
      "2025-05-18 04:44:49.703686: train_loss -0.9487\n",
      "2025-05-18 04:44:49.703802: val_loss -0.9332\n",
      "2025-05-18 04:44:49.703834: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-18 04:44:49.703866: Epoch time: 108.93 s\n",
      "2025-05-18 04:44:49.703887: Yayy! New best EMA pseudo Dice: 0.9711999893188477\n",
      "2025-05-18 04:44:50.415642: \n",
      "2025-05-18 04:44:50.415980: Epoch 83\n",
      "2025-05-18 04:44:50.416057: Current learning rate: 0.00925\n",
      "2025-05-18 04:46:39.345989: train_loss -0.9498\n",
      "2025-05-18 04:46:39.346168: val_loss -0.9305\n",
      "2025-05-18 04:46:39.346204: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-18 04:46:39.346236: Epoch time: 108.93 s\n",
      "2025-05-18 04:46:39.346257: Yayy! New best EMA pseudo Dice: 0.9714000225067139\n",
      "2025-05-18 04:46:40.058789: \n",
      "2025-05-18 04:46:40.058941: Epoch 84\n",
      "2025-05-18 04:46:40.059006: Current learning rate: 0.00924\n",
      "2025-05-18 04:48:28.982939: train_loss -0.9498\n",
      "2025-05-18 04:48:28.983066: val_loss -0.9303\n",
      "2025-05-18 04:48:28.983100: Pseudo dice [np.float32(0.9718)]\n",
      "Epoch time: 108.92 s983132: \n",
      "2025-05-18 04:48:28.983245: Yayy! New best EMA pseudo Dice: 0.9714999794960022\n",
      "2025-05-18 04:48:29.703883: \n",
      "2025-05-18 04:48:29.704019: Epoch 85\n",
      "2025-05-18 04:48:29.704088: Current learning rate: 0.00923\n",
      "2025-05-18 04:50:18.733361: train_loss -0.9502\n",
      "2025-05-18 04:50:18.733578: val_loss -0.9319\n",
      "2025-05-18 04:50:18.733623: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-18 04:50:18.733825: Epoch time: 109.03 s\n",
      "2025-05-18 04:50:18.733899: Yayy! New best EMA pseudo Dice: 0.9714999794960022\n",
      "2025-05-18 04:50:19.490648: \n",
      "2025-05-18 04:50:19.490739: Epoch 86\n",
      "2025-05-18 04:50:19.490803: Current learning rate: 0.00922\n",
      "2025-05-18 04:52:08.423706: train_loss -0.95\n",
      "2025-05-18 04:52:08.423835: val_loss -0.9277\n",
      "2025-05-18 04:52:08.423871: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-18 04:52:08.423906: Epoch time: 108.93 s\n",
      "2025-05-18 04:52:08.943847: \n",
      "2025-05-18 04:52:08.943931: Epoch 87\n",
      "2025-05-18 04:52:08.943997: Current learning rate: 0.00921\n",
      "2025-05-18 04:53:57.923300: train_loss -0.9468\n",
      "2025-05-18 04:53:57.923439: val_loss -0.9352\n",
      "2025-05-18 04:53:57.923475: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-18 04:53:57.923509: Epoch time: 108.98 s\n",
      "2025-05-18 04:53:57.923531: Yayy! New best EMA pseudo Dice: 0.9717000126838684\n",
      "2025-05-18 04:53:58.634690: \n",
      "2025-05-18 04:53:58.634771: Epoch 88\n",
      "2025-05-18 04:53:58.634835: Current learning rate: 0.0092\n",
      "2025-05-18 04:55:47.665818: train_loss -0.9509\n",
      "2025-05-18 04:55:47.666206: val_loss -0.9416\n",
      "2025-05-18 04:55:47.666271: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-18 04:55:47.666310: Epoch time: 109.03 s\n",
      "2025-05-18 04:55:47.666334: Yayy! New best EMA pseudo Dice: 0.972000002861023\n",
      "2025-05-18 04:55:48.569588: \n",
      "2025-05-18 04:55:48.569700: Epoch 89\n",
      "2025-05-18 04:55:48.569774: Current learning rate: 0.0092\n",
      "2025-05-18 04:57:37.460922: train_loss -0.9517\n",
      "2025-05-18 04:57:37.461101: val_loss -0.9203\n",
      "2025-05-18 04:57:37.461149: Pseudo dice [np.float32(0.9679)]\n",
      "2025-05-18 04:57:37.461184: Epoch time: 108.89 s\n",
      "2025-05-18 04:57:37.974072: \n",
      "2025-05-18 04:57:37.974230: Epoch 90\n",
      "2025-05-18 04:57:37.974330: Current learning rate: 0.00919\n",
      "2025-05-18 04:59:26.909206: train_loss -0.9434\n",
      "2025-05-18 04:59:26.909331: val_loss -0.938\n",
      "2025-05-18 04:59:26.909363: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-18 04:59:26.909396: Epoch time: 108.94 s\n",
      "2025-05-18 04:59:27.416632: \n",
      "2025-05-18 04:59:27.416722: Epoch 91\n",
      "2025-05-18 04:59:27.416782: Current learning rate: 0.00918\n",
      "2025-05-18 05:01:16.443163: train_loss -0.9462\n",
      "2025-05-18 05:01:16.443288: val_loss -0.9279\n",
      "2025-05-18 05:01:16.443322: Pseudo dice [np.float32(0.9687)]\n",
      "2025-05-18 05:01:16.443355: Epoch time: 109.03 s\n",
      "2025-05-18 05:01:16.942677: \n",
      "2025-05-18 05:01:16.942895: Epoch 92\n",
      "2025-05-18 05:01:16.943004: Current learning rate: 0.00917\n",
      "2025-05-18 05:03:05.851901: train_loss -0.9482\n",
      "2025-05-18 05:03:05.852041: val_loss -0.9406\n",
      "2025-05-18 05:03:05.852076: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-18 05:03:05.852216: Epoch time: 108.91 s\n",
      "2025-05-18 05:03:06.359453: \n",
      "2025-05-18 05:03:06.359600: Epoch 93\n",
      "2025-05-18 05:03:06.359664: Current learning rate: 0.00916\n",
      "2025-05-18 05:04:55.343488: train_loss -0.9489\n",
      "2025-05-18 05:04:55.343769: val_loss -0.9379\n",
      "2025-05-18 05:04:55.343845: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-18 05:04:55.343951: Epoch time: 108.98 s\n",
      "2025-05-18 05:04:55.344009: Yayy! New best EMA pseudo Dice: 0.972100019454956\n",
      "2025-05-18 05:04:56.061291: \n",
      "2025-05-18 05:04:56.061386: Epoch 94\n",
      "2025-05-18 05:04:56.061450: Current learning rate: 0.00915\n",
      "2025-05-18 05:06:45.024564: train_loss -0.9469\n",
      "2025-05-18 05:06:45.024697: val_loss -0.9308\n",
      "2025-05-18 05:06:45.024731: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-18 05:06:45.024766: Epoch time: 108.96 s\n",
      "2025-05-18 05:06:45.024786: Yayy! New best EMA pseudo Dice: 0.972100019454956\n",
      "2025-05-18 05:06:45.736512: \n",
      "2025-05-18 05:06:45.736598: Epoch 95\n",
      "2025-05-18 05:06:45.736659: Current learning rate: 0.00914\n",
      "2025-05-18 05:08:34.727152: train_loss -0.9479\n",
      "2025-05-18 05:08:34.727266: val_loss -0.9378\n",
      "2025-05-18 05:08:34.727307: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-18 05:08:34.727347: Epoch time: 108.99 s\n",
      "2025-05-18 05:08:34.727380: Yayy! New best EMA pseudo Dice: 0.9721999764442444\n",
      "2025-05-18 05:08:35.435624: \n",
      "2025-05-18 05:08:35.435868: Epoch 96\n",
      "2025-05-18 05:08:35.435954: Current learning rate: 0.00913\n",
      "2025-05-18 05:10:24.378556: train_loss -0.95\n",
      "2025-05-18 05:10:24.378699: val_loss -0.9325\n",
      "2025-05-18 05:10:24.378735: Pseudo dice [np.float32(0.9704)]\n",
      "2025-05-18 05:10:24.378771: Epoch time: 108.94 s\n",
      "2025-05-18 05:10:24.884786: \n",
      "2025-05-18 05:10:24.885052: Epoch 97\n",
      "2025-05-18 05:10:24.885131: Current learning rate: 0.00912\n",
      "2025-05-18 05:12:13.854740: train_loss -0.9465\n",
      "2025-05-18 05:12:13.854880: val_loss -0.9293\n",
      "2025-05-18 05:12:13.854921: Pseudo dice [np.float32(0.9693)]\n",
      "2025-05-18 05:12:13.854956: Epoch time: 108.97 s\n",
      "2025-05-18 05:12:14.358146: \n",
      "2025-05-18 05:12:14.358232: Epoch 98\n",
      "2025-05-18 05:12:14.358302: Current learning rate: 0.00911\n",
      "2025-05-18 05:14:03.417490: train_loss -0.9503\n",
      "2025-05-18 05:14:03.417612: val_loss -0.9394\n",
      "2025-05-18 05:14:03.417646: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-18 05:14:03.417681: Epoch time: 109.06 s\n",
      "2025-05-18 05:14:03.923366: \n",
      "2025-05-18 05:14:03.923534: Epoch 99\n",
      "2025-05-18 05:14:03.923779: Current learning rate: 0.0091\n",
      "2025-05-18 05:15:52.916540: train_loss -0.9512\n",
      "2025-05-18 05:15:52.916665: val_loss -0.943\n",
      "2025-05-18 05:15:52.916698: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 05:15:52.916730: Epoch time: 108.99 s\n",
      "2025-05-18 05:15:53.121676: Yayy! New best EMA pseudo Dice: 0.9722999930381775\n",
      "2025-05-18 05:15:53.838453: \n",
      "2025-05-18 05:15:53.838651: Epoch 100\n",
      "2025-05-18 05:15:53.838744: Current learning rate: 0.0091\n",
      "2025-05-18 05:17:42.792176: train_loss -0.9516\n",
      "2025-05-18 05:17:42.792298: val_loss -0.9457\n",
      "2025-05-18 05:17:42.792331: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-18 05:17:42.792364: Epoch time: 108.95 s\n",
      "2025-05-18 05:17:42.792385: Yayy! New best EMA pseudo Dice: 0.9728000164031982\n",
      "2025-05-18 05:17:43.727669: \n",
      "2025-05-18 05:17:43.727813: Epoch 101\n",
      "2025-05-18 05:17:43.727912: Current learning rate: 0.00909\n",
      "2025-05-18 05:19:32.756851: train_loss -0.9508\n",
      "2025-05-18 05:19:32.756977: val_loss -0.9359\n",
      "2025-05-18 05:19:32.757011: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-18 05:19:32.757045: Epoch time: 109.03 s\n",
      "2025-05-18 05:19:33.265990: \n",
      "2025-05-18 05:19:33.266128: Epoch 102\n",
      "2025-05-18 05:19:33.266193: Current learning rate: 0.00908\n",
      "2025-05-18 05:21:22.167995: train_loss -0.9501\n",
      "2025-05-18 05:21:22.168146: val_loss -0.9464\n",
      "2025-05-18 05:21:22.168287: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 05:21:22.168387: Epoch time: 108.9 s\n",
      "2025-05-18 05:21:22.168549: Yayy! New best EMA pseudo Dice: 0.9732000231742859\n",
      "2025-05-18 05:21:22.883283: \n",
      "2025-05-18 05:21:22.883375: Epoch 103\n",
      "2025-05-18 05:21:22.883467: Current learning rate: 0.00907\n",
      "2025-05-18 05:23:11.894927: train_loss -0.9537\n",
      "2025-05-18 05:23:11.895052: val_loss -0.9372\n",
      "2025-05-18 05:23:11.895088: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 05:23:11.895123: Epoch time: 109.01 s\n",
      "2025-05-18 05:23:11.895144: Yayy! New best EMA pseudo Dice: 0.9733999967575073\n",
      "2025-05-18 05:23:12.618136: \n",
      "2025-05-18 05:23:12.618233: Epoch 104\n",
      "2025-05-18 05:23:12.618420: Current learning rate: 0.00906\n",
      "2025-05-18 05:25:01.638673: train_loss -0.9513\n",
      "2025-05-18 05:25:01.638845: val_loss -0.9376\n",
      "2025-05-18 05:25:01.638883: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-18 05:25:01.638919: Epoch time: 109.02 s\n",
      "2025-05-18 05:25:02.149090: \n",
      "2025-05-18 05:25:02.149180: Epoch 105\n",
      "2025-05-18 05:25:02.149245: Current learning rate: 0.00905\n",
      "2025-05-18 05:26:51.101766: train_loss -0.9466\n",
      "2025-05-18 05:26:51.101941: val_loss -0.9312\n",
      "2025-05-18 05:26:51.101973: Pseudo dice [np.float32(0.971)]\n",
      "2025-05-18 05:26:51.102004: Epoch time: 108.95 s\n",
      "2025-05-18 05:26:51.611406: \n",
      "2025-05-18 05:26:51.611512: Epoch 106\n",
      "2025-05-18 05:26:51.611577: Current learning rate: 0.00904\n",
      "2025-05-18 05:28:40.613483: train_loss -0.9417\n",
      "2025-05-18 05:28:40.613609: val_loss -0.936\n",
      "2025-05-18 05:28:40.613644: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-18 05:28:40.613677: Epoch time: 109.0 s\n",
      "2025-05-18 05:28:41.115671: \n",
      "2025-05-18 05:28:41.115818: Epoch 107\n",
      "Current learning rate: 0.00903\n",
      "2025-05-18 05:30:30.124766: train_loss -0.9426\n",
      "2025-05-18 05:30:30.124894: val_loss -0.9274\n",
      "2025-05-18 05:30:30.125061: Pseudo dice [np.float32(0.9708)]\n",
      "2025-05-18 05:30:30.125159: Epoch time: 109.01 s\n",
      "2025-05-18 05:30:30.633151: \n",
      "2025-05-18 05:30:30.633867: Epoch 108\n",
      "2025-05-18 05:30:30.633943: Current learning rate: 0.00902\n",
      "2025-05-18 05:32:19.670093: train_loss -0.9516\n",
      "2025-05-18 05:32:19.670203: val_loss -0.9326\n",
      "2025-05-18 05:32:19.670233: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-18 05:32:19.670264: Epoch time: 109.04 s\n",
      "2025-05-18 05:32:20.178748: \n",
      "2025-05-18 05:32:20.178901: Epoch 109\n",
      "2025-05-18 05:32:20.178969: Current learning rate: 0.00901\n",
      "2025-05-18 05:34:09.170831: train_loss -0.952\n",
      "2025-05-18 05:34:09.170966: val_loss -0.9418\n",
      "2025-05-18 05:34:09.171002: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 05:34:09.171039: Epoch time: 108.99 s\n",
      "2025-05-18 05:34:09.682251: \n",
      "2025-05-18 05:34:09.682417: Epoch 110\n",
      "2025-05-18 05:34:09.682490: Current learning rate: 0.009\n",
      "2025-05-18 05:35:58.670516: train_loss -0.9528\n",
      "2025-05-18 05:35:58.670641: val_loss -0.9388\n",
      "2025-05-18 05:35:58.670674: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-18 05:35:58.670708: Epoch time: 108.99 s\n",
      "2025-05-18 05:35:59.182095: \n",
      "2025-05-18 05:35:59.182181: Epoch 111\n",
      "2025-05-18 05:35:59.182244: Current learning rate: 0.009\n",
      "2025-05-18 05:37:48.232017: train_loss -0.9528\n",
      "2025-05-18 05:37:48.232238: val_loss -0.9481\n",
      "2025-05-18 05:37:48.232282: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-18 05:37:48.232323: Epoch time: 109.05 s\n",
      "2025-05-18 05:37:48.232349: Yayy! New best EMA pseudo Dice: 0.9736999869346619\n",
      "2025-05-18 05:37:48.970478: \n",
      "2025-05-18 05:37:48.970652: Epoch 112\n",
      "2025-05-18 05:37:48.970715: Current learning rate: 0.00899\n",
      "2025-05-18 05:39:37.921371: train_loss -0.945\n",
      "2025-05-18 05:39:37.921580: val_loss -0.9475\n",
      "2025-05-18 05:39:37.921655: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 05:39:37.921694: Epoch time: 108.95 s\n",
      "2025-05-18 05:39:37.921715: Yayy! New best EMA pseudo Dice: 0.9740999937057495\n",
      "2025-05-18 05:39:38.663164: \n",
      "2025-05-18 05:39:38.663245: Epoch 113\n",
      "2025-05-18 05:39:38.663313: Current learning rate: 0.00898\n",
      "2025-05-18 05:41:27.658034: train_loss -0.9235\n",
      "2025-05-18 05:41:27.658166: val_loss -0.876\n",
      "2025-05-18 05:41:27.658206: Pseudo dice [np.float32(0.9507)]\n",
      "2025-05-18 05:41:27.658245: Epoch time: 109.0 s\n",
      "2025-05-18 05:41:28.382000: \n",
      "2025-05-18 05:41:28.382091: Epoch 114\n",
      "2025-05-18 05:41:28.382159: Current learning rate: 0.00897\n",
      "2025-05-18 05:43:17.370043: train_loss -0.9144\n",
      "2025-05-18 05:43:17.370183: val_loss -0.9054\n",
      "2025-05-18 05:43:17.370219: Pseudo dice [np.float32(0.9621)]\n",
      "2025-05-18 05:43:17.370252: Epoch time: 108.99 s\n",
      "2025-05-18 05:43:17.892525: \n",
      "2025-05-18 05:43:17.892615: Epoch 115\n",
      "2025-05-18 05:43:17.892678: Current learning rate: 0.00896\n",
      "2025-05-18 05:45:06.902922: train_loss -0.9383\n",
      "2025-05-18 05:45:06.903044: val_loss -0.9403\n",
      "2025-05-18 05:45:06.903076: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-18 05:45:06.903119: Epoch time: 109.01 s\n",
      "2025-05-18 05:45:07.417211: \n",
      "2025-05-18 05:45:07.417376: Epoch 116\n",
      "2025-05-18 05:45:07.417446: Current learning rate: 0.00895\n",
      "2025-05-18 05:46:56.384176: train_loss -0.9382\n",
      "2025-05-18 05:46:56.384297: val_loss -0.934\n",
      "2025-05-18 05:46:56.384333: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-18 05:46:56.384364: Epoch time: 108.97 s\n",
      "2025-05-18 05:46:56.903279: \n",
      "2025-05-18 05:46:56.903557: Epoch 117\n",
      "2025-05-18 05:46:56.903633: Current learning rate: 0.00894\n",
      "2025-05-18 05:48:45.859262: train_loss -0.9074\n",
      "2025-05-18 05:48:45.859389: val_loss -0.9052\n",
      "2025-05-18 05:48:45.859422: Pseudo dice [np.float32(0.962)]\n",
      "2025-05-18 05:48:45.859455: Epoch time: 108.96 s\n",
      "2025-05-18 05:48:46.368545: \n",
      "2025-05-18 05:48:46.368643: Epoch 118\n",
      "2025-05-18 05:48:46.368707: Current learning rate: 0.00893\n",
      "2025-05-18 05:50:35.348584: train_loss -0.9186\n",
      "2025-05-18 05:50:35.348773: val_loss -0.9147\n",
      "2025-05-18 05:50:35.348810: Pseudo dice [np.float32(0.9678)]\n",
      "2025-05-18 05:50:35.348845: Epoch time: 108.98 s\n",
      "2025-05-18 05:50:35.859079: \n",
      "2025-05-18 05:50:35.859409: Epoch 119\n",
      "2025-05-18 05:50:35.859599: Current learning rate: 0.00892\n",
      "2025-05-18 05:52:24.805743: train_loss -0.9352\n",
      "2025-05-18 05:52:24.805866: val_loss -0.9247\n",
      "2025-05-18 05:52:24.805899: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-18 05:52:24.805932: Epoch time: 108.95 s\n",
      "2025-05-18 05:52:25.323309: \n",
      "2025-05-18 05:52:25.323399: Epoch 120\n",
      "2025-05-18 05:52:25.323461: Current learning rate: 0.00891\n",
      "2025-05-18 05:54:14.305729: train_loss -0.9408\n",
      "2025-05-18 05:54:14.305911: val_loss -0.9355\n",
      "2025-05-18 05:54:14.305945: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-18 05:54:14.305979: Epoch time: 108.98 s\n",
      "2025-05-18 05:54:14.821992: \n",
      "2025-05-18 05:54:14.822117: Epoch 121\n",
      "2025-05-18 05:54:14.822247: Current learning rate: 0.0089\n",
      "2025-05-18 05:56:03.810991: train_loss -0.9459\n",
      "2025-05-18 05:56:03.811160: val_loss -0.9364\n",
      "2025-05-18 05:56:03.811193: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-18 05:56:03.811256: Epoch time: 108.99 s\n",
      "2025-05-18 05:56:04.327528: \n",
      "2025-05-18 05:56:04.327684: Epoch 122\n",
      "2025-05-18 05:56:04.327750: Current learning rate: 0.00889\n",
      "2025-05-18 05:57:53.267633: train_loss -0.9493\n",
      "2025-05-18 05:57:53.267757: val_loss -0.9365\n",
      "2025-05-18 05:57:53.267790: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 05:57:53.267834: Epoch time: 108.94 s\n",
      "2025-05-18 05:57:53.771660: \n",
      "2025-05-18 05:57:53.771744: Epoch 123\n",
      "2025-05-18 05:57:53.771807: Current learning rate: 0.00889\n",
      "2025-05-18 05:59:42.738353: train_loss -0.9456\n",
      "2025-05-18 05:59:42.738484: val_loss -0.918\n",
      "2025-05-18 05:59:42.738520: Pseudo dice [np.float32(0.9649)]\n",
      "2025-05-18 05:59:42.738553: Epoch time: 108.97 s\n",
      "2025-05-18 05:59:43.256273: \n",
      "2025-05-18 05:59:43.256356: Epoch 124\n",
      "2025-05-18 05:59:43.256420: Current learning rate: 0.00888\n",
      "2025-05-18 06:01:32.269158: train_loss -0.9417\n",
      "2025-05-18 06:01:32.269285: val_loss -0.9215\n",
      "2025-05-18 06:01:32.269319: Pseudo dice [np.float32(0.9691)]\n",
      "2025-05-18 06:01:32.269352: Epoch time: 109.01 s\n",
      "2025-05-18 06:01:32.777050: \n",
      "2025-05-18 06:01:32.777509: Epoch 125\n",
      "2025-05-18 06:01:32.777582: Current learning rate: 0.00887\n",
      "2025-05-18 06:03:21.670171: train_loss -0.941\n",
      "2025-05-18 06:03:21.670485: val_loss -0.9339\n",
      "2025-05-18 06:03:21.670584: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-18 06:03:21.670636: Epoch time: 108.89 s\n",
      "2025-05-18 06:03:22.186235: \n",
      "2025-05-18 06:03:22.186445: Epoch 126\n",
      "2025-05-18 06:03:22.186602: Current learning rate: 0.00886\n",
      "2025-05-18 06:05:11.203506: train_loss -0.9494\n",
      "2025-05-18 06:05:11.203773: val_loss -0.9416\n",
      "2025-05-18 06:05:11.203867: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 06:05:11.203944: Epoch time: 109.02 s\n",
      "2025-05-18 06:05:11.908841: \n",
      "2025-05-18 06:05:11.909003: Epoch 127\n",
      "2025-05-18 06:05:11.909075: Current learning rate: 0.00885\n",
      "2025-05-18 06:07:00.955461: train_loss -0.9235\n",
      "2025-05-18 06:07:00.955637: val_loss -0.8893\n",
      "2025-05-18 06:07:00.955673: Pseudo dice [np.float32(0.9579)]\n",
      "2025-05-18 06:07:00.955731: Epoch time: 109.05 s\n",
      "2025-05-18 06:07:01.469979: \n",
      "2025-05-18 06:07:01.470118: Epoch 128\n",
      "2025-05-18 06:07:01.470185: Current learning rate: 0.00884\n",
      "2025-05-18 06:08:50.362109: train_loss -0.9373\n",
      "2025-05-18 06:08:50.362231: val_loss -0.9376\n",
      "2025-05-18 06:08:50.362402: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 06:08:50.362472: Epoch time: 108.89 s\n",
      "2025-05-18 06:08:50.881199: \n",
      "2025-05-18 06:08:50.881354: Epoch 129\n",
      "2025-05-18 06:08:50.881423: Current learning rate: 0.00883\n",
      "2025-05-18 06:10:39.831236: train_loss -0.944\n",
      "2025-05-18 06:10:39.831401: val_loss -0.9327\n",
      "2025-05-18 06:10:39.831434: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-18 06:10:39.831465: Epoch time: 108.95 s\n",
      "2025-05-18 06:10:40.350071: \n",
      "2025-05-18 06:10:40.350225: Epoch 130\n",
      "2025-05-18 06:10:40.350308: Current learning rate: 0.00882\n",
      "2025-05-18 06:12:29.301280: train_loss -0.9508\n",
      "2025-05-18 06:12:29.301401: val_loss -0.9419\n",
      "2025-05-18 06:12:29.301433: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-18 06:12:29.301466: Epoch time: 108.95 s\n",
      "2025-05-18 06:12:29.809489: \n",
      "2025-05-18 06:12:29.809587: Epoch 131\n",
      "2025-05-18 06:12:29.809663: Current learning rate: 0.00881\n",
      "2025-05-18 06:14:18.814597: train_loss -0.9525\n",
      "2025-05-18 06:14:18.814727: val_loss -0.9279\n",
      "[np.float32(0.9701)]814762: Pseudo dice \n",
      "2025-05-18 06:14:18.814966: Epoch time: 109.01 s\n",
      "2025-05-18 06:14:19.336968: \n",
      "2025-05-18 06:14:19.337060: Epoch 132\n",
      "2025-05-18 06:14:19.337121: Current learning rate: 0.0088\n",
      "2025-05-18 06:16:08.243841: train_loss -0.952\n",
      "2025-05-18 06:16:08.243980: val_loss -0.9462\n",
      "2025-05-18 06:16:08.244020: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 06:16:08.244059: Epoch time: 108.91 s\n",
      "2025-05-18 06:16:08.757683: \n",
      "2025-05-18 06:16:08.757785: Epoch 133\n",
      "2025-05-18 06:16:08.757850: Current learning rate: 0.00879\n",
      "2025-05-18 06:17:57.729278: train_loss -0.9511\n",
      "2025-05-18 06:17:57.729591: val_loss -0.939\n",
      "2025-05-18 06:17:57.729629: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 06:17:57.729662: Epoch time: 108.97 s\n",
      "2025-05-18 06:17:58.241738: \n",
      "2025-05-18 06:17:58.241834: Epoch 134\n",
      "2025-05-18 06:17:58.241897: Current learning rate: 0.00879\n",
      "2025-05-18 06:19:47.234540: train_loss -0.9534\n",
      "2025-05-18 06:19:47.234668: val_loss -0.9372\n",
      "2025-05-18 06:19:47.234702: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-18 06:19:47.234753: Epoch time: 108.99 s\n",
      "2025-05-18 06:19:47.765151: \n",
      "2025-05-18 06:19:47.765330: Epoch 135\n",
      "2025-05-18 06:19:47.765450: Current learning rate: 0.00878\n",
      "2025-05-18 06:21:36.699492: train_loss -0.9499\n",
      "2025-05-18 06:21:36.699614: val_loss -0.9208\n",
      "2025-05-18 06:21:36.699647: Pseudo dice [np.float32(0.9671)]\n",
      "2025-05-18 06:21:36.699679: Epoch time: 108.93 s\n",
      "2025-05-18 06:21:37.212400: \n",
      "2025-05-18 06:21:37.212492: Epoch 136\n",
      "2025-05-18 06:21:37.212561: Current learning rate: 0.00877\n",
      "2025-05-18 06:23:26.178333: train_loss -0.9403\n",
      "2025-05-18 06:23:26.178460: val_loss -0.9241\n",
      "2025-05-18 06:23:26.178496: Pseudo dice [np.float32(0.9685)]\n",
      "2025-05-18 06:23:26.178530: Epoch time: 108.97 s\n",
      "2025-05-18 06:23:26.707357: \n",
      "2025-05-18 06:23:26.707498: Epoch 137\n",
      "2025-05-18 06:23:26.707578: Current learning rate: 0.00876\n",
      "2025-05-18 06:25:15.652917: train_loss -0.9487\n",
      "2025-05-18 06:25:15.653051: val_loss -0.9337\n",
      "2025-05-18 06:25:15.653085: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-18 06:25:15.653120: Epoch time: 108.95 s\n",
      "2025-05-18 06:25:16.174409: \n",
      "2025-05-18 06:25:16.174489: Epoch 138\n",
      "2025-05-18 06:25:16.174552: Current learning rate: 0.00875\n",
      "2025-05-18 06:27:05.067163: train_loss -0.9324\n",
      "2025-05-18 06:27:05.067287: val_loss -0.9348\n",
      "2025-05-18 06:27:05.067322: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-18 06:27:05.067356: Epoch time: 108.89 s\n",
      "2025-05-18 06:27:05.795516: \n",
      "2025-05-18 06:27:05.795843: Epoch 139\n",
      "2025-05-18 06:27:05.795948: Current learning rate: 0.00874\n",
      "2025-05-18 06:28:54.764458: train_loss -0.9449\n",
      "2025-05-18 06:28:54.764640: val_loss -0.9438\n",
      "2025-05-18 06:28:54.764673: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 06:28:54.764714: Epoch time: 108.97 s\n",
      "2025-05-18 06:28:55.286901: \n",
      "2025-05-18 06:28:55.287062: Epoch 140\n",
      "2025-05-18 06:28:55.287128: Current learning rate: 0.00873\n",
      "2025-05-18 06:30:44.170658: train_loss -0.9522\n",
      "2025-05-18 06:30:44.170788: val_loss -0.9425\n",
      "2025-05-18 06:30:44.170823: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-18 06:30:44.170857: Epoch time: 108.88 s\n",
      "2025-05-18 06:30:44.693607: \n",
      "2025-05-18 06:30:44.693944: Epoch 141\n",
      "2025-05-18 06:30:44.694038: Current learning rate: 0.00872\n",
      "2025-05-18 06:32:33.679606: train_loss -0.9509\n",
      "2025-05-18 06:32:33.679738: val_loss -0.9362\n",
      "2025-05-18 06:32:33.679775: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-18 06:32:33.679808: Epoch time: 108.99 s\n",
      "2025-05-18 06:32:34.201345: \n",
      "2025-05-18 06:32:34.201524: Epoch 142\n",
      "2025-05-18 06:32:34.201598: Current learning rate: 0.00871\n",
      "2025-05-18 06:34:23.127826: train_loss -0.9521\n",
      "2025-05-18 06:34:23.127956: val_loss -0.9475\n",
      "2025-05-18 06:34:23.127990: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 06:34:23.128023: Epoch time: 108.93 s\n",
      "2025-05-18 06:34:23.647150: \n",
      "2025-05-18 06:34:23.647318: Epoch 143\n",
      "2025-05-18 06:34:23.647383: Current learning rate: 0.0087\n",
      "2025-05-18 06:36:12.642154: train_loss -0.9542\n",
      "2025-05-18 06:36:12.642429: val_loss -0.9468\n",
      "2025-05-18 06:36:12.642466: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-18 06:36:12.642506: Epoch time: 109.0 s\n",
      "2025-05-18 06:36:13.170212: \n",
      "2025-05-18 06:36:13.170426: Epoch 144\n",
      "2025-05-18 06:36:13.170497: Current learning rate: 0.00869\n",
      "2025-05-18 06:38:02.186244: train_loss -0.9561\n",
      "2025-05-18 06:38:02.186364: val_loss -0.9445\n",
      "2025-05-18 06:38:02.186397: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 06:38:02.186430: Epoch time: 109.02 s\n",
      "2025-05-18 06:38:02.711025: \n",
      "2025-05-18 06:38:02.711142: Epoch 145\n",
      "2025-05-18 06:38:02.711246: Current learning rate: 0.00868\n",
      "2025-05-18 06:39:51.709995: train_loss -0.9538\n",
      "2025-05-18 06:39:51.710210: val_loss -0.934\n",
      "2025-05-18 06:39:51.710280: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-18 06:39:51.710353: Epoch time: 109.0 s\n",
      "2025-05-18 06:39:52.227153: \n",
      "2025-05-18 06:39:52.227316: Epoch 146\n",
      "2025-05-18 06:39:52.227389: Current learning rate: 0.00868\n",
      "2025-05-18 06:41:41.203642: train_loss -0.9535\n",
      "2025-05-18 06:41:41.203793: val_loss -0.9365\n",
      "2025-05-18 06:41:41.203831: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-18 06:41:41.203894: Epoch time: 108.98 s\n",
      "2025-05-18 06:41:41.720733: \n",
      "2025-05-18 06:41:41.721053: Epoch 147\n",
      "2025-05-18 06:41:41.721140: Current learning rate: 0.00867\n",
      "2025-05-18 06:43:30.699284: train_loss -0.9541\n",
      "2025-05-18 06:43:30.699430: val_loss -0.9373\n",
      "2025-05-18 06:43:30.699538: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-18 06:43:30.699613: Epoch time: 108.98 s\n",
      "2025-05-18 06:43:31.226803: \n",
      "2025-05-18 06:43:31.226882: Epoch 148\n",
      "2025-05-18 06:43:31.226945: Current learning rate: 0.00866\n",
      "2025-05-18 06:45:20.148705: train_loss -0.9555\n",
      "2025-05-18 06:45:20.148900: val_loss -0.9455\n",
      "2025-05-18 06:45:20.148938: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-18 06:45:20.148970: Epoch time: 108.92 s\n",
      "2025-05-18 06:45:20.662161: \n",
      "2025-05-18 06:45:20.662245: Epoch 149\n",
      "2025-05-18 06:45:20.662317: Current learning rate: 0.00865\n",
      "2025-05-18 06:47:09.666077: train_loss -0.9537\n",
      "2025-05-18 06:47:09.666289: val_loss -0.9368\n",
      "2025-05-18 06:47:09.666394: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-18 06:47:09.666503: Epoch time: 109.0 s\n",
      "2025-05-18 06:47:10.387765: \n",
      "2025-05-18 06:47:10.387900: Epoch 150\n",
      "2025-05-18 06:47:10.387967: Current learning rate: 0.00864\n",
      "2025-05-18 06:48:59.429405: train_loss -0.954\n",
      "2025-05-18 06:48:59.429526: val_loss -0.9323\n",
      "2025-05-18 06:48:59.429563: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-18 06:48:59.429601: Epoch time: 109.04 s\n",
      "2025-05-18 06:49:00.155328: \n",
      "2025-05-18 06:49:00.155674: Epoch 151\n",
      "2025-05-18 06:49:00.155829: Current learning rate: 0.00863\n",
      "2025-05-18 06:50:49.152208: train_loss -0.9562\n",
      "2025-05-18 06:50:49.152332: val_loss -0.9386\n",
      "2025-05-18 06:50:49.152364: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 06:50:49.152395: Epoch time: 109.0 s\n",
      "2025-05-18 06:50:49.675604: \n",
      "2025-05-18 06:50:49.675823: Epoch 152\n",
      "2025-05-18 06:50:49.675947: Current learning rate: 0.00862\n",
      "2025-05-18 06:52:38.636153: train_loss -0.9555\n",
      "2025-05-18 06:52:38.636278: val_loss -0.9368\n",
      "2025-05-18 06:52:38.636315: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-18 06:52:38.636348: Epoch time: 108.96 s\n",
      "2025-05-18 06:52:39.159901: \n",
      "2025-05-18 06:52:39.159992: Epoch 153\n",
      "2025-05-18 06:52:39.160054: Current learning rate: 0.00861\n",
      "2025-05-18 06:54:28.105506: train_loss -0.9548\n",
      "2025-05-18 06:54:28.105684: val_loss -0.942\n",
      "2025-05-18 06:54:28.105718: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-18 06:54:28.105751: Epoch time: 108.95 s\n",
      "2025-05-18 06:54:28.639524: \n",
      "2025-05-18 06:54:28.639637: Epoch 154\n",
      "2025-05-18 06:54:28.639723: Current learning rate: 0.0086\n",
      "2025-05-18 06:56:17.630992: train_loss -0.9528\n",
      "2025-05-18 06:56:17.631129: val_loss -0.943\n",
      "2025-05-18 06:56:17.631161: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 06:56:17.631195: Epoch time: 108.99 s\n",
      "2025-05-18 06:56:17.631242: Yayy! New best EMA pseudo Dice: 0.9743000268936157\n",
      "2025-05-18 06:56:18.395849: \n",
      "2025-05-18 06:56:18.396028: Epoch 155\n",
      "2025-05-18 06:56:18.396139: Current learning rate: 0.00859\n",
      "2025-05-18 06:58:07.361624: train_loss -0.9456\n",
      "2025-05-18 06:58:07.361795: val_loss -0.937\n",
      "2025-05-18 06:58:07.361830: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-18 06:58:07.361871: Epoch time: 108.97 s\n",
      "2025-05-18 06:58:07.896370: \n",
      "2025-05-18 06:58:07.896544: Epoch 156\n",
      "2025-05-18 06:58:07.896765: Current learning rate: 0.00858\n",
      "2025-05-18 06:59:56.899361: train_loss -0.9543\n",
      "2025-05-18 06:59:56.899530: val_loss -0.943\n",
      "2025-05-18 06:59:56.899698: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-18 06:59:56.899938: Epoch time: 109.0 s\n",
      "2025-05-18 06:59:56.900078: Yayy! New best EMA pseudo Dice: 0.974399983882904\n",
      "2025-05-18 06:59:57.666728: \n",
      "2025-05-18 06:59:57.667006: Epoch 157\n",
      "2025-05-18 06:59:57.667073: Current learning rate: 0.00858\n",
      "2025-05-18 07:01:46.710974: train_loss -0.9535\n",
      "2025-05-18 07:01:46.711103: val_loss -0.931\n",
      "2025-05-18 07:01:46.711139: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-18 07:01:46.711173: Epoch time: 109.04 s\n",
      "2025-05-18 07:01:47.247661: \n",
      "2025-05-18 07:01:47.247858: Epoch 158\n",
      "2025-05-18 07:01:47.247931: Current learning rate: 0.00857\n",
      "2025-05-18 07:03:36.128590: train_loss -0.9557\n",
      "2025-05-18 07:03:36.128807: val_loss -0.9442\n",
      "2025-05-18 07:03:36.128886: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 07:03:36.128928: Epoch time: 108.88 s\n",
      "2025-05-18 07:03:36.654682: \n",
      "2025-05-18 07:03:36.654772: Epoch 159\n",
      "2025-05-18 07:03:36.654835: Current learning rate: 0.00856\n",
      "2025-05-18 07:05:25.602757: train_loss -0.956\n",
      "2025-05-18 07:05:25.602878: val_loss -0.9446\n",
      "2025-05-18 07:05:25.602912: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-18 07:05:25.602945: Epoch time: 108.95 s\n",
      "2025-05-18 07:05:25.602966: Yayy! New best EMA pseudo Dice: 0.9747999906539917\n",
      "2025-05-18 07:05:26.342154: \n",
      "2025-05-18 07:05:26.342737: Epoch 160\n",
      "2025-05-18 07:05:26.342877: Current learning rate: 0.00855\n",
      "2025-05-18 07:07:15.336610: train_loss -0.9573\n",
      "2025-05-18 07:07:15.336756: val_loss -0.9431\n",
      "2025-05-18 07:07:15.336803: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 07:07:15.336851: Epoch time: 108.99 s\n",
      "2025-05-18 07:07:15.336902: Yayy! New best EMA pseudo Dice: 0.9749000072479248\n",
      "2025-05-18 07:07:16.081152: \n",
      "2025-05-18 07:07:16.081236: Epoch 161\n",
      "2025-05-18 07:07:16.081298: Current learning rate: 0.00854\n",
      "2025-05-18 07:09:05.048082: train_loss -0.9566\n",
      "2025-05-18 07:09:05.048203: val_loss -0.93\n",
      "2025-05-18 07:09:05.048237: Pseudo dice [np.float32(0.9713)]\n",
      "2025-05-18 07:09:05.048270: Epoch time: 108.97 s\n",
      "2025-05-18 07:09:05.574843: \n",
      "2025-05-18 07:09:05.575002: Epoch 162\n",
      "2025-05-18 07:09:05.575100: Current learning rate: 0.00853\n",
      "2025-05-18 07:10:54.557811: train_loss -0.9567\n",
      "2025-05-18 07:10:54.558007: val_loss -0.9513\n",
      "2025-05-18 07:10:54.558049: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-18 07:10:54.558081: Epoch time: 108.98 s\n",
      "2025-05-18 07:10:54.558103: Yayy! New best EMA pseudo Dice: 0.9750000238418579\n",
      "2025-05-18 07:10:55.503212: \n",
      "2025-05-18 07:10:55.503471: Epoch 163\n",
      "2025-05-18 07:10:55.503546: Current learning rate: 0.00852\n",
      "2025-05-18 07:12:44.526381: train_loss -0.9547\n",
      "2025-05-18 07:12:44.526508: val_loss -0.9351\n",
      "2025-05-18 07:12:44.526555: Pseudo dice [np.float32(0.9703)]\n",
      "2025-05-18 07:12:44.526590: Epoch time: 109.02 s\n",
      "2025-05-18 07:12:45.049447: \n",
      "2025-05-18 07:12:45.049603: Epoch 164\n",
      "2025-05-18 07:12:45.049670: Current learning rate: 0.00851\n",
      "2025-05-18 07:14:34.031632: train_loss -0.9536\n",
      "2025-05-18 07:14:34.031796: val_loss -0.9398\n",
      "2025-05-18 07:14:34.031934: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 07:14:34.032075: Epoch time: 108.98 s\n",
      "2025-05-18 07:14:34.555772: \n",
      "2025-05-18 07:14:34.555905: Epoch 165\n",
      "2025-05-18 07:14:34.555970: Current learning rate: 0.0085\n",
      "2025-05-18 07:16:23.571382: train_loss -0.9552\n",
      "2025-05-18 07:16:23.571550: val_loss -0.9391\n",
      "2025-05-18 07:16:23.571585: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-18 07:16:23.571620: Epoch time: 109.02 s\n",
      "2025-05-18 07:16:24.093347: \n",
      "2025-05-18 07:16:24.093531: Epoch 166\n",
      "2025-05-18 07:16:24.093610: Current learning rate: 0.00849\n",
      "2025-05-18 07:18:13.057904: train_loss -0.9561\n",
      "2025-05-18 07:18:13.058068: val_loss -0.9416\n",
      "2025-05-18 07:18:13.058103: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-18 07:18:13.058136: Epoch time: 108.97 s\n",
      "2025-05-18 07:18:13.571386: \n",
      "2025-05-18 07:18:13.571482: Epoch 167\n",
      "2025-05-18 07:18:13.571548: Current learning rate: 0.00848\n",
      "2025-05-18 07:20:02.541468: train_loss -0.9582\n",
      "2025-05-18 07:20:02.541592: val_loss -0.9446\n",
      "2025-05-18 07:20:02.541626: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 07:20:02.541659: Epoch time: 108.97 s\n",
      "2025-05-18 07:20:03.072477: \n",
      "2025-05-18 07:20:03.072579: Epoch 168\n",
      "2025-05-18 07:20:03.072644: Current learning rate: 0.00847\n",
      "2025-05-18 07:21:52.043245: train_loss -0.9556\n",
      "2025-05-18 07:21:52.043381: val_loss -0.9433\n",
      "2025-05-18 07:21:52.043414: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 07:21:52.043447: Epoch time: 108.97 s\n",
      "2025-05-18 07:21:52.567502: \n",
      "2025-05-18 07:21:52.567592: Epoch 169\n",
      "2025-05-18 07:21:52.567659: Current learning rate: 0.00847\n",
      "2025-05-18 07:23:41.552863: train_loss -0.9579\n",
      "2025-05-18 07:23:41.553005: val_loss -0.9376\n",
      "2025-05-18 07:23:41.553042: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-18 07:23:41.553076: Epoch time: 108.99 s\n",
      "2025-05-18 07:23:42.072108: \n",
      "2025-05-18 07:23:42.072262: Epoch 170\n",
      "2025-05-18 07:23:42.072330: Current learning rate: 0.00846\n",
      "2025-05-18 07:25:31.080801: train_loss -0.9552\n",
      "2025-05-18 07:25:31.080920: val_loss -0.9372\n",
      "2025-05-18 07:25:31.080998: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 07:25:31.081089: Epoch time: 109.01 s\n",
      "2025-05-18 07:25:31.613302: \n",
      "2025-05-18 07:25:31.613443: Epoch 171\n",
      "2025-05-18 07:25:31.613518: Current learning rate: 0.00845\n",
      "2025-05-18 07:27:20.540858: train_loss -0.9391\n",
      "2025-05-18 07:27:20.540977: val_loss -0.9144\n",
      "2025-05-18 07:27:20.541008: Pseudo dice [np.float32(0.9666)]\n",
      "2025-05-18 07:27:20.541041: Epoch time: 108.93 s\n",
      "2025-05-18 07:27:21.063700: \n",
      "2025-05-18 07:27:21.063779: Epoch 172\n",
      "2025-05-18 07:27:21.063841: Current learning rate: 0.00844\n",
      "2025-05-18 07:29:10.042310: train_loss -0.9319\n",
      "2025-05-18 07:29:10.042456: val_loss -0.9187\n",
      "2025-05-18 07:29:10.042635: Pseudo dice [np.float32(0.9666)]\n",
      "2025-05-18 07:29:10.042756: Epoch time: 108.98 s\n",
      "2025-05-18 07:29:10.569672: \n",
      "2025-05-18 07:29:10.569755: Epoch 173\n",
      "2025-05-18 07:29:10.569840: Current learning rate: 0.00843\n",
      "2025-05-18 07:30:59.651035: train_loss -0.8964\n",
      "2025-05-18 07:30:59.651440: val_loss -0.9117\n",
      "2025-05-18 07:30:59.651485: Pseudo dice [np.float32(0.961)]\n",
      "2025-05-18 07:30:59.651519: Epoch time: 109.08 s\n",
      "2025-05-18 07:31:00.173568: \n",
      "2025-05-18 07:31:00.173661: Epoch 174\n",
      "2025-05-18 07:31:00.173744: Current learning rate: 0.00842\n",
      "2025-05-18 07:32:49.076127: train_loss -0.9239\n",
      "2025-05-18 07:32:49.076260: val_loss -0.9299\n",
      "2025-05-18 07:32:49.076296: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-18 07:32:49.076328: Epoch time: 108.9 s\n",
      "2025-05-18 07:32:49.798295: \n",
      "2025-05-18 07:32:49.798409: Epoch 175\n",
      "2025-05-18 07:32:49.798474: Current learning rate: 0.00841\n",
      "2025-05-18 07:34:38.796194: train_loss -0.9397\n",
      "2025-05-18 07:34:38.796309: val_loss -0.9295\n",
      "2025-05-18 07:34:38.796343: Pseudo dice [np.float32(0.9722)]\n",
      "2025-05-18 07:34:38.796376: Epoch time: 109.0 s\n",
      "2025-05-18 07:34:39.327154: \n",
      "2025-05-18 07:34:39.327321: Epoch 176\n",
      "2025-05-18 07:34:39.327388: Current learning rate: 0.0084\n",
      "2025-05-18 07:36:28.248397: train_loss -0.9343\n",
      "2025-05-18 07:36:28.248519: val_loss -0.9266\n",
      "2025-05-18 07:36:28.248549: Pseudo dice [np.float32(0.968)]\n",
      "2025-05-18 07:36:28.248581: Epoch time: 108.92 s\n",
      "2025-05-18 07:36:28.764704: \n",
      "2025-05-18 07:36:28.764885: Epoch 177\n",
      "2025-05-18 07:36:28.764998: Current learning rate: 0.00839\n",
      "2025-05-18 07:38:17.729065: train_loss -0.9145\n",
      "2025-05-18 07:38:17.729187: val_loss -0.9184\n",
      "2025-05-18 07:38:17.729221: Pseudo dice [np.float32(0.9644)]\n",
      "2025-05-18 07:38:17.729254: Epoch time: 108.96 s\n",
      "2025-05-18 07:38:18.256232: \n",
      "2025-05-18 07:38:18.256321: Epoch 178\n",
      "2025-05-18 07:38:18.256385: Current learning rate: 0.00838\n",
      "2025-05-18 07:40:07.203285: train_loss -0.9245\n",
      "2025-05-18 07:40:07.203399: val_loss -0.9109\n",
      "2025-05-18 07:40:07.203431: Pseudo dice [np.float32(0.9633)]\n",
      "2025-05-18 07:40:07.203475: Epoch time: 108.95 s\n",
      "2025-05-18 07:40:07.741913: \n",
      "2025-05-18 07:40:07.742044: Epoch 179\n",
      "2025-05-18 07:40:07.742109: Current learning rate: 0.00837\n",
      "2025-05-18 07:41:56.668459: train_loss -0.9251\n",
      "2025-05-18 07:41:56.668581: val_loss -0.9235\n",
      "2025-05-18 07:41:56.668613: Pseudo dice [np.float32(0.9685)]\n",
      "2025-05-18 07:41:56.668646: Epoch time: 108.93 s\n",
      "2025-05-18 07:41:57.207707: \n",
      "2025-05-18 07:41:57.207885: Epoch 180\n",
      "2025-05-18 07:41:57.208085: Current learning rate: 0.00836\n",
      "2025-05-18 07:43:46.215238: train_loss -0.9458\n",
      "2025-05-18 07:43:46.215495: val_loss -0.9438\n",
      "2025-05-18 07:43:46.215666: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 07:43:46.215838: Epoch time: 109.01 s\n",
      "2025-05-18 07:43:46.748466: \n",
      "2025-05-18 07:43:46.748555: Epoch 181\n",
      "2025-05-18 07:43:46.748627: Current learning rate: 0.00836\n",
      "2025-05-18 07:45:35.660892: train_loss -0.9497\n",
      "2025-05-18 07:45:35.661020: val_loss -0.9522\n",
      "2025-05-18 07:45:35.661053: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-18 07:45:35.661085: Epoch time: 108.91 s\n",
      "2025-05-18 07:45:36.189516: \n",
      "2025-05-18 07:45:36.189849: Epoch 182\n",
      "2025-05-18 07:45:36.189931: Current learning rate: 0.00835\n",
      "2025-05-18 07:47:25.127487: train_loss -0.9503\n",
      "2025-05-18 07:47:25.127717: val_loss -0.9364\n",
      "2025-05-18 07:47:25.127789: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-18 07:47:25.127829: Epoch time: 108.94 s\n",
      "2025-05-18 07:47:25.649754: \n",
      "2025-05-18 07:47:25.649838: Epoch 183\n",
      "2025-05-18 07:47:25.649901: Current learning rate: 0.00834\n",
      "2025-05-18 07:49:14.637848: train_loss -0.9527\n",
      "2025-05-18 07:49:14.638168: val_loss -0.9407\n",
      "2025-05-18 07:49:14.638227: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-18 07:49:14.638270: Epoch time: 108.99 s\n",
      "2025-05-18 07:49:15.156904: \n",
      "2025-05-18 07:49:15.156990: Epoch 184\n",
      "2025-05-18 07:49:15.157141: Current learning rate: 0.00833\n",
      "2025-05-18 07:51:04.084580: train_loss -0.9538\n",
      "2025-05-18 07:51:04.084697: val_loss -0.935\n",
      "2025-05-18 07:51:04.084729: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-18 07:51:04.084762: Epoch time: 108.93 s\n",
      "2025-05-18 07:51:04.606774: \n",
      "2025-05-18 07:51:04.606941: Epoch 185\n",
      "2025-05-18 07:51:04.607116: Current learning rate: 0.00832\n",
      "2025-05-18 07:52:53.572316: train_loss -0.9474\n",
      "2025-05-18 07:52:53.572444: val_loss -0.9404\n",
      "2025-05-18 07:52:53.572479: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-18 07:52:53.572514: Epoch time: 108.97 s\n",
      "2025-05-18 07:52:54.101331: \n",
      "2025-05-18 07:52:54.101414: Epoch 186\n",
      "2025-05-18 07:52:54.101479: Current learning rate: 0.00831\n",
      "2025-05-18 07:54:43.101426: train_loss -0.9526\n",
      "2025-05-18 07:54:43.101548: val_loss -0.9336\n",
      "2025-05-18 07:54:43.101581: Pseudo dice [np.float32(0.9717)]\n",
      "2025-05-18 07:54:43.101612: Epoch time: 109.0 s\n",
      "2025-05-18 07:54:43.622254: \n",
      "2025-05-18 07:54:43.622623: Epoch 187\n",
      "2025-05-18 07:54:43.622711: Current learning rate: 0.0083\n",
      "2025-05-18 07:56:32.552582: train_loss -0.9531\n",
      "2025-05-18 07:56:32.552712: val_loss -0.9407\n",
      "2025-05-18 07:56:32.552747: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 07:56:32.552908: Epoch time: 108.93 s\n",
      "2025-05-18 07:56:33.275221: \n",
      "2025-05-18 07:56:33.275355: Epoch 188\n",
      "2025-05-18 07:56:33.275445: Current learning rate: 0.00829\n",
      "2025-05-18 07:58:22.226199: train_loss -0.9555\n",
      "2025-05-18 07:58:22.226319: val_loss -0.9451\n",
      "2025-05-18 07:58:22.226507: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 07:58:22.226594: Epoch time: 108.95 s\n",
      "2025-05-18 07:58:22.753392: \n",
      "2025-05-18 07:58:22.753624: Epoch 189\n",
      "2025-05-18 07:58:22.753711: Current learning rate: 0.00828\n",
      "2025-05-18 08:00:11.666927: train_loss -0.955\n",
      "2025-05-18 08:00:11.667203: val_loss -0.9445\n",
      "2025-05-18 08:00:11.667239: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 08:00:11.667271: Epoch time: 108.91 s\n",
      "2025-05-18 08:00:12.194861: \n",
      "2025-05-18 08:00:12.195185: Epoch 190\n",
      "2025-05-18 08:00:12.195288: Current learning rate: 0.00827\n",
      "2025-05-18 08:02:01.137639: train_loss -0.9555\n",
      "2025-05-18 08:02:01.137762: val_loss -0.927\n",
      "2025-05-18 08:02:01.137795: Pseudo dice [np.float32(0.9707)]\n",
      "2025-05-18 08:02:01.137828: Epoch time: 108.94 s\n",
      "2025-05-18 08:02:01.666919: \n",
      "2025-05-18 08:02:01.667249: Epoch 191\n",
      "2025-05-18 08:02:01.667334: Current learning rate: 0.00826\n",
      "2025-05-18 08:03:50.688982: train_loss -0.9557\n",
      "2025-05-18 08:03:50.689101: val_loss -0.9314\n",
      "2025-05-18 08:03:50.689134: Pseudo dice [np.float32(0.9714)]\n",
      "2025-05-18 08:03:50.689165: Epoch time: 109.02 s\n",
      "2025-05-18 08:03:51.218737: \n",
      "2025-05-18 08:03:51.218920: Epoch 192\n",
      "2025-05-18 08:03:51.218987: Current learning rate: 0.00825\n",
      "2025-05-18 08:05:40.191751: train_loss -0.9572\n",
      "2025-05-18 08:05:40.191972: val_loss -0.9333\n",
      "2025-05-18 08:05:40.192041: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-18 08:05:40.192080: Epoch time: 108.97 s\n",
      "2025-05-18 08:05:40.713305: \n",
      "2025-05-18 08:05:40.713468: Epoch 193\n",
      "2025-05-18 08:05:40.713554: Current learning rate: 0.00824\n",
      "2025-05-18 08:07:29.729603: train_loss -0.9584\n",
      "2025-05-18 08:07:29.729728: val_loss -0.9421\n",
      "2025-05-18 08:07:29.729760: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 08:07:29.729794: Epoch time: 109.02 s\n",
      "2025-05-18 08:07:30.258908: \n",
      "2025-05-18 08:07:30.258999: Epoch 194\n",
      "2025-05-18 08:07:30.259063: Current learning rate: 0.00824\n",
      "2025-05-18 08:09:19.240599: train_loss -0.9559\n",
      "2025-05-18 08:09:19.240721: val_loss -0.9358\n",
      "2025-05-18 08:09:19.240753: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-18 08:09:19.240785: Epoch time: 108.98 s\n",
      "2025-05-18 08:09:19.766310: \n",
      "2025-05-18 08:09:19.766438: Epoch 195\n",
      "2025-05-18 08:09:19.766608: Current learning rate: 0.00823\n",
      "2025-05-18 08:11:08.783185: train_loss -0.9567\n",
      "2025-05-18 08:11:08.783324: val_loss -0.9437\n",
      "2025-05-18 08:11:08.783459: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 08:11:08.783546: Epoch time: 109.02 s\n",
      "2025-05-18 08:11:09.316181: \n",
      "2025-05-18 08:11:09.316271: Epoch 196\n",
      "2025-05-18 08:11:09.316334: Current learning rate: 0.00822\n",
      "2025-05-18 08:12:58.350060: train_loss -0.9172\n",
      "2025-05-18 08:12:58.350258: val_loss -0.9257\n",
      "2025-05-18 08:12:58.350291: Pseudo dice [np.float32(0.9698)]\n",
      "2025-05-18 08:12:58.350324: Epoch time: 109.03 s\n",
      "2025-05-18 08:12:58.875357: \n",
      "2025-05-18 08:12:58.875524: Epoch 197\n",
      "2025-05-18 08:12:58.875614: Current learning rate: 0.00821\n",
      "2025-05-18 08:14:47.780197: train_loss -0.947\n",
      "2025-05-18 08:14:47.780315: val_loss -0.9315\n",
      "2025-05-18 08:14:47.780349: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-18 08:14:47.780380: Epoch time: 108.91 s\n",
      "2025-05-18 08:14:48.305670: \n",
      "2025-05-18 08:14:48.305821: Epoch 198\n",
      "2025-05-18 08:14:48.305912: Current learning rate: 0.0082\n",
      "2025-05-18 08:16:37.279868: train_loss -0.9512\n",
      "2025-05-18 08:16:37.279984: val_loss -0.9365\n",
      "2025-05-18 08:16:37.280235: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 08:16:37.280279: Epoch time: 108.97 s\n",
      "2025-05-18 08:16:38.012273: \n",
      "2025-05-18 08:16:38.012550: Epoch 199\n",
      "2025-05-18 08:16:38.012649: Current learning rate: 0.00819\n",
      "2025-05-18 08:18:26.970121: train_loss -0.9537\n",
      "2025-05-18 08:18:26.970298: val_loss -0.9335\n",
      "2025-05-18 08:18:26.970443: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-18 08:18:26.970496: Epoch time: 108.96 s\n",
      "2025-05-18 08:18:27.697071: \n",
      "2025-05-18 08:18:27.697157: Epoch 200\n",
      "2025-05-18 08:18:27.697225: Current learning rate: 0.00818\n",
      "2025-05-18 08:20:16.648042: train_loss -0.9339\n",
      "2025-05-18 08:20:16.648176: val_loss -0.8828\n",
      "2025-05-18 08:20:16.648215: Pseudo dice [np.float32(0.9519)]\n",
      "2025-05-18 08:20:16.648246: Epoch time: 108.95 s\n",
      "2025-05-18 08:20:17.206492: \n",
      "2025-05-18 08:20:17.206767: Epoch 201\n",
      "2025-05-18 08:20:17.206848: Current learning rate: 0.00817\n",
      "2025-05-18 08:22:06.186338: train_loss -0.9239\n",
      "2025-05-18 08:22:06.186499: val_loss -0.9155\n",
      "2025-05-18 08:22:06.186534: Pseudo dice [np.float32(0.9648)]\n",
      "2025-05-18 08:22:06.186567: Epoch time: 108.98 s\n",
      "2025-05-18 08:22:06.719940: \n",
      "2025-05-18 08:22:06.720067: Epoch 202\n",
      "2025-05-18 08:22:06.720143: Current learning rate: 0.00816\n",
      "2025-05-18 08:23:55.744389: train_loss -0.9254\n",
      "2025-05-18 08:23:55.744505: val_loss -0.9083\n",
      "2025-05-18 08:23:55.744579: Pseudo dice [np.float32(0.964)]\n",
      "2025-05-18 08:23:55.744619: Epoch time: 109.03 s\n",
      "2025-05-18 08:23:56.283785: \n",
      "2025-05-18 08:23:56.283946: Epoch 203\n",
      "2025-05-18 08:23:56.284109: Current learning rate: 0.00815\n",
      "2025-05-18 08:25:45.310375: train_loss -0.9409\n",
      "2025-05-18 08:25:45.310604: val_loss -0.9359\n",
      "2025-05-18 08:25:45.310641: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 08:25:45.310680: Epoch time: 109.03 s\n",
      "2025-05-18 08:25:45.837755: \n",
      "2025-05-18 08:25:45.837859: Epoch 204\n",
      "2025-05-18 08:25:45.837924: Current learning rate: 0.00814\n",
      "2025-05-18 08:27:34.804901: train_loss -0.9447\n",
      "2025-05-18 08:27:34.805021: val_loss -0.9296\n",
      "2025-05-18 08:27:34.805053: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-18 08:27:34.805088: Epoch time: 108.97 s\n",
      "2025-05-18 08:27:35.336661: \n",
      "2025-05-18 08:27:35.336833: Epoch 205\n",
      "2025-05-18 08:27:35.336899: Current learning rate: 0.00813\n",
      "2025-05-18 08:29:24.311832: train_loss -0.9481\n",
      "2025-05-18 08:29:24.311982: val_loss -0.9424\n",
      "2025-05-18 08:29:24.312072: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-18 08:29:24.312128: Epoch time: 108.98 s\n",
      "2025-05-18 08:29:24.822439: \n",
      "2025-05-18 08:29:24.822779: Epoch 206\n",
      "2025-05-18 08:29:24.822870: Current learning rate: 0.00813\n",
      "2025-05-18 08:31:13.849593: train_loss -0.9525\n",
      "2025-05-18 08:31:13.849717: val_loss -0.9383\n",
      "2025-05-18 08:31:13.849747: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-18 08:31:13.849781: Epoch time: 109.03 s\n",
      "2025-05-18 08:31:14.353414: \n",
      "2025-05-18 08:31:14.353503: Epoch 207\n",
      "2025-05-18 08:31:14.353568: Current learning rate: 0.00812\n",
      "2025-05-18 08:33:03.300992: train_loss -0.9364\n",
      "2025-05-18 08:33:03.301393: val_loss -0.9155\n",
      "2025-05-18 08:33:03.301433: Pseudo dice [np.float32(0.9658)]\n",
      "2025-05-18 08:33:03.301467: Epoch time: 108.95 s\n",
      "2025-05-18 08:33:03.805510: \n",
      "2025-05-18 08:33:03.805607: Epoch 208\n",
      "2025-05-18 08:33:03.805670: Current learning rate: 0.00811\n",
      "2025-05-18 08:34:52.811635: train_loss -0.9437\n",
      "2025-05-18 08:34:52.811754: val_loss -0.9367\n",
      "2025-05-18 08:34:52.811788: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 08:34:52.811822: Epoch time: 109.01 s\n",
      "2025-05-18 08:34:53.321088: \n",
      "2025-05-18 08:34:53.321244: Epoch 209\n",
      "2025-05-18 08:34:53.321400: Current learning rate: 0.0081\n",
      "2025-05-18 08:36:42.245179: train_loss -0.9413\n",
      "2025-05-18 08:36:42.245317: val_loss -0.9114\n",
      "2025-05-18 08:36:42.245589: Pseudo dice [np.float32(0.9636)]\n",
      "2025-05-18 08:36:42.245684: Epoch time: 108.92 s\n",
      "2025-05-18 08:36:42.754317: \n",
      "2025-05-18 08:36:42.754446: Epoch 210\n",
      "2025-05-18 08:36:42.754531: Current learning rate: 0.00809\n",
      "2025-05-18 08:38:31.721451: train_loss -0.939\n",
      "2025-05-18 08:38:31.721625: val_loss -0.934\n",
      "2025-05-18 08:38:31.721705: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-18 08:38:31.721761: Epoch time: 108.97 s\n",
      "2025-05-18 08:38:32.229675: \n",
      "2025-05-18 08:38:32.229855: Epoch 211\n",
      "2025-05-18 08:38:32.229920: Current learning rate: 0.00808\n",
      "2025-05-18 08:40:21.009802: train_loss -0.9462\n",
      "2025-05-18 08:40:21.009984: val_loss -0.9289\n",
      "2025-05-18 08:40:21.010017: Pseudo dice [np.float32(0.9702)]\n",
      "2025-05-18 08:40:21.010058: Epoch time: 108.78 s\n",
      "2025-05-18 08:40:21.712217: \n",
      "2025-05-18 08:40:21.712413: Epoch 212\n",
      "2025-05-18 08:40:21.712533: Current learning rate: 0.00807\n",
      "2025-05-18 08:42:10.672500: train_loss -0.9437\n",
      "2025-05-18 08:42:10.672682: val_loss -0.9381\n",
      "2025-05-18 08:42:10.672715: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-18 08:42:10.672749: Epoch time: 108.96 s\n",
      "2025-05-18 08:42:11.180982: \n",
      "2025-05-18 08:42:11.181168: Epoch 213\n",
      "2025-05-18 08:42:11.181268: Current learning rate: 0.00806\n",
      "2025-05-18 08:44:00.145006: train_loss -0.9523\n",
      "2025-05-18 08:44:00.145190: val_loss -0.9419\n",
      "2025-05-18 08:44:00.145226: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-18 08:44:00.145259: Epoch time: 108.96 s\n",
      "2025-05-18 08:44:00.666883: \n",
      "2025-05-18 08:44:00.667042: Epoch 214\n",
      "2025-05-18 08:44:00.667109: Current learning rate: 0.00805\n",
      "2025-05-18 08:45:49.625956: train_loss -0.9551\n",
      "2025-05-18 08:45:49.626175: val_loss -0.9444\n",
      "2025-05-18 08:45:49.626210: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 08:45:49.626243: Epoch time: 108.96 s\n",
      "2025-05-18 08:45:50.145690: \n",
      "2025-05-18 08:45:50.145776: Epoch 215\n",
      "2025-05-18 08:45:50.145838: Current learning rate: 0.00804\n",
      "2025-05-18 08:47:39.127609: train_loss -0.9557\n",
      "2025-05-18 08:47:39.127734: val_loss -0.9423\n",
      "2025-05-18 08:47:39.127845: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 08:47:39.127899: Epoch time: 108.98 s\n",
      "2025-05-18 08:47:39.637883: \n",
      "2025-05-18 08:47:39.637972: Epoch 216\n",
      "2025-05-18 08:47:39.638038: Current learning rate: 0.00803\n",
      "2025-05-18 08:49:28.592454: train_loss -0.9566\n",
      "2025-05-18 08:49:28.592629: val_loss -0.9412\n",
      "2025-05-18 08:49:28.592663: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-18 08:49:28.592696: Epoch time: 108.96 s\n",
      "2025-05-18 08:49:29.125870: \n",
      "2025-05-18 08:49:29.126035: Epoch 217\n",
      "2025-05-18 08:49:29.126113: Current learning rate: 0.00802\n",
      "2025-05-18 08:51:18.077772: train_loss -0.9549\n",
      "2025-05-18 08:51:18.077896: val_loss -0.9267\n",
      "2025-05-18 08:51:18.077938: Pseudo dice [np.float32(0.9698)]\n",
      "2025-05-18 08:51:18.077973: Epoch time: 108.95 s\n",
      "2025-05-18 08:51:18.593074: \n",
      "2025-05-18 08:51:18.593286: Epoch 218\n",
      "2025-05-18 08:51:18.593371: Current learning rate: 0.00801\n",
      "2025-05-18 08:53:07.564524: train_loss -0.9567\n",
      "2025-05-18 08:53:07.564737: val_loss -0.9435\n",
      "2025-05-18 08:53:07.564825: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 08:53:07.565013: Epoch time: 108.97 s\n",
      "2025-05-18 08:53:08.080829: \n",
      "2025-05-18 08:53:08.080918: Epoch 219\n",
      "2025-05-18 08:53:08.080979: Current learning rate: 0.00801\n",
      "2025-05-18 08:54:57.165463: train_loss -0.9546\n",
      "2025-05-18 08:54:57.165668: val_loss -0.9395\n",
      "2025-05-18 08:54:57.165704: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-18 08:54:57.165738: Epoch time: 109.09 s\n",
      "2025-05-18 08:54:57.674155: \n",
      "2025-05-18 08:54:57.674244: Epoch 220\n",
      "2025-05-18 08:54:57.674307: Current learning rate: 0.008\n",
      "2025-05-18 08:56:46.605188: train_loss -0.9567\n",
      "2025-05-18 08:56:46.605309: val_loss -0.9405\n",
      "2025-05-18 08:56:46.605341: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-18 08:56:46.605375: Epoch time: 108.93 s\n",
      "2025-05-18 08:56:47.119003: \n",
      "2025-05-18 08:56:47.119096: Epoch 221\n",
      "2025-05-18 08:56:47.119160: Current learning rate: 0.00799\n",
      "2025-05-18 08:58:36.116320: train_loss -0.9564\n",
      "2025-05-18 08:58:36.116502: val_loss -0.9464\n",
      "2025-05-18 08:58:36.116535: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 08:58:36.116568: Epoch time: 109.0 s\n",
      "2025-05-18 08:58:36.626848: \n",
      "2025-05-18 08:58:36.626941: Epoch 222\n",
      "2025-05-18 08:58:36.627022: Current learning rate: 0.00798\n",
      "2025-05-18 09:00:25.576856: train_loss -0.9525\n",
      "2025-05-18 09:00:25.577045: val_loss -0.9378\n",
      "2025-05-18 09:00:25.577085: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 09:00:25.577121: Epoch time: 108.95 s\n",
      "2025-05-18 09:00:26.084633: \n",
      "2025-05-18 09:00:26.085032: Epoch 223\n",
      "2025-05-18 09:00:26.085401: Current learning rate: 0.00797\n",
      "2025-05-18 09:02:15.119496: train_loss -0.9531\n",
      "2025-05-18 09:02:15.119616: val_loss -0.935\n",
      "2025-05-18 09:02:15.119651: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-18 09:02:15.119684: Epoch time: 109.04 s\n",
      "2025-05-18 09:02:15.634207: \n",
      "2025-05-18 09:02:15.634289: Epoch 224\n",
      "2025-05-18 09:02:15.634352: Current learning rate: 0.00796\n",
      "2025-05-18 09:04:04.594070: train_loss -0.9532\n",
      "2025-05-18 09:04:04.594193: val_loss -0.9479\n",
      "2025-05-18 09:04:04.594227: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-18 09:04:04.594261: Epoch time: 108.96 s\n",
      "2025-05-18 09:04:05.290569: \n",
      "2025-05-18 09:04:05.290743: Epoch 225\n",
      "2025-05-18 09:04:05.290830: Current learning rate: 0.00795\n",
      "2025-05-18 09:05:54.315242: train_loss -0.9534\n",
      "2025-05-18 09:05:54.315366: val_loss -0.941\n",
      "2025-05-18 09:05:54.315399: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 09:05:54.315431: Epoch time: 109.03 s\n",
      "2025-05-18 09:05:54.840400: \n",
      "2025-05-18 09:05:54.840672: Epoch 226\n",
      "2025-05-18 09:05:54.840746: Current learning rate: 0.00794\n",
      "2025-05-18 09:07:43.848412: train_loss -0.9503\n",
      "2025-05-18 09:07:43.848539: val_loss -0.9369\n",
      "2025-05-18 09:07:43.848569: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-18 09:07:43.848601: Epoch time: 109.01 s\n",
      "2025-05-18 09:07:44.355102: \n",
      "2025-05-18 09:07:44.355261: Epoch 227\n",
      "2025-05-18 09:07:44.355329: Current learning rate: 0.00793\n",
      "2025-05-18 09:09:33.298931: train_loss -0.9528\n",
      "2025-05-18 09:09:33.299065: val_loss -0.9349\n",
      "2025-05-18 09:09:33.299100: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-18 09:09:33.299134: Epoch time: 108.94 s\n",
      "2025-05-18 09:09:33.806329: \n",
      "2025-05-18 09:09:33.806477: Epoch 228\n",
      "2025-05-18 09:09:33.806543: Current learning rate: 0.00792\n",
      "2025-05-18 09:11:22.845022: train_loss -0.956\n",
      "2025-05-18 09:11:22.845239: val_loss -0.9463\n",
      "2025-05-18 09:11:22.845280: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 09:11:22.845331: Epoch time: 109.04 s\n",
      "2025-05-18 09:11:23.352483: \n",
      "2025-05-18 09:11:23.352667: Epoch 229\n",
      "2025-05-18 09:11:23.352821: Current learning rate: 0.00791\n",
      "2025-05-18 09:13:12.323921: train_loss -0.956\n",
      "2025-05-18 09:13:12.324232: val_loss -0.948\n",
      "2025-05-18 09:13:12.324428: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-18 09:13:12.324476: Epoch time: 108.97 s\n",
      "2025-05-18 09:13:12.829150: \n",
      "2025-05-18 09:13:12.829308: Epoch 230\n",
      "2025-05-18 09:13:12.829499: Current learning rate: 0.0079\n",
      "2025-05-18 09:15:01.740459: train_loss -0.9568\n",
      "2025-05-18 09:15:01.740598: val_loss -0.9498\n",
      "2025-05-18 09:15:01.740697: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 09:15:01.740931: Epoch time: 108.91 s\n",
      "2025-05-18 09:15:02.244171: \n",
      "2025-05-18 09:15:02.244318: Epoch 231\n",
      "2025-05-18 09:15:02.244386: Current learning rate: 0.00789\n",
      "2025-05-18 09:16:51.269782: train_loss -0.9581\n",
      "2025-05-18 09:16:51.269910: val_loss -0.9433\n",
      "2025-05-18 09:16:51.269946: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 09:16:51.269980: Epoch time: 109.03 s\n",
      "2025-05-18 09:16:51.270000: Yayy! New best EMA pseudo Dice: 0.9750999808311462\n",
      "2025-05-18 09:16:51.992772: \n",
      "2025-05-18 09:16:51.993022: Epoch 232\n",
      "2025-05-18 09:16:51.993103: Current learning rate: 0.00789\n",
      "2025-05-18 09:18:41.008488: train_loss -0.9576\n",
      "2025-05-18 09:18:41.008698: val_loss -0.9463\n",
      "2025-05-18 09:18:41.008770: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 09:18:41.008812: Epoch time: 109.02 s\n",
      "2025-05-18 09:18:41.008835: Yayy! New best EMA pseudo Dice: 0.9753999710083008\n",
      "2025-05-18 09:18:41.726260: \n",
      "2025-05-18 09:18:41.726409: Epoch 233\n",
      "2025-05-18 09:18:41.726490: Current learning rate: 0.00788\n",
      "2025-05-18 09:20:30.641938: train_loss -0.9577\n",
      "2025-05-18 09:20:30.642092: val_loss -0.9424\n",
      "2025-05-18 09:20:30.642127: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-18 09:20:30.642160: Epoch time: 108.92 s\n",
      "2025-05-18 09:20:31.138688: \n",
      "2025-05-18 09:20:31.138772: Epoch 234\n",
      "2025-05-18 09:20:31.138836: Current learning rate: 0.00787\n",
      "2025-05-18 09:22:20.075497: train_loss -0.9581\n",
      "2025-05-18 09:22:20.075619: val_loss -0.9539\n",
      "2025-05-18 09:22:20.075668: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-18 09:22:20.075725: Epoch time: 108.94 s\n",
      "2025-05-18 09:22:20.075761: Yayy! New best EMA pseudo Dice: 0.9757999777793884\n",
      "2025-05-18 09:22:20.793818: \n",
      "2025-05-18 09:22:20.794356: Epoch 235\n",
      "2025-05-18 09:22:20.794442: Current learning rate: 0.00786\n",
      "2025-05-18 09:24:09.804906: train_loss -0.9598\n",
      "2025-05-18 09:24:09.805043: val_loss -0.9486\n",
      "2025-05-18 09:24:09.805094: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-18 09:24:09.805158: Epoch time: 109.01 s\n",
      "2025-05-18 09:24:09.805264: Yayy! New best EMA pseudo Dice: 0.9761000275611877\n",
      "2025-05-18 09:24:10.523034: \n",
      "2025-05-18 09:24:10.523118: Epoch 236\n",
      "2025-05-18 09:24:10.523181: Current learning rate: 0.00785\n",
      "2025-05-18 09:25:59.626681: train_loss -0.9607\n",
      "2025-05-18 09:25:59.626865: val_loss -0.9477\n",
      "2025-05-18 09:25:59.626898: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 09:25:59.626932: Epoch time: 109.1 s\n",
      "2025-05-18 09:25:59.626958: Yayy! New best EMA pseudo Dice: 0.9763000011444092\n",
      "2025-05-18 09:26:00.354060: \n",
      "2025-05-18 09:26:00.354141: Epoch 237\n",
      "2025-05-18 09:26:00.354204: Current learning rate: 0.00784\n",
      "2025-05-18 09:27:49.316027: train_loss -0.9572\n",
      "2025-05-18 09:27:49.316141: val_loss -0.9452\n",
      "2025-05-18 09:27:49.316175: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-18 09:27:49.316208: Epoch time: 108.96 s\n",
      "2025-05-18 09:27:50.015854: \n",
      "2025-05-18 09:27:50.015967: Epoch 238\n",
      "2025-05-18 09:27:50.016068: Current learning rate: 0.00783\n",
      "2025-05-18 09:29:39.036176: train_loss -0.9564\n",
      "2025-05-18 09:29:39.036293: val_loss -0.946\n",
      "2025-05-18 09:29:39.036324: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 09:29:39.036356: Epoch time: 109.02 s\n",
      "2025-05-18 09:29:39.542201: \n",
      "2025-05-18 09:29:39.542439: Epoch 239\n",
      "2025-05-18 09:29:39.542510: Current learning rate: 0.00782\n",
      "2025-05-18 09:31:28.593582: train_loss -0.9583\n",
      "2025-05-18 09:31:28.593759: val_loss -0.9445\n",
      "2025-05-18 09:31:28.593793: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 09:31:28.593827: Epoch time: 109.05 s\n",
      "2025-05-18 09:31:29.102351: \n",
      "2025-05-18 09:31:29.102591: Epoch 240\n",
      "2025-05-18 09:31:29.102790: Current learning rate: 0.00781\n",
      "2025-05-18 09:33:18.076398: train_loss -0.9577\n",
      "2025-05-18 09:33:18.076521: val_loss -0.9404\n",
      "2025-05-18 09:33:18.076554: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 09:33:18.076586: Epoch time: 108.97 s\n",
      "2025-05-18 09:33:18.588809: \n",
      "2025-05-18 09:33:18.588944: Epoch 241\n",
      "2025-05-18 09:33:18.589009: Current learning rate: 0.0078\n",
      "2025-05-18 09:35:07.628517: train_loss -0.959\n",
      "2025-05-18 09:35:07.628634: val_loss -0.9517\n",
      "2025-05-18 09:35:07.628665: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-18 09:35:07.628699: Epoch time: 109.04 s\n",
      "2025-05-18 09:35:07.628720: Yayy! New best EMA pseudo Dice: 0.9763000011444092\n",
      "2025-05-18 09:35:08.358992: \n",
      "2025-05-18 09:35:08.359151: Epoch 242\n",
      "2025-05-18 09:35:08.359223: Current learning rate: 0.00779\n",
      "2025-05-18 09:36:57.395289: train_loss -0.9587\n",
      "2025-05-18 09:36:57.395645: val_loss -0.9519\n",
      "2025-05-18 09:36:57.395787: Pseudo dice [np.float32(0.9798)]\n",
      "2025-05-18 09:36:57.395831: Epoch time: 109.04 s\n",
      "2025-05-18 09:36:57.395853: Yayy! New best EMA pseudo Dice: 0.9767000079154968\n",
      "2025-05-18 09:36:58.119849: \n",
      "2025-05-18 09:36:58.120074: Epoch 243\n",
      "2025-05-18 09:36:58.120162: Current learning rate: 0.00778\n",
      "2025-05-18 09:38:47.044884: train_loss -0.9559\n",
      "2025-05-18 09:38:47.045073: val_loss -0.9351\n",
      "2025-05-18 09:38:47.045107: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-18 09:38:47.045140: Epoch time: 108.93 s\n",
      "2025-05-18 09:38:47.553929: \n",
      "2025-05-18 09:38:47.554089: Epoch 244\n",
      "2025-05-18 09:38:47.554162: Current learning rate: 0.00777\n",
      "2025-05-18 09:40:36.540216: train_loss -0.9595\n",
      "2025-05-18 09:40:36.540346: val_loss -0.9465\n",
      "2025-05-18 09:40:36.540459: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 09:40:36.540563: Epoch time: 108.99 s\n",
      "2025-05-18 09:40:37.054654: \n",
      "2025-05-18 09:40:37.054802: Epoch 245\n",
      "2025-05-18 09:40:37.054913: Current learning rate: 0.00777\n",
      "2025-05-18 09:42:26.013067: train_loss -0.9579\n",
      "2025-05-18 09:42:26.013191: val_loss -0.9485\n",
      "2025-05-18 09:42:26.013226: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-18 09:42:26.013258: Epoch time: 108.96 s\n",
      "2025-05-18 09:42:26.516441: \n",
      "2025-05-18 09:42:26.516531: Epoch 246\n",
      "2025-05-18 09:42:26.516597: Current learning rate: 0.00776\n",
      "2025-05-18 09:44:15.540931: train_loss -0.9573\n",
      "2025-05-18 09:44:15.541065: val_loss -0.9441\n",
      "2025-05-18 09:44:15.541260: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 09:44:15.541426: Epoch time: 109.03 s\n",
      "2025-05-18 09:44:16.055615: \n",
      "2025-05-18 09:44:16.055704: Epoch 247\n",
      "2025-05-18 09:44:16.055770: Current learning rate: 0.00775\n",
      "2025-05-18 09:46:05.106668: train_loss -0.9544\n",
      "2025-05-18 09:46:05.106858: val_loss -0.9356\n",
      "2025-05-18 09:46:05.106900: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-18 09:46:05.106936: Epoch time: 109.05 s\n",
      "2025-05-18 09:46:05.624151: \n",
      "2025-05-18 09:46:05.624238: Epoch 248\n",
      "2025-05-18 09:46:05.624300: Current learning rate: 0.00774\n",
      "2025-05-18 09:47:54.623507: train_loss -0.9554\n",
      "2025-05-18 09:47:54.623627: val_loss -0.9284\n",
      "2025-05-18 09:47:54.623662: Pseudo dice [np.float32(0.9706)]\n",
      "2025-05-18 09:47:54.623693: Epoch time: 109.0 s\n",
      "2025-05-18 09:47:55.125712: \n",
      "2025-05-18 09:47:55.125792: Epoch 249\n",
      "2025-05-18 09:47:55.125857: Current learning rate: 0.00773\n",
      "2025-05-18 09:49:44.161320: train_loss -0.9547\n",
      "2025-05-18 09:49:44.161555: val_loss -0.9418\n",
      "2025-05-18 09:49:44.161597: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-18 09:49:44.161629: Epoch time: 109.04 s\n",
      "2025-05-18 09:49:44.884135: \n",
      "2025-05-18 09:49:44.884222: Epoch 250\n",
      "2025-05-18 09:49:44.884285: Current learning rate: 0.00772\n",
      "2025-05-18 09:51:33.889474: train_loss -0.9566\n",
      "2025-05-18 09:51:33.889607: val_loss -0.9374\n",
      "2025-05-18 09:51:33.889642: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-18 09:51:33.889676: Epoch time: 109.01 s\n",
      "2025-05-18 09:51:34.589690: \n",
      "2025-05-18 09:51:34.589826: Epoch 251\n",
      "2025-05-18 09:51:34.589954: Current learning rate: 0.00771\n",
      "2025-05-18 09:53:23.665941: train_loss -0.9565\n",
      "2025-05-18 09:53:23.666118: val_loss -0.9474\n",
      "2025-05-18 09:53:23.666152: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 09:53:23.666204: Epoch time: 109.08 s\n",
      "2025-05-18 09:53:24.174946: \n",
      "2025-05-18 09:53:24.175111: Epoch 252\n",
      "2025-05-18 09:53:24.175230: Current learning rate: 0.0077\n",
      "2025-05-18 09:55:13.206751: train_loss -0.9567\n",
      "2025-05-18 09:55:13.206952: val_loss -0.9492\n",
      "2025-05-18 09:55:13.206990: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-18 09:55:13.207036: Epoch time: 109.03 s\n",
      "2025-05-18 09:55:13.717890: \n",
      "2025-05-18 09:55:13.718040: Epoch 253\n",
      "2025-05-18 09:55:13.718110: Current learning rate: 0.00769\n",
      "2025-05-18 09:57:02.659270: train_loss -0.9503\n",
      "2025-05-18 09:57:02.659390: val_loss -0.9251\n",
      "[np.float32(0.9673)]659426: Pseudo dice \n",
      "2025-05-18 09:57:02.659527: Epoch time: 108.94 s\n",
      "2025-05-18 09:57:03.171904: \n",
      "2025-05-18 09:57:03.172050: Epoch 254\n",
      "2025-05-18 09:57:03.172219: Current learning rate: 0.00768\n",
      "2025-05-18 09:58:53.142012: train_loss -0.9245\n",
      "2025-05-18 09:58:53.142188: val_loss -0.9331\n",
      "2025-05-18 09:58:53.142220: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-18 09:58:53.142253: Epoch time: 109.97 s\n",
      "2025-05-18 09:58:53.657602: \n",
      "2025-05-18 09:58:53.657698: Epoch 255\n",
      "2025-05-18 09:58:53.657761: Current learning rate: 0.00767\n",
      "2025-05-18 10:00:43.044236: train_loss -0.9227\n",
      "2025-05-18 10:00:43.044359: val_loss -0.9128\n",
      "2025-05-18 10:00:43.044455: Pseudo dice [np.float32(0.9661)]\n",
      "2025-05-18 10:00:43.044574: Epoch time: 109.39 s\n",
      "2025-05-18 10:00:43.552247: \n",
      "2025-05-18 10:00:43.552513: Epoch 256\n",
      "2025-05-18 10:00:43.552585: Current learning rate: 0.00766\n",
      "2025-05-18 10:02:32.883967: train_loss -0.939\n",
      "2025-05-18 10:02:32.884090: val_loss -0.9342\n",
      "2025-05-18 10:02:32.884125: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-18 10:02:32.884160: Epoch time: 109.33 s\n",
      "2025-05-18 10:02:33.392562: \n",
      "2025-05-18 10:02:33.392725: Epoch 257\n",
      "2025-05-18 10:02:33.392805: Current learning rate: 0.00765\n",
      "2025-05-18 10:04:22.813867: train_loss -0.9495\n",
      "2025-05-18 10:04:22.813998: val_loss -0.939\n",
      "2025-05-18 10:04:22.814033: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-18 10:04:22.814065: Epoch time: 109.42 s\n",
      "2025-05-18 10:04:23.327973: \n",
      "2025-05-18 10:04:23.328072: Epoch 258\n",
      "2025-05-18 10:04:23.328139: Current learning rate: 0.00764\n",
      "2025-05-18 10:06:12.642139: train_loss -0.954\n",
      "2025-05-18 10:06:12.642272: val_loss -0.9467\n",
      "2025-05-18 10:06:12.642315: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 10:06:12.642372: Epoch time: 109.31 s\n",
      "2025-05-18 10:06:13.152988: \n",
      "2025-05-18 10:06:13.153077: Epoch 259\n",
      "2025-05-18 10:06:13.153139: Current learning rate: 0.00764\n",
      "2025-05-18 10:08:02.485634: train_loss -0.9558\n",
      "2025-05-18 10:08:02.485758: val_loss -0.9472\n",
      "2025-05-18 10:08:02.485898: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-18 10:08:02.485973: Epoch time: 109.33 s\n",
      "2025-05-18 10:08:02.999084: \n",
      "2025-05-18 10:08:02.999213: Epoch 260\n",
      "2025-05-18 10:08:02.999280: Current learning rate: 0.00763\n",
      "2025-05-18 10:09:52.277664: train_loss -0.9564\n",
      "2025-05-18 10:09:52.277908: val_loss -0.9529\n",
      "2025-05-18 10:09:52.277956: Pseudo dice [np.float32(0.9797)]\n",
      "2025-05-18 10:09:52.277997: Epoch time: 109.28 s\n",
      "2025-05-18 10:09:52.783572: \n",
      "2025-05-18 10:09:52.783733: Epoch 261\n",
      "2025-05-18 10:09:52.783840: Current learning rate: 0.00762\n",
      "2025-05-18 10:11:42.096025: train_loss -0.9563\n",
      "2025-05-18 10:11:42.096155: val_loss -0.9373\n",
      "2025-05-18 10:11:42.096189: Pseudo dice [np.float32(0.9725)]\n",
      "2025-05-18 10:11:42.096224: Epoch time: 109.31 s\n",
      "2025-05-18 10:11:42.615814: \n",
      "2025-05-18 10:11:42.615953: Epoch 262\n",
      "2025-05-18 10:11:42.616115: Current learning rate: 0.00761\n",
      "2025-05-18 10:13:31.962386: train_loss -0.9581\n",
      "2025-05-18 10:13:31.962504: val_loss -0.9446\n",
      "2025-05-18 10:13:31.962539: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 10:13:31.962574: Epoch time: 109.35 s\n",
      "2025-05-18 10:13:32.472579: \n",
      "2025-05-18 10:13:32.472839: Epoch 263\n",
      "2025-05-18 10:13:32.473097: Current learning rate: 0.0076\n",
      "2025-05-18 10:15:21.756543: train_loss -0.9526\n",
      "2025-05-18 10:15:21.756772: val_loss -0.9508\n",
      "2025-05-18 10:15:21.756815: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-18 10:15:21.756849: Epoch time: 109.28 s\n",
      "2025-05-18 10:15:22.472157: \n",
      "2025-05-18 10:15:22.472250: Epoch 264\n",
      "2025-05-18 10:15:22.472313: Current learning rate: 0.00759\n",
      "2025-05-18 10:17:11.837815: train_loss -0.9579\n",
      "2025-05-18 10:17:11.837940: val_loss -0.9455\n",
      "2025-05-18 10:17:11.837974: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-18 10:17:11.838009: Epoch time: 109.37 s\n",
      "2025-05-18 10:17:12.350713: \n",
      "2025-05-18 10:17:12.350871: Epoch 265\n",
      "2025-05-18 10:17:12.350936: Current learning rate: 0.00758\n",
      "2025-05-18 10:19:01.745439: train_loss -0.9566\n",
      "2025-05-18 10:19:01.745833: val_loss -0.9316\n",
      "2025-05-18 10:19:01.745877: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-18 10:19:01.745922: Epoch time: 109.4 s\n",
      "2025-05-18 10:19:02.266057: \n",
      "2025-05-18 10:19:02.266187: Epoch 266\n",
      "2025-05-18 10:19:02.266273: Current learning rate: 0.00757\n",
      "2025-05-18 10:20:51.597926: train_loss -0.9582\n",
      "2025-05-18 10:20:51.598056: val_loss -0.9433\n",
      "2025-05-18 10:20:51.598089: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 10:20:51.598124: Epoch time: 109.33 s\n",
      "2025-05-18 10:20:52.123674: \n",
      "2025-05-18 10:20:52.123810: Epoch 267\n",
      "2025-05-18 10:20:52.123874: Current learning rate: 0.00756\n",
      "2025-05-18 10:22:41.473118: train_loss -0.9601\n",
      "2025-05-18 10:22:41.473244: val_loss -0.9459\n",
      "2025-05-18 10:22:41.473280: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 10:22:41.473313: Epoch time: 109.35 s\n",
      "2025-05-18 10:22:41.987504: \n",
      "2025-05-18 10:22:41.987595: Epoch 268\n",
      "2025-05-18 10:22:41.987656: Current learning rate: 0.00755\n",
      "2025-05-18 10:24:31.337587: train_loss -0.9579\n",
      "2025-05-18 10:24:31.337766: val_loss -0.9451\n",
      "2025-05-18 10:24:31.337807: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 10:24:31.337839: Epoch time: 109.35 s\n",
      "2025-05-18 10:24:31.852584: \n",
      "2025-05-18 10:24:31.852692: Epoch 269\n",
      "2025-05-18 10:24:31.852757: Current learning rate: 0.00754\n",
      "2025-05-18 10:26:21.119533: train_loss -0.959\n",
      "2025-05-18 10:26:21.119674: val_loss -0.9359\n",
      "2025-05-18 10:26:21.119713: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-18 10:26:21.119745: Epoch time: 109.27 s\n",
      "2025-05-18 10:26:21.634345: \n",
      "2025-05-18 10:26:21.634519: Epoch 270\n",
      "2025-05-18 10:26:21.634589: Current learning rate: 0.00753\n",
      "2025-05-18 10:28:10.896174: train_loss -0.9605\n",
      "2025-05-18 10:28:10.896556: val_loss -0.9385\n",
      "2025-05-18 10:28:10.896629: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 10:28:10.896676: Epoch time: 109.26 s\n",
      "2025-05-18 10:28:11.405985: \n",
      "2025-05-18 10:28:11.406070: Epoch 271\n",
      "2025-05-18 10:28:11.406132: Current learning rate: 0.00752\n",
      "2025-05-18 10:30:00.896530: train_loss -0.9585\n",
      "2025-05-18 10:30:00.896653: val_loss -0.9486\n",
      "2025-05-18 10:30:00.896686: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-18 10:30:00.896718: Epoch time: 109.49 s\n",
      "2025-05-18 10:30:01.402518: \n",
      "2025-05-18 10:30:01.402606: Epoch 272\n",
      "2025-05-18 10:30:01.402669: Current learning rate: 0.00751\n",
      "2025-05-18 10:31:50.762386: train_loss -0.9613\n",
      "2025-05-18 10:31:50.762549: val_loss -0.944\n",
      "2025-05-18 10:31:50.762586: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 10:31:50.762620: Epoch time: 109.36 s\n",
      "2025-05-18 10:31:51.279027: \n",
      "2025-05-18 10:31:51.279251: Epoch 273\n",
      "2025-05-18 10:31:51.279323: Current learning rate: 0.00751\n",
      "2025-05-18 10:33:40.612415: train_loss -0.959\n",
      "2025-05-18 10:33:40.612533: val_loss -0.9412\n",
      "2025-05-18 10:33:40.612567: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 10:33:40.612600: Epoch time: 109.33 s\n",
      "2025-05-18 10:33:41.129872: \n",
      "2025-05-18 10:33:41.130022: Epoch 274\n",
      "2025-05-18 10:33:41.130123: Current learning rate: 0.0075\n",
      "2025-05-18 10:35:30.630071: train_loss -0.9569\n",
      "2025-05-18 10:35:30.630235: val_loss -0.9412\n",
      "2025-05-18 10:35:30.630267: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 10:35:30.630302: Epoch time: 109.5 s\n",
      "2025-05-18 10:35:31.148389: \n",
      "2025-05-18 10:35:31.148539: Epoch 275\n",
      "2025-05-18 10:35:31.148628: Current learning rate: 0.00749\n",
      "2025-05-18 10:37:20.482857: train_loss -0.96\n",
      "2025-05-18 10:37:20.482982: val_loss -0.9441\n",
      "2025-05-18 10:37:20.483110: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 10:37:20.483152: Epoch time: 109.34 s\n",
      "2025-05-18 10:37:20.991999: \n",
      "2025-05-18 10:37:20.992140: Epoch 276\n",
      "2025-05-18 10:37:20.992213: Current learning rate: 0.00748\n",
      "2025-05-18 10:39:10.307625: train_loss -0.96\n",
      "2025-05-18 10:39:10.307783: val_loss -0.9355\n",
      "2025-05-18 10:39:10.307850: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-18 10:39:10.308045: Epoch time: 109.32 s\n",
      "2025-05-18 10:39:11.023236: \n",
      "2025-05-18 10:39:11.023333: Epoch 277\n",
      "2025-05-18 10:39:11.023397: Current learning rate: 0.00747\n",
      "2025-05-18 10:41:00.419733: train_loss -0.9584\n",
      "2025-05-18 10:41:00.419979: val_loss -0.9422\n",
      "2025-05-18 10:41:00.420022: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 10:41:00.420057: Epoch time: 109.4 s\n",
      "2025-05-18 10:41:00.927569: \n",
      "2025-05-18 10:41:00.927663: Epoch 278\n",
      "2025-05-18 10:41:00.927727: Current learning rate: 0.00746\n",
      "2025-05-18 10:42:50.292239: train_loss -0.9593\n",
      "2025-05-18 10:42:50.292365: val_loss -0.9475\n",
      "[np.float32(0.9772)]292400: Pseudo dice \n",
      "2025-05-18 10:42:50.292505: Epoch time: 109.37 s\n",
      "2025-05-18 10:42:50.812411: \n",
      "2025-05-18 10:42:50.812560: Epoch 279\n",
      "2025-05-18 10:42:50.812629: Current learning rate: 0.00745\n",
      "2025-05-18 10:44:40.139116: train_loss -0.9594\n",
      "2025-05-18 10:44:40.139304: val_loss -0.944\n",
      "2025-05-18 10:44:40.139338: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 10:44:40.139369: Epoch time: 109.33 s\n",
      "2025-05-18 10:44:40.647745: \n",
      "2025-05-18 10:44:40.647889: Epoch 280\n",
      "2025-05-18 10:44:40.647962: Current learning rate: 0.00744\n",
      "2025-05-18 10:46:30.036883: train_loss -0.9596\n",
      "2025-05-18 10:46:30.037075: val_loss -0.9492\n",
      "2025-05-18 10:46:30.037115: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 10:46:30.037150: Epoch time: 109.39 s\n",
      "2025-05-18 10:46:30.556487: \n",
      "2025-05-18 10:46:30.556677: Epoch 281\n",
      "2025-05-18 10:46:30.556756: Current learning rate: 0.00743\n",
      "2025-05-18 10:48:19.914395: train_loss -0.9588\n",
      "2025-05-18 10:48:19.914565: val_loss -0.9399\n",
      "2025-05-18 10:48:19.914598: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-18 10:48:19.914632: Epoch time: 109.36 s\n",
      "2025-05-18 10:48:20.437620: \n",
      "2025-05-18 10:48:20.437769: Epoch 282\n",
      "2025-05-18 10:48:20.437836: Current learning rate: 0.00742\n",
      "2025-05-18 10:50:09.801627: train_loss -0.9591\n",
      "2025-05-18 10:50:09.801836: val_loss -0.9434\n",
      "2025-05-18 10:50:09.801876: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 10:50:09.801910: Epoch time: 109.36 s\n",
      "2025-05-18 10:50:10.315815: \n",
      "2025-05-18 10:50:10.315904: Epoch 283\n",
      "2025-05-18 10:50:10.315976: Current learning rate: 0.00741\n",
      "2025-05-18 10:51:59.653314: train_loss -0.9599\n",
      "2025-05-18 10:51:59.653437: val_loss -0.9397\n",
      "2025-05-18 10:51:59.653469: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 10:51:59.653500: Epoch time: 109.34 s\n",
      "2025-05-18 10:52:00.175625: \n",
      "2025-05-18 10:52:00.175778: Epoch 284\n",
      "2025-05-18 10:52:00.175891: Current learning rate: 0.0074\n",
      "2025-05-18 10:53:49.562231: train_loss -0.9599\n",
      "2025-05-18 10:53:49.562491: val_loss -0.9469\n",
      "2025-05-18 10:53:49.562529: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-18 10:53:49.562563: Epoch time: 109.39 s\n",
      "2025-05-18 10:53:50.077423: \n",
      "2025-05-18 10:53:50.077527: Epoch 285\n",
      "2025-05-18 10:53:50.077591: Current learning rate: 0.00739\n",
      "2025-05-18 10:55:39.460204: train_loss -0.96\n",
      "2025-05-18 10:55:39.460384: val_loss -0.9506\n",
      "2025-05-18 10:55:39.460419: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-18 10:55:39.460456: Epoch time: 109.38 s\n",
      "2025-05-18 10:55:39.974037: \n",
      "2025-05-18 10:55:39.974213: Epoch 286\n",
      "2025-05-18 10:55:39.974278: Current learning rate: 0.00738\n",
      "2025-05-18 10:57:29.296419: train_loss -0.9606\n",
      "2025-05-18 10:57:29.296540: val_loss -0.9438\n",
      "2025-05-18 10:57:29.296572: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 10:57:29.296605: Epoch time: 109.32 s\n",
      "2025-05-18 10:57:29.812853: \n",
      "2025-05-18 10:57:29.812936: Epoch 287\n",
      "2025-05-18 10:57:29.812997: Current learning rate: 0.00738\n",
      "2025-05-18 10:59:19.217968: train_loss -0.9597\n",
      "2025-05-18 10:59:19.218087: val_loss -0.9434\n",
      "2025-05-18 10:59:19.218120: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-18 10:59:19.218161: Epoch time: 109.41 s\n",
      "2025-05-18 10:59:19.741498: \n",
      "2025-05-18 10:59:19.741643: Epoch 288\n",
      "2025-05-18 10:59:19.741722: Current learning rate: 0.00737\n",
      "2025-05-18 11:01:09.042175: train_loss -0.9601\n",
      "2025-05-18 11:01:09.042294: val_loss -0.939\n",
      "2025-05-18 11:01:09.042326: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-18 11:01:09.042356: Epoch time: 109.3 s\n",
      "2025-05-18 11:01:09.764854: \n",
      "2025-05-18 11:01:09.765209: Epoch 289\n",
      "2025-05-18 11:01:09.765321: Current learning rate: 0.00736\n",
      "2025-05-18 11:02:59.235275: train_loss -0.959\n",
      "2025-05-18 11:02:59.235403: val_loss -0.9391\n",
      "2025-05-18 11:02:59.235437: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-18 11:02:59.235469: Epoch time: 109.47 s\n",
      "2025-05-18 11:02:59.755851: \n",
      "2025-05-18 11:02:59.755939: Epoch 290\n",
      "2025-05-18 11:02:59.756009: Current learning rate: 0.00735\n",
      "2025-05-18 11:04:49.162013: train_loss -0.9602\n",
      "2025-05-18 11:04:49.162306: val_loss -0.9509\n",
      "2025-05-18 11:04:49.162413: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 11:04:49.162531: Epoch time: 109.41 s\n",
      "2025-05-18 11:04:49.683005: \n",
      "2025-05-18 11:04:49.683134: Epoch 291\n",
      "2025-05-18 11:04:49.683199: Current learning rate: 0.00734\n",
      "2025-05-18 11:06:39.048942: train_loss -0.9574\n",
      "2025-05-18 11:06:39.049226: val_loss -0.9417\n",
      "2025-05-18 11:06:39.049264: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 11:06:39.049299: Epoch time: 109.37 s\n",
      "2025-05-18 11:06:39.570789: \n",
      "2025-05-18 11:06:39.571125: Epoch 292\n",
      "2025-05-18 11:06:39.571200: Current learning rate: 0.00733\n",
      "2025-05-18 11:08:28.989159: train_loss -0.9581\n",
      "2025-05-18 11:08:28.989332: val_loss -0.9309\n",
      "2025-05-18 11:08:28.989368: Pseudo dice [np.float32(0.9713)]\n",
      "2025-05-18 11:08:28.989401: Epoch time: 109.42 s\n",
      "2025-05-18 11:08:29.514175: \n",
      "2025-05-18 11:08:29.514337: Epoch 293\n",
      "2025-05-18 11:08:29.514410: Current learning rate: 0.00732\n",
      "2025-05-18 11:10:18.835935: train_loss -0.9583\n",
      "2025-05-18 11:10:18.836130: val_loss -0.9448\n",
      "2025-05-18 11:10:18.836191: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-18 11:10:18.836234: Epoch time: 109.32 s\n",
      "2025-05-18 11:10:19.359445: \n",
      "2025-05-18 11:10:19.359653: Epoch 294\n",
      "2025-05-18 11:10:19.359723: Current learning rate: 0.00731\n",
      "2025-05-18 11:12:08.720230: train_loss -0.9602\n",
      "2025-05-18 11:12:08.720370: val_loss -0.9329\n",
      "2025-05-18 11:12:08.720407: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-18 11:12:08.720440: Epoch time: 109.36 s\n",
      "2025-05-18 11:12:09.240492: \n",
      "2025-05-18 11:12:09.240735: Epoch 295\n",
      "2025-05-18 11:12:09.241030: Current learning rate: 0.0073\n",
      "2025-05-18 11:13:58.616235: train_loss -0.9575\n",
      "2025-05-18 11:13:58.616361: val_loss -0.9451\n",
      "2025-05-18 11:13:58.616392: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 11:13:58.616423: Epoch time: 109.38 s\n",
      "2025-05-18 11:13:59.138238: \n",
      "2025-05-18 11:13:59.138361: Epoch 296\n",
      "2025-05-18 11:13:59.138429: Current learning rate: 0.00729\n",
      "2025-05-18 11:15:48.481162: train_loss -0.9584\n",
      "2025-05-18 11:15:48.481359: val_loss -0.9458\n",
      "2025-05-18 11:15:48.481394: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 11:15:48.481426: Epoch time: 109.34 s\n",
      "2025-05-18 11:15:48.999563: \n",
      "2025-05-18 11:15:48.999779: Epoch 297\n",
      "2025-05-18 11:15:48.999861: Current learning rate: 0.00728\n",
      "2025-05-18 11:17:38.331200: train_loss -0.9581\n",
      "2025-05-18 11:17:38.331339: val_loss -0.9404\n",
      "2025-05-18 11:17:38.331465: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-18 11:17:38.331547: Epoch time: 109.33 s\n",
      "2025-05-18 11:17:38.846292: \n",
      "2025-05-18 11:17:38.846375: Epoch 298\n",
      "2025-05-18 11:17:38.846434: Current learning rate: 0.00727\n",
      "2025-05-18 11:19:28.265868: train_loss -0.958\n",
      "2025-05-18 11:19:28.265991: val_loss -0.9427\n",
      "2025-05-18 11:19:28.266025: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-18 11:19:28.266069: Epoch time: 109.42 s\n",
      "2025-05-18 11:19:28.797182: \n",
      "2025-05-18 11:19:28.797326: Epoch 299\n",
      "2025-05-18 11:19:28.797404: Current learning rate: 0.00726\n",
      "2025-05-18 11:21:18.143376: train_loss -0.961\n",
      "2025-05-18 11:21:18.143560: val_loss -0.9428\n",
      "2025-05-18 11:21:18.143605: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 11:21:18.143641: Epoch time: 109.35 s\n",
      "2025-05-18 11:21:18.893148: \n",
      "2025-05-18 11:21:18.893235: Epoch 300\n",
      "2025-05-18 11:21:18.893298: Current learning rate: 0.00725\n",
      "2025-05-18 11:23:08.274528: train_loss -0.959\n",
      "2025-05-18 11:23:08.274702: val_loss -0.9368\n",
      "2025-05-18 11:23:08.274743: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-18 11:23:08.274784: Epoch time: 109.38 s\n",
      "2025-05-18 11:23:08.797211: \n",
      "2025-05-18 11:23:08.797291: Epoch 301\n",
      "2025-05-18 11:23:08.797354: Current learning rate: 0.00724\n",
      "2025-05-18 11:24:58.137141: train_loss -0.9604\n",
      "2025-05-18 11:24:58.137263: val_loss -0.9367\n",
      "2025-05-18 11:24:58.137298: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-18 11:24:58.137339: Epoch time: 109.34 s\n",
      "2025-05-18 11:24:58.869605: \n",
      "2025-05-18 11:24:58.869708: Epoch 302\n",
      "2025-05-18 11:24:58.869774: Current learning rate: 0.00724\n",
      "2025-05-18 11:26:48.176838: train_loss -0.9597\n",
      "2025-05-18 11:26:48.177047: val_loss -0.946\n",
      "2025-05-18 11:26:48.177173: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 11:26:48.177298: Epoch time: 109.31 s\n",
      "2025-05-18 11:26:48.698954: \n",
      "2025-05-18 11:26:48.699047: Epoch 303\n",
      "2025-05-18 11:26:48.699111: Current learning rate: 0.00723\n",
      "2025-05-18 11:28:38.194079: train_loss -0.9618\n",
      "2025-05-18 11:28:38.194242: val_loss -0.9465\n",
      "[np.float32(0.9783)]194275: Pseudo dice \n",
      "2025-05-18 11:28:38.194359: Epoch time: 109.5 s\n",
      "2025-05-18 11:28:38.725578: \n",
      "2025-05-18 11:28:38.725948: Epoch 304\n",
      "2025-05-18 11:28:38.726044: Current learning rate: 0.00722\n",
      "2025-05-18 11:30:28.093550: train_loss -0.9599\n",
      "2025-05-18 11:30:28.093853: val_loss -0.9493\n",
      "2025-05-18 11:30:28.093935: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 11:30:28.093976: Epoch time: 109.37 s\n",
      "2025-05-18 11:30:28.613180: \n",
      "2025-05-18 11:30:28.613274: Epoch 305\n",
      "2025-05-18 11:30:28.613336: Current learning rate: 0.00721\n",
      "2025-05-18 11:32:17.961680: train_loss -0.9582\n",
      "2025-05-18 11:32:17.961809: val_loss -0.9486\n",
      "2025-05-18 11:32:17.961838: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 11:32:17.961872: Epoch time: 109.35 s\n",
      "2025-05-18 11:32:18.478163: \n",
      "2025-05-18 11:32:18.478307: Epoch 306\n",
      "2025-05-18 11:32:18.478388: Current learning rate: 0.0072\n",
      "2025-05-18 11:34:07.874856: train_loss -0.9583\n",
      "2025-05-18 11:34:07.874977: val_loss -0.9406\n",
      "2025-05-18 11:34:07.875012: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 11:34:07.875048: Epoch time: 109.4 s\n",
      "2025-05-18 11:34:08.398811: \n",
      "2025-05-18 11:34:08.398962: Epoch 307\n",
      "2025-05-18 11:34:08.399032: Current learning rate: 0.00719\n",
      "2025-05-18 11:35:57.771007: train_loss -0.962\n",
      "2025-05-18 11:35:57.771323: val_loss -0.9454\n",
      "2025-05-18 11:35:57.771487: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-18 11:35:57.771750: Epoch time: 109.37 s\n",
      "2025-05-18 11:35:58.295862: \n",
      "2025-05-18 11:35:58.296068: Epoch 308\n",
      "2025-05-18 11:35:58.296167: Current learning rate: 0.00718\n",
      "2025-05-18 11:37:47.791218: train_loss -0.9625\n",
      "2025-05-18 11:37:47.791403: val_loss -0.944\n",
      "2025-05-18 11:37:47.791518: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 11:37:47.791600: Epoch time: 109.5 s\n",
      "2025-05-18 11:37:48.308352: \n",
      "2025-05-18 11:37:48.308514: Epoch 309\n",
      "2025-05-18 11:37:48.308586: Current learning rate: 0.00717\n",
      "2025-05-18 11:39:37.658695: train_loss -0.9599\n",
      "2025-05-18 11:39:37.658902: val_loss -0.9418\n",
      "2025-05-18 11:39:37.658970: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-18 11:39:37.659154: Epoch time: 109.35 s\n",
      "2025-05-18 11:39:38.179579: \n",
      "2025-05-18 11:39:38.179782: Epoch 310\n",
      "2025-05-18 11:39:38.179875: Current learning rate: 0.00716\n",
      "2025-05-18 11:41:27.559033: train_loss -0.9579\n",
      "2025-05-18 11:41:27.559332: val_loss -0.943\n",
      "2025-05-18 11:41:27.559375: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 11:41:27.559408: Epoch time: 109.38 s\n",
      "2025-05-18 11:41:28.082148: \n",
      "2025-05-18 11:41:28.082358: Epoch 311\n",
      "2025-05-18 11:41:28.082522: Current learning rate: 0.00715\n",
      "2025-05-18 11:43:17.509085: train_loss -0.9608\n",
      "2025-05-18 11:43:17.509212: val_loss -0.9395\n",
      "2025-05-18 11:43:17.509354: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 11:43:17.509440: Epoch time: 109.43 s\n",
      "2025-05-18 11:43:18.028877: \n",
      "2025-05-18 11:43:18.028970: Epoch 312\n",
      "2025-05-18 11:43:18.029032: Current learning rate: 0.00714\n",
      "2025-05-18 11:45:07.315121: train_loss -0.9621\n",
      "2025-05-18 11:45:07.315320: val_loss -0.9487\n",
      "2025-05-18 11:45:07.315354: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 11:45:07.315388: Epoch time: 109.29 s\n",
      "2025-05-18 11:45:07.838435: \n",
      "2025-05-18 11:45:07.838647: Epoch 313\n",
      "2025-05-18 11:45:07.838713: Current learning rate: 0.00713\n",
      "2025-05-18 11:46:57.183700: train_loss -0.9603\n",
      "2025-05-18 11:46:57.183835: val_loss -0.9374\n",
      "2025-05-18 11:46:57.183867: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-18 11:46:57.183901: Epoch time: 109.35 s\n",
      "2025-05-18 11:46:57.706308: \n",
      "2025-05-18 11:46:57.706600: Epoch 314\n",
      "2025-05-18 11:46:57.706715: Current learning rate: 0.00712\n",
      "2025-05-18 11:48:47.062159: train_loss -0.9599\n",
      "2025-05-18 11:48:47.062276: val_loss -0.9448\n",
      "2025-05-18 11:48:47.062307: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 11:48:47.062339: Epoch time: 109.36 s\n",
      "2025-05-18 11:48:47.788758: \n",
      "2025-05-18 11:48:47.788902: Epoch 315\n",
      "2025-05-18 11:48:47.788987: Current learning rate: 0.00711\n",
      "2025-05-18 11:50:37.092865: train_loss -0.9615\n",
      "2025-05-18 11:50:37.092987: val_loss -0.9405\n",
      "2025-05-18 11:50:37.093020: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-18 11:50:37.093056: Epoch time: 109.3 s\n",
      "2025-05-18 11:50:37.621429: \n",
      "2025-05-18 11:50:37.621529: Epoch 316\n",
      "2025-05-18 11:50:37.621595: Current learning rate: 0.0071\n",
      "2025-05-18 11:52:27.003638: train_loss -0.9611\n",
      "2025-05-18 11:52:27.003904: val_loss -0.9304\n",
      "2025-05-18 11:52:27.004072: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-18 11:52:27.004153: Epoch time: 109.38 s\n",
      "2025-05-18 11:52:27.525230: \n",
      "2025-05-18 11:52:27.525397: Epoch 317\n",
      "2025-05-18 11:52:27.525501: Current learning rate: 0.0071\n",
      "2025-05-18 11:54:16.873325: train_loss -0.9588\n",
      "2025-05-18 11:54:16.873447: val_loss -0.9428\n",
      "2025-05-18 11:54:16.873481: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-18 11:54:16.873514: Epoch time: 109.35 s\n",
      "2025-05-18 11:54:17.395007: \n",
      "2025-05-18 11:54:17.395163: Epoch 318\n",
      "2025-05-18 11:54:17.395230: Current learning rate: 0.00709\n",
      "2025-05-18 11:56:06.759886: train_loss -0.9611\n",
      "2025-05-18 11:56:06.760014: val_loss -0.9339\n",
      "2025-05-18 11:56:06.760072: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-18 11:56:06.760111: Epoch time: 109.37 s\n",
      "2025-05-18 11:56:07.289872: \n",
      "2025-05-18 11:56:07.289973: Epoch 319\n",
      "2025-05-18 11:56:07.290037: Current learning rate: 0.00708\n",
      "2025-05-18 11:57:56.610506: train_loss -0.9597\n",
      "2025-05-18 11:57:56.610629: val_loss -0.953\n",
      "2025-05-18 11:57:56.610662: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-18 11:57:56.610694: Epoch time: 109.32 s\n",
      "2025-05-18 11:57:57.130004: \n",
      "2025-05-18 11:57:57.130107: Epoch 320\n",
      "2025-05-18 11:57:57.130172: Current learning rate: 0.00707\n",
      "2025-05-18 11:59:46.492020: train_loss -0.9621\n",
      "2025-05-18 11:59:46.492148: val_loss -0.9485\n",
      "2025-05-18 11:59:46.492182: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 11:59:46.492216: Epoch time: 109.36 s\n",
      "2025-05-18 11:59:47.025135: \n",
      "2025-05-18 11:59:47.025356: Epoch 321\n",
      "2025-05-18 11:59:47.025439: Current learning rate: 0.00706\n",
      "2025-05-18 12:01:36.407925: train_loss -0.9598\n",
      "2025-05-18 12:01:36.408050: val_loss -0.9384\n",
      "2025-05-18 12:01:36.408082: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-18 12:01:36.408114: Epoch time: 109.38 s\n",
      "2025-05-18 12:01:36.936555: \n",
      "2025-05-18 12:01:36.936650: Epoch 322\n",
      "2025-05-18 12:01:36.936724: Current learning rate: 0.00705\n",
      "2025-05-18 12:03:26.363163: train_loss -0.9602\n",
      "2025-05-18 12:03:26.363283: val_loss -0.9387\n",
      "2025-05-18 12:03:26.363315: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-18 12:03:26.363350: Epoch time: 109.43 s\n",
      "2025-05-18 12:03:26.894690: \n",
      "2025-05-18 12:03:26.894857: Epoch 323\n",
      "2025-05-18 12:03:26.894968: Current learning rate: 0.00704\n",
      "2025-05-18 12:05:16.250356: train_loss -0.9614\n",
      "2025-05-18 12:05:16.250511: val_loss -0.9455\n",
      "2025-05-18 12:05:16.250550: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 12:05:16.250616: Epoch time: 109.36 s\n",
      "2025-05-18 12:05:16.780195: \n",
      "2025-05-18 12:05:16.780282: Epoch 324\n",
      "2025-05-18 12:05:16.780351: Current learning rate: 0.00703\n",
      "2025-05-18 12:07:06.126224: train_loss -0.9591\n",
      "2025-05-18 12:07:06.126338: val_loss -0.9366\n",
      "2025-05-18 12:07:06.126378: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-18 12:07:06.126426: Epoch time: 109.35 s\n",
      "2025-05-18 12:07:06.644527: \n",
      "2025-05-18 12:07:06.644676: Epoch 325\n",
      "2025-05-18 12:07:06.644744: Current learning rate: 0.00702\n",
      "2025-05-18 12:08:55.911783: train_loss -0.9603\n",
      "2025-05-18 12:08:55.911911: val_loss -0.9395\n",
      "2025-05-18 12:08:55.911942: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 12:08:55.912023: Epoch time: 109.27 s\n",
      "2025-05-18 12:08:56.437837: \n",
      "2025-05-18 12:08:56.437919: Epoch 326\n",
      "2025-05-18 12:08:56.437985: Current learning rate: 0.00701\n",
      "2025-05-18 12:10:45.904208: train_loss -0.9592\n",
      "2025-05-18 12:10:45.904337: val_loss -0.9416\n",
      "2025-05-18 12:10:45.904368: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 12:10:45.904401: Epoch time: 109.47 s\n",
      "2025-05-18 12:10:46.630740: \n",
      "2025-05-18 12:10:46.630837: Epoch 327\n",
      "2025-05-18 12:10:46.630906: Current learning rate: 0.007\n",
      "2025-05-18 12:12:35.997435: train_loss -0.9572\n",
      "2025-05-18 12:12:35.997704: val_loss -0.9314\n",
      "2025-05-18 12:12:35.997770: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-18 12:12:35.997812: Epoch time: 109.37 s\n",
      "2025-05-18 12:12:36.513914: \n",
      "2025-05-18 12:12:36.514346: Epoch 328\n",
      "2025-05-18 12:12:36.514560: Current learning rate: 0.00699\n",
      "2025-05-18 12:14:25.822545: train_loss -0.9593\n",
      "2025-05-18 12:14:25.822718: val_loss -0.9404\n",
      "[np.float32(0.9732)]822752: Pseudo dice \n",
      "2025-05-18 12:14:25.822855: Epoch time: 109.31 s\n",
      "2025-05-18 12:14:26.351846: \n",
      "2025-05-18 12:14:26.351939: Epoch 329\n",
      "2025-05-18 12:14:26.352003: Current learning rate: 0.00698\n",
      "2025-05-18 12:16:15.678616: train_loss -0.9619\n",
      "2025-05-18 12:16:15.678780: val_loss -0.9504\n",
      "2025-05-18 12:16:15.678811: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-18 12:16:15.678844: Epoch time: 109.33 s\n",
      "2025-05-18 12:16:16.206904: \n",
      "2025-05-18 12:16:16.207086: Epoch 330\n",
      "2025-05-18 12:16:16.207155: Current learning rate: 0.00697\n",
      "2025-05-18 12:18:05.554647: train_loss -0.9595\n",
      "2025-05-18 12:18:05.554772: val_loss -0.9456\n",
      "2025-05-18 12:18:05.554807: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 12:18:05.554840: Epoch time: 109.35 s\n",
      "2025-05-18 12:18:06.074318: \n",
      "2025-05-18 12:18:06.074520: Epoch 331\n",
      "2025-05-18 12:18:06.074590: Current learning rate: 0.00696\n",
      "2025-05-18 12:19:55.450258: train_loss -0.9602\n",
      "2025-05-18 12:19:55.450396: val_loss -0.9385\n",
      "2025-05-18 12:19:55.450432: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 12:19:55.450555: Epoch time: 109.38 s\n",
      "2025-05-18 12:19:55.977210: \n",
      "2025-05-18 12:19:55.977429: Epoch 332\n",
      "2025-05-18 12:19:55.977500: Current learning rate: 0.00696\n",
      "2025-05-18 12:21:45.290230: train_loss -0.961\n",
      "2025-05-18 12:21:45.290359: val_loss -0.9564\n",
      "2025-05-18 12:21:45.290394: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-18 12:21:45.290428: Epoch time: 109.31 s\n",
      "2025-05-18 12:21:45.811401: \n",
      "2025-05-18 12:21:45.811503: Epoch 333\n",
      "2025-05-18 12:21:45.811567: Current learning rate: 0.00695\n",
      "2025-05-18 12:23:35.197362: train_loss -0.9614\n",
      "2025-05-18 12:23:35.197486: val_loss -0.951\n",
      "2025-05-18 12:23:35.197521: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-18 12:23:35.197556: Epoch time: 109.39 s\n",
      "2025-05-18 12:23:35.729028: \n",
      "2025-05-18 12:23:35.729119: Epoch 334\n",
      "2025-05-18 12:23:35.729183: Current learning rate: 0.00694\n",
      "2025-05-18 12:25:25.093893: train_loss -0.9589\n",
      "2025-05-18 12:25:25.094014: val_loss -0.9369\n",
      "2025-05-18 12:25:25.094046: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-18 12:25:25.094079: Epoch time: 109.37 s\n",
      "2025-05-18 12:25:25.621454: \n",
      "2025-05-18 12:25:25.621618: Epoch 335\n",
      "2025-05-18 12:25:25.621741: Current learning rate: 0.00693\n",
      "2025-05-18 12:27:15.081480: train_loss -0.9553\n",
      "2025-05-18 12:27:15.081600: val_loss -0.9394\n",
      "2025-05-18 12:27:15.081636: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-18 12:27:15.081669: Epoch time: 109.46 s\n",
      "2025-05-18 12:27:15.612826: \n",
      "2025-05-18 12:27:15.613125: Epoch 336\n",
      "2025-05-18 12:27:15.613302: Current learning rate: 0.00692\n",
      "2025-05-18 12:29:05.042031: train_loss -0.946\n",
      "2025-05-18 12:29:05.042150: val_loss -0.9224\n",
      "2025-05-18 12:29:05.042182: Pseudo dice [np.float32(0.9682)]\n",
      "2025-05-18 12:29:05.042224: Epoch time: 109.43 s\n",
      "2025-05-18 12:29:05.564998: \n",
      "2025-05-18 12:29:05.565085: Epoch 337\n",
      "2025-05-18 12:29:05.565217: Current learning rate: 0.00691\n",
      "2025-05-18 12:30:54.913833: train_loss -0.9481\n",
      "2025-05-18 12:30:54.913959: val_loss -0.9261\n",
      "2025-05-18 12:30:54.913992: Pseudo dice [np.float32(0.9699)]\n",
      "2025-05-18 12:30:54.914024: Epoch time: 109.35 s\n",
      "2025-05-18 12:30:55.439622: \n",
      "2025-05-18 12:30:55.439719: Epoch 338\n",
      "2025-05-18 12:30:55.439785: Current learning rate: 0.0069\n",
      "2025-05-18 12:32:44.744018: train_loss -0.9434\n",
      "2025-05-18 12:32:44.744143: val_loss -0.9358\n",
      "2025-05-18 12:32:44.744177: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-18 12:32:44.744210: Epoch time: 109.3 s\n",
      "2025-05-18 12:32:45.479845: \n",
      "2025-05-18 12:32:45.480049: Epoch 339\n",
      "2025-05-18 12:32:45.480137: Current learning rate: 0.00689\n",
      "2025-05-18 12:34:34.802749: train_loss -0.9466\n",
      "2025-05-18 12:34:34.802871: val_loss -0.9381\n",
      "2025-05-18 12:34:34.802907: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-18 12:34:34.802939: Epoch time: 109.32 s\n",
      "2025-05-18 12:34:35.332881: \n",
      "2025-05-18 12:34:35.333029: Epoch 340\n",
      "2025-05-18 12:34:35.333094: Current learning rate: 0.00688\n",
      "2025-05-18 12:36:24.773102: train_loss -0.9501\n",
      "2025-05-18 12:36:24.773398: val_loss -0.9405\n",
      "2025-05-18 12:36:24.773553: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 12:36:24.773649: Epoch time: 109.44 s\n",
      "2025-05-18 12:36:25.295549: \n",
      "2025-05-18 12:36:25.295775: Epoch 341\n",
      "2025-05-18 12:36:25.295847: Current learning rate: 0.00687\n",
      "2025-05-18 12:38:14.618405: train_loss -0.9191\n",
      "2025-05-18 12:38:14.618536: val_loss -0.9072\n",
      "2025-05-18 12:38:14.618567: Pseudo dice [np.float32(0.9616)]\n",
      "2025-05-18 12:38:14.618601: Epoch time: 109.32 s\n",
      "2025-05-18 12:38:15.147383: \n",
      "2025-05-18 12:38:15.147479: Epoch 342\n",
      "2025-05-18 12:38:15.147543: Current learning rate: 0.00686\n",
      "2025-05-18 12:40:04.499711: train_loss -0.8785\n",
      "2025-05-18 12:40:04.499831: val_loss -0.8833\n",
      "2025-05-18 12:40:04.499862: Pseudo dice [np.float32(0.9541)]\n",
      "2025-05-18 12:40:04.499894: Epoch time: 109.35 s\n",
      "2025-05-18 12:40:05.035748: \n",
      "2025-05-18 12:40:05.035865: Epoch 343\n",
      "2025-05-18 12:40:05.035987: Current learning rate: 0.00685\n",
      "2025-05-18 12:41:54.408905: train_loss -0.9068\n",
      "2025-05-18 12:41:54.409068: val_loss -0.7946\n",
      "2025-05-18 12:41:54.409099: Pseudo dice [np.float32(0.9233)]\n",
      "2025-05-18 12:41:54.409131: Epoch time: 109.37 s\n",
      "2025-05-18 12:41:54.932588: \n",
      "2025-05-18 12:41:54.932691: Epoch 344\n",
      "2025-05-18 12:41:54.932759: Current learning rate: 0.00684\n",
      "2025-05-18 12:43:44.267958: train_loss -0.9048\n",
      "2025-05-18 12:43:44.268125: val_loss -0.9269\n",
      "2025-05-18 12:43:44.268160: Pseudo dice [np.float32(0.9702)]\n",
      "2025-05-18 12:43:44.268280: Epoch time: 109.34 s\n",
      "2025-05-18 12:43:44.803368: \n",
      "2025-05-18 12:43:44.803514: Epoch 345\n",
      "2025-05-18 12:43:44.803579: Current learning rate: 0.00683\n",
      "2025-05-18 12:45:34.108429: train_loss -0.9337\n",
      "2025-05-18 12:45:34.108609: val_loss -0.9272\n",
      "2025-05-18 12:45:34.108644: Pseudo dice [np.float32(0.9705)]\n",
      "2025-05-18 12:45:34.108678: Epoch time: 109.31 s\n",
      "2025-05-18 12:45:34.642077: \n",
      "2025-05-18 12:45:34.642172: Epoch 346\n",
      "2025-05-18 12:45:34.642234: Current learning rate: 0.00682\n",
      "2025-05-18 12:47:24.128684: train_loss -0.9439\n",
      "2025-05-18 12:47:24.128893: val_loss -0.9372\n",
      "2025-05-18 12:47:24.128927: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-18 12:47:24.128960: Epoch time: 109.49 s\n",
      "2025-05-18 12:47:24.654534: \n",
      "2025-05-18 12:47:24.654621: Epoch 347\n",
      "2025-05-18 12:47:24.654689: Current learning rate: 0.00681\n",
      "2025-05-18 12:49:13.980438: train_loss -0.9479\n",
      "2025-05-18 12:49:13.980559: val_loss -0.9372\n",
      "2025-05-18 12:49:13.980592: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-18 12:49:13.980624: Epoch time: 109.33 s\n",
      "2025-05-18 12:49:14.508989: \n",
      "2025-05-18 12:49:14.509078: Epoch 348\n",
      "2025-05-18 12:49:14.509136: Current learning rate: 0.0068\n",
      "2025-05-18 12:51:03.812607: train_loss -0.9526\n",
      "2025-05-18 12:51:03.812770: val_loss -0.9313\n",
      "2025-05-18 12:51:03.812987: Pseudo dice [np.float32(0.9706)]\n",
      "2025-05-18 12:51:03.813065: Epoch time: 109.3 s\n",
      "2025-05-18 12:51:04.340239: \n",
      "2025-05-18 12:51:04.340581: Epoch 349\n",
      "2025-05-18 12:51:04.340652: Current learning rate: 0.0068\n",
      "2025-05-18 12:52:53.690568: train_loss -0.9503\n",
      "2025-05-18 12:52:53.690691: val_loss -0.9109\n",
      "2025-05-18 12:52:53.690725: Pseudo dice [np.float32(0.9654)]\n",
      "2025-05-18 12:52:53.690760: Epoch time: 109.35 s\n",
      "2025-05-18 12:52:54.427609: \n",
      "2025-05-18 12:52:54.427767: Epoch 350\n",
      "2025-05-18 12:52:54.427842: Current learning rate: 0.00679\n",
      "2025-05-18 12:54:45.189325: train_loss -0.9515\n",
      "2025-05-18 12:54:45.189447: val_loss -0.9354\n",
      "2025-05-18 12:54:45.189481: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-18 12:54:45.189514: Epoch time: 110.76 s\n",
      "2025-05-18 12:54:45.715471: \n",
      "2025-05-18 12:54:45.715551: Epoch 351\n",
      "2025-05-18 12:54:45.715615: Current learning rate: 0.00678\n",
      "2025-05-18 12:56:35.212136: train_loss -0.9552\n",
      "2025-05-18 12:56:35.212263: val_loss -0.9443\n",
      "2025-05-18 12:56:35.212297: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-18 12:56:35.212330: Epoch time: 109.5 s\n",
      "2025-05-18 12:56:35.950001: \n",
      "2025-05-18 12:56:35.950272: Epoch 352\n",
      "2025-05-18 12:56:35.950413: Current learning rate: 0.00677\n",
      "2025-05-18 12:58:25.264523: train_loss -0.9453\n",
      "2025-05-18 12:58:25.264647: val_loss -0.9269\n",
      "2025-05-18 12:58:25.264677: Pseudo dice [np.float32(0.9701)]\n",
      "2025-05-18 12:58:25.264721: Epoch time: 109.32 s\n",
      "2025-05-18 12:58:25.794851: \n",
      "2025-05-18 12:58:25.794949: Epoch 353\n",
      "2025-05-18 12:58:25.795011: Current learning rate: 0.00676\n",
      "2025-05-18 13:00:15.112200: train_loss -0.9419\n",
      "2025-05-18 13:00:15.112632: val_loss -0.9293\n",
      "[np.float32(0.9711)]112672: Pseudo dice \n",
      "2025-05-18 13:00:15.112823: Epoch time: 109.32 s\n",
      "2025-05-18 13:00:15.644181: \n",
      "2025-05-18 13:00:15.644279: Epoch 354\n",
      "2025-05-18 13:00:15.644343: Current learning rate: 0.00675\n",
      "2025-05-18 13:02:05.034331: train_loss -0.9465\n",
      "2025-05-18 13:02:05.034489: val_loss -0.943\n",
      "2025-05-18 13:02:05.034649: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 13:02:05.034725: Epoch time: 109.39 s\n",
      "2025-05-18 13:02:05.563547: \n",
      "2025-05-18 13:02:05.563795: Epoch 355\n",
      "2025-05-18 13:02:05.563959: Current learning rate: 0.00674\n",
      "2025-05-18 13:03:54.866841: train_loss -0.9437\n",
      "2025-05-18 13:03:54.866967: val_loss -0.9283\n",
      "2025-05-18 13:03:54.867000: Pseudo dice [np.float32(0.9723)]\n",
      "2025-05-18 13:03:54.867047: Epoch time: 109.3 s\n",
      "2025-05-18 13:03:55.398139: \n",
      "2025-05-18 13:03:55.398247: Epoch 356\n",
      "2025-05-18 13:03:55.398322: Current learning rate: 0.00673\n",
      "2025-05-18 13:05:44.780601: train_loss -0.9424\n",
      "2025-05-18 13:05:44.780955: val_loss -0.929\n",
      "2025-05-18 13:05:44.780993: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-18 13:05:44.781026: Epoch time: 109.38 s\n",
      "2025-05-18 13:05:45.307290: \n",
      "2025-05-18 13:05:45.307384: Epoch 357\n",
      "2025-05-18 13:05:45.307501: Current learning rate: 0.00672\n",
      "2025-05-18 13:07:34.622809: train_loss -0.9499\n",
      "2025-05-18 13:07:34.622955: val_loss -0.9421\n",
      "2025-05-18 13:07:34.622991: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 13:07:34.623023: Epoch time: 109.32 s\n",
      "2025-05-18 13:07:35.156589: \n",
      "2025-05-18 13:07:35.156749: Epoch 358\n",
      "2025-05-18 13:07:35.156826: Current learning rate: 0.00671\n",
      "2025-05-18 13:09:24.565490: train_loss -0.9537\n",
      "2025-05-18 13:09:24.565614: val_loss -0.935\n",
      "2025-05-18 13:09:24.565647: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-18 13:09:24.565683: Epoch time: 109.41 s\n",
      "2025-05-18 13:09:25.096525: \n",
      "2025-05-18 13:09:25.096746: Epoch 359\n",
      "2025-05-18 13:09:25.096824: Current learning rate: 0.0067\n",
      "2025-05-18 13:11:14.438487: train_loss -0.9551\n",
      "2025-05-18 13:11:14.438679: val_loss -0.943\n",
      "2025-05-18 13:11:14.438800: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 13:11:14.438842: Epoch time: 109.34 s\n",
      "2025-05-18 13:11:14.963700: \n",
      "2025-05-18 13:11:14.963936: Epoch 360\n",
      "2025-05-18 13:11:14.964049: Current learning rate: 0.00669\n",
      "2025-05-18 13:13:04.273344: train_loss -0.9566\n",
      "2025-05-18 13:13:04.273468: val_loss -0.9457\n",
      "2025-05-18 13:13:04.273502: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 13:13:04.273536: Epoch time: 109.31 s\n",
      "2025-05-18 13:13:04.801995: \n",
      "2025-05-18 13:13:04.802175: Epoch 361\n",
      "2025-05-18 13:13:04.802246: Current learning rate: 0.00668\n",
      "2025-05-18 13:14:54.080450: train_loss -0.953\n",
      "2025-05-18 13:14:54.080584: val_loss -0.9069\n",
      "2025-05-18 13:14:54.080617: Pseudo dice [np.float32(0.9626)]\n",
      "2025-05-18 13:14:54.080651: Epoch time: 109.28 s\n",
      "2025-05-18 13:14:54.607739: \n",
      "2025-05-18 13:14:54.607872: Epoch 362\n",
      "2025-05-18 13:14:54.608009: Current learning rate: 0.00667\n",
      "2025-05-18 13:16:44.057546: train_loss -0.9447\n",
      "2025-05-18 13:16:44.057666: val_loss -0.9345\n",
      "2025-05-18 13:16:44.057698: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-18 13:16:44.057733: Epoch time: 109.45 s\n",
      "2025-05-18 13:16:44.589766: \n",
      "2025-05-18 13:16:44.589847: Epoch 363\n",
      "2025-05-18 13:16:44.589909: Current learning rate: 0.00666\n",
      "2025-05-18 13:18:33.873825: train_loss -0.9414\n",
      "2025-05-18 13:18:33.874023: val_loss -0.9243\n",
      "2025-05-18 13:18:33.874257: Pseudo dice [np.float32(0.9693)]\n",
      "2025-05-18 13:18:33.874348: Epoch time: 109.28 s\n",
      "2025-05-18 13:18:34.612407: \n",
      "2025-05-18 13:18:34.612506: Epoch 364\n",
      "2025-05-18 13:18:34.612571: Current learning rate: 0.00665\n",
      "2025-05-18 13:20:23.944199: train_loss -0.9465\n",
      "2025-05-18 13:20:23.944330: val_loss -0.9311\n",
      "2025-05-18 13:20:23.944364: Pseudo dice [np.float32(0.971)]\n",
      "2025-05-18 13:20:23.944396: Epoch time: 109.33 s\n",
      "2025-05-18 13:20:24.476780: \n",
      "2025-05-18 13:20:24.476873: Epoch 365\n",
      "2025-05-18 13:20:24.476937: Current learning rate: 0.00665\n",
      "2025-05-18 13:22:13.797628: train_loss -0.9477\n",
      "2025-05-18 13:22:13.797750: val_loss -0.9252\n",
      "2025-05-18 13:22:13.797781: Pseudo dice [np.float32(0.9671)]\n",
      "2025-05-18 13:22:13.797816: Epoch time: 109.32 s\n",
      "2025-05-18 13:22:14.323516: \n",
      "2025-05-18 13:22:14.323665: Epoch 366\n",
      "2025-05-18 13:22:14.323734: Current learning rate: 0.00664\n",
      "2025-05-18 13:24:03.662983: train_loss -0.924\n",
      "2025-05-18 13:24:03.663142: val_loss -0.8832\n",
      "2025-05-18 13:24:03.663174: Pseudo dice [np.float32(0.958)]\n",
      "2025-05-18 13:24:03.663206: Epoch time: 109.34 s\n",
      "2025-05-18 13:24:04.197104: \n",
      "2025-05-18 13:24:04.197262: Epoch 367\n",
      "2025-05-18 13:24:04.197337: Current learning rate: 0.00663\n",
      "2025-05-18 13:25:53.532997: train_loss -0.9227\n",
      "2025-05-18 13:25:53.533157: val_loss -0.8394\n",
      "2025-05-18 13:25:53.533188: Pseudo dice [np.float32(0.9412)]\n",
      "2025-05-18 13:25:53.533220: Epoch time: 109.34 s\n",
      "2025-05-18 13:25:54.057250: \n",
      "2025-05-18 13:25:54.057353: Epoch 368\n",
      "2025-05-18 13:25:54.057416: Current learning rate: 0.00662\n",
      "2025-05-18 13:27:43.367247: train_loss -0.9281\n",
      "2025-05-18 13:27:43.367385: val_loss -0.9275\n",
      "2025-05-18 13:27:43.367419: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-18 13:27:43.367453: Epoch time: 109.31 s\n",
      "2025-05-18 13:27:43.898974: \n",
      "2025-05-18 13:27:43.899298: Epoch 369\n",
      "2025-05-18 13:27:43.899389: Current learning rate: 0.00661\n",
      "2025-05-18 13:29:33.264954: train_loss -0.9462\n",
      "2025-05-18 13:29:33.265099: val_loss -0.9366\n",
      "2025-05-18 13:29:33.265139: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 13:29:33.265174: Epoch time: 109.37 s\n",
      "2025-05-18 13:29:33.794008: \n",
      "2025-05-18 13:29:33.794237: Epoch 370\n",
      "2025-05-18 13:29:33.794313: Current learning rate: 0.0066\n",
      "2025-05-18 13:31:23.125903: train_loss -0.9514\n",
      "2025-05-18 13:31:23.126246: val_loss -0.9349\n",
      "2025-05-18 13:31:23.126369: Pseudo dice [np.float32(0.9716)]\n",
      "2025-05-18 13:31:23.126419: Epoch time: 109.33 s\n",
      "2025-05-18 13:31:23.648490: \n",
      "2025-05-18 13:31:23.648718: Epoch 371\n",
      "2025-05-18 13:31:23.648798: Current learning rate: 0.00659\n",
      "2025-05-18 13:33:12.967987: train_loss -0.9544\n",
      "2025-05-18 13:33:12.968194: val_loss -0.948\n",
      "2025-05-18 13:33:12.968229: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 13:33:12.968262: Epoch time: 109.32 s\n",
      "2025-05-18 13:33:13.497773: \n",
      "2025-05-18 13:33:13.497859: Epoch 372\n",
      "2025-05-18 13:33:13.497926: Current learning rate: 0.00658\n",
      "2025-05-18 13:35:02.811236: train_loss -0.9569\n",
      "2025-05-18 13:35:02.811355: val_loss -0.9496\n",
      "2025-05-18 13:35:02.811385: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-18 13:35:02.811445: Epoch time: 109.31 s\n",
      "2025-05-18 13:35:03.336130: \n",
      "2025-05-18 13:35:03.336220: Epoch 373\n",
      "2025-05-18 13:35:03.336284: Current learning rate: 0.00657\n",
      "2025-05-18 13:36:52.681963: train_loss -0.9543\n",
      "2025-05-18 13:36:52.682079: val_loss -0.9411\n",
      "2025-05-18 13:36:52.682119: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-18 13:36:52.682152: Epoch time: 109.35 s\n",
      "2025-05-18 13:36:53.215313: \n",
      "2025-05-18 13:36:53.215457: Epoch 374\n",
      "2025-05-18 13:36:53.215520: Current learning rate: 0.00656\n",
      "2025-05-18 13:38:42.484977: train_loss -0.9503\n",
      "2025-05-18 13:38:42.485169: val_loss -0.9331\n",
      "2025-05-18 13:38:42.485202: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-18 13:38:42.485247: Epoch time: 109.27 s\n",
      "2025-05-18 13:38:43.012775: \n",
      "2025-05-18 13:38:43.012957: Epoch 375\n",
      "2025-05-18 13:38:43.013035: Current learning rate: 0.00655\n",
      "2025-05-18 13:40:32.343210: train_loss -0.955\n",
      "2025-05-18 13:40:32.343383: val_loss -0.9382\n",
      "2025-05-18 13:40:32.343413: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-18 13:40:32.343446: Epoch time: 109.33 s\n",
      "2025-05-18 13:40:33.049385: \n",
      "2025-05-18 13:40:33.049515: Epoch 376\n",
      "2025-05-18 13:40:33.049587: Current learning rate: 0.00654\n",
      "2025-05-18 13:42:22.374753: train_loss -0.9556\n",
      "2025-05-18 13:42:22.374947: val_loss -0.9356\n",
      "2025-05-18 13:42:22.374984: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 13:42:22.375018: Epoch time: 109.33 s\n",
      "2025-05-18 13:42:22.905766: \n",
      "2025-05-18 13:42:22.906010: Epoch 377\n",
      "2025-05-18 13:42:22.906221: Current learning rate: 0.00653\n",
      "2025-05-18 13:44:12.214897: train_loss -0.9561\n",
      "2025-05-18 13:44:12.215087: val_loss -0.947\n",
      "2025-05-18 13:44:12.215127: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-18 13:44:12.215168: Epoch time: 109.31 s\n",
      "2025-05-18 13:44:12.749052: \n",
      "2025-05-18 13:44:12.749338: Epoch 378\n",
      "2025-05-18 13:44:12.749464: Current learning rate: 0.00652\n",
      "2025-05-18 13:46:02.037480: train_loss -0.9565\n",
      "2025-05-18 13:46:02.037598: val_loss -0.9322\n",
      "[np.float32(0.9733)]037629: Pseudo dice \n",
      "2025-05-18 13:46:02.037792: Epoch time: 109.29 s\n",
      "2025-05-18 13:46:02.570619: \n",
      "2025-05-18 13:46:02.570762: Epoch 379\n",
      "2025-05-18 13:46:02.570824: Current learning rate: 0.00651\n",
      "2025-05-18 13:47:52.032459: train_loss -0.9585\n",
      "2025-05-18 13:47:52.032701: val_loss -0.9337\n",
      "2025-05-18 13:47:52.032766: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-18 13:47:52.032805: Epoch time: 109.46 s\n",
      "2025-05-18 13:47:52.564767: \n",
      "2025-05-18 13:47:52.564867: Epoch 380\n",
      "2025-05-18 13:47:52.564955: Current learning rate: 0.0065\n",
      "2025-05-18 13:49:41.915632: train_loss -0.9571\n",
      "2025-05-18 13:49:41.915770: val_loss -0.9467\n",
      "2025-05-18 13:49:41.915808: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 13:49:41.915838: Epoch time: 109.35 s\n",
      "2025-05-18 13:49:42.443685: \n",
      "2025-05-18 13:49:42.443892: Epoch 381\n",
      "2025-05-18 13:49:42.443962: Current learning rate: 0.00649\n",
      "2025-05-18 13:51:31.729289: train_loss -0.9606\n",
      "2025-05-18 13:51:31.729409: val_loss -0.9385\n",
      "2025-05-18 13:51:31.729442: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-18 13:51:31.729474: Epoch time: 109.29 s\n",
      "2025-05-18 13:51:32.263218: \n",
      "2025-05-18 13:51:32.263603: Epoch 382\n",
      "2025-05-18 13:51:32.263748: Current learning rate: 0.00648\n",
      "2025-05-18 13:53:21.629456: train_loss -0.9516\n",
      "2025-05-18 13:53:21.629574: val_loss -0.934\n",
      "2025-05-18 13:53:21.629614: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-18 13:53:21.629653: Epoch time: 109.37 s\n",
      "2025-05-18 13:53:22.157064: \n",
      "2025-05-18 13:53:22.157160: Epoch 383\n",
      "2025-05-18 13:53:22.157227: Current learning rate: 0.00648\n",
      "2025-05-18 13:55:11.580131: train_loss -0.9507\n",
      "2025-05-18 13:55:11.580289: val_loss -0.9342\n",
      "2025-05-18 13:55:11.580335: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-18 13:55:11.580397: Epoch time: 109.42 s\n",
      "2025-05-18 13:55:12.113018: \n",
      "2025-05-18 13:55:12.113110: Epoch 384\n",
      "2025-05-18 13:55:12.113175: Current learning rate: 0.00647\n",
      "2025-05-18 13:57:01.408451: train_loss -0.9566\n",
      "2025-05-18 13:57:01.408577: val_loss -0.9454\n",
      "2025-05-18 13:57:01.408606: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 13:57:01.408639: Epoch time: 109.3 s\n",
      "2025-05-18 13:57:01.945913: \n",
      "2025-05-18 13:57:01.946224: Epoch 385\n",
      "2025-05-18 13:57:01.946462: Current learning rate: 0.00646\n",
      "2025-05-18 13:58:51.280030: train_loss -0.936\n",
      "2025-05-18 13:58:51.280154: val_loss -0.9248\n",
      "2025-05-18 13:58:51.280187: Pseudo dice [np.float32(0.9691)]\n",
      "2025-05-18 13:58:51.280228: Epoch time: 109.33 s\n",
      "2025-05-18 13:58:51.822734: \n",
      "2025-05-18 13:58:51.822883: Epoch 386\n",
      "2025-05-18 13:58:51.822947: Current learning rate: 0.00645\n",
      "2025-05-18 14:00:41.133497: train_loss -0.9484\n",
      "2025-05-18 14:00:41.133615: val_loss -0.9449\n",
      "2025-05-18 14:00:41.133691: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-18 14:00:41.133730: Epoch time: 109.31 s\n",
      "2025-05-18 14:00:41.670614: \n",
      "2025-05-18 14:00:41.670848: Epoch 387\n",
      "2025-05-18 14:00:41.670931: Current learning rate: 0.00644\n",
      "2025-05-18 14:02:31.042887: train_loss -0.9535\n",
      "2025-05-18 14:02:31.043068: val_loss -0.9375\n",
      "2025-05-18 14:02:31.043106: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-18 14:02:31.043139: Epoch time: 109.37 s\n",
      "2025-05-18 14:02:31.579522: \n",
      "2025-05-18 14:02:31.579623: Epoch 388\n",
      "2025-05-18 14:02:31.579687: Current learning rate: 0.00643\n",
      "2025-05-18 14:04:20.886032: train_loss -0.9547\n",
      "2025-05-18 14:04:20.886199: val_loss -0.9506\n",
      "2025-05-18 14:04:20.886258: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-18 14:04:20.886295: Epoch time: 109.31 s\n",
      "2025-05-18 14:04:21.424814: \n",
      "2025-05-18 14:04:21.424907: Epoch 389\n",
      "2025-05-18 14:04:21.424969: Current learning rate: 0.00642\n",
      "2025-05-18 14:06:10.765481: train_loss -0.9513\n",
      "2025-05-18 14:06:10.765598: val_loss -0.9381\n",
      "2025-05-18 14:06:10.765798: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 14:06:10.765882: Epoch time: 109.34 s\n",
      "2025-05-18 14:06:11.303001: \n",
      "2025-05-18 14:06:11.303154: Epoch 390\n",
      "2025-05-18 14:06:11.303234: Current learning rate: 0.00641\n",
      "2025-05-18 14:08:00.596912: train_loss -0.9542\n",
      "2025-05-18 14:08:00.597027: val_loss -0.9454\n",
      "2025-05-18 14:08:00.597061: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 14:08:00.597094: Epoch time: 109.29 s\n",
      "2025-05-18 14:08:01.134943: \n",
      "2025-05-18 14:08:01.135103: Epoch 391\n",
      "2025-05-18 14:08:01.135168: Current learning rate: 0.0064\n",
      "2025-05-18 14:09:50.575935: train_loss -0.9564\n",
      "2025-05-18 14:09:50.576086: val_loss -0.9403\n",
      "2025-05-18 14:09:50.576124: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-18 14:09:50.576195: Epoch time: 109.44 s\n",
      "2025-05-18 14:09:51.112039: \n",
      "2025-05-18 14:09:51.112180: Epoch 392\n",
      "2025-05-18 14:09:51.112250: Current learning rate: 0.00639\n",
      "2025-05-18 14:11:40.471904: train_loss -0.9576\n",
      "2025-05-18 14:11:40.472034: val_loss -0.9364\n",
      "2025-05-18 14:11:40.472065: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 14:11:40.472098: Epoch time: 109.36 s\n",
      "2025-05-18 14:11:41.003359: \n",
      "2025-05-18 14:11:41.003452: Epoch 393\n",
      "2025-05-18 14:11:41.003539: Current learning rate: 0.00638\n",
      "2025-05-18 14:13:30.338828: train_loss -0.9551\n",
      "2025-05-18 14:13:30.339018: val_loss -0.9329\n",
      "2025-05-18 14:13:30.339057: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-18 14:13:30.339090: Epoch time: 109.34 s\n",
      "2025-05-18 14:13:30.871714: \n",
      "2025-05-18 14:13:30.871859: Epoch 394\n",
      "2025-05-18 14:13:30.871946: Current learning rate: 0.00637\n",
      "2025-05-18 14:15:21.450593: train_loss -0.9526\n",
      "2025-05-18 14:15:21.450714: val_loss -0.9413\n",
      "2025-05-18 14:15:21.450746: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-18 14:15:21.450779: Epoch time: 110.58 s\n",
      "2025-05-18 14:15:21.998923: \n",
      "2025-05-18 14:15:21.999200: Epoch 395\n",
      "2025-05-18 14:15:21.999272: Current learning rate: 0.00636\n",
      "2025-05-18 14:17:11.463420: train_loss -0.9585\n",
      "2025-05-18 14:17:11.463559: val_loss -0.932\n",
      "2025-05-18 14:17:11.463596: Pseudo dice [np.float32(0.9714)]\n",
      "2025-05-18 14:17:11.463632: Epoch time: 109.47 s\n",
      "2025-05-18 14:17:12.003838: \n",
      "2025-05-18 14:17:12.004065: Epoch 396\n",
      "2025-05-18 14:17:12.004204: Current learning rate: 0.00635\n",
      "2025-05-18 14:19:01.464697: train_loss -0.9576\n",
      "2025-05-18 14:19:01.464836: val_loss -0.9496\n",
      "2025-05-18 14:19:01.464869: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-18 14:19:01.464903: Epoch time: 109.46 s\n",
      "2025-05-18 14:19:02.013660: \n",
      "2025-05-18 14:19:02.013803: Epoch 397\n",
      "2025-05-18 14:19:02.013931: Current learning rate: 0.00634\n",
      "2025-05-18 14:20:51.362234: train_loss -0.9581\n",
      "2025-05-18 14:20:51.362359: val_loss -0.9396\n",
      "2025-05-18 14:20:51.362391: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-18 14:20:51.362422: Epoch time: 109.35 s\n",
      "2025-05-18 14:20:51.910034: \n",
      "2025-05-18 14:20:51.910177: Epoch 398\n",
      "2025-05-18 14:20:51.910249: Current learning rate: 0.00633\n",
      "2025-05-18 14:22:41.353291: train_loss -0.9594\n",
      "2025-05-18 14:22:41.353416: val_loss -0.9405\n",
      "2025-05-18 14:22:41.353451: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 14:22:41.353484: Epoch time: 109.44 s\n",
      "2025-05-18 14:22:42.105765: \n",
      "2025-05-18 14:22:42.106152: Epoch 399\n",
      "2025-05-18 14:22:42.106267: Current learning rate: 0.00632\n",
      "2025-05-18 14:24:31.558553: train_loss -0.9572\n",
      "2025-05-18 14:24:31.558700: val_loss -0.9359\n",
      "2025-05-18 14:24:31.558735: Pseudo dice [np.float32(0.9716)]\n",
      "2025-05-18 14:24:31.558768: Epoch time: 109.45 s\n",
      "2025-05-18 14:24:32.324112: \n",
      "2025-05-18 14:24:32.324246: Epoch 400\n",
      "2025-05-18 14:24:32.324311: Current learning rate: 0.00631\n",
      "2025-05-18 14:26:21.771983: train_loss -0.9569\n",
      "2025-05-18 14:26:21.772193: val_loss -0.9455\n",
      "2025-05-18 14:26:21.772236: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 14:26:21.772274: Epoch time: 109.45 s\n",
      "2025-05-18 14:26:22.320027: \n",
      "2025-05-18 14:26:22.320118: Epoch 401\n",
      "2025-05-18 14:26:22.320185: Current learning rate: 0.0063\n",
      "2025-05-18 14:28:12.217359: train_loss -0.9575\n",
      "2025-05-18 14:28:12.217574: val_loss -0.9395\n",
      "2025-05-18 14:28:12.217610: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-18 14:28:12.217646: Epoch time: 109.9 s\n",
      "2025-05-18 14:28:12.769300: \n",
      "2025-05-18 14:28:12.769528: Epoch 402\n",
      "2025-05-18 14:28:12.769710: Current learning rate: 0.0063\n",
      "2025-05-18 14:30:02.209940: train_loss -0.957\n",
      "2025-05-18 14:30:02.210203: val_loss -0.9492\n",
      "2025-05-18 14:30:02.210318: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-18 14:30:02.210383: Epoch time: 109.44 s\n",
      "2025-05-18 14:30:02.758649: \n",
      "2025-05-18 14:30:02.758752: Epoch 403\n",
      "2025-05-18 14:30:02.758815: Current learning rate: 0.00629\n",
      "2025-05-18 14:31:52.176366: train_loss -0.9585\n",
      "2025-05-18 14:31:52.176654: val_loss -0.9418\n",
      "[np.float32(0.9755)]176720: Pseudo dice \n",
      "2025-05-18 14:31:52.176878: Epoch time: 109.42 s\n",
      "2025-05-18 14:31:52.726732: \n",
      "2025-05-18 14:31:52.727152: Epoch 404\n",
      "2025-05-18 14:31:52.727277: Current learning rate: 0.00628\n",
      "2025-05-18 14:33:42.180212: train_loss -0.9598\n",
      "2025-05-18 14:33:42.180364: val_loss -0.9465\n",
      "2025-05-18 14:33:42.180396: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 14:33:42.180430: Epoch time: 109.45 s\n",
      "2025-05-18 14:33:42.730787: \n",
      "2025-05-18 14:33:42.730891: Epoch 405\n",
      "2025-05-18 14:33:42.730954: Current learning rate: 0.00627\n",
      "2025-05-18 14:35:32.218770: train_loss -0.962\n",
      "2025-05-18 14:35:32.218954: val_loss -0.9524\n",
      "2025-05-18 14:35:32.219094: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-18 14:35:32.219144: Epoch time: 109.49 s\n",
      "2025-05-18 14:35:32.759048: \n",
      "2025-05-18 14:35:32.759199: Epoch 406\n",
      "2025-05-18 14:35:32.759296: Current learning rate: 0.00626\n",
      "2025-05-18 14:37:22.208760: train_loss -0.9597\n",
      "2025-05-18 14:37:22.208911: val_loss -0.9378\n",
      "2025-05-18 14:37:22.209012: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-18 14:37:22.209107: Epoch time: 109.45 s\n",
      "2025-05-18 14:37:22.755497: \n",
      "2025-05-18 14:37:22.755764: Epoch 407\n",
      "2025-05-18 14:37:22.755914: Current learning rate: 0.00625\n",
      "2025-05-18 14:39:12.195029: train_loss -0.9603\n",
      "2025-05-18 14:39:12.195155: val_loss -0.9484\n",
      "2025-05-18 14:39:12.195189: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 14:39:12.195222: Epoch time: 109.44 s\n",
      "2025-05-18 14:39:12.737925: \n",
      "2025-05-18 14:39:12.738081: Epoch 408\n",
      "2025-05-18 14:39:12.738150: Current learning rate: 0.00624\n",
      "2025-05-18 14:41:02.197881: train_loss -0.96\n",
      "2025-05-18 14:41:02.198004: val_loss -0.9482\n",
      "2025-05-18 14:41:02.198038: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 14:41:02.198071: Epoch time: 109.46 s\n",
      "2025-05-18 14:41:02.741464: \n",
      "2025-05-18 14:41:02.741590: Epoch 409\n",
      "2025-05-18 14:41:02.741653: Current learning rate: 0.00623\n",
      "2025-05-18 14:42:52.214587: train_loss -0.9624\n",
      "2025-05-18 14:42:52.214712: val_loss -0.9533\n",
      "2025-05-18 14:42:52.214746: Pseudo dice [np.float32(0.9803)]\n",
      "2025-05-18 14:42:52.214778: Epoch time: 109.47 s\n",
      "2025-05-18 14:42:52.768834: \n",
      "2025-05-18 14:42:52.768921: Epoch 410\n",
      "2025-05-18 14:42:52.768985: Current learning rate: 0.00622\n",
      "2025-05-18 14:44:42.172458: train_loss -0.96\n",
      "2025-05-18 14:44:42.172579: val_loss -0.9437\n",
      "2025-05-18 14:44:42.172613: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-18 14:44:42.172646: Epoch time: 109.4 s\n",
      "2025-05-18 14:44:42.913413: \n",
      "2025-05-18 14:44:42.913526: Epoch 411\n",
      "2025-05-18 14:44:42.913590: Current learning rate: 0.00621\n",
      "2025-05-18 14:46:32.351655: train_loss -0.9611\n",
      "2025-05-18 14:46:32.351778: val_loss -0.9403\n",
      "2025-05-18 14:46:32.351814: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-18 14:46:32.351848: Epoch time: 109.44 s\n",
      "2025-05-18 14:46:32.877231: \n",
      "2025-05-18 14:46:32.877326: Epoch 412\n",
      "2025-05-18 14:46:32.877390: Current learning rate: 0.0062\n",
      "2025-05-18 14:48:22.459258: train_loss -0.9608\n",
      "2025-05-18 14:48:22.459508: val_loss -0.9456\n",
      "2025-05-18 14:48:22.459569: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 14:48:22.459609: Epoch time: 109.58 s\n",
      "2025-05-18 14:48:22.990479: \n",
      "2025-05-18 14:48:22.990636: Epoch 413\n",
      "2025-05-18 14:48:22.990701: Current learning rate: 0.00619\n",
      "2025-05-18 14:50:12.408471: train_loss -0.9612\n",
      "2025-05-18 14:50:12.408590: val_loss -0.945\n",
      "2025-05-18 14:50:12.408621: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 14:50:12.408653: Epoch time: 109.42 s\n",
      "2025-05-18 14:50:12.933097: \n",
      "2025-05-18 14:50:12.933356: Epoch 414\n",
      "2025-05-18 14:50:12.933568: Current learning rate: 0.00618\n",
      "2025-05-18 14:52:02.389774: train_loss -0.9598\n",
      "2025-05-18 14:52:02.389996: val_loss -0.9383\n",
      "2025-05-18 14:52:02.390031: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-18 14:52:02.390095: Epoch time: 109.46 s\n",
      "2025-05-18 14:52:02.919561: \n",
      "2025-05-18 14:52:02.919691: Epoch 415\n",
      "2025-05-18 14:52:02.919775: Current learning rate: 0.00617\n",
      "2025-05-18 14:53:52.367001: train_loss -0.9579\n",
      "2025-05-18 14:53:52.367189: val_loss -0.946\n",
      "2025-05-18 14:53:52.367223: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 14:53:52.367256: Epoch time: 109.45 s\n",
      "2025-05-18 14:53:52.891008: \n",
      "2025-05-18 14:53:52.891146: Epoch 416\n",
      "2025-05-18 14:53:52.891219: Current learning rate: 0.00616\n",
      "2025-05-18 14:55:42.474368: train_loss -0.9586\n",
      "2025-05-18 14:55:42.474567: val_loss -0.9301\n",
      "2025-05-18 14:55:42.474600: Pseudo dice [np.float32(0.9702)]\n",
      "2025-05-18 14:55:42.474717: Epoch time: 109.58 s\n",
      "2025-05-18 14:55:43.001198: \n",
      "2025-05-18 14:55:43.001281: Epoch 417\n",
      "2025-05-18 14:55:43.001343: Current learning rate: 0.00615\n",
      "2025-05-18 14:57:32.442590: train_loss -0.9578\n",
      "2025-05-18 14:57:32.442752: val_loss -0.9433\n",
      "2025-05-18 14:57:32.442784: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 14:57:32.442818: Epoch time: 109.44 s\n",
      "2025-05-18 14:57:32.970726: \n",
      "2025-05-18 14:57:32.970919: Epoch 418\n",
      "2025-05-18 14:57:32.970990: Current learning rate: 0.00614\n",
      "2025-05-18 14:59:22.458983: train_loss -0.9581\n",
      "2025-05-18 14:59:22.459311: val_loss -0.9512\n",
      "2025-05-18 14:59:22.459354: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-18 14:59:22.459389: Epoch time: 109.49 s\n",
      "2025-05-18 14:59:22.983371: \n",
      "2025-05-18 14:59:22.983465: Epoch 419\n",
      "2025-05-18 14:59:22.983611: Current learning rate: 0.00613\n",
      "2025-05-18 15:01:12.440782: train_loss -0.9593\n",
      "2025-05-18 15:01:12.440913: val_loss -0.9418\n",
      "2025-05-18 15:01:12.440958: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-18 15:01:12.440994: Epoch time: 109.46 s\n",
      "2025-05-18 15:01:12.975133: \n",
      "2025-05-18 15:01:12.975346: Epoch 420\n",
      "2025-05-18 15:01:12.975511: Current learning rate: 0.00612\n",
      "2025-05-18 15:03:02.397771: train_loss -0.9566\n",
      "2025-05-18 15:03:02.398214: val_loss -0.9443\n",
      "2025-05-18 15:03:02.398327: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-18 15:03:02.398372: Epoch time: 109.42 s\n",
      "2025-05-18 15:03:02.926555: \n",
      "2025-05-18 15:03:02.926826: Epoch 421\n",
      "2025-05-18 15:03:02.926896: Current learning rate: 0.00612\n",
      "2025-05-18 15:04:52.405450: train_loss -0.9581\n",
      "2025-05-18 15:04:52.405564: val_loss -0.9448\n",
      "2025-05-18 15:04:52.405596: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 15:04:52.405628: Epoch time: 109.48 s\n",
      "2025-05-18 15:04:52.924357: \n",
      "2025-05-18 15:04:52.924439: Epoch 422\n",
      "2025-05-18 15:04:52.924572: Current learning rate: 0.00611\n",
      "2025-05-18 15:06:42.404950: train_loss -0.9606\n",
      "2025-05-18 15:06:42.405083: val_loss -0.9348\n",
      "2025-05-18 15:06:42.405123: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-18 15:06:42.405156: Epoch time: 109.48 s\n",
      "2025-05-18 15:06:42.931122: \n",
      "2025-05-18 15:06:42.931396: Epoch 423\n",
      "2025-05-18 15:06:42.931645: Current learning rate: 0.0061\n",
      "2025-05-18 15:08:32.504248: train_loss -0.961\n",
      "2025-05-18 15:08:32.504365: val_loss -0.9516\n",
      "2025-05-18 15:08:32.504399: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-18 15:08:32.504433: Epoch time: 109.57 s\n",
      "2025-05-18 15:08:33.232488: \n",
      "2025-05-18 15:08:33.232599: Epoch 424\n",
      "2025-05-18 15:08:33.232661: Current learning rate: 0.00609\n",
      "2025-05-18 15:10:22.715641: train_loss -0.9574\n",
      "2025-05-18 15:10:22.715770: val_loss -0.935\n",
      "2025-05-18 15:10:22.715806: Pseudo dice [np.float32(0.9717)]\n",
      "2025-05-18 15:10:22.715841: Epoch time: 109.48 s\n",
      "2025-05-18 15:10:23.238888: \n",
      "2025-05-18 15:10:23.238997: Epoch 425\n",
      "2025-05-18 15:10:23.239137: Current learning rate: 0.00608\n",
      "2025-05-18 15:12:12.700322: train_loss -0.9584\n",
      "2025-05-18 15:12:12.700452: val_loss -0.942\n",
      "2025-05-18 15:12:12.700487: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-18 15:12:12.700519: Epoch time: 109.46 s\n",
      "2025-05-18 15:12:13.226018: \n",
      "2025-05-18 15:12:13.226339: Epoch 426\n",
      "2025-05-18 15:12:13.226415: Current learning rate: 0.00607\n",
      "2025-05-18 15:14:02.686487: train_loss -0.9604\n",
      "2025-05-18 15:14:02.686636: val_loss -0.943\n",
      "2025-05-18 15:14:02.686671: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 15:14:02.686703: Epoch time: 109.46 s\n",
      "2025-05-18 15:14:03.214984: \n",
      "2025-05-18 15:14:03.215079: Epoch 427\n",
      "2025-05-18 15:14:03.215143: Current learning rate: 0.00606\n",
      "2025-05-18 15:15:52.646395: train_loss -0.9608\n",
      "2025-05-18 15:15:52.646711: val_loss -0.9295\n",
      "2025-05-18 15:15:52.646754: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-18 15:15:52.646789: Epoch time: 109.43 s\n",
      "2025-05-18 15:15:53.167946: \n",
      "2025-05-18 15:15:53.168253: Epoch 428\n",
      "2025-05-18 15:15:53.168434: Current learning rate: 0.00605\n",
      "2025-05-18 15:17:42.733002: train_loss -0.9604\n",
      "2025-05-18 15:17:42.733124: val_loss -0.9465\n",
      "[np.float32(0.9776)]733159: Pseudo dice \n",
      "2025-05-18 15:17:42.733344: Epoch time: 109.57 s\n",
      "2025-05-18 15:17:43.266150: \n",
      "2025-05-18 15:17:43.266450: Epoch 429\n",
      "2025-05-18 15:17:43.266555: Current learning rate: 0.00604\n",
      "2025-05-18 15:19:32.769841: train_loss -0.9596\n",
      "2025-05-18 15:19:32.769961: val_loss -0.942\n",
      "2025-05-18 15:19:32.769992: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 15:19:32.770026: Epoch time: 109.5 s\n",
      "2025-05-18 15:19:33.304370: \n",
      "2025-05-18 15:19:33.304533: Epoch 430\n",
      "2025-05-18 15:19:33.304600: Current learning rate: 0.00603\n",
      "2025-05-18 15:21:22.720330: train_loss -0.9592\n",
      "2025-05-18 15:21:22.720508: val_loss -0.9455\n",
      "2025-05-18 15:21:22.720539: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 15:21:22.720571: Epoch time: 109.42 s\n",
      "2025-05-18 15:21:23.251319: \n",
      "2025-05-18 15:21:23.251470: Epoch 431\n",
      "2025-05-18 15:21:23.251729: Current learning rate: 0.00602\n",
      "2025-05-18 15:23:12.707208: train_loss -0.9607\n",
      "2025-05-18 15:23:12.707368: val_loss -0.9511\n",
      "2025-05-18 15:23:12.707401: Pseudo dice [np.float32(0.9798)]\n",
      "2025-05-18 15:23:12.707434: Epoch time: 109.46 s\n",
      "2025-05-18 15:23:13.233098: \n",
      "2025-05-18 15:23:13.233181: Epoch 432\n",
      "2025-05-18 15:23:13.233246: Current learning rate: 0.00601\n",
      "2025-05-18 15:25:02.725149: train_loss -0.9622\n",
      "2025-05-18 15:25:02.725272: val_loss -0.9479\n",
      "2025-05-18 15:25:02.725305: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 15:25:02.725334: Epoch time: 109.49 s\n",
      "2025-05-18 15:25:03.254244: \n",
      "2025-05-18 15:25:03.254329: Epoch 433\n",
      "2025-05-18 15:25:03.254394: Current learning rate: 0.006\n",
      "2025-05-18 15:26:52.650656: train_loss -0.9595\n",
      "2025-05-18 15:26:52.650844: val_loss -0.9519\n",
      "2025-05-18 15:26:52.650978: Pseudo dice [np.float32(0.9805)]\n",
      "2025-05-18 15:26:52.651024: Epoch time: 109.4 s\n",
      "2025-05-18 15:26:53.185916: \n",
      "2025-05-18 15:26:53.186075: Epoch 434\n",
      "2025-05-18 15:26:53.186151: Current learning rate: 0.00599\n",
      "2025-05-18 15:28:42.748459: train_loss -0.9619\n",
      "2025-05-18 15:28:42.748579: val_loss -0.9438\n",
      "2025-05-18 15:28:42.748610: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 15:28:42.748650: Epoch time: 109.56 s\n",
      "2025-05-18 15:28:43.278935: \n",
      "2025-05-18 15:28:43.279230: Epoch 435\n",
      "2025-05-18 15:28:43.279303: Current learning rate: 0.00598\n",
      "2025-05-18 15:30:32.744440: train_loss -0.9617\n",
      "2025-05-18 15:30:32.744553: val_loss -0.9379\n",
      "2025-05-18 15:30:32.744585: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-18 15:30:32.744617: Epoch time: 109.47 s\n",
      "2025-05-18 15:30:33.270176: \n",
      "2025-05-18 15:30:33.270307: Epoch 436\n",
      "2025-05-18 15:30:33.270426: Current learning rate: 0.00597\n",
      "2025-05-18 15:32:22.678560: train_loss -0.9628\n",
      "2025-05-18 15:32:22.678675: val_loss -0.9481\n",
      "2025-05-18 15:32:22.678708: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 15:32:22.678739: Epoch time: 109.41 s\n",
      "2025-05-18 15:32:23.399426: \n",
      "2025-05-18 15:32:23.399521: Epoch 437\n",
      "2025-05-18 15:32:23.399588: Current learning rate: 0.00596\n",
      "2025-05-18 15:34:12.859438: train_loss -0.9621\n",
      "2025-05-18 15:34:12.859568: val_loss -0.9473\n",
      "2025-05-18 15:34:12.859601: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 15:34:12.859635: Epoch time: 109.46 s\n",
      "2025-05-18 15:34:13.389253: \n",
      "2025-05-18 15:34:13.389435: Epoch 438\n",
      "2025-05-18 15:34:13.389504: Current learning rate: 0.00595\n",
      "2025-05-18 15:36:02.865577: train_loss -0.9623\n",
      "2025-05-18 15:36:02.865783: val_loss -0.9437\n",
      "2025-05-18 15:36:02.865824: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 15:36:02.865860: Epoch time: 109.48 s\n",
      "2025-05-18 15:36:03.407582: \n",
      "2025-05-18 15:36:03.407685: Epoch 439\n",
      "2025-05-18 15:36:03.407751: Current learning rate: 0.00594\n",
      "2025-05-18 15:37:52.901225: train_loss -0.9638\n",
      "2025-05-18 15:37:52.901377: val_loss -0.9475\n",
      "2025-05-18 15:37:52.901490: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-18 15:37:52.901571: Epoch time: 109.49 s\n",
      "2025-05-18 15:37:52.901602: Yayy! New best EMA pseudo Dice: 0.9767000079154968\n",
      "2025-05-18 15:37:53.667453: \n",
      "2025-05-18 15:37:53.667611: Epoch 440\n",
      "2025-05-18 15:37:53.667676: Current learning rate: 0.00593\n",
      "2025-05-18 15:39:43.140651: train_loss -0.9632\n",
      "2025-05-18 15:39:43.140768: val_loss -0.942\n",
      "2025-05-18 15:39:43.140799: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 15:39:43.140833: Epoch time: 109.47 s\n",
      "2025-05-18 15:39:43.669191: \n",
      "2025-05-18 15:39:43.669291: Epoch 441\n",
      "2025-05-18 15:39:43.669353: Current learning rate: 0.00592\n",
      "2025-05-18 15:41:33.290265: train_loss -0.9618\n",
      "2025-05-18 15:41:33.290401: val_loss -0.9407\n",
      "2025-05-18 15:41:33.290435: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 15:41:33.290467: Epoch time: 109.62 s\n",
      "2025-05-18 15:41:33.815449: \n",
      "2025-05-18 15:41:33.815548: Epoch 442\n",
      "2025-05-18 15:41:33.815613: Current learning rate: 0.00592\n",
      "2025-05-18 15:43:23.395276: train_loss -0.9618\n",
      "2025-05-18 15:43:23.395522: val_loss -0.9504\n",
      "2025-05-18 15:43:23.395566: Pseudo dice [np.float32(0.98)]\n",
      "2025-05-18 15:43:23.395602: Epoch time: 109.58 s\n",
      "2025-05-18 15:43:23.395624: Yayy! New best EMA pseudo Dice: 0.9768999814987183\n",
      "2025-05-18 15:43:24.145073: \n",
      "2025-05-18 15:43:24.145168: Epoch 443\n",
      "2025-05-18 15:43:24.145235: Current learning rate: 0.00591\n",
      "2025-05-18 15:45:13.663967: train_loss -0.9623\n",
      "2025-05-18 15:45:13.664168: val_loss -0.9435\n",
      "2025-05-18 15:45:13.664236: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-18 15:45:13.664276: Epoch time: 109.52 s\n",
      "2025-05-18 15:45:14.194511: \n",
      "2025-05-18 15:45:14.194667: Epoch 444\n",
      "2025-05-18 15:45:14.194736: Current learning rate: 0.0059\n",
      "2025-05-18 15:47:03.736194: train_loss -0.9613\n",
      "2025-05-18 15:47:03.736331: val_loss -0.9376\n",
      "2025-05-18 15:47:03.736365: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 15:47:03.736398: Epoch time: 109.54 s\n",
      "2025-05-18 15:47:04.261229: \n",
      "2025-05-18 15:47:04.261368: Epoch 445\n",
      "2025-05-18 15:47:04.261439: Current learning rate: 0.00589\n",
      "2025-05-18 15:48:53.753400: train_loss -0.9639\n",
      "2025-05-18 15:48:53.753542: val_loss -0.9418\n",
      "2025-05-18 15:48:53.753577: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 15:48:53.753611: Epoch time: 109.49 s\n",
      "2025-05-18 15:48:54.279565: \n",
      "2025-05-18 15:48:54.279716: Epoch 446\n",
      "2025-05-18 15:48:54.279786: Current learning rate: 0.00588\n",
      "2025-05-18 15:50:43.762538: train_loss -0.9631\n",
      "2025-05-18 15:50:43.762773: val_loss -0.9484\n",
      "2025-05-18 15:50:43.762813: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-18 15:50:43.762845: Epoch time: 109.48 s\n",
      "2025-05-18 15:50:44.299906: \n",
      "2025-05-18 15:50:44.300051: Epoch 447\n",
      "2025-05-18 15:50:44.300122: Current learning rate: 0.00587\n",
      "2025-05-18 15:52:33.808191: train_loss -0.9632\n",
      "2025-05-18 15:52:33.808378: val_loss -0.9483\n",
      "2025-05-18 15:52:33.808822: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 15:52:33.808894: Epoch time: 109.51 s\n",
      "2025-05-18 15:52:33.808930: Yayy! New best EMA pseudo Dice: 0.9768999814987183\n",
      "2025-05-18 15:52:34.578574: \n",
      "2025-05-18 15:52:34.578975: Epoch 448\n",
      "2025-05-18 15:52:34.579123: Current learning rate: 0.00586\n",
      "2025-05-18 15:54:24.092824: train_loss -0.963\n",
      "2025-05-18 15:54:24.093006: val_loss -0.9446\n",
      "2025-05-18 15:54:24.093039: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-18 15:54:24.093073: Epoch time: 109.51 s\n",
      "2025-05-18 15:54:24.630142: \n",
      "2025-05-18 15:54:24.630305: Epoch 449\n",
      "2025-05-18 15:54:24.630377: Current learning rate: 0.00585\n",
      "2025-05-18 15:56:14.148370: train_loss -0.962\n",
      "2025-05-18 15:56:14.148500: val_loss -0.958\n",
      "2025-05-18 15:56:14.148640: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-18 15:56:14.148901: Epoch time: 109.52 s\n",
      "2025-05-18 15:56:14.387653: Yayy! New best EMA pseudo Dice: 0.9772999882698059\n",
      "2025-05-18 15:56:15.394260: \n",
      "2025-05-18 15:56:15.394494: Epoch 450\n",
      "2025-05-18 15:56:15.394590: Current learning rate: 0.00584\n",
      "2025-05-18 15:58:04.886258: train_loss -0.964\n",
      "2025-05-18 15:58:04.886512: val_loss -0.9418\n",
      "2025-05-18 15:58:04.886594: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-18 15:58:04.886640: Epoch time: 109.49 s\n",
      "2025-05-18 15:58:05.420941: \n",
      "2025-05-18 15:58:05.421165: Epoch 451\n",
      "2025-05-18 15:58:05.421250: Current learning rate: 0.00583\n",
      "2025-05-18 15:59:54.914329: train_loss -0.9646\n",
      "2025-05-18 15:59:54.914458: val_loss -0.9501\n",
      "2025-05-18 15:59:54.914493: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-18 15:59:54.914527: Epoch time: 109.49 s\n",
      "2025-05-18 15:59:54.914549: Yayy! New best EMA pseudo Dice: 0.9772999882698059\n",
      "2025-05-18 15:59:55.686708: \n",
      "2025-05-18 15:59:55.686949: Epoch 452\n",
      "2025-05-18 15:59:55.687026: Current learning rate: 0.00582\n",
      "2025-05-18 16:01:45.205976: train_loss -0.9634\n",
      "2025-05-18 16:01:45.206115: val_loss -0.944\n",
      "2025-05-18 16:01:45.206214: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-18 16:01:45.206274: Epoch time: 109.52 s\n",
      "2025-05-18 16:01:45.750241: \n",
      "2025-05-18 16:01:45.750408: Epoch 453\n",
      "2025-05-18 16:01:45.750486: Current learning rate: 0.00581\n",
      "2025-05-18 16:03:35.398126: train_loss -0.9639\n",
      "2025-05-18 16:03:35.398265: val_loss -0.9464\n",
      "2025-05-18 16:03:35.398343: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 16:03:35.398383: Epoch time: 109.65 s\n",
      "2025-05-18 16:03:35.940562: \n",
      "2025-05-18 16:03:35.940663: Epoch 454\n",
      "2025-05-18 16:03:35.940728: Current learning rate: 0.0058\n",
      "2025-05-18 16:05:25.484162: train_loss -0.9631\n",
      "2025-05-18 16:05:25.484308: val_loss -0.9477\n",
      "2025-05-18 16:05:25.484346: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 16:05:25.484383: Epoch time: 109.54 s\n",
      "2025-05-18 16:05:26.022252: \n",
      "2025-05-18 16:05:26.022362: Epoch 455\n",
      "2025-05-18 16:05:26.022430: Current learning rate: 0.00579\n",
      "2025-05-18 16:07:15.547943: train_loss -0.9635\n",
      "2025-05-18 16:07:15.548090: val_loss -0.9442\n",
      "2025-05-18 16:07:15.548126: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 16:07:15.548167: Epoch time: 109.53 s\n",
      "2025-05-18 16:07:16.091823: \n",
      "2025-05-18 16:07:16.091976: Epoch 456\n",
      "2025-05-18 16:07:16.092051: Current learning rate: 0.00578\n",
      "2025-05-18 16:09:05.605876: train_loss -0.9631\n",
      "2025-05-18 16:09:05.606002: val_loss -0.9577\n",
      "2025-05-18 16:09:05.606037: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-18 16:09:05.606071: Epoch time: 109.51 s\n",
      "2025-05-18 16:09:05.606093: Yayy! New best EMA pseudo Dice: 0.9776999950408936\n",
      "2025-05-18 16:09:06.387049: \n",
      "2025-05-18 16:09:06.387473: Epoch 457\n",
      "2025-05-18 16:09:06.387663: Current learning rate: 0.00577\n",
      "2025-05-18 16:10:55.926657: train_loss -0.9634\n",
      "2025-05-18 16:10:55.926833: val_loss -0.9433\n",
      "2025-05-18 16:10:55.926867: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 16:10:55.926900: Epoch time: 109.54 s\n",
      "2025-05-18 16:10:56.469342: \n",
      "2025-05-18 16:10:56.469444: Epoch 458\n",
      "2025-05-18 16:10:56.469512: Current learning rate: 0.00576\n",
      "2025-05-18 16:12:46.137079: train_loss -0.9647\n",
      "2025-05-18 16:12:46.137196: val_loss -0.944\n",
      "2025-05-18 16:12:46.137228: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-18 16:12:46.137260: Epoch time: 109.67 s\n",
      "2025-05-18 16:12:46.650906: \n",
      "2025-05-18 16:12:46.651046: Epoch 459\n",
      "2025-05-18 16:12:46.651127: Current learning rate: 0.00575\n",
      "2025-05-18 16:14:36.085398: train_loss -0.962\n",
      "2025-05-18 16:14:36.085587: val_loss -0.9429\n",
      "2025-05-18 16:14:36.085620: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 16:14:36.085660: Epoch time: 109.43 s\n",
      "2025-05-18 16:14:36.624249: \n",
      "2025-05-18 16:14:36.624346: Epoch 460\n",
      "2025-05-18 16:14:36.624422: Current learning rate: 0.00574\n",
      "2025-05-18 16:16:26.149298: train_loss -0.9613\n",
      "2025-05-18 16:16:26.149446: val_loss -0.9391\n",
      "2025-05-18 16:16:26.149481: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-18 16:16:26.149515: Epoch time: 109.53 s\n",
      "2025-05-18 16:16:26.687571: \n",
      "2025-05-18 16:16:26.687873: Epoch 461\n",
      "2025-05-18 16:16:26.687984: Current learning rate: 0.00573\n",
      "2025-05-18 16:18:16.241410: train_loss -0.9613\n",
      "2025-05-18 16:18:16.241548: val_loss -0.9454\n",
      "2025-05-18 16:18:16.241584: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 16:18:16.241620: Epoch time: 109.55 s\n",
      "2025-05-18 16:18:17.025811: \n",
      "2025-05-18 16:18:17.026213: Epoch 462\n",
      "2025-05-18 16:18:17.026461: Current learning rate: 0.00572\n",
      "2025-05-18 16:20:06.565592: train_loss -0.9642\n",
      "2025-05-18 16:20:06.565792: val_loss -0.9393\n",
      "2025-05-18 16:20:06.565830: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 16:20:06.565866: Epoch time: 109.54 s\n",
      "2025-05-18 16:20:07.099557: \n",
      "2025-05-18 16:20:07.099749: Epoch 463\n",
      "2025-05-18 16:20:07.099852: Current learning rate: 0.00571\n",
      "2025-05-18 16:21:57.344776: train_loss -0.9657\n",
      "2025-05-18 16:21:57.344924: val_loss -0.9496\n",
      "2025-05-18 16:21:57.344959: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-18 16:21:57.344994: Epoch time: 110.25 s\n",
      "2025-05-18 16:21:57.879007: \n",
      "2025-05-18 16:21:57.879116: Epoch 464\n",
      "2025-05-18 16:21:57.879184: Current learning rate: 0.0057\n",
      "2025-05-18 16:23:47.572570: train_loss -0.9646\n",
      "2025-05-18 16:23:47.572722: val_loss -0.9349\n",
      "2025-05-18 16:23:47.572826: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-18 16:23:47.572871: Epoch time: 109.69 s\n",
      "2025-05-18 16:23:48.113924: \n",
      "2025-05-18 16:23:48.114165: Epoch 465\n",
      "2025-05-18 16:23:48.114286: Current learning rate: 0.0057\n",
      "2025-05-18 16:25:37.796712: train_loss -0.9634\n",
      "2025-05-18 16:25:37.796852: val_loss -0.9504\n",
      "2025-05-18 16:25:37.796882: Pseudo dice [np.float32(0.9795)]\n",
      "2025-05-18 16:25:37.796917: Epoch time: 109.68 s\n",
      "2025-05-18 16:25:38.328920: \n",
      "2025-05-18 16:25:38.329250: Epoch 466\n",
      "2025-05-18 16:25:38.329332: Current learning rate: 0.00569\n",
      "2025-05-18 16:27:27.796702: train_loss -0.962\n",
      "2025-05-18 16:27:27.796887: val_loss -0.9346\n",
      "2025-05-18 16:27:27.796920: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-18 16:27:27.796955: Epoch time: 109.47 s\n",
      "2025-05-18 16:27:28.327835: \n",
      "2025-05-18 16:27:28.327931: Epoch 467\n",
      "2025-05-18 16:27:28.328000: Current learning rate: 0.00568\n",
      "2025-05-18 16:29:17.762388: train_loss -0.965\n",
      "2025-05-18 16:29:17.762740: val_loss -0.9475\n",
      "2025-05-18 16:29:17.762787: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-18 16:29:17.762826: Epoch time: 109.44 s\n",
      "2025-05-18 16:29:18.302148: \n",
      "2025-05-18 16:29:18.302501: Epoch 468\n",
      "2025-05-18 16:29:18.302834: Current learning rate: 0.00567\n",
      "2025-05-18 16:31:07.704371: train_loss -0.962\n",
      "2025-05-18 16:31:07.704531: val_loss -0.9543\n",
      "2025-05-18 16:31:07.704572: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-18 16:31:07.704612: Epoch time: 109.4 s\n",
      "2025-05-18 16:31:08.230769: \n",
      "2025-05-18 16:31:08.230864: Epoch 469\n",
      "2025-05-18 16:31:08.230932: Current learning rate: 0.00566\n",
      "2025-05-18 16:32:57.623100: train_loss -0.9642\n",
      "2025-05-18 16:32:57.623239: val_loss -0.936\n",
      "2025-05-18 16:32:57.623395: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-18 16:32:57.623447: Epoch time: 109.39 s\n",
      "2025-05-18 16:32:58.143719: \n",
      "2025-05-18 16:32:58.144079: Epoch 470\n",
      "2025-05-18 16:32:58.144218: Current learning rate: 0.00565\n",
      "2025-05-18 16:34:47.585216: train_loss -0.9636\n",
      "2025-05-18 16:34:47.585349: val_loss -0.9298\n",
      "2025-05-18 16:34:47.585383: Pseudo dice [np.float32(0.9719)]\n",
      "2025-05-18 16:34:47.585415: Epoch time: 109.44 s\n",
      "2025-05-18 16:34:48.118830: \n",
      "2025-05-18 16:34:48.119121: Epoch 471\n",
      "2025-05-18 16:34:48.119203: Current learning rate: 0.00564\n",
      "2025-05-18 16:36:37.166917: train_loss -0.9651\n",
      "2025-05-18 16:36:37.167045: val_loss -0.9406\n",
      "2025-05-18 16:36:37.167192: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-18 16:36:37.167250: Epoch time: 109.05 s\n",
      "2025-05-18 16:36:37.677448: \n",
      "2025-05-18 16:36:37.677592: Epoch 472\n",
      "2025-05-18 16:36:37.677662: Current learning rate: 0.00563\n",
      "2025-05-18 16:38:26.693459: train_loss -0.9623\n",
      "2025-05-18 16:38:26.693691: val_loss -0.9434\n",
      "2025-05-18 16:38:26.693735: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 16:38:26.693836: Epoch time: 109.02 s\n",
      "2025-05-18 16:38:27.198568: \n",
      "2025-05-18 16:38:27.198670: Epoch 473\n",
      "2025-05-18 16:38:27.198731: Current learning rate: 0.00562\n",
      "2025-05-18 16:40:16.226118: train_loss -0.9639\n",
      "2025-05-18 16:40:16.226240: val_loss -0.9456\n",
      "2025-05-18 16:40:16.226275: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-18 16:40:16.226307: Epoch time: 109.03 s\n",
      "2025-05-18 16:40:16.741215: \n",
      "2025-05-18 16:40:16.741294: Epoch 474\n",
      "2025-05-18 16:40:16.741370: Current learning rate: 0.00561\n",
      "2025-05-18 16:42:05.794096: train_loss -0.9661\n",
      "2025-05-18 16:42:05.794223: val_loss -0.9523\n",
      "2025-05-18 16:42:05.794258: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-18 16:42:05.794292: Epoch time: 109.05 s\n",
      "2025-05-18 16:42:06.309520: \n",
      "2025-05-18 16:42:06.309602: Epoch 475\n",
      "2025-05-18 16:42:06.309667: Current learning rate: 0.0056\n",
      "2025-05-18 16:43:55.375329: train_loss -0.9663\n",
      "2025-05-18 16:43:55.375528: val_loss -0.9475\n",
      "2025-05-18 16:43:55.375565: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-18 16:43:55.375600: Epoch time: 109.07 s\n",
      "2025-05-18 16:43:56.116617: \n",
      "2025-05-18 16:43:56.116821: Epoch 476\n",
      "2025-05-18 16:43:56.116910: Current learning rate: 0.00559\n",
      "2025-05-18 16:45:45.198094: train_loss -0.9649\n",
      "2025-05-18 16:45:45.198263: val_loss -0.9452\n",
      "2025-05-18 16:45:45.198318: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-18 16:45:45.198352: Epoch time: 109.08 s\n",
      "2025-05-18 16:45:45.717708: \n",
      "2025-05-18 16:45:45.717969: Epoch 477\n",
      "Current learning rate: 0.00558\n",
      "2025-05-18 16:47:34.781047: train_loss -0.965\n",
      "2025-05-18 16:47:34.781198: val_loss -0.9402\n",
      "2025-05-18 16:47:34.781235: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-18 16:47:34.781270: Epoch time: 109.06 s\n",
      "2025-05-18 16:47:35.302910: \n",
      "2025-05-18 16:47:35.303085: Epoch 478\n",
      "2025-05-18 16:47:35.303158: Current learning rate: 0.00557\n",
      "2025-05-18 16:49:24.409927: train_loss -0.9646\n",
      "2025-05-18 16:49:24.410056: val_loss -0.9327\n",
      "2025-05-18 16:49:24.410089: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-18 16:49:24.410123: Epoch time: 109.11 s\n",
      "2025-05-18 16:49:24.954811: \n",
      "2025-05-18 16:49:24.954913: Epoch 479\n",
      "2025-05-18 16:49:24.954978: Current learning rate: 0.00556\n",
      "2025-05-18 16:51:14.008980: train_loss -0.9669\n",
      "2025-05-18 16:51:14.009150: val_loss -0.9428\n",
      "2025-05-18 16:51:14.009187: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 16:51:14.009222: Epoch time: 109.05 s\n",
      "2025-05-18 16:51:14.543685: \n",
      "2025-05-18 16:51:14.543842: Epoch 480\n",
      "2025-05-18 16:51:14.543914: Current learning rate: 0.00555\n",
      "2025-05-18 16:53:03.577195: train_loss -0.9637\n",
      "2025-05-18 16:53:03.577327: val_loss -0.9562\n",
      "2025-05-18 16:53:03.577361: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-18 16:53:03.577393: Epoch time: 109.03 s\n",
      "2025-05-18 16:53:04.109673: \n",
      "2025-05-18 16:53:04.109854: Epoch 481\n",
      "2025-05-18 16:53:04.109922: Current learning rate: 0.00554\n",
      "2025-05-18 16:54:53.167584: train_loss -0.9642\n",
      "2025-05-18 16:54:53.167734: val_loss -0.9434\n",
      "2025-05-18 16:54:53.167769: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 16:54:53.167803: Epoch time: 109.06 s\n",
      "2025-05-18 16:54:53.700651: \n",
      "2025-05-18 16:54:53.701011: Epoch 482\n",
      "2025-05-18 16:54:53.701137: Current learning rate: 0.00553\n",
      "2025-05-18 16:56:42.718541: train_loss -0.9651\n",
      "2025-05-18 16:56:42.718783: val_loss -0.9461\n",
      "2025-05-18 16:56:42.718827: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-18 16:56:42.718862: Epoch time: 109.02 s\n",
      "2025-05-18 16:56:43.255726: \n",
      "2025-05-18 16:56:43.255896: Epoch 483\n",
      "2025-05-18 16:56:43.255966: Current learning rate: 0.00552\n",
      "2025-05-18 16:58:32.311859: train_loss -0.965\n",
      "2025-05-18 16:58:32.311992: val_loss -0.958\n",
      "2025-05-18 16:58:32.312119: Pseudo dice [np.float32(0.983)]\n",
      "2025-05-18 16:58:32.312334: Epoch time: 109.06 s\n",
      "2025-05-18 16:58:32.849054: \n",
      "2025-05-18 16:58:32.849265: Epoch 484\n",
      "2025-05-18 16:58:32.849418: Current learning rate: 0.00551\n",
      "2025-05-18 17:00:21.869433: train_loss -0.9647\n",
      "2025-05-18 17:00:21.869572: val_loss -0.9497\n",
      "2025-05-18 17:00:21.869606: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-18 17:00:21.869642: Epoch time: 109.02 s\n",
      "2025-05-18 17:00:21.869663: Yayy! New best EMA pseudo Dice: 0.9776999950408936\n",
      "2025-05-18 17:00:22.682969: \n",
      "2025-05-18 17:00:22.683282: Epoch 485\n",
      "2025-05-18 17:00:22.683411: Current learning rate: 0.0055\n",
      "2025-05-18 17:02:11.676000: train_loss -0.965\n",
      "2025-05-18 17:02:11.676165: val_loss -0.9395\n",
      "2025-05-18 17:02:11.676199: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 17:02:11.676231: Epoch time: 108.99 s\n",
      "2025-05-18 17:02:12.205610: \n",
      "2025-05-18 17:02:12.205897: Epoch 486\n",
      "2025-05-18 17:02:12.206165: Current learning rate: 0.00549\n",
      "2025-05-18 17:04:01.272571: train_loss -0.965\n",
      "2025-05-18 17:04:01.272720: val_loss -0.9432\n",
      "2025-05-18 17:04:01.272756: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 17:04:01.272790: Epoch time: 109.07 s\n",
      "2025-05-18 17:04:01.814812: \n",
      "2025-05-18 17:04:01.814912: Epoch 487\n",
      "2025-05-18 17:04:01.814980: Current learning rate: 0.00548\n",
      "2025-05-18 17:05:50.843880: train_loss -0.9648\n",
      "2025-05-18 17:05:50.844085: val_loss -0.9488\n",
      "2025-05-18 17:05:50.844146: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-18 17:05:50.844185: Epoch time: 109.03 s\n",
      "2025-05-18 17:05:51.385426: \n",
      "2025-05-18 17:05:51.385522: Epoch 488\n",
      "2025-05-18 17:05:51.385592: Current learning rate: 0.00547\n",
      "2025-05-18 17:07:40.426999: train_loss -0.9651\n",
      "2025-05-18 17:07:40.427140: val_loss -0.9414\n",
      "2025-05-18 17:07:40.427176: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 17:07:40.427208: Epoch time: 109.04 s\n",
      "2025-05-18 17:07:41.203519: \n",
      "2025-05-18 17:07:41.203707: Epoch 489\n",
      "2025-05-18 17:07:41.203776: Current learning rate: 0.00546\n",
      "2025-05-18 17:09:30.244080: train_loss -0.9644\n",
      "2025-05-18 17:09:30.244217: val_loss -0.9459\n",
      "2025-05-18 17:09:30.244257: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 17:09:30.244324: Epoch time: 109.04 s\n",
      "2025-05-18 17:09:30.782226: \n",
      "2025-05-18 17:09:30.782336: Epoch 490\n",
      "2025-05-18 17:09:30.782404: Current learning rate: 0.00546\n",
      "2025-05-18 17:11:19.919390: train_loss -0.9648\n",
      "2025-05-18 17:11:19.919560: val_loss -0.9391\n",
      "2025-05-18 17:11:19.919599: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-18 17:11:19.919647: Epoch time: 109.14 s\n",
      "2025-05-18 17:11:20.506335: \n",
      "2025-05-18 17:11:20.506447: Epoch 491\n",
      "2025-05-18 17:11:20.506516: Current learning rate: 0.00545\n",
      "2025-05-18 17:13:09.683429: train_loss -0.9626\n",
      "2025-05-18 17:13:09.683748: val_loss -0.9433\n",
      "2025-05-18 17:13:09.683857: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 17:13:09.683935: Epoch time: 109.18 s\n",
      "2025-05-18 17:13:10.283559: \n",
      "2025-05-18 17:13:10.284153: Epoch 492\n",
      "2025-05-18 17:13:10.284390: Current learning rate: 0.00544\n",
      "2025-05-18 17:14:59.465448: train_loss -0.9635\n",
      "2025-05-18 17:14:59.465620: val_loss -0.9454\n",
      "2025-05-18 17:14:59.465659: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-18 17:14:59.465695: Epoch time: 109.18 s\n",
      "2025-05-18 17:15:00.055188: \n",
      "2025-05-18 17:15:00.055615: Epoch 493\n",
      "2025-05-18 17:15:00.056000: Current learning rate: 0.00543\n",
      "2025-05-18 17:16:49.243952: train_loss -0.9644\n",
      "2025-05-18 17:16:49.244311: val_loss -0.944\n",
      "2025-05-18 17:16:49.244363: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-18 17:16:49.244401: Epoch time: 109.19 s\n",
      "2025-05-18 17:16:49.823581: \n",
      "2025-05-18 17:16:49.823723: Epoch 494\n",
      "2025-05-18 17:16:49.823800: Current learning rate: 0.00542\n",
      "2025-05-18 17:18:38.982057: train_loss -0.9662\n",
      "2025-05-18 17:18:38.982227: val_loss -0.9462\n",
      "2025-05-18 17:18:38.982263: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 17:18:38.982301: Epoch time: 109.16 s\n",
      "2025-05-18 17:18:39.566921: \n",
      "2025-05-18 17:18:39.567192: Epoch 495\n",
      "2025-05-18 17:18:39.567476: Current learning rate: 0.00541\n",
      "2025-05-18 17:20:28.681222: train_loss -0.9659\n",
      "2025-05-18 17:20:28.681349: val_loss -0.9458\n",
      "2025-05-18 17:20:28.681383: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-18 17:20:28.681444: Epoch time: 109.11 s\n",
      "2025-05-18 17:20:29.219236: \n",
      "2025-05-18 17:20:29.219642: Epoch 496\n",
      "2025-05-18 17:20:29.219753: Current learning rate: 0.0054\n",
      "2025-05-18 17:22:18.323232: train_loss -0.9634\n",
      "2025-05-18 17:22:18.323381: val_loss -0.9484\n",
      "2025-05-18 17:22:18.323416: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-18 17:22:18.323450: Epoch time: 109.1 s\n",
      "2025-05-18 17:22:18.899887: \n",
      "2025-05-18 17:22:18.900001: Epoch 497\n",
      "2025-05-18 17:22:18.900123: Current learning rate: 0.00539\n",
      "2025-05-18 17:24:08.025255: train_loss -0.9625\n",
      "2025-05-18 17:24:08.025414: val_loss -0.9368\n",
      "2025-05-18 17:24:08.025449: Pseudo dice [np.float32(0.9743)]\n",
      "2025-05-18 17:24:08.025484: Epoch time: 109.13 s\n",
      "2025-05-18 17:24:08.599799: \n",
      "2025-05-18 17:24:08.599916: Epoch 498\n",
      "2025-05-18 17:24:08.599995: Current learning rate: 0.00538\n",
      "2025-05-18 17:25:57.645789: train_loss -0.9637\n",
      "2025-05-18 17:25:57.645911: val_loss -0.9257\n",
      "2025-05-18 17:25:57.645945: Pseudo dice [np.float32(0.9702)]\n",
      "2025-05-18 17:25:57.645979: Epoch time: 109.05 s\n",
      "2025-05-18 17:25:58.158347: \n",
      "2025-05-18 17:25:58.158563: Epoch 499\n",
      "2025-05-18 17:25:58.158643: Current learning rate: 0.00537\n",
      "2025-05-18 17:27:47.168702: train_loss -0.9628\n",
      "2025-05-18 17:27:47.168819: val_loss -0.9418\n",
      "2025-05-18 17:27:47.168854: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 17:27:47.168889: Epoch time: 109.01 s\n",
      "2025-05-18 17:27:47.912551: \n",
      "2025-05-18 17:27:47.912639: Epoch 500\n",
      "2025-05-18 17:27:47.912704: Current learning rate: 0.00536\n",
      "2025-05-18 17:29:36.916779: train_loss -0.964\n",
      "2025-05-18 17:29:36.916900: val_loss -0.9394\n",
      "2025-05-18 17:29:36.916931: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 17:29:36.916963: Epoch time: 109.0 s\n",
      "2025-05-18 17:29:37.435030: \n",
      "2025-05-18 17:29:37.435172: Epoch 501\n",
      "2025-05-18 17:29:37.435308: Current learning rate: 0.00535\n",
      "2025-05-18 17:31:26.446628: train_loss -0.9596\n",
      "2025-05-18 17:31:26.446813: val_loss -0.9441\n",
      "2025-05-18 17:31:26.446845: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-18 17:31:26.446877: Epoch time: 109.01 s\n",
      "2025-05-18 17:31:27.145480: \n",
      "2025-05-18 17:31:27.145660: Epoch 502\n",
      "2025-05-18 17:31:27.145744: Current learning rate: 0.00534\n",
      "2025-05-18 17:33:16.156411: train_loss -0.9648\n",
      "2025-05-18 17:33:16.156539: val_loss -0.9546\n",
      "2025-05-18 17:33:16.156571: Pseudo dice [np.float32(0.9809)]\n",
      "2025-05-18 17:33:16.156605: Epoch time: 109.01 s\n",
      "2025-05-18 17:33:16.677444: \n",
      "2025-05-18 17:33:16.677542: Epoch 503\n",
      "2025-05-18 17:33:16.677629: Current learning rate: 0.00533\n",
      "2025-05-18 17:35:05.698597: train_loss -0.9632\n",
      "2025-05-18 17:35:05.698727: val_loss -0.9433\n",
      "2025-05-18 17:35:05.698780: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 17:35:05.698818: Epoch time: 109.02 s\n",
      "2025-05-18 17:35:06.211905: \n",
      "2025-05-18 17:35:06.212065: Epoch 504\n",
      "2025-05-18 17:35:06.212135: Current learning rate: 0.00532\n",
      "2025-05-18 17:36:55.267396: train_loss -0.9619\n",
      "2025-05-18 17:36:55.267518: val_loss -0.9265\n",
      "2025-05-18 17:36:55.267551: Pseudo dice [np.float32(0.9705)]\n",
      "2025-05-18 17:36:55.267589: Epoch time: 109.06 s\n",
      "2025-05-18 17:36:55.795668: \n",
      "2025-05-18 17:36:55.795762: Epoch 505\n",
      "2025-05-18 17:36:55.795825: Current learning rate: 0.00531\n",
      "2025-05-18 17:38:44.843376: train_loss -0.9577\n",
      "2025-05-18 17:38:44.843501: val_loss -0.943\n",
      "2025-05-18 17:38:44.843536: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-18 17:38:44.843570: Epoch time: 109.05 s\n",
      "2025-05-18 17:38:45.355016: \n",
      "2025-05-18 17:38:45.355185: Epoch 506\n",
      "2025-05-18 17:38:45.355278: Current learning rate: 0.0053\n",
      "2025-05-18 17:40:34.327092: train_loss -0.9606\n",
      "2025-05-18 17:40:34.327217: val_loss -0.9014\n",
      "2025-05-18 17:40:34.327257: Pseudo dice [np.float32(0.9558)]\n",
      "2025-05-18 17:40:34.327288: Epoch time: 108.97 s\n",
      "2025-05-18 17:40:34.847572: \n",
      "2025-05-18 17:40:34.848015: Epoch 507\n",
      "2025-05-18 17:40:34.848264: Current learning rate: 0.00529\n",
      "2025-05-18 17:42:23.909854: train_loss -0.9546\n",
      "2025-05-18 17:42:23.910017: val_loss -0.9463\n",
      "2025-05-18 17:42:23.910049: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-18 17:42:23.910082: Epoch time: 109.06 s\n",
      "2025-05-18 17:42:24.437383: \n",
      "2025-05-18 17:42:24.437644: Epoch 508\n",
      "2025-05-18 17:42:24.437755: Current learning rate: 0.00528\n",
      "2025-05-18 17:44:13.502047: train_loss -0.9583\n",
      "2025-05-18 17:44:13.502190: val_loss -0.9442\n",
      "2025-05-18 17:44:13.502224: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 17:44:13.502260: Epoch time: 109.07 s\n",
      "2025-05-18 17:44:14.022586: \n",
      "2025-05-18 17:44:14.022834: Epoch 509\n",
      "2025-05-18 17:44:14.023009: Current learning rate: 0.00527\n",
      "2025-05-18 17:46:03.093613: train_loss -0.9566\n",
      "2025-05-18 17:46:03.093810: val_loss -0.9418\n",
      "2025-05-18 17:46:03.093848: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-18 17:46:03.093883: Epoch time: 109.07 s\n",
      "2025-05-18 17:46:03.617117: \n",
      "2025-05-18 17:46:03.617439: Epoch 510\n",
      "2025-05-18 17:46:03.617535: Current learning rate: 0.00526\n",
      "2025-05-18 17:47:52.700233: train_loss -0.9571\n",
      "2025-05-18 17:47:52.700402: val_loss -0.9379\n",
      "2025-05-18 17:47:52.700436: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-18 17:47:52.700471: Epoch time: 109.08 s\n",
      "2025-05-18 17:47:53.230962: \n",
      "2025-05-18 17:47:53.231286: Epoch 511\n",
      "2025-05-18 17:47:53.231499: Current learning rate: 0.00525\n",
      "2025-05-18 17:49:42.318726: train_loss -0.9545\n",
      "2025-05-18 17:49:42.318855: val_loss -0.9348\n",
      "2025-05-18 17:49:42.318888: Pseudo dice [np.float32(0.9713)]\n",
      "2025-05-18 17:49:42.318922: Epoch time: 109.09 s\n",
      "2025-05-18 17:49:42.851570: \n",
      "2025-05-18 17:49:42.851713: Epoch 512\n",
      "2025-05-18 17:49:42.851791: Current learning rate: 0.00524\n",
      "2025-05-18 17:51:31.884674: train_loss -0.9624\n",
      "2025-05-18 17:51:31.884846: val_loss -0.9415\n",
      "2025-05-18 17:51:31.884878: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-18 17:51:31.884943: Epoch time: 109.03 s\n",
      "2025-05-18 17:51:32.415447: \n",
      "2025-05-18 17:51:32.415539: Epoch 513\n",
      "2025-05-18 17:51:32.415604: Current learning rate: 0.00523\n",
      "2025-05-18 17:53:21.432571: train_loss -0.9622\n",
      "2025-05-18 17:53:21.432702: val_loss -0.9368\n",
      "2025-05-18 17:53:21.432736: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-18 17:53:21.432772: Epoch time: 109.02 s\n",
      "2025-05-18 17:53:21.973991: \n",
      "2025-05-18 17:53:21.974597: Epoch 514\n",
      "2025-05-18 17:53:21.974738: Current learning rate: 0.00522\n",
      "2025-05-18 17:55:11.026109: train_loss -0.9647\n",
      "2025-05-18 17:55:11.026272: val_loss -0.9564\n",
      "2025-05-18 17:55:11.026311: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-18 17:55:11.026346: Epoch time: 109.05 s\n",
      "2025-05-18 17:55:11.771807: \n",
      "2025-05-18 17:55:11.771992: Epoch 515\n",
      "2025-05-18 17:55:11.772069: Current learning rate: 0.00521\n",
      "2025-05-18 17:57:00.838778: train_loss -0.9644\n",
      "2025-05-18 17:57:00.838950: val_loss -0.9435\n",
      "2025-05-18 17:57:00.838982: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-18 17:57:00.839016: Epoch time: 109.07 s\n",
      "2025-05-18 17:57:01.375892: \n",
      "2025-05-18 17:57:01.376066: Epoch 516\n",
      "2025-05-18 17:57:01.376146: Current learning rate: 0.0052\n",
      "2025-05-18 17:58:50.437384: train_loss -0.9633\n",
      "2025-05-18 17:58:50.437520: val_loss -0.9498\n",
      "2025-05-18 17:58:50.437561: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 17:58:50.437599: Epoch time: 109.06 s\n",
      "2025-05-18 17:58:50.989935: \n",
      "2025-05-18 17:58:50.990028: Epoch 517\n",
      "2025-05-18 17:58:50.990091: Current learning rate: 0.00519\n",
      "2025-05-18 18:00:40.016434: train_loss -0.9638\n",
      "2025-05-18 18:00:40.016790: val_loss -0.944\n",
      "2025-05-18 18:00:40.016898: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 18:00:40.016942: Epoch time: 109.03 s\n",
      "2025-05-18 18:00:40.572129: \n",
      "2025-05-18 18:00:40.572583: Epoch 518\n",
      "2025-05-18 18:00:40.572708: Current learning rate: 0.00518\n",
      "2025-05-18 18:02:29.591721: train_loss -0.9645\n",
      "2025-05-18 18:02:29.591852: val_loss -0.9451\n",
      "2025-05-18 18:02:29.591886: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 18:02:29.591926: Epoch time: 109.02 s\n",
      "2025-05-18 18:02:30.133524: \n",
      "2025-05-18 18:02:30.133692: Epoch 519\n",
      "2025-05-18 18:02:30.133758: Current learning rate: 0.00518\n",
      "2025-05-18 18:04:19.191627: train_loss -0.964\n",
      "2025-05-18 18:04:19.191765: val_loss -0.946\n",
      "2025-05-18 18:04:19.191798: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 18:04:19.191833: Epoch time: 109.06 s\n",
      "2025-05-18 18:04:19.744297: \n",
      "2025-05-18 18:04:19.744572: Epoch 520\n",
      "2025-05-18 18:04:19.744723: Current learning rate: 0.00517\n",
      "2025-05-18 18:06:08.789850: train_loss -0.9605\n",
      "2025-05-18 18:06:08.790033: val_loss -0.9508\n",
      "2025-05-18 18:06:08.790067: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-18 18:06:08.790100: Epoch time: 109.05 s\n",
      "2025-05-18 18:06:09.335193: \n",
      "2025-05-18 18:06:09.335305: Epoch 521\n",
      "2025-05-18 18:06:09.335374: Current learning rate: 0.00516\n",
      "2025-05-18 18:07:58.340696: train_loss -0.9642\n",
      "2025-05-18 18:07:58.340809: val_loss -0.9382\n",
      "2025-05-18 18:07:58.340839: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 18:07:58.340873: Epoch time: 109.01 s\n",
      "2025-05-18 18:07:58.869154: \n",
      "2025-05-18 18:07:58.869242: Epoch 522\n",
      "2025-05-18 18:07:58.869303: Current learning rate: 0.00515\n",
      "2025-05-18 18:09:47.867977: train_loss -0.9654\n",
      "2025-05-18 18:09:47.868121: val_loss -0.9385\n",
      "2025-05-18 18:09:47.868155: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-18 18:09:47.868188: Epoch time: 109.0 s\n",
      "2025-05-18 18:09:48.401323: \n",
      "2025-05-18 18:09:48.401633: Epoch 523\n",
      "2025-05-18 18:09:48.401726: Current learning rate: 0.00514\n",
      "2025-05-18 18:11:37.443306: train_loss -0.9657\n",
      "2025-05-18 18:11:37.443441: val_loss -0.9498\n",
      "2025-05-18 18:11:37.443475: Pseudo dice [np.float32(0.9803)]\n",
      "2025-05-18 18:11:37.443510: Epoch time: 109.04 s\n",
      "2025-05-18 18:11:37.980699: \n",
      "2025-05-18 18:11:37.980865: Epoch 524\n",
      "2025-05-18 18:11:37.980945: Current learning rate: 0.00513\n",
      "2025-05-18 18:13:27.029508: train_loss -0.9646\n",
      "2025-05-18 18:13:27.029644: val_loss -0.9402\n",
      "2025-05-18 18:13:27.029815: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-18 18:13:27.029959: Epoch time: 109.05 s\n",
      "2025-05-18 18:13:27.568442: \n",
      "2025-05-18 18:13:27.568534: Epoch 525\n",
      "2025-05-18 18:13:27.568599: Current learning rate: 0.00512\n",
      "2025-05-18 18:15:16.593551: train_loss -0.9654\n",
      "2025-05-18 18:15:16.593687: val_loss -0.9408\n",
      "2025-05-18 18:15:16.593719: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 18:15:16.593753: Epoch time: 109.03 s\n",
      "2025-05-18 18:15:17.133910: \n",
      "2025-05-18 18:15:17.134100: Epoch 526\n",
      "2025-05-18 18:15:17.134171: Current learning rate: 0.00511\n",
      "2025-05-18 18:17:06.174082: train_loss -0.9647\n",
      "2025-05-18 18:17:06.174350: val_loss -0.9384\n",
      "2025-05-18 18:17:06.174390: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-18 18:17:06.174423: Epoch time: 109.04 s\n",
      "2025-05-18 18:17:06.718623: \n",
      "2025-05-18 18:17:06.718708: Epoch 527\n",
      "2025-05-18 18:17:06.718773: Current learning rate: 0.0051\n",
      "2025-05-18 18:18:55.745261: train_loss -0.9658\n",
      "2025-05-18 18:18:55.745517: val_loss -0.9532\n",
      "2025-05-18 18:18:55.745559: Pseudo dice [np.float32(0.9812)]\n",
      "2025-05-18 18:18:55.745593: Epoch time: 109.03 s\n",
      "2025-05-18 18:18:56.517350: \n",
      "2025-05-18 18:18:56.517524: Epoch 528\n",
      "2025-05-18 18:18:56.517647: Current learning rate: 0.00509\n",
      "2025-05-18 18:20:45.504750: train_loss -0.9651\n",
      "2025-05-18 18:20:45.504884: val_loss -0.9432\n",
      "2025-05-18 18:20:45.504921: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 18:20:45.504955: Epoch time: 108.99 s\n",
      "2025-05-18 18:20:46.048507: \n",
      "2025-05-18 18:20:46.048609: Epoch 529\n",
      "2025-05-18 18:20:46.048675: Current learning rate: 0.00508\n",
      "2025-05-18 18:22:35.039647: train_loss -0.9663\n",
      "2025-05-18 18:22:35.039835: val_loss -0.9405\n",
      "2025-05-18 18:22:35.039868: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-18 18:22:35.039903: Epoch time: 108.99 s\n",
      "2025-05-18 18:22:35.582214: \n",
      "2025-05-18 18:22:35.582458: Epoch 530\n",
      "2025-05-18 18:22:35.582578: Current learning rate: 0.00507\n",
      "2025-05-18 18:24:24.622208: train_loss -0.9672\n",
      "2025-05-18 18:24:24.622407: val_loss -0.945\n",
      "2025-05-18 18:24:24.622451: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 18:24:24.622581: Epoch time: 109.04 s\n",
      "2025-05-18 18:24:25.151204: \n",
      "2025-05-18 18:24:25.151401: Epoch 531\n",
      "2025-05-18 18:24:25.151475: Current learning rate: 0.00506\n",
      "2025-05-18 18:26:14.138105: train_loss -0.9655\n",
      "2025-05-18 18:26:14.138239: val_loss -0.9409\n",
      "2025-05-18 18:26:14.138274: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 18:26:14.138342: Epoch time: 108.99 s\n",
      "2025-05-18 18:26:14.665836: \n",
      "2025-05-18 18:26:14.666189: Epoch 532\n",
      "2025-05-18 18:26:14.666447: Current learning rate: 0.00505\n",
      "2025-05-18 18:28:03.714236: train_loss -0.9671\n",
      "2025-05-18 18:28:03.714355: val_loss -0.9406\n",
      "2025-05-18 18:28:03.714390: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 18:28:03.714422: Epoch time: 109.05 s\n",
      "2025-05-18 18:28:04.228739: \n",
      "2025-05-18 18:28:04.228882: Epoch 533\n",
      "2025-05-18 18:28:04.228947: Current learning rate: 0.00504\n",
      "2025-05-18 18:29:53.265415: train_loss -0.9654\n",
      "2025-05-18 18:29:53.265701: val_loss -0.9532\n",
      "2025-05-18 18:29:53.265762: Pseudo dice [np.float32(0.9803)]\n",
      "2025-05-18 18:29:53.265802: Epoch time: 109.04 s\n",
      "2025-05-18 18:29:53.786116: \n",
      "2025-05-18 18:29:53.786209: Epoch 534\n",
      "2025-05-18 18:29:53.786272: Current learning rate: 0.00503\n",
      "2025-05-18 18:31:42.736340: train_loss -0.9652\n",
      "2025-05-18 18:31:42.736460: val_loss -0.9396\n",
      "2025-05-18 18:31:42.736493: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 18:31:42.736586: Epoch time: 108.95 s\n",
      "2025-05-18 18:31:43.250880: \n",
      "2025-05-18 18:31:43.251032: Epoch 535\n",
      "2025-05-18 18:31:43.251105: Current learning rate: 0.00502\n",
      "2025-05-18 18:33:32.226058: train_loss -0.9644\n",
      "2025-05-18 18:33:32.226178: val_loss -0.9491\n",
      "2025-05-18 18:33:32.226210: Pseudo dice [np.float32(0.9797)]\n",
      "2025-05-18 18:33:32.226243: Epoch time: 108.98 s\n",
      "2025-05-18 18:33:32.751224: \n",
      "2025-05-18 18:33:32.751304: Epoch 536\n",
      "2025-05-18 18:33:32.751368: Current learning rate: 0.00501\n",
      "2025-05-18 18:35:21.739289: train_loss -0.9657\n",
      "2025-05-18 18:35:21.739412: val_loss -0.951\n",
      "2025-05-18 18:35:21.739445: Pseudo dice [np.float32(0.9804)]\n",
      "2025-05-18 18:35:21.739476: Epoch time: 108.99 s\n",
      "2025-05-18 18:35:22.252691: \n",
      "2025-05-18 18:35:22.252833: Epoch 537\n",
      "2025-05-18 18:35:22.252908: Current learning rate: 0.005\n",
      "2025-05-18 18:37:11.225546: train_loss -0.9661\n",
      "2025-05-18 18:37:11.225670: val_loss -0.9459\n",
      "2025-05-18 18:37:11.225703: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-18 18:37:11.225737: Epoch time: 108.97 s\n",
      "2025-05-18 18:37:11.745442: \n",
      "2025-05-18 18:37:11.745764: Epoch 538\n",
      "2025-05-18 18:37:11.745867: Current learning rate: 0.00499\n",
      "2025-05-18 18:39:00.700527: train_loss -0.9664\n",
      "2025-05-18 18:39:00.700705: val_loss -0.9446\n",
      "2025-05-18 18:39:00.700738: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 18:39:00.700771: Epoch time: 108.96 s\n",
      "2025-05-18 18:39:01.215943: \n",
      "2025-05-18 18:39:01.216021: Epoch 539\n",
      "2025-05-18 18:39:01.216082: Current learning rate: 0.00498\n",
      "2025-05-18 18:40:50.222431: train_loss -0.9669\n",
      "2025-05-18 18:40:50.222582: val_loss -0.9517\n",
      "2025-05-18 18:40:50.222677: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-18 18:40:50.222748: Epoch time: 109.01 s\n",
      "2025-05-18 18:40:50.748944: \n",
      "2025-05-18 18:40:50.749236: Epoch 540\n",
      "2025-05-18 18:40:50.749352: Current learning rate: 0.00497\n",
      "2025-05-18 18:42:39.763386: train_loss -0.9665\n",
      "2025-05-18 18:42:39.763607: val_loss -0.9455\n",
      "2025-05-18 18:42:39.763646: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-18 18:42:39.763685: Epoch time: 109.01 s\n",
      "2025-05-18 18:42:40.472075: \n",
      "2025-05-18 18:42:40.472364: Epoch 541\n",
      "2025-05-18 18:42:40.472533: Current learning rate: 0.00496\n",
      "2025-05-18 18:44:29.427391: train_loss -0.9661\n",
      "2025-05-18 18:44:29.427511: val_loss -0.9434\n",
      "2025-05-18 18:44:29.427599: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-18 18:44:29.427692: Epoch time: 108.96 s\n",
      "2025-05-18 18:44:29.944286: \n",
      "2025-05-18 18:44:29.944382: Epoch 542\n",
      "2025-05-18 18:44:29.944447: Current learning rate: 0.00495\n",
      "2025-05-18 18:46:18.908133: train_loss -0.9645\n",
      "2025-05-18 18:46:18.908306: val_loss -0.9494\n",
      "2025-05-18 18:46:18.908395: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 18:46:18.908432: Epoch time: 108.96 s\n",
      "2025-05-18 18:46:19.430256: \n",
      "2025-05-18 18:46:19.430507: Epoch 543\n",
      "2025-05-18 18:46:19.430594: Current learning rate: 0.00494\n",
      "2025-05-18 18:48:08.425147: train_loss -0.9667\n",
      "2025-05-18 18:48:08.425327: val_loss -0.9434\n",
      "2025-05-18 18:48:08.425474: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 18:48:08.425559: Epoch time: 109.0 s\n",
      "2025-05-18 18:48:08.939926: \n",
      "2025-05-18 18:48:08.940018: Epoch 544\n",
      "2025-05-18 18:48:08.940081: Current learning rate: 0.00493\n",
      "2025-05-18 18:49:58.010152: train_loss -0.9677\n",
      "2025-05-18 18:49:58.010275: val_loss -0.9367\n",
      "2025-05-18 18:49:58.010307: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-18 18:49:58.010338: Epoch time: 109.07 s\n",
      "2025-05-18 18:49:58.529310: \n",
      "2025-05-18 18:49:58.529419: Epoch 545\n",
      "2025-05-18 18:49:58.529573: Current learning rate: 0.00492\n",
      "2025-05-18 18:51:47.503057: train_loss -0.9674\n",
      "2025-05-18 18:51:47.503220: val_loss -0.9492\n",
      "2025-05-18 18:51:47.503382: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-18 18:51:47.503452: Epoch time: 108.97 s\n",
      "2025-05-18 18:51:48.023916: \n",
      "2025-05-18 18:51:48.024198: Epoch 546\n",
      "2025-05-18 18:51:48.024297: Current learning rate: 0.00491\n",
      "2025-05-18 18:53:37.074503: train_loss -0.9689\n",
      "2025-05-18 18:53:37.075024: val_loss -0.9318\n",
      "2025-05-18 18:53:37.075147: Pseudo dice [np.float32(0.973)]\n",
      "2025-05-18 18:53:37.075193: Epoch time: 109.05 s\n",
      "2025-05-18 18:53:37.597750: \n",
      "2025-05-18 18:53:37.597926: Epoch 547\n",
      "2025-05-18 18:53:37.598006: Current learning rate: 0.0049\n",
      "2025-05-18 18:55:26.639619: train_loss -0.9665\n",
      "2025-05-18 18:55:26.639793: val_loss -0.9414\n",
      "2025-05-18 18:55:26.639891: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 18:55:26.639935: Epoch time: 109.04 s\n",
      "2025-05-18 18:55:27.168671: \n",
      "2025-05-18 18:55:27.168802: Epoch 548\n",
      "2025-05-18 18:55:27.168907: Current learning rate: 0.00489\n",
      "2025-05-18 18:57:16.140574: train_loss -0.9669\n",
      "2025-05-18 18:57:16.140751: val_loss -0.9523\n",
      "2025-05-18 18:57:16.140784: Pseudo dice [np.float32(0.9803)]\n",
      "2025-05-18 18:57:16.140815: Epoch time: 108.97 s\n",
      "2025-05-18 18:57:16.658290: \n",
      "2025-05-18 18:57:16.658390: Epoch 549\n",
      "2025-05-18 18:57:16.658453: Current learning rate: 0.00488\n",
      "2025-05-18 18:59:05.675970: train_loss -0.9601\n",
      "2025-05-18 18:59:05.676101: val_loss -0.9437\n",
      "2025-05-18 18:59:05.676138: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-18 18:59:05.676173: Epoch time: 109.02 s\n",
      "2025-05-18 18:59:06.421419: \n",
      "2025-05-18 18:59:06.421623: Epoch 550\n",
      "2025-05-18 18:59:06.421711: Current learning rate: 0.00487\n",
      "2025-05-18 19:00:55.432785: train_loss -0.9618\n",
      "2025-05-18 19:00:55.432905: val_loss -0.9419\n",
      "2025-05-18 19:00:55.432938: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 19:00:55.432970: Epoch time: 109.01 s\n",
      "2025-05-18 19:00:55.951497: \n",
      "2025-05-18 19:00:55.951583: Epoch 551\n",
      "2025-05-18 19:00:55.951647: Current learning rate: 0.00486\n",
      "2025-05-18 19:02:44.883148: train_loss -0.9631\n",
      "2025-05-18 19:02:44.883269: val_loss -0.9401\n",
      "2025-05-18 19:02:44.883302: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 19:02:44.883336: Epoch time: 108.93 s\n",
      "2025-05-18 19:02:45.399306: \n",
      "2025-05-18 19:02:45.399431: Epoch 552\n",
      "2025-05-18 19:02:45.399842: Current learning rate: 0.00485\n",
      "2025-05-18 19:04:34.346496: train_loss -0.9598\n",
      "2025-05-18 19:04:34.346661: val_loss -0.9438\n",
      "2025-05-18 19:04:34.346695: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 19:04:34.346729: Epoch time: 108.95 s\n",
      "2025-05-18 19:04:35.063968: \n",
      "2025-05-18 19:04:35.064090: Epoch 553\n",
      "2025-05-18 19:04:35.064157: Current learning rate: 0.00484\n",
      "2025-05-18 19:06:24.029179: train_loss -0.9638\n",
      "2025-05-18 19:06:24.029447: val_loss -0.941\n",
      "2025-05-18 19:06:24.029488: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 19:06:24.029520: Epoch time: 108.97 s\n",
      "2025-05-18 19:06:24.555972: \n",
      "2025-05-18 19:06:24.556066: Epoch 554\n",
      "2025-05-18 19:06:24.556177: Current learning rate: 0.00484\n",
      "2025-05-18 19:08:13.569143: train_loss -0.9663\n",
      "2025-05-18 19:08:13.569258: val_loss -0.9422\n",
      "2025-05-18 19:08:13.569289: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 19:08:13.569321: Epoch time: 109.01 s\n",
      "2025-05-18 19:08:14.090890: \n",
      "2025-05-18 19:08:14.090991: Epoch 555\n",
      "2025-05-18 19:08:14.091057: Current learning rate: 0.00483\n",
      "2025-05-18 19:10:03.054597: train_loss -0.9645\n",
      "2025-05-18 19:10:03.054766: val_loss -0.9538\n",
      "2025-05-18 19:10:03.054808: Pseudo dice [np.float32(0.9809)]\n",
      "2025-05-18 19:10:03.054841: Epoch time: 108.96 s\n",
      "2025-05-18 19:10:03.577399: \n",
      "2025-05-18 19:10:03.577552: Epoch 556\n",
      "2025-05-18 19:10:03.577628: Current learning rate: 0.00482\n",
      "2025-05-18 19:11:52.595997: train_loss -0.9641\n",
      "2025-05-18 19:11:52.596263: val_loss -0.9385\n",
      "2025-05-18 19:11:52.596318: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 19:11:52.596357: Epoch time: 109.02 s\n",
      "2025-05-18 19:11:53.119710: \n",
      "2025-05-18 19:11:53.119894: Epoch 557\n",
      "2025-05-18 19:11:53.119992: Current learning rate: 0.00481\n",
      "2025-05-18 19:13:42.157255: train_loss -0.9646\n",
      "2025-05-18 19:13:42.157376: val_loss -0.9489\n",
      "2025-05-18 19:13:42.157407: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-18 19:13:42.157467: Epoch time: 109.04 s\n",
      "2025-05-18 19:13:42.678665: \n",
      "2025-05-18 19:13:42.678809: Epoch 558\n",
      "2025-05-18 19:13:42.678878: Current learning rate: 0.0048\n",
      "2025-05-18 19:15:31.642310: train_loss -0.9304\n",
      "2025-05-18 19:15:31.642502: val_loss -0.9192\n",
      "2025-05-18 19:15:31.642558: Pseudo dice [np.float32(0.9695)]\n",
      "2025-05-18 19:15:31.642598: Epoch time: 108.96 s\n",
      "2025-05-18 19:15:32.167948: \n",
      "2025-05-18 19:15:32.168136: Epoch 559\n",
      "2025-05-18 19:15:32.168211: Current learning rate: 0.00479\n",
      "2025-05-18 19:17:21.194718: train_loss -0.8942\n",
      "2025-05-18 19:17:21.194965: val_loss -0.9233\n",
      "2025-05-18 19:17:21.195006: Pseudo dice [np.float32(0.9672)]\n",
      "2025-05-18 19:17:21.195042: Epoch time: 109.03 s\n",
      "2025-05-18 19:17:21.720618: \n",
      "2025-05-18 19:17:21.721096: Epoch 560\n",
      "2025-05-18 19:17:21.721198: Current learning rate: 0.00478\n",
      "2025-05-18 19:19:10.733460: train_loss -0.8615\n",
      "2025-05-18 19:19:10.733582: val_loss -0.8722\n",
      "2025-05-18 19:19:10.733615: Pseudo dice [np.float32(0.951)]\n",
      "2025-05-18 19:19:10.733647: Epoch time: 109.01 s\n",
      "2025-05-18 19:19:11.257505: \n",
      "2025-05-18 19:19:11.257595: Epoch 561\n",
      "2025-05-18 19:19:11.257657: Current learning rate: 0.00477\n",
      "2025-05-18 19:21:00.244795: train_loss -0.8786\n",
      "2025-05-18 19:21:00.244956: val_loss -0.8991\n",
      "2025-05-18 19:21:00.245117: Pseudo dice [np.float32(0.9623)]\n",
      "2025-05-18 19:21:00.245203: Epoch time: 108.99 s\n",
      "2025-05-18 19:21:00.760067: \n",
      "2025-05-18 19:21:00.760360: Epoch 562\n",
      "2025-05-18 19:21:00.760459: Current learning rate: 0.00476\n",
      "2025-05-18 19:22:49.706751: train_loss -0.8642\n",
      "2025-05-18 19:22:49.707051: val_loss -0.8495\n",
      "2025-05-18 19:22:49.707108: Pseudo dice [np.float32(0.94)]\n",
      "2025-05-18 19:22:49.707149: Epoch time: 108.95 s\n",
      "2025-05-18 19:22:50.225279: \n",
      "2025-05-18 19:22:50.225524: Epoch 563\n",
      "2025-05-18 19:22:50.225610: Current learning rate: 0.00475\n",
      "2025-05-18 19:24:39.204147: train_loss -0.9147\n",
      "2025-05-18 19:24:39.204268: val_loss -0.914\n",
      "2025-05-18 19:24:39.204423: Pseudo dice [np.float32(0.9662)]\n",
      "2025-05-18 19:24:39.204495: Epoch time: 108.98 s\n",
      "2025-05-18 19:24:39.722527: \n",
      "2025-05-18 19:24:39.722671: Epoch 564\n",
      "2025-05-18 19:24:39.722790: Current learning rate: 0.00474\n",
      "2025-05-18 19:26:28.638963: train_loss -0.9083\n",
      "2025-05-18 19:26:28.639087: val_loss -0.8345\n",
      "2025-05-18 19:26:28.639119: Pseudo dice [np.float32(0.9374)]\n",
      "2025-05-18 19:26:28.639151: Epoch time: 108.92 s\n",
      "2025-05-18 19:26:29.158254: \n",
      "2025-05-18 19:26:29.158357: Epoch 565\n",
      "2025-05-18 19:26:29.158421: Current learning rate: 0.00473\n",
      "2025-05-18 19:28:18.067689: train_loss -0.9317\n",
      "2025-05-18 19:28:18.067806: val_loss -0.9344\n",
      "2025-05-18 19:28:18.067837: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-18 19:28:18.067882: Epoch time: 108.91 s\n",
      "2025-05-18 19:28:18.788614: \n",
      "2025-05-18 19:28:18.788801: Epoch 566\n",
      "2025-05-18 19:28:18.788888: Current learning rate: 0.00472\n",
      "2025-05-18 19:30:07.704061: train_loss -0.9422\n",
      "2025-05-18 19:30:07.704225: val_loss -0.9335\n",
      "2025-05-18 19:30:07.704257: Pseudo dice [np.float32(0.972)]\n",
      "2025-05-18 19:30:07.704290: Epoch time: 108.92 s\n",
      "2025-05-18 19:30:08.227278: \n",
      "2025-05-18 19:30:08.227415: Epoch 567\n",
      "2025-05-18 19:30:08.227615: Current learning rate: 0.00471\n",
      "2025-05-18 19:31:57.285240: train_loss -0.9454\n",
      "2025-05-18 19:31:57.285449: val_loss -0.9347\n",
      "2025-05-18 19:31:57.285622: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-18 19:31:57.285707: Epoch time: 109.06 s\n",
      "2025-05-18 19:31:57.812232: \n",
      "2025-05-18 19:31:57.812425: Epoch 568\n",
      "2025-05-18 19:31:57.812490: Current learning rate: 0.0047\n",
      "2025-05-18 19:33:46.843457: train_loss -0.9501\n",
      "2025-05-18 19:33:46.843580: val_loss -0.9383\n",
      "2025-05-18 19:33:46.843612: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-18 19:33:46.843645: Epoch time: 109.03 s\n",
      "2025-05-18 19:33:47.362802: \n",
      "2025-05-18 19:33:47.362958: Epoch 569\n",
      "2025-05-18 19:33:47.363036: Current learning rate: 0.00469\n",
      "2025-05-18 19:35:36.371264: train_loss -0.9504\n",
      "2025-05-18 19:35:36.371389: val_loss -0.9431\n",
      "2025-05-18 19:35:36.371423: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 19:35:36.371456: Epoch time: 109.01 s\n",
      "2025-05-18 19:35:36.892975: \n",
      "2025-05-18 19:35:36.893231: Epoch 570\n",
      "2025-05-18 19:35:36.893378: Current learning rate: 0.00468\n",
      "2025-05-18 19:37:25.949209: train_loss -0.9509\n",
      "2025-05-18 19:37:25.949329: val_loss -0.9375\n",
      "2025-05-18 19:37:25.949361: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-18 19:37:25.949392: Epoch time: 109.06 s\n",
      "2025-05-18 19:37:26.471012: \n",
      "2025-05-18 19:37:26.471132: Epoch 571\n",
      "2025-05-18 19:37:26.471217: Current learning rate: 0.00467\n",
      "2025-05-18 19:39:15.408351: train_loss -0.9556\n",
      "2025-05-18 19:39:15.408522: val_loss -0.9275\n",
      "2025-05-18 19:39:15.408689: Pseudo dice [np.float32(0.9698)]\n",
      "2025-05-18 19:39:15.409182: Epoch time: 108.94 s\n",
      "2025-05-18 19:39:15.936591: \n",
      "2025-05-18 19:39:15.936749: Epoch 572\n",
      "2025-05-18 19:39:15.936816: Current learning rate: 0.00466\n",
      "2025-05-18 19:41:04.931132: train_loss -0.9543\n",
      "2025-05-18 19:41:04.931251: val_loss -0.9479\n",
      "2025-05-18 19:41:04.931282: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-18 19:41:04.931314: Epoch time: 109.0 s\n",
      "2025-05-18 19:41:05.466028: \n",
      "2025-05-18 19:41:05.466181: Epoch 573\n",
      "2025-05-18 19:41:05.466247: Current learning rate: 0.00465\n",
      "2025-05-18 19:42:54.469645: train_loss -0.954\n",
      "2025-05-18 19:42:54.469805: val_loss -0.936\n",
      "2025-05-18 19:42:54.469845: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-18 19:42:54.469886: Epoch time: 109.0 s\n",
      "2025-05-18 19:42:54.996454: \n",
      "2025-05-18 19:42:54.996539: Epoch 574\n",
      "2025-05-18 19:42:54.996604: Current learning rate: 0.00464\n",
      "2025-05-18 19:44:44.005115: train_loss -0.9521\n",
      "2025-05-18 19:44:44.005235: val_loss -0.9495\n",
      "2025-05-18 19:44:44.005269: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-18 19:44:44.005302: Epoch time: 109.01 s\n",
      "2025-05-18 19:44:44.529665: \n",
      "2025-05-18 19:44:44.529818: Epoch 575\n",
      "2025-05-18 19:44:44.529886: Current learning rate: 0.00463\n",
      "2025-05-18 19:46:33.601180: train_loss -0.9463\n",
      "2025-05-18 19:46:33.601765: val_loss -0.9407\n",
      "2025-05-18 19:46:33.601806: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-18 19:46:33.601842: Epoch time: 109.07 s\n",
      "2025-05-18 19:46:34.131530: \n",
      "2025-05-18 19:46:34.131620: Epoch 576\n",
      "2025-05-18 19:46:34.131684: Current learning rate: 0.00462\n",
      "2025-05-18 19:48:23.162583: train_loss -0.9403\n",
      "2025-05-18 19:48:23.162718: val_loss -0.9225\n",
      "2025-05-18 19:48:23.162750: Pseudo dice [np.float32(0.9676)]\n",
      "2025-05-18 19:48:23.162783: Epoch time: 109.03 s\n",
      "2025-05-18 19:48:23.692613: \n",
      "2025-05-18 19:48:23.692771: Epoch 577\n",
      "2025-05-18 19:48:23.693213: Current learning rate: 0.00461\n",
      "2025-05-18 19:50:12.731315: train_loss -0.9494\n",
      "2025-05-18 19:50:12.731445: val_loss -0.9381\n",
      "2025-05-18 19:50:12.731478: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-18 19:50:12.731514: Epoch time: 109.04 s\n",
      "2025-05-18 19:50:13.268017: \n",
      "2025-05-18 19:50:13.268101: Epoch 578\n",
      "2025-05-18 19:50:13.268163: Current learning rate: 0.0046\n",
      "2025-05-18 19:52:02.288259: train_loss -0.9534\n",
      "2025-05-18 19:52:02.288418: val_loss -0.9348\n",
      "2025-05-18 19:52:02.288450: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-18 19:52:02.288486: Epoch time: 109.02 s\n",
      "2025-05-18 19:52:03.032631: \n",
      "2025-05-18 19:52:03.032843: Epoch 579\n",
      "2025-05-18 19:52:03.033003: Current learning rate: 0.00459\n",
      "2025-05-18 19:53:52.821826: train_loss -0.9549\n",
      "2025-05-18 19:53:52.821955: val_loss -0.9377\n",
      "2025-05-18 19:53:52.821990: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-18 19:53:52.822025: Epoch time: 109.79 s\n",
      "2025-05-18 19:53:53.377581: \n",
      "2025-05-18 19:53:53.377852: Epoch 580\n",
      "2025-05-18 19:53:53.377977: Current learning rate: 0.00458\n",
      "2025-05-18 19:55:42.879439: train_loss -0.9539\n",
      "2025-05-18 19:55:42.879574: val_loss -0.9379\n",
      "2025-05-18 19:55:42.879607: Pseudo dice [np.float32(0.9744)]\n",
      "2025-05-18 19:55:42.879650: Epoch time: 109.5 s\n",
      "2025-05-18 19:55:43.422778: \n",
      "2025-05-18 19:55:43.422876: Epoch 581\n",
      "2025-05-18 19:55:43.422941: Current learning rate: 0.00457\n",
      "2025-05-18 19:57:32.843856: train_loss -0.9543\n",
      "2025-05-18 19:57:32.843986: val_loss -0.9297\n",
      "2025-05-18 19:57:32.844036: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-18 19:57:32.844072: Epoch time: 109.42 s\n",
      "2025-05-18 19:57:33.397403: \n",
      "2025-05-18 19:57:33.397506: Epoch 582\n",
      "2025-05-18 19:57:33.397573: Current learning rate: 0.00456\n",
      "2025-05-18 19:59:22.930414: train_loss -0.9557\n",
      "2025-05-18 19:59:22.930560: val_loss -0.9458\n",
      "2025-05-18 19:59:22.930595: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 19:59:22.930630: Epoch time: 109.53 s\n",
      "2025-05-18 19:59:23.491412: \n",
      "2025-05-18 19:59:23.491581: Epoch 583\n",
      "2025-05-18 19:59:23.491666: Current learning rate: 0.00455\n",
      "2025-05-18 20:01:12.929747: train_loss -0.9557\n",
      "2025-05-18 20:01:12.929906: val_loss -0.9256\n",
      "2025-05-18 20:01:12.929941: Pseudo dice [np.float32(0.9694)]\n",
      "2025-05-18 20:01:12.929978: Epoch time: 109.44 s\n",
      "2025-05-18 20:01:13.494004: \n",
      "2025-05-18 20:01:13.494203: Epoch 584\n",
      "2025-05-18 20:01:13.494278: Current learning rate: 0.00454\n",
      "2025-05-18 20:03:02.984434: train_loss -0.9535\n",
      "2025-05-18 20:03:02.984576: val_loss -0.9459\n",
      "2025-05-18 20:03:02.984609: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 20:03:02.984644: Epoch time: 109.49 s\n",
      "2025-05-18 20:03:03.548744: \n",
      "2025-05-18 20:03:03.548837: Epoch 585\n",
      "2025-05-18 20:03:03.548905: Current learning rate: 0.00453\n",
      "2025-05-18 20:04:53.023329: train_loss -0.9559\n",
      "2025-05-18 20:04:53.023510: val_loss -0.9387\n",
      "2025-05-18 20:04:53.023712: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-18 20:04:53.023833: Epoch time: 109.48 s\n",
      "2025-05-18 20:04:53.589675: \n",
      "2025-05-18 20:04:53.589867: Epoch 586\n",
      "2025-05-18 20:04:53.589945: Current learning rate: 0.00452\n",
      "2025-05-18 20:06:43.051377: train_loss -0.9565\n",
      "2025-05-18 20:06:43.051569: val_loss -0.9436\n",
      "2025-05-18 20:06:43.051745: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-18 20:06:43.051826: Epoch time: 109.46 s\n",
      "2025-05-18 20:06:43.614874: \n",
      "2025-05-18 20:06:43.614969: Epoch 587\n",
      "2025-05-18 20:06:43.615034: Current learning rate: 0.00451\n",
      "2025-05-18 20:08:33.107823: train_loss -0.956\n",
      "2025-05-18 20:08:33.107972: val_loss -0.9355\n",
      "2025-05-18 20:08:33.108006: Pseudo dice [np.float32(0.9724)]\n",
      "2025-05-18 20:08:33.108040: Epoch time: 109.49 s\n",
      "2025-05-18 20:08:33.678474: \n",
      "2025-05-18 20:08:33.678580: Epoch 588\n",
      "2025-05-18 20:08:33.678649: Current learning rate: 0.0045\n",
      "2025-05-18 20:10:23.134569: train_loss -0.9573\n",
      "2025-05-18 20:10:23.134763: val_loss -0.95\n",
      "2025-05-18 20:10:23.134797: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-18 20:10:23.134830: Epoch time: 109.46 s\n",
      "2025-05-18 20:10:23.703435: \n",
      "2025-05-18 20:10:23.703528: Epoch 589\n",
      "2025-05-18 20:10:23.703598: Current learning rate: 0.00449\n",
      "2025-05-18 20:12:13.127015: train_loss -0.9581\n",
      "2025-05-18 20:12:13.127142: val_loss -0.9351\n",
      "2025-05-18 20:12:13.127174: Pseudo dice [np.float32(0.9726)]\n",
      "2025-05-18 20:12:13.127208: Epoch time: 109.42 s\n",
      "2025-05-18 20:12:13.659463: \n",
      "2025-05-18 20:12:13.659603: Epoch 590\n",
      "2025-05-18 20:12:13.659680: Current learning rate: 0.00448\n",
      "2025-05-18 20:14:03.132769: train_loss -0.9597\n",
      "2025-05-18 20:14:03.132962: val_loss -0.9545\n",
      "2025-05-18 20:14:03.133002: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-18 20:14:03.133039: Epoch time: 109.47 s\n",
      "2025-05-18 20:14:03.695547: \n",
      "2025-05-18 20:14:03.695777: Epoch 591\n",
      "2025-05-18 20:14:03.695860: Current learning rate: 0.00447\n",
      "2025-05-18 20:15:53.168251: train_loss -0.9586\n",
      "2025-05-18 20:15:53.168441: val_loss -0.9524\n",
      "2025-05-18 20:15:53.168473: Pseudo dice [np.float32(0.98)]\n",
      "2025-05-18 20:15:53.168507: Epoch time: 109.47 s\n",
      "2025-05-18 20:15:53.994976: \n",
      "2025-05-18 20:15:53.995157: Epoch 592\n",
      "2025-05-18 20:15:53.995233: Current learning rate: 0.00446\n",
      "2025-05-18 20:17:43.506988: train_loss -0.9614\n",
      "2025-05-18 20:17:43.507160: val_loss -0.9457\n",
      "2025-05-18 20:17:43.507200: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 20:17:43.507236: Epoch time: 109.51 s\n",
      "2025-05-18 20:17:44.068412: \n",
      "2025-05-18 20:17:44.068767: Epoch 593\n",
      "2025-05-18 20:17:44.068939: Current learning rate: 0.00445\n",
      "2025-05-18 20:19:33.567345: train_loss -0.9598\n",
      "2025-05-18 20:19:33.567486: val_loss -0.9468\n",
      "2025-05-18 20:19:33.567534: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 20:19:33.567570: Epoch time: 109.5 s\n",
      "2025-05-18 20:19:34.123945: \n",
      "2025-05-18 20:19:34.124047: Epoch 594\n",
      "2025-05-18 20:19:34.124114: Current learning rate: 0.00444\n",
      "2025-05-18 20:21:23.625605: train_loss -0.9589\n",
      "2025-05-18 20:21:23.625741: val_loss -0.9507\n",
      "2025-05-18 20:21:23.625776: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-18 20:21:23.625811: Epoch time: 109.5 s\n",
      "2025-05-18 20:21:24.188976: \n",
      "2025-05-18 20:21:24.189376: Epoch 595\n",
      "2025-05-18 20:21:24.189478: Current learning rate: 0.00443\n",
      "2025-05-18 20:23:13.683710: train_loss -0.9593\n",
      "2025-05-18 20:23:13.684031: val_loss -0.9488\n",
      "2025-05-18 20:23:13.684069: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-18 20:23:13.684104: Epoch time: 109.5 s\n",
      "2025-05-18 20:23:14.243111: \n",
      "2025-05-18 20:23:14.243223: Epoch 596\n",
      "2025-05-18 20:23:14.243288: Current learning rate: 0.00442\n",
      "2025-05-18 20:25:03.727914: train_loss -0.961\n",
      "2025-05-18 20:25:03.728228: val_loss -0.9445\n",
      "2025-05-18 20:25:03.728297: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-18 20:25:03.728353: Epoch time: 109.49 s\n",
      "2025-05-18 20:25:04.282536: \n",
      "2025-05-18 20:25:04.282646: Epoch 597\n",
      "2025-05-18 20:25:04.282715: Current learning rate: 0.00441\n",
      "2025-05-18 20:26:53.745055: train_loss -0.9603\n",
      "2025-05-18 20:26:53.745217: val_loss -0.9549\n",
      "2025-05-18 20:26:53.745267: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-18 20:26:53.745307: Epoch time: 109.46 s\n",
      "2025-05-18 20:26:54.300041: \n",
      "2025-05-18 20:26:54.300148: Epoch 598\n",
      "2025-05-18 20:26:54.300218: Current learning rate: 0.0044\n",
      "2025-05-18 20:28:43.823473: train_loss -0.9614\n",
      "2025-05-18 20:28:43.823643: val_loss -0.9449\n",
      "2025-05-18 20:28:43.823675: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 20:28:43.823707: Epoch time: 109.52 s\n",
      "2025-05-18 20:28:44.368136: \n",
      "2025-05-18 20:28:44.368225: Epoch 599\n",
      "2025-05-18 20:28:44.368300: Current learning rate: 0.00439\n",
      "2025-05-18 20:30:33.825214: train_loss -0.9618\n",
      "2025-05-18 20:30:33.825339: val_loss -0.9347\n",
      "2025-05-18 20:30:33.825372: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-18 20:30:33.825405: Epoch time: 109.46 s\n",
      "2025-05-18 20:30:34.599125: \n",
      "2025-05-18 20:30:34.599316: Epoch 600\n",
      "2025-05-18 20:30:34.599688: Current learning rate: 0.00438\n",
      "2025-05-18 20:32:24.050164: train_loss -0.9609\n",
      "2025-05-18 20:32:24.050285: val_loss -0.9475\n",
      "2025-05-18 20:32:24.050318: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 20:32:24.050351: Epoch time: 109.45 s\n",
      "2025-05-18 20:32:24.585147: \n",
      "2025-05-18 20:32:24.585231: Epoch 601\n",
      "2025-05-18 20:32:24.585293: Current learning rate: 0.00437\n",
      "2025-05-18 20:34:13.968599: train_loss -0.9617\n",
      "2025-05-18 20:34:13.968968: val_loss -0.9525\n",
      "2025-05-18 20:34:13.969078: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-18 20:34:13.969203: Epoch time: 109.38 s\n",
      "2025-05-18 20:34:14.507558: \n",
      "2025-05-18 20:34:14.508041: Epoch 602\n",
      "2025-05-18 20:34:14.508206: Current learning rate: 0.00436\n",
      "2025-05-18 20:36:03.947749: train_loss -0.9615\n",
      "2025-05-18 20:36:03.947870: val_loss -0.9442\n",
      "2025-05-18 20:36:03.947900: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-18 20:36:03.948012: Epoch time: 109.44 s\n",
      "2025-05-18 20:36:04.488861: \n",
      "2025-05-18 20:36:04.489035: Epoch 603\n",
      "2025-05-18 20:36:04.489109: Current learning rate: 0.00435\n",
      "2025-05-18 20:37:53.940897: train_loss -0.9613\n",
      "2025-05-18 20:37:53.941030: val_loss -0.9491\n",
      "2025-05-18 20:37:53.941064: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 20:37:53.941097: Epoch time: 109.45 s\n",
      "2025-05-18 20:37:54.674926: \n",
      "2025-05-18 20:37:54.675178: Epoch 604\n",
      "2025-05-18 20:37:54.675270: Current learning rate: 0.00434\n",
      "2025-05-18 20:39:44.066274: train_loss -0.9609\n",
      "2025-05-18 20:39:44.066418: val_loss -0.9441\n",
      "2025-05-18 20:39:44.066470: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 20:39:44.066509: Epoch time: 109.39 s\n",
      "2025-05-18 20:39:44.604730: \n",
      "2025-05-18 20:39:44.604891: Epoch 605\n",
      "2025-05-18 20:39:44.604963: Current learning rate: 0.00433\n",
      "2025-05-18 20:41:34.082689: train_loss -0.9625\n",
      "2025-05-18 20:41:34.082812: val_loss -0.9388\n",
      "2025-05-18 20:41:34.082845: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-18 20:41:34.082878: Epoch time: 109.48 s\n",
      "2025-05-18 20:41:34.624458: \n",
      "2025-05-18 20:41:34.624642: Epoch 606\n",
      "2025-05-18 20:41:34.624747: Current learning rate: 0.00432\n",
      "2025-05-18 20:43:24.120388: train_loss -0.9609\n",
      "2025-05-18 20:43:24.120567: val_loss -0.9388\n",
      "2025-05-18 20:43:24.120600: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-18 20:43:24.120632: Epoch time: 109.5 s\n",
      "2025-05-18 20:43:24.657383: \n",
      "2025-05-18 20:43:24.657702: Epoch 607\n",
      "2025-05-18 20:43:24.657791: Current learning rate: 0.00431\n",
      "2025-05-18 20:45:14.155478: train_loss -0.9615\n",
      "2025-05-18 20:45:14.155606: val_loss -0.9438\n",
      "2025-05-18 20:45:14.155641: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 20:45:14.155674: Epoch time: 109.5 s\n",
      "2025-05-18 20:45:14.694620: \n",
      "2025-05-18 20:45:14.694711: Epoch 608\n",
      "2025-05-18 20:45:14.694776: Current learning rate: 0.0043\n",
      "2025-05-18 20:47:04.205016: train_loss -0.9636\n",
      "2025-05-18 20:47:04.205140: val_loss -0.9422\n",
      "2025-05-18 20:47:04.205174: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 20:47:04.205209: Epoch time: 109.51 s\n",
      "2025-05-18 20:47:04.744172: \n",
      "2025-05-18 20:47:04.744277: Epoch 609\n",
      "2025-05-18 20:47:04.744350: Current learning rate: 0.00429\n",
      "2025-05-18 20:48:54.255935: train_loss -0.9617\n",
      "2025-05-18 20:48:54.256313: val_loss -0.9396\n",
      "2025-05-18 20:48:54.256457: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-18 20:48:54.256527: Epoch time: 109.51 s\n",
      "2025-05-18 20:48:54.797106: \n",
      "2025-05-18 20:48:54.797199: Epoch 610\n",
      "2025-05-18 20:48:54.797262: Current learning rate: 0.00429\n",
      "2025-05-18 20:50:44.310206: train_loss -0.962\n",
      "2025-05-18 20:50:44.310404: val_loss -0.9429\n",
      "2025-05-18 20:50:44.310441: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 20:50:44.310475: Epoch time: 109.51 s\n",
      "2025-05-18 20:50:44.855873: \n",
      "2025-05-18 20:50:44.856183: Epoch 611\n",
      "2025-05-18 20:50:44.856472: Current learning rate: 0.00428\n",
      "2025-05-18 20:52:34.375357: train_loss -0.9609\n",
      "2025-05-18 20:52:34.375509: val_loss -0.9438\n",
      "2025-05-18 20:52:34.375544: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-18 20:52:34.375578: Epoch time: 109.52 s\n",
      "2025-05-18 20:52:34.912481: \n",
      "2025-05-18 20:52:34.912631: Epoch 612\n",
      "2025-05-18 20:52:34.912694: Current learning rate: 0.00427\n",
      "2025-05-18 20:54:24.388490: train_loss -0.9629\n",
      "2025-05-18 20:54:24.388631: val_loss -0.953\n",
      "2025-05-18 20:54:24.388664: Pseudo dice [np.float32(0.9805)]\n",
      "2025-05-18 20:54:24.388698: Epoch time: 109.48 s\n",
      "2025-05-18 20:54:24.935424: \n",
      "2025-05-18 20:54:24.935521: Epoch 613\n",
      "2025-05-18 20:54:24.935590: Current learning rate: 0.00426\n",
      "2025-05-18 20:56:14.429373: train_loss -0.9626\n",
      "2025-05-18 20:56:14.429497: val_loss -0.9443\n",
      "2025-05-18 20:56:14.429530: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-18 20:56:14.429565: Epoch time: 109.49 s\n",
      "2025-05-18 20:56:14.973914: \n",
      "2025-05-18 20:56:14.974010: Epoch 614\n",
      "2025-05-18 20:56:14.974077: Current learning rate: 0.00425\n",
      "2025-05-18 20:58:04.487103: train_loss -0.9642\n",
      "2025-05-18 20:58:04.487242: val_loss -0.9516\n",
      "2025-05-18 20:58:04.487276: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-18 20:58:04.487311: Epoch time: 109.51 s\n",
      "2025-05-18 20:58:05.037275: \n",
      "2025-05-18 20:58:05.037430: Epoch 615\n",
      "2025-05-18 20:58:05.037509: Current learning rate: 0.00424\n",
      "2025-05-18 20:59:54.529772: train_loss -0.9634\n",
      "2025-05-18 20:59:54.529900: val_loss -0.9522\n",
      "2025-05-18 20:59:54.529935: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-18 20:59:54.529969: Epoch time: 109.49 s\n",
      "2025-05-18 20:59:55.085348: \n",
      "2025-05-18 20:59:55.085490: Epoch 616\n",
      "2025-05-18 20:59:55.085560: Current learning rate: 0.00423\n",
      "2025-05-18 21:01:44.547771: train_loss -0.9629\n",
      "2025-05-18 21:01:44.547909: val_loss -0.9448\n",
      "2025-05-18 21:01:44.547945: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-18 21:01:44.547989: Epoch time: 109.46 s\n",
      "2025-05-18 21:01:45.337415: \n",
      "2025-05-18 21:01:45.337790: Epoch 617\n",
      "2025-05-18 21:01:45.337879: Current learning rate: 0.00422\n",
      "2025-05-18 21:03:34.874089: train_loss -0.9624\n",
      "2025-05-18 21:03:34.874227: val_loss -0.9407\n",
      "2025-05-18 21:03:34.874263: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-18 21:03:34.874298: Epoch time: 109.54 s\n",
      "2025-05-18 21:03:35.436771: \n",
      "2025-05-18 21:03:35.436885: Epoch 618\n",
      "2025-05-18 21:03:35.436953: Current learning rate: 0.00421\n",
      "2025-05-18 21:05:24.970834: train_loss -0.963\n",
      "2025-05-18 21:05:24.970985: val_loss -0.9507\n",
      "2025-05-18 21:05:24.971022: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-18 21:05:24.971055: Epoch time: 109.53 s\n",
      "2025-05-18 21:05:25.539329: \n",
      "2025-05-18 21:05:25.539433: Epoch 619\n",
      "2025-05-18 21:05:25.539509: Current learning rate: 0.0042\n",
      "2025-05-18 21:07:15.122990: train_loss -0.9634\n",
      "2025-05-18 21:07:15.123134: val_loss -0.9427\n",
      "2025-05-18 21:07:15.123169: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-18 21:07:15.123205: Epoch time: 109.58 s\n",
      "2025-05-18 21:07:15.717821: \n",
      "2025-05-18 21:07:15.717939: Epoch 620\n",
      "2025-05-18 21:07:15.718015: Current learning rate: 0.00419\n",
      "2025-05-18 21:09:05.392280: train_loss -0.9642\n",
      "2025-05-18 21:09:05.392458: val_loss -0.951\n",
      "2025-05-18 21:09:05.392493: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-18 21:09:05.392528: Epoch time: 109.68 s\n",
      "2025-05-18 21:09:05.985826: \n",
      "2025-05-18 21:09:05.986229: Epoch 621\n",
      "2025-05-18 21:09:05.986398: Current learning rate: 0.00418\n",
      "2025-05-18 21:10:55.614370: train_loss -0.9656\n",
      "2025-05-18 21:10:55.614658: val_loss -0.9366\n",
      "2025-05-18 21:10:55.614697: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-18 21:10:55.614735: Epoch time: 109.63 s\n",
      "2025-05-18 21:10:56.205660: \n",
      "2025-05-18 21:10:56.205889: Epoch 622\n",
      "2025-05-18 21:10:56.205998: Current learning rate: 0.00417\n",
      "2025-05-18 21:12:45.750204: train_loss -0.9646\n",
      "2025-05-18 21:12:45.750334: val_loss -0.9359\n",
      "2025-05-18 21:12:45.750368: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-18 21:12:45.750403: Epoch time: 109.55 s\n",
      "2025-05-18 21:12:46.304566: \n",
      "2025-05-18 21:12:46.304656: Epoch 623\n",
      "2025-05-18 21:12:46.304722: Current learning rate: 0.00416\n",
      "2025-05-18 21:14:35.797482: train_loss -0.964\n",
      "2025-05-18 21:14:35.797623: val_loss -0.9408\n",
      "2025-05-18 21:14:35.797659: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 21:14:35.797694: Epoch time: 109.49 s\n",
      "2025-05-18 21:14:36.348715: \n",
      "2025-05-18 21:14:36.348996: Epoch 624\n",
      "2025-05-18 21:14:36.349065: Current learning rate: 0.00415\n",
      "2025-05-18 21:16:25.843196: train_loss -0.9643\n",
      "2025-05-18 21:16:25.843400: val_loss -0.9431\n",
      "2025-05-18 21:16:25.843441: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 21:16:25.843481: Epoch time: 109.5 s\n",
      "2025-05-18 21:16:26.385864: \n",
      "2025-05-18 21:16:26.386222: Epoch 625\n",
      "2025-05-18 21:16:26.386320: Current learning rate: 0.00414\n",
      "2025-05-18 21:18:15.868766: train_loss -0.9651\n",
      "2025-05-18 21:18:15.868898: val_loss -0.9452\n",
      "2025-05-18 21:18:15.868932: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 21:18:15.868965: Epoch time: 109.48 s\n",
      "2025-05-18 21:18:16.431532: \n",
      "2025-05-18 21:18:16.431677: Epoch 626\n",
      "2025-05-18 21:18:16.431746: Current learning rate: 0.00413\n",
      "2025-05-18 21:20:05.912239: train_loss -0.9636\n",
      "2025-05-18 21:20:05.912539: val_loss -0.9473\n",
      "2025-05-18 21:20:05.912676: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 21:20:05.912722: Epoch time: 109.48 s\n",
      "2025-05-18 21:20:06.462683: \n",
      "2025-05-18 21:20:06.463095: Epoch 627\n",
      "2025-05-18 21:20:06.463319: Current learning rate: 0.00412\n",
      "2025-05-18 21:21:55.963993: train_loss -0.9653\n",
      "2025-05-18 21:21:55.964131: val_loss -0.9354\n",
      "2025-05-18 21:21:55.964164: Pseudo dice [np.float32(0.9727)]\n",
      "2025-05-18 21:21:55.964198: Epoch time: 109.5 s\n",
      "2025-05-18 21:21:56.521532: \n",
      "2025-05-18 21:21:56.521667: Epoch 628\n",
      "2025-05-18 21:21:56.521821: Current learning rate: 0.00411\n",
      "2025-05-18 21:23:46.009822: train_loss -0.9656\n",
      "2025-05-18 21:23:46.010126: val_loss -0.9401\n",
      "2025-05-18 21:23:46.010166: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 21:23:46.010201: Epoch time: 109.49 s\n",
      "2025-05-18 21:23:46.819744: \n",
      "2025-05-18 21:23:46.820032: Epoch 629\n",
      "2025-05-18 21:23:46.820173: Current learning rate: 0.0041\n",
      "2025-05-18 21:25:36.359301: train_loss -0.9657\n",
      "2025-05-18 21:25:36.359436: val_loss -0.9498\n",
      "2025-05-18 21:25:36.359472: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-18 21:25:36.359506: Epoch time: 109.54 s\n",
      "2025-05-18 21:25:36.913896: \n",
      "2025-05-18 21:25:36.914052: Epoch 630\n",
      "2025-05-18 21:25:36.914121: Current learning rate: 0.00409\n",
      "2025-05-18 21:27:26.422755: train_loss -0.964\n",
      "2025-05-18 21:27:26.422899: val_loss -0.9475\n",
      "2025-05-18 21:27:26.422935: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 21:27:26.422973: Epoch time: 109.51 s\n",
      "2025-05-18 21:27:26.973860: \n",
      "2025-05-18 21:27:26.974043: Epoch 631\n",
      "2025-05-18 21:27:26.974115: Current learning rate: 0.00408\n",
      "2025-05-18 21:29:16.470317: train_loss -0.9657\n",
      "2025-05-18 21:29:16.470441: val_loss -0.9484\n",
      "2025-05-18 21:29:16.470537: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 21:29:16.470650: Epoch time: 109.5 s\n",
      "2025-05-18 21:29:17.004045: \n",
      "2025-05-18 21:29:17.004140: Epoch 632\n",
      "2025-05-18 21:29:17.004205: Current learning rate: 0.00407\n",
      "2025-05-18 21:31:06.477513: train_loss -0.9665\n",
      "2025-05-18 21:31:06.477764: val_loss -0.9494\n",
      "2025-05-18 21:31:06.477814: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-18 21:31:06.477883: Epoch time: 109.47 s\n",
      "2025-05-18 21:31:07.014590: \n",
      "2025-05-18 21:31:07.014683: Epoch 633\n",
      "2025-05-18 21:31:07.014767: Current learning rate: 0.00406\n",
      "2025-05-18 21:32:56.450002: train_loss -0.965\n",
      "2025-05-18 21:32:56.450125: val_loss -0.9394\n",
      "2025-05-18 21:32:56.450157: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 21:32:56.450193: Epoch time: 109.44 s\n",
      "2025-05-18 21:32:56.994596: \n",
      "2025-05-18 21:32:56.994830: Epoch 634\n",
      "2025-05-18 21:32:56.994911: Current learning rate: 0.00405\n",
      "2025-05-18 21:34:46.451556: train_loss -0.9648\n",
      "2025-05-18 21:34:46.451727: val_loss -0.9427\n",
      "2025-05-18 21:34:46.451855: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 21:34:46.451955: Epoch time: 109.46 s\n",
      "2025-05-18 21:34:46.996453: \n",
      "2025-05-18 21:34:46.996572: Epoch 635\n",
      "2025-05-18 21:34:46.996635: Current learning rate: 0.00404\n",
      "2025-05-18 21:36:36.484228: train_loss -0.9656\n",
      "2025-05-18 21:36:36.484378: val_loss -0.9516\n",
      "2025-05-18 21:36:36.484488: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-18 21:36:36.484542: Epoch time: 109.49 s\n",
      "2025-05-18 21:36:37.028204: \n",
      "2025-05-18 21:36:37.028336: Epoch 636\n",
      "2025-05-18 21:36:37.028421: Current learning rate: 0.00403\n",
      "2025-05-18 21:38:26.422854: train_loss -0.9654\n",
      "2025-05-18 21:38:26.422977: val_loss -0.9469\n",
      "2025-05-18 21:38:26.423011: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-18 21:38:26.423043: Epoch time: 109.4 s\n",
      "2025-05-18 21:38:26.959364: \n",
      "2025-05-18 21:38:26.959451: Epoch 637\n",
      "2025-05-18 21:38:26.959514: Current learning rate: 0.00402\n",
      "2025-05-18 21:40:16.384336: train_loss -0.9654\n",
      "2025-05-18 21:40:16.384766: val_loss -0.9427\n",
      "2025-05-18 21:40:16.384851: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-18 21:40:16.384896: Epoch time: 109.43 s\n",
      "2025-05-18 21:40:16.923746: \n",
      "2025-05-18 21:40:16.923903: Epoch 638\n",
      "2025-05-18 21:40:16.923976: Current learning rate: 0.00401\n",
      "2025-05-18 21:42:06.321720: train_loss -0.9642\n",
      "2025-05-18 21:42:06.321836: val_loss -0.9495\n",
      "2025-05-18 21:42:06.321867: Pseudo dice [np.float32(0.9797)]\n",
      "2025-05-18 21:42:06.321901: Epoch time: 109.4 s\n",
      "2025-05-18 21:42:06.856422: \n",
      "2025-05-18 21:42:06.856511: Epoch 639\n",
      "2025-05-18 21:42:06.856589: Current learning rate: 0.004\n",
      "2025-05-18 21:43:56.285737: train_loss -0.9646\n",
      "2025-05-18 21:43:56.285906: val_loss -0.9486\n",
      "2025-05-18 21:43:56.285940: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 21:43:56.285971: Epoch time: 109.43 s\n",
      "2025-05-18 21:43:56.818519: \n",
      "2025-05-18 21:43:56.818658: Epoch 640\n",
      "2025-05-18 21:43:56.818729: Current learning rate: 0.00399\n",
      "2025-05-18 21:45:46.237397: train_loss -0.9651\n",
      "2025-05-18 21:45:46.237623: val_loss -0.9374\n",
      "2025-05-18 21:45:46.237666: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-18 21:45:46.237700: Epoch time: 109.42 s\n",
      "2025-05-18 21:45:46.773498: \n",
      "2025-05-18 21:45:46.773844: Epoch 641\n",
      "2025-05-18 21:45:46.773993: Current learning rate: 0.00398\n",
      "2025-05-18 21:47:36.287942: train_loss -0.9647\n",
      "2025-05-18 21:47:36.288235: val_loss -0.9441\n",
      "2025-05-18 21:47:36.288348: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-18 21:47:36.288434: Epoch time: 109.51 s\n",
      "2025-05-18 21:47:37.045642: \n",
      "2025-05-18 21:47:37.045791: Epoch 642\n",
      "2025-05-18 21:47:37.045940: Current learning rate: 0.00397\n",
      "2025-05-18 21:49:26.575688: train_loss -0.9646\n",
      "2025-05-18 21:49:26.575820: val_loss -0.9452\n",
      "2025-05-18 21:49:26.575901: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 21:49:26.575943: Epoch time: 109.53 s\n",
      "2025-05-18 21:49:27.125203: \n",
      "2025-05-18 21:49:27.125650: Epoch 643\n",
      "2025-05-18 21:49:27.125793: Current learning rate: 0.00396\n",
      "2025-05-18 21:51:16.594194: train_loss -0.9655\n",
      "2025-05-18 21:51:16.594353: val_loss -0.9493\n",
      "2025-05-18 21:51:16.594455: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-18 21:51:16.594499: Epoch time: 109.47 s\n",
      "2025-05-18 21:51:17.156883: \n",
      "2025-05-18 21:51:17.157040: Epoch 644\n",
      "2025-05-18 21:51:17.157116: Current learning rate: 0.00395\n",
      "2025-05-18 21:53:06.658562: train_loss -0.9659\n",
      "2025-05-18 21:53:06.658744: val_loss -0.9458\n",
      "2025-05-18 21:53:06.658779: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 21:53:06.658816: Epoch time: 109.5 s\n",
      "2025-05-18 21:53:07.212466: \n",
      "2025-05-18 21:53:07.212564: Epoch 645\n",
      "2025-05-18 21:53:07.212630: Current learning rate: 0.00394\n",
      "2025-05-18 21:54:56.758666: train_loss -0.9664\n",
      "2025-05-18 21:54:56.758857: val_loss -0.9442\n",
      "2025-05-18 21:54:56.758893: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-18 21:54:56.758925: Epoch time: 109.55 s\n",
      "2025-05-18 21:54:57.310877: \n",
      "2025-05-18 21:54:57.311126: Epoch 646\n",
      "2025-05-18 21:54:57.311200: Current learning rate: 0.00393\n",
      "2025-05-18 21:56:46.785925: train_loss -0.9672\n",
      "2025-05-18 21:56:46.786120: val_loss -0.9324\n",
      "2025-05-18 21:56:46.786155: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-18 21:56:46.786189: Epoch time: 109.48 s\n",
      "2025-05-18 21:56:47.350027: \n",
      "2025-05-18 21:56:47.350424: Epoch 647\n",
      "2025-05-18 21:56:47.350526: Current learning rate: 0.00392\n",
      "2025-05-18 21:58:36.899060: train_loss -0.9675\n",
      "2025-05-18 21:58:36.899189: val_loss -0.9446\n",
      "2025-05-18 21:58:36.899222: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 21:58:36.899256: Epoch time: 109.55 s\n",
      "2025-05-18 21:58:37.466065: \n",
      "2025-05-18 21:58:37.466289: Epoch 648\n",
      "2025-05-18 21:58:37.466366: Current learning rate: 0.00391\n",
      "2025-05-18 22:00:26.952322: train_loss -0.9658\n",
      "2025-05-18 22:00:26.952539: val_loss -0.9463\n",
      "2025-05-18 22:00:26.952587: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 22:00:26.952626: Epoch time: 109.49 s\n",
      "2025-05-18 22:00:27.515383: \n",
      "2025-05-18 22:00:27.515556: Epoch 649\n",
      "2025-05-18 22:00:27.515632: Current learning rate: 0.0039\n",
      "2025-05-18 22:02:17.016199: train_loss -0.9665\n",
      "2025-05-18 22:02:17.016344: val_loss -0.9463\n",
      "2025-05-18 22:02:17.016493: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 22:02:17.016566: Epoch time: 109.5 s\n",
      "2025-05-18 22:02:17.821397: \n",
      "2025-05-18 22:02:17.821593: Epoch 650\n",
      "2025-05-18 22:02:17.821683: Current learning rate: 0.00389\n",
      "2025-05-18 22:04:07.290324: train_loss -0.9676\n",
      "2025-05-18 22:04:07.290580: val_loss -0.9443\n",
      "2025-05-18 22:04:07.290621: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-18 22:04:07.290657: Epoch time: 109.47 s\n",
      "2025-05-18 22:04:07.855514: \n",
      "2025-05-18 22:04:07.855686: Epoch 651\n",
      "2025-05-18 22:04:07.855770: Current learning rate: 0.00388\n",
      "2025-05-18 22:05:57.376399: train_loss -0.9661\n",
      "2025-05-18 22:05:57.376531: val_loss -0.9463\n",
      "2025-05-18 22:05:57.376565: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-18 22:05:57.376599: Epoch time: 109.52 s\n",
      "2025-05-18 22:05:57.944466: \n",
      "2025-05-18 22:05:57.944690: Epoch 652\n",
      "2025-05-18 22:05:57.944787: Current learning rate: 0.00387\n",
      "2025-05-18 22:07:47.345212: train_loss -0.9662\n",
      "2025-05-18 22:07:47.345377: val_loss -0.9411\n",
      "2025-05-18 22:07:47.345410: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 22:07:47.345443: Epoch time: 109.4 s\n",
      "2025-05-18 22:07:47.908806: \n",
      "2025-05-18 22:07:47.909322: Epoch 653\n",
      "2025-05-18 22:07:47.909558: Current learning rate: 0.00386\n",
      "2025-05-18 22:09:37.285661: train_loss -0.9662\n",
      "2025-05-18 22:09:37.286050: val_loss -0.9383\n",
      "2025-05-18 22:09:37.286117: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-18 22:09:37.286160: Epoch time: 109.38 s\n",
      "2025-05-18 22:09:37.848903: \n",
      "2025-05-18 22:09:37.848994: Epoch 654\n",
      "2025-05-18 22:09:37.849059: Current learning rate: 0.00385\n",
      "2025-05-18 22:11:27.235757: train_loss -0.9672\n",
      "2025-05-18 22:11:27.235894: val_loss -0.9453\n",
      "2025-05-18 22:11:27.235929: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-18 22:11:27.235966: Epoch time: 109.39 s\n",
      "2025-05-18 22:11:28.049283: \n",
      "2025-05-18 22:11:28.049400: Epoch 655\n",
      "2025-05-18 22:11:28.049479: Current learning rate: 0.00384\n",
      "2025-05-18 22:13:17.475363: train_loss -0.9671\n",
      "2025-05-18 22:13:17.475501: val_loss -0.9496\n",
      "2025-05-18 22:13:17.475532: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-18 22:13:17.475566: Epoch time: 109.43 s\n",
      "2025-05-18 22:13:18.046828: \n",
      "2025-05-18 22:13:18.047014: Epoch 656\n",
      "2025-05-18 22:13:18.047086: Current learning rate: 0.00383\n",
      "2025-05-18 22:15:07.407480: train_loss -0.9674\n",
      "2025-05-18 22:15:07.407728: val_loss -0.9519\n",
      "2025-05-18 22:15:07.407792: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-18 22:15:07.407833: Epoch time: 109.36 s\n",
      "2025-05-18 22:15:07.956660: \n",
      "2025-05-18 22:15:07.957044: Epoch 657\n",
      "2025-05-18 22:15:07.957144: Current learning rate: 0.00382\n",
      "2025-05-18 22:16:56.966581: train_loss -0.9667\n",
      "2025-05-18 22:16:56.966966: val_loss -0.9427\n",
      "2025-05-18 22:16:56.967088: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 22:16:56.967146: Epoch time: 109.01 s\n",
      "2025-05-18 22:16:57.523363: \n",
      "2025-05-18 22:16:57.523469: Epoch 658\n",
      "2025-05-18 22:16:57.523535: Current learning rate: 0.00381\n",
      "2025-05-18 22:18:46.570994: train_loss -0.9666\n",
      "2025-05-18 22:18:46.571142: val_loss -0.9364\n",
      "2025-05-18 22:18:46.571181: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-18 22:18:46.571217: Epoch time: 109.05 s\n",
      "2025-05-18 22:18:47.129262: \n",
      "2025-05-18 22:18:47.129615: Epoch 659\n",
      "2025-05-18 22:18:47.129692: Current learning rate: 0.0038\n",
      "2025-05-18 22:20:36.159803: train_loss -0.9654\n",
      "2025-05-18 22:20:36.160000: val_loss -0.9457\n",
      "2025-05-18 22:20:36.160035: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-18 22:20:36.160068: Epoch time: 109.03 s\n",
      "2025-05-18 22:20:36.715915: \n",
      "2025-05-18 22:20:36.716269: Epoch 660\n",
      "2025-05-18 22:20:36.716398: Current learning rate: 0.00379\n",
      "2025-05-18 22:22:25.782609: train_loss -0.967\n",
      "2025-05-18 22:22:25.782729: val_loss -0.9471\n",
      "2025-05-18 22:22:25.782761: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-18 22:22:25.782794: Epoch time: 109.07 s\n",
      "2025-05-18 22:22:26.340685: \n",
      "2025-05-18 22:22:26.340889: Epoch 661\n",
      "2025-05-18 22:22:26.340988: Current learning rate: 0.00378\n",
      "2025-05-18 22:24:15.389399: train_loss -0.9669\n",
      "2025-05-18 22:24:15.389551: val_loss -0.9389\n",
      "2025-05-18 22:24:15.389584: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-18 22:24:15.389619: Epoch time: 109.05 s\n",
      "2025-05-18 22:24:15.949864: \n",
      "2025-05-18 22:24:15.950219: Epoch 662\n",
      "2025-05-18 22:24:15.950308: Current learning rate: 0.00377\n",
      "2025-05-18 22:26:04.999625: train_loss -0.9627\n",
      "2025-05-18 22:26:04.999813: val_loss -0.9451\n",
      "2025-05-18 22:26:04.999848: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-18 22:26:04.999885: Epoch time: 109.05 s\n",
      "2025-05-18 22:26:05.560358: \n",
      "2025-05-18 22:26:05.560500: Epoch 663\n",
      "2025-05-18 22:26:05.560571: Current learning rate: 0.00376\n",
      "2025-05-18 22:27:54.575721: train_loss -0.9624\n",
      "2025-05-18 22:27:54.576051: val_loss -0.9417\n",
      "2025-05-18 22:27:54.576098: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-18 22:27:54.576136: Epoch time: 109.02 s\n",
      "2025-05-18 22:27:55.134513: \n",
      "2025-05-18 22:27:55.134607: Epoch 664\n",
      "2025-05-18 22:27:55.134673: Current learning rate: 0.00375\n",
      "2025-05-18 22:29:44.152670: train_loss -0.965\n",
      "2025-05-18 22:29:44.153132: val_loss -0.9457\n",
      "2025-05-18 22:29:44.153177: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-18 22:29:44.153209: Epoch time: 109.02 s\n",
      "2025-05-18 22:29:44.710300: \n",
      "2025-05-18 22:29:44.710414: Epoch 665\n",
      "2025-05-18 22:29:44.710489: Current learning rate: 0.00374\n",
      "2025-05-18 22:31:33.727391: train_loss -0.9663\n",
      "2025-05-18 22:31:33.727548: val_loss -0.9401\n",
      "2025-05-18 22:31:33.727586: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-18 22:31:33.727628: Epoch time: 109.02 s\n",
      "2025-05-18 22:31:34.279787: \n",
      "2025-05-18 22:31:34.279893: Epoch 666\n",
      "2025-05-18 22:31:34.280188: Current learning rate: 0.00373\n",
      "2025-05-18 22:33:23.286307: train_loss -0.9644\n",
      "2025-05-18 22:33:23.286437: val_loss -0.9429\n",
      "2025-05-18 22:33:23.286468: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-18 22:33:23.286502: Epoch time: 109.01 s\n",
      "2025-05-18 22:33:24.087276: \n",
      "2025-05-18 22:33:24.087380: Epoch 667\n",
      "2025-05-18 22:33:24.087448: Current learning rate: 0.00372\n",
      "2025-05-18 22:35:13.047619: train_loss -0.9642\n",
      "2025-05-18 22:35:13.047994: val_loss -0.9447\n",
      "2025-05-18 22:35:13.048051: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-18 22:35:13.048090: Epoch time: 108.96 s\n",
      "2025-05-18 22:35:13.590490: \n",
      "2025-05-18 22:35:13.590654: Epoch 668\n",
      "2025-05-18 22:35:13.590754: Current learning rate: 0.00371\n",
      "2025-05-18 22:37:02.535778: train_loss -0.968\n",
      "2025-05-18 22:37:02.535954: val_loss -0.9534\n",
      "2025-05-18 22:37:02.535987: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-18 22:37:02.536021: Epoch time: 108.95 s\n",
      "2025-05-18 22:37:03.077416: \n",
      "2025-05-18 22:37:03.077577: Epoch 669\n",
      "2025-05-18 22:37:03.077656: Current learning rate: 0.0037\n",
      "2025-05-18 22:38:51.962086: train_loss -0.9664\n",
      "2025-05-18 22:38:51.962255: val_loss -0.9528\n",
      "2025-05-18 22:38:51.962294: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-18 22:38:51.962353: Epoch time: 108.89 s\n",
      "2025-05-18 22:38:52.507234: \n",
      "2025-05-18 22:38:52.507383: Epoch 670\n",
      "2025-05-18 22:38:52.507455: Current learning rate: 0.00369\n",
      "2025-05-18 22:40:41.467282: train_loss -0.9664\n",
      "2025-05-18 22:40:41.467600: val_loss -0.9502\n",
      "2025-05-18 22:40:41.467710: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 22:40:41.467870: Epoch time: 108.96 s\n",
      "2025-05-18 22:40:42.015333: \n",
      "2025-05-18 22:40:42.015513: Epoch 671\n",
      "2025-05-18 22:40:42.015709: Current learning rate: 0.00368\n",
      "2025-05-18 22:42:30.991749: train_loss -0.9668\n",
      "2025-05-18 22:42:30.991925: val_loss -0.9412\n",
      "2025-05-18 22:42:30.991958: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 22:42:30.991998: Epoch time: 108.98 s\n",
      "2025-05-18 22:42:31.544393: \n",
      "2025-05-18 22:42:31.544527: Epoch 672\n",
      "2025-05-18 22:42:31.544622: Current learning rate: 0.00367\n",
      "2025-05-18 22:44:20.523978: train_loss -0.9669\n",
      "2025-05-18 22:44:20.524096: val_loss -0.9405\n",
      "2025-05-18 22:44:20.524125: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 22:44:20.524156: Epoch time: 108.98 s\n",
      "2025-05-18 22:44:21.071228: \n",
      "2025-05-18 22:44:21.071327: Epoch 673\n",
      "2025-05-18 22:44:21.071390: Current learning rate: 0.00366\n",
      "2025-05-18 22:46:10.041333: train_loss -0.967\n",
      "2025-05-18 22:46:10.041484: val_loss -0.9371\n",
      "2025-05-18 22:46:10.041631: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-18 22:46:10.041737: Epoch time: 108.97 s\n",
      "2025-05-18 22:46:10.584642: \n",
      "2025-05-18 22:46:10.584920: Epoch 674\n",
      "2025-05-18 22:46:10.585178: Current learning rate: 0.00365\n",
      "2025-05-18 22:47:59.609313: train_loss -0.9642\n",
      "2025-05-18 22:47:59.609481: val_loss -0.9396\n",
      "2025-05-18 22:47:59.609515: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 22:47:59.609549: Epoch time: 109.03 s\n",
      "2025-05-18 22:48:00.163248: \n",
      "2025-05-18 22:48:00.163420: Epoch 675\n",
      "2025-05-18 22:48:00.163494: Current learning rate: 0.00364\n",
      "2025-05-18 22:49:49.205092: train_loss -0.9573\n",
      "2025-05-18 22:49:49.205293: val_loss -0.9457\n",
      "2025-05-18 22:49:49.205327: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-18 22:49:49.205359: Epoch time: 109.04 s\n",
      "2025-05-18 22:49:49.751856: \n",
      "2025-05-18 22:49:49.751950: Epoch 676\n",
      "2025-05-18 22:49:49.752028: Current learning rate: 0.00363\n",
      "2025-05-18 22:51:38.801610: train_loss -0.9551\n",
      "2025-05-18 22:51:38.801847: val_loss -0.934\n",
      "2025-05-18 22:51:38.801890: Pseudo dice [np.float32(0.9729)]\n",
      "2025-05-18 22:51:38.801927: Epoch time: 109.05 s\n",
      "2025-05-18 22:51:39.355109: \n",
      "2025-05-18 22:51:39.355247: Epoch 677\n",
      "2025-05-18 22:51:39.355330: Current learning rate: 0.00362\n",
      "2025-05-18 22:53:28.418377: train_loss -0.9611\n",
      "2025-05-18 22:53:28.418499: val_loss -0.9384\n",
      "2025-05-18 22:53:28.418528: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-18 22:53:28.418562: Epoch time: 109.06 s\n",
      "2025-05-18 22:53:28.974606: \n",
      "2025-05-18 22:53:28.974822: Epoch 678\n",
      "2025-05-18 22:53:28.974894: Current learning rate: 0.00361\n",
      "2025-05-18 22:55:17.976006: train_loss -0.9642\n",
      "2025-05-18 22:55:17.976148: val_loss -0.9443\n",
      "2025-05-18 22:55:17.976186: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-18 22:55:17.976220: Epoch time: 109.0 s\n",
      "2025-05-18 22:55:18.536706: \n",
      "2025-05-18 22:55:18.536796: Epoch 679\n",
      "2025-05-18 22:55:18.536861: Current learning rate: 0.0036\n",
      "2025-05-18 22:57:07.537523: train_loss -0.9631\n",
      "2025-05-18 22:57:07.537655: val_loss -0.9506\n",
      "2025-05-18 22:57:07.537689: Pseudo dice [np.float32(0.9804)]\n",
      "2025-05-18 22:57:07.537722: Epoch time: 109.0 s\n",
      "2025-05-18 22:57:08.298988: \n",
      "2025-05-18 22:57:08.299154: Epoch 680\n",
      "2025-05-18 22:57:08.299238: Current learning rate: 0.00359\n",
      "2025-05-18 22:58:57.304021: train_loss -0.9551\n",
      "2025-05-18 22:58:57.304153: val_loss -0.9136\n",
      "2025-05-18 22:58:57.304188: Pseudo dice [np.float32(0.9653)]\n",
      "2025-05-18 22:58:57.304222: Epoch time: 109.01 s\n",
      "2025-05-18 22:58:57.857342: \n",
      "2025-05-18 22:58:57.857603: Epoch 681\n",
      "2025-05-18 22:58:57.857703: Current learning rate: 0.00358\n",
      "2025-05-18 23:00:46.849788: train_loss -0.8899\n",
      "2025-05-18 23:00:46.850054: val_loss -0.8773\n",
      "2025-05-18 23:00:46.850210: Pseudo dice [np.float32(0.9512)]\n",
      "2025-05-18 23:00:46.850261: Epoch time: 108.99 s\n",
      "2025-05-18 23:00:47.412721: \n",
      "2025-05-18 23:00:47.412897: Epoch 682\n",
      "2025-05-18 23:00:47.412974: Current learning rate: 0.00357\n",
      "2025-05-18 23:02:36.420719: train_loss -0.9324\n",
      "2025-05-18 23:02:36.420856: val_loss -0.9281\n",
      "2025-05-18 23:02:36.420891: Pseudo dice [np.float32(0.9702)]\n",
      "2025-05-18 23:02:36.420925: Epoch time: 109.01 s\n",
      "2025-05-18 23:02:36.990309: \n",
      "2025-05-18 23:02:36.990484: Epoch 683\n",
      "2025-05-18 23:02:36.990555: Current learning rate: 0.00356\n",
      "2025-05-18 23:04:25.991437: train_loss -0.9423\n",
      "2025-05-18 23:04:25.991683: val_loss -0.939\n",
      "2025-05-18 23:04:25.991727: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-18 23:04:25.991764: Epoch time: 109.0 s\n",
      "2025-05-18 23:04:26.560880: \n",
      "2025-05-18 23:04:26.560993: Epoch 684\n",
      "2025-05-18 23:04:26.561295: Current learning rate: 0.00355\n",
      "2025-05-18 23:06:15.616241: train_loss -0.9513\n",
      "2025-05-18 23:06:15.616396: val_loss -0.9415\n",
      "2025-05-18 23:06:15.616725: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-18 23:06:15.616833: Epoch time: 109.06 s\n",
      "2025-05-18 23:06:16.228927: \n",
      "2025-05-18 23:06:16.229204: Epoch 685\n",
      "2025-05-18 23:06:16.229368: Current learning rate: 0.00354\n",
      "2025-05-18 23:08:05.339411: train_loss -0.9562\n",
      "2025-05-18 23:08:05.339565: val_loss -0.9425\n",
      "2025-05-18 23:08:05.339601: Pseudo dice [np.float32(0.9757)]\n",
      "2025-05-18 23:08:05.339638: Epoch time: 109.11 s\n",
      "2025-05-18 23:08:05.973210: \n",
      "2025-05-18 23:08:05.973328: Epoch 686\n",
      "2025-05-18 23:08:05.973401: Current learning rate: 0.00353\n",
      "2025-05-18 23:09:55.057199: train_loss -0.9559\n",
      "2025-05-18 23:09:55.057364: val_loss -0.9294\n",
      "2025-05-18 23:09:55.057398: Pseudo dice [np.float32(0.9715)]\n",
      "2025-05-18 23:09:55.057432: Epoch time: 109.08 s\n",
      "2025-05-18 23:09:55.664503: \n",
      "2025-05-18 23:09:55.664708: Epoch 687\n",
      "2025-05-18 23:09:55.664795: Current learning rate: 0.00352\n",
      "2025-05-18 23:11:44.718313: train_loss -0.9564\n",
      "2025-05-18 23:11:44.718673: val_loss -0.934\n",
      "2025-05-18 23:11:44.718714: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-18 23:11:44.718750: Epoch time: 109.05 s\n",
      "2025-05-18 23:11:45.286246: \n",
      "2025-05-18 23:11:45.286394: Epoch 688\n",
      "2025-05-18 23:11:45.286473: Current learning rate: 0.00351\n",
      "2025-05-18 23:13:34.249254: train_loss -0.9579\n",
      "2025-05-18 23:13:34.249412: val_loss -0.9449\n",
      "2025-05-18 23:13:34.249495: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-18 23:13:34.249536: Epoch time: 108.96 s\n",
      "2025-05-18 23:13:34.811472: \n",
      "2025-05-18 23:13:34.811866: Epoch 689\n",
      "2025-05-18 23:13:34.811955: Current learning rate: 0.0035\n",
      "2025-05-18 23:15:23.750146: train_loss -0.9585\n",
      "2025-05-18 23:15:23.750478: val_loss -0.9342\n",
      "2025-05-18 23:15:23.750514: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-18 23:15:23.750547: Epoch time: 108.94 s\n",
      "2025-05-18 23:15:24.296173: \n",
      "2025-05-18 23:15:24.296314: Epoch 690\n",
      "2025-05-18 23:15:24.296385: Current learning rate: 0.00349\n",
      "2025-05-18 23:17:13.244461: train_loss -0.96\n",
      "2025-05-18 23:17:13.244647: val_loss -0.9464\n",
      "2025-05-18 23:17:13.244680: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 23:17:13.244713: Epoch time: 108.95 s\n",
      "2025-05-18 23:17:13.807713: \n",
      "2025-05-18 23:17:13.807875: Epoch 691\n",
      "2025-05-18 23:17:13.807960: Current learning rate: 0.00348\n",
      "2025-05-18 23:19:02.789243: train_loss -0.9574\n",
      "2025-05-18 23:19:02.789367: val_loss -0.9413\n",
      "2025-05-18 23:19:02.789398: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 23:19:02.789430: Epoch time: 108.98 s\n",
      "2025-05-18 23:19:03.574157: \n",
      "2025-05-18 23:19:03.574320: Epoch 692\n",
      "2025-05-18 23:19:03.574391: Current learning rate: 0.00346\n",
      "2025-05-18 23:20:52.429298: train_loss -0.9623\n",
      "2025-05-18 23:20:52.429440: val_loss -0.9472\n",
      "2025-05-18 23:20:52.429473: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-18 23:20:52.429506: Epoch time: 108.86 s\n",
      "2025-05-18 23:20:52.999047: \n",
      "2025-05-18 23:20:52.999151: Epoch 693\n",
      "2025-05-18 23:20:52.999228: Current learning rate: 0.00345\n",
      "2025-05-18 23:22:41.953566: train_loss -0.9609\n",
      "2025-05-18 23:22:41.953690: val_loss -0.9552\n",
      "2025-05-18 23:22:41.953754: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-18 23:22:41.953804: Epoch time: 108.96 s\n",
      "2025-05-18 23:22:42.506454: \n",
      "2025-05-18 23:22:42.506762: Epoch 694\n",
      "2025-05-18 23:22:42.506866: Current learning rate: 0.00344\n",
      "2025-05-18 23:24:31.454596: train_loss -0.9638\n",
      "2025-05-18 23:24:31.454720: val_loss -0.9352\n",
      "2025-05-18 23:24:31.454755: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-18 23:24:31.454788: Epoch time: 108.95 s\n",
      "2025-05-18 23:24:32.002210: \n",
      "2025-05-18 23:24:32.002514: Epoch 695\n",
      "2025-05-18 23:24:32.002619: Current learning rate: 0.00343\n",
      "2025-05-18 23:26:20.951928: train_loss -0.964\n",
      "2025-05-18 23:26:20.952044: val_loss -0.9463\n",
      "2025-05-18 23:26:20.952076: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-18 23:26:20.952109: Epoch time: 108.95 s\n",
      "2025-05-18 23:26:21.496428: \n",
      "2025-05-18 23:26:21.496576: Epoch 696\n",
      "2025-05-18 23:26:21.496642: Current learning rate: 0.00342\n",
      "2025-05-18 23:28:10.406425: train_loss -0.9579\n",
      "2025-05-18 23:28:10.406543: val_loss -0.9394\n",
      "2025-05-18 23:28:10.406576: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-18 23:28:10.406609: Epoch time: 108.91 s\n",
      "2025-05-18 23:28:10.958241: \n",
      "2025-05-18 23:28:10.958420: Epoch 697\n",
      "2025-05-18 23:28:10.958546: Current learning rate: 0.00341\n",
      "2025-05-18 23:29:59.969292: train_loss -0.9579\n",
      "2025-05-18 23:29:59.969451: val_loss -0.9236\n",
      "2025-05-18 23:29:59.969571: Pseudo dice [np.float32(0.9698)]\n",
      "2025-05-18 23:29:59.969649: Epoch time: 109.01 s\n",
      "2025-05-18 23:30:00.516827: \n",
      "2025-05-18 23:30:00.517004: Epoch 698\n",
      "2025-05-18 23:30:00.517111: Current learning rate: 0.0034\n",
      "2025-05-18 23:31:49.498907: train_loss -0.9621\n",
      "2025-05-18 23:31:49.499025: val_loss -0.9399\n",
      "2025-05-18 23:31:49.499056: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-18 23:31:49.499090: Epoch time: 108.98 s\n",
      "2025-05-18 23:31:50.048385: \n",
      "2025-05-18 23:31:50.048522: Epoch 699\n",
      "2025-05-18 23:31:50.048590: Current learning rate: 0.00339\n",
      "2025-05-18 23:33:38.932951: train_loss -0.9622\n",
      "2025-05-18 23:33:38.933081: val_loss -0.9553\n",
      "2025-05-18 23:33:38.933111: Pseudo dice [np.float32(0.9809)]\n",
      "2025-05-18 23:33:38.933143: Epoch time: 108.89 s\n",
      "2025-05-18 23:33:39.711374: \n",
      "2025-05-18 23:33:39.711506: Epoch 700\n",
      "2025-05-18 23:33:39.711571: Current learning rate: 0.00338\n",
      "2025-05-18 23:35:28.612715: train_loss -0.9628\n",
      "2025-05-18 23:35:28.612845: val_loss -0.9477\n",
      "2025-05-18 23:35:28.612881: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-18 23:35:28.612913: Epoch time: 108.9 s\n",
      "2025-05-18 23:35:29.152135: \n",
      "2025-05-18 23:35:29.152288: Epoch 701\n",
      "2025-05-18 23:35:29.152372: Current learning rate: 0.00337\n",
      "2025-05-18 23:37:18.152197: train_loss -0.9632\n",
      "2025-05-18 23:37:18.152314: val_loss -0.9427\n",
      "2025-05-18 23:37:18.152348: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-18 23:37:18.152380: Epoch time: 109.0 s\n",
      "2025-05-18 23:37:18.704894: \n",
      "2025-05-18 23:37:18.705070: Epoch 702\n",
      "2025-05-18 23:37:18.705155: Current learning rate: 0.00336\n",
      "2025-05-18 23:39:07.633002: train_loss -0.9644\n",
      "2025-05-18 23:39:07.633184: val_loss -0.9447\n",
      "2025-05-18 23:39:07.633217: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-18 23:39:07.633251: Epoch time: 108.93 s\n",
      "2025-05-18 23:39:08.182199: \n",
      "2025-05-18 23:39:08.182443: Epoch 703\n",
      "2025-05-18 23:39:08.182579: Current learning rate: 0.00335\n",
      "2025-05-18 23:40:57.205892: train_loss -0.9628\n",
      "2025-05-18 23:40:57.206059: val_loss -0.9507\n",
      "2025-05-18 23:40:57.206097: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-18 23:40:57.206130: Epoch time: 109.02 s\n",
      "2025-05-18 23:40:57.950224: \n",
      "2025-05-18 23:40:57.950537: Epoch 704\n",
      "2025-05-18 23:40:57.950663: Current learning rate: 0.00334\n",
      "2025-05-18 23:42:46.924831: train_loss -0.964\n",
      "2025-05-18 23:42:46.924944: val_loss -0.9409\n",
      "2025-05-18 23:42:46.925033: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-18 23:42:46.925141: Epoch time: 108.98 s\n",
      "2025-05-18 23:42:47.475838: \n",
      "2025-05-18 23:42:47.476081: Epoch 705\n",
      "2025-05-18 23:42:47.476149: Current learning rate: 0.00333\n",
      "2025-05-18 23:44:36.384320: train_loss -0.9641\n",
      "2025-05-18 23:44:36.384495: val_loss -0.943\n",
      "2025-05-18 23:44:36.384531: Pseudo dice [np.float32(0.976)]\n",
      "2025-05-18 23:44:36.384562: Epoch time: 108.91 s\n",
      "2025-05-18 23:44:36.931619: \n",
      "2025-05-18 23:44:36.931775: Epoch 706\n",
      "2025-05-18 23:44:36.931874: Current learning rate: 0.00332\n",
      "2025-05-18 23:46:25.846932: train_loss -0.9631\n",
      "2025-05-18 23:46:25.847065: val_loss -0.9485\n",
      "2025-05-18 23:46:25.847101: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-18 23:46:25.847135: Epoch time: 108.92 s\n",
      "2025-05-18 23:46:26.398989: \n",
      "2025-05-18 23:46:26.399264: Epoch 707\n",
      "2025-05-18 23:46:26.399383: Current learning rate: 0.00331\n",
      "2025-05-18 23:48:15.461052: train_loss -0.9654\n",
      "2025-05-18 23:48:15.461171: val_loss -0.9458\n",
      "2025-05-18 23:48:15.461205: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 23:48:15.461237: Epoch time: 109.06 s\n",
      "2025-05-18 23:48:16.024302: \n",
      "2025-05-18 23:48:16.024706: Epoch 708\n",
      "2025-05-18 23:48:16.024806: Current learning rate: 0.0033\n",
      "2025-05-18 23:50:05.060169: train_loss -0.9661\n",
      "2025-05-18 23:50:05.060313: val_loss -0.9449\n",
      "2025-05-18 23:50:05.060351: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-18 23:50:05.060383: Epoch time: 109.04 s\n",
      "2025-05-18 23:50:05.618273: \n",
      "2025-05-18 23:50:05.618472: Epoch 709\n",
      "2025-05-18 23:50:05.618543: Current learning rate: 0.00329\n",
      "2025-05-18 23:51:54.669578: train_loss -0.9652\n",
      "2025-05-18 23:51:54.669778: val_loss -0.9451\n",
      "2025-05-18 23:51:54.669814: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-18 23:51:54.669848: Epoch time: 109.05 s\n",
      "2025-05-18 23:51:55.225016: \n",
      "2025-05-18 23:51:55.225268: Epoch 710\n",
      "2025-05-18 23:51:55.225346: Current learning rate: 0.00328\n",
      "2025-05-18 23:53:44.268395: train_loss -0.9647\n",
      "2025-05-18 23:53:44.268523: val_loss -0.9481\n",
      "2025-05-18 23:53:44.268559: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-18 23:53:44.268599: Epoch time: 109.04 s\n",
      "2025-05-18 23:53:44.822296: \n",
      "2025-05-18 23:53:44.822450: Epoch 711\n",
      "2025-05-18 23:53:44.822527: Current learning rate: 0.00327\n",
      "2025-05-18 23:55:33.818222: train_loss -0.9669\n",
      "2025-05-18 23:55:33.818344: val_loss -0.9535\n",
      "2025-05-18 23:55:33.818371: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-18 23:55:33.818402: Epoch time: 109.0 s\n",
      "2025-05-18 23:55:34.377656: \n",
      "2025-05-18 23:55:34.377876: Epoch 712\n",
      "2025-05-18 23:55:34.377950: Current learning rate: 0.00326\n",
      "2025-05-18 23:57:23.374058: train_loss -0.9652\n",
      "2025-05-18 23:57:23.374184: val_loss -0.9422\n",
      "2025-05-18 23:57:23.374217: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-18 23:57:23.374249: Epoch time: 109.0 s\n",
      "2025-05-18 23:57:23.928618: \n",
      "2025-05-18 23:57:23.928779: Epoch 713\n",
      "2025-05-18 23:57:23.928865: Current learning rate: 0.00325\n",
      "2025-05-18 23:59:12.922702: train_loss -0.9659\n",
      "2025-05-18 23:59:12.922833: val_loss -0.9434\n",
      "2025-05-18 23:59:12.922865: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-18 23:59:12.922898: Epoch time: 108.99 s\n",
      "2025-05-18 23:59:13.483915: \n",
      "2025-05-18 23:59:13.484056: Epoch 714\n",
      "2025-05-18 23:59:13.484128: Current learning rate: 0.00324\n",
      "2025-05-19 00:01:02.487593: train_loss -0.9673\n",
      "2025-05-19 00:01:02.487708: val_loss -0.9479\n",
      "2025-05-19 00:01:02.487739: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-19 00:01:02.487780: Epoch time: 109.0 s\n",
      "2025-05-19 00:01:03.053571: \n",
      "2025-05-19 00:01:03.053724: Epoch 715\n",
      "2025-05-19 00:01:03.053896: Current learning rate: 0.00323\n",
      "2025-05-19 00:02:52.024411: train_loss -0.9653\n",
      "2025-05-19 00:02:52.024537: val_loss -0.951\n",
      "2025-05-19 00:02:52.024571: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-19 00:02:52.024606: Epoch time: 108.97 s\n",
      "2025-05-19 00:02:52.819882: \n",
      "2025-05-19 00:02:52.820004: Epoch 716\n",
      "2025-05-19 00:02:52.820086: Current learning rate: 0.00322\n",
      "2025-05-19 00:04:41.815329: train_loss -0.9678\n",
      "2025-05-19 00:04:41.815625: val_loss -0.9353\n",
      "2025-05-19 00:04:41.815753: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-19 00:04:41.815795: Epoch time: 109.0 s\n",
      "2025-05-19 00:04:42.384854: \n",
      "2025-05-19 00:04:42.385035: Epoch 717\n",
      "2025-05-19 00:04:42.385106: Current learning rate: 0.00321\n",
      "2025-05-19 00:06:31.434099: train_loss -0.9643\n",
      "2025-05-19 00:06:31.434225: val_loss -0.9471\n",
      "2025-05-19 00:06:31.434376: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-19 00:06:31.434435: Epoch time: 109.05 s\n",
      "2025-05-19 00:06:31.976640: \n",
      "2025-05-19 00:06:31.976913: Epoch 718\n",
      "2025-05-19 00:06:31.977021: Current learning rate: 0.0032\n",
      "2025-05-19 00:08:20.933918: train_loss -0.9678\n",
      "2025-05-19 00:08:20.934181: val_loss -0.9371\n",
      "2025-05-19 00:08:20.934301: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-19 00:08:20.934346: Epoch time: 108.96 s\n",
      "2025-05-19 00:08:21.506371: \n",
      "2025-05-19 00:08:21.506547: Epoch 719\n",
      "2025-05-19 00:08:21.506739: Current learning rate: 0.00319\n",
      "2025-05-19 00:10:10.516333: train_loss -0.9665\n",
      "2025-05-19 00:10:10.516541: val_loss -0.941\n",
      "2025-05-19 00:10:10.516583: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-19 00:10:10.516620: Epoch time: 109.01 s\n",
      "2025-05-19 00:10:11.082592: \n",
      "2025-05-19 00:10:11.082717: Epoch 720\n",
      "2025-05-19 00:10:11.082834: Current learning rate: 0.00318\n",
      "2025-05-19 00:12:00.148058: train_loss -0.9665\n",
      "2025-05-19 00:12:00.148393: val_loss -0.9378\n",
      "2025-05-19 00:12:00.148479: Pseudo dice [np.float32(0.9738)]\n",
      "2025-05-19 00:12:00.148521: Epoch time: 109.07 s\n",
      "2025-05-19 00:12:00.716192: \n",
      "2025-05-19 00:12:00.716302: Epoch 721\n",
      "2025-05-19 00:12:00.716364: Current learning rate: 0.00317\n",
      "2025-05-19 00:13:49.722583: train_loss -0.9659\n",
      "2025-05-19 00:13:49.722729: val_loss -0.9422\n",
      "2025-05-19 00:13:49.722761: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-19 00:13:49.722796: Epoch time: 109.01 s\n",
      "2025-05-19 00:13:50.290201: \n",
      "2025-05-19 00:13:50.290331: Epoch 722\n",
      "2025-05-19 00:13:50.290397: Current learning rate: 0.00316\n",
      "2025-05-19 00:15:39.257619: train_loss -0.9675\n",
      "2025-05-19 00:15:39.257739: val_loss -0.9444\n",
      "2025-05-19 00:15:39.257784: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-19 00:15:39.257819: Epoch time: 108.97 s\n",
      "2025-05-19 00:15:39.815760: \n",
      "2025-05-19 00:15:39.816052: Epoch 723\n",
      "2025-05-19 00:15:39.816133: Current learning rate: 0.00315\n",
      "2025-05-19 00:17:28.782981: train_loss -0.9668\n",
      "2025-05-19 00:17:28.783179: val_loss -0.9446\n",
      "2025-05-19 00:17:28.783223: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-19 00:17:28.783259: Epoch time: 108.97 s\n",
      "2025-05-19 00:17:29.328765: \n",
      "2025-05-19 00:17:29.328934: Epoch 724\n",
      "2025-05-19 00:17:29.329260: Current learning rate: 0.00314\n",
      "2025-05-19 00:19:18.332480: train_loss -0.9675\n",
      "2025-05-19 00:19:18.332633: val_loss -0.9465\n",
      "2025-05-19 00:19:18.332666: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-19 00:19:18.332699: Epoch time: 109.0 s\n",
      "2025-05-19 00:19:18.886185: \n",
      "2025-05-19 00:19:18.886502: Epoch 725\n",
      "2025-05-19 00:19:18.886737: Current learning rate: 0.00313\n",
      "2025-05-19 00:21:07.747214: train_loss -0.967\n",
      "2025-05-19 00:21:07.747334: val_loss -0.9252\n",
      "2025-05-19 00:21:07.747366: Pseudo dice [np.float32(0.9702)]\n",
      "2025-05-19 00:21:07.747398: Epoch time: 108.86 s\n",
      "2025-05-19 00:21:08.293099: \n",
      "2025-05-19 00:21:08.293236: Epoch 726\n",
      "2025-05-19 00:21:08.293300: Current learning rate: 0.00312\n",
      "2025-05-19 00:22:57.210388: train_loss -0.9654\n",
      "2025-05-19 00:22:57.210507: val_loss -0.942\n",
      "2025-05-19 00:22:57.210539: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-19 00:22:57.210576: Epoch time: 108.92 s\n",
      "2025-05-19 00:22:57.754098: \n",
      "2025-05-19 00:22:57.754337: Epoch 727\n",
      "2025-05-19 00:22:57.754425: Current learning rate: 0.00311\n",
      "2025-05-19 00:24:46.745981: train_loss -0.9675\n",
      "2025-05-19 00:24:46.746099: val_loss -0.9434\n",
      "2025-05-19 00:24:46.746286: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-19 00:24:46.746365: Epoch time: 108.99 s\n",
      "2025-05-19 00:24:47.475906: \n",
      "2025-05-19 00:24:47.476132: Epoch 728\n",
      "2025-05-19 00:24:47.476221: Current learning rate: 0.0031\n",
      "2025-05-19 00:26:36.436108: train_loss -0.9673\n",
      "2025-05-19 00:26:36.436248: val_loss -0.943\n",
      "2025-05-19 00:26:36.436421: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-19 00:26:36.436636: Epoch time: 108.96 s\n",
      "2025-05-19 00:26:36.983164: \n",
      "2025-05-19 00:26:36.983313: Epoch 729\n",
      "2025-05-19 00:26:36.983383: Current learning rate: 0.00309\n",
      "2025-05-19 00:28:25.975337: train_loss -0.9688\n",
      "2025-05-19 00:28:25.975467: val_loss -0.9379\n",
      "2025-05-19 00:28:25.975505: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-19 00:28:25.975538: Epoch time: 108.99 s\n",
      "2025-05-19 00:28:26.521644: \n",
      "2025-05-19 00:28:26.521986: Epoch 730\n",
      "2025-05-19 00:28:26.522076: Current learning rate: 0.00308\n",
      "2025-05-19 00:30:16.517988: train_loss -0.968\n",
      "2025-05-19 00:30:16.518115: val_loss -0.9399\n",
      "2025-05-19 00:30:16.518150: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-19 00:30:16.518182: Epoch time: 110.0 s\n",
      "2025-05-19 00:30:17.076967: \n",
      "2025-05-19 00:30:17.077324: Epoch 731\n",
      "2025-05-19 00:30:17.077436: Current learning rate: 0.00307\n",
      "2025-05-19 00:32:06.449271: train_loss -0.9673\n",
      "2025-05-19 00:32:06.449405: val_loss -0.9429\n",
      "2025-05-19 00:32:06.449439: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-19 00:32:06.449510: Epoch time: 109.37 s\n",
      "2025-05-19 00:32:07.010758: \n",
      "2025-05-19 00:32:07.011008: Epoch 732\n",
      "2025-05-19 00:32:07.011106: Current learning rate: 0.00306\n",
      "2025-05-19 00:33:56.331097: train_loss -0.9671\n",
      "2025-05-19 00:33:56.331233: val_loss -0.955\n",
      "2025-05-19 00:33:56.331266: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-19 00:33:56.331299: Epoch time: 109.32 s\n",
      "2025-05-19 00:33:56.902192: \n",
      "2025-05-19 00:33:56.902350: Epoch 733\n",
      "2025-05-19 00:33:56.902421: Current learning rate: 0.00305\n",
      "2025-05-19 00:35:46.276607: train_loss -0.9681\n",
      "2025-05-19 00:35:46.276744: val_loss -0.9406\n",
      "2025-05-19 00:35:46.276777: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-19 00:35:46.276811: Epoch time: 109.37 s\n",
      "2025-05-19 00:35:46.835551: \n",
      "2025-05-19 00:35:46.835700: Epoch 734\n",
      "2025-05-19 00:35:46.835780: Current learning rate: 0.00304\n",
      "2025-05-19 00:37:36.212891: train_loss -0.9673\n",
      "2025-05-19 00:37:36.213009: val_loss -0.9579\n",
      "2025-05-19 00:37:36.213139: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-19 00:37:36.213182: Epoch time: 109.38 s\n",
      "2025-05-19 00:37:36.774875: \n",
      "2025-05-19 00:37:36.775321: Epoch 735\n",
      "2025-05-19 00:37:36.775406: Current learning rate: 0.00303\n",
      "2025-05-19 00:39:26.084554: train_loss -0.9643\n",
      "2025-05-19 00:39:26.084676: val_loss -0.9365\n",
      "2025-05-19 00:39:26.084709: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-19 00:39:26.084742: Epoch time: 109.31 s\n",
      "2025-05-19 00:39:26.648766: \n",
      "2025-05-19 00:39:26.648859: Epoch 736\n",
      "2025-05-19 00:39:26.648926: Current learning rate: 0.00302\n",
      "2025-05-19 00:41:15.970570: train_loss -0.9651\n",
      "2025-05-19 00:41:15.970966: val_loss -0.9349\n",
      "2025-05-19 00:41:15.971008: Pseudo dice [np.float32(0.9713)]\n",
      "2025-05-19 00:41:15.971041: Epoch time: 109.32 s\n",
      "2025-05-19 00:41:16.525824: \n",
      "2025-05-19 00:41:16.526382: Epoch 737\n",
      "2025-05-19 00:41:16.526642: Current learning rate: 0.00301\n",
      "2025-05-19 00:43:05.871061: train_loss -0.9523\n",
      "2025-05-19 00:43:05.871447: val_loss -0.9374\n",
      "2025-05-19 00:43:05.871500: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-19 00:43:05.871653: Epoch time: 109.35 s\n",
      "2025-05-19 00:43:06.431033: \n",
      "2025-05-19 00:43:06.431204: Epoch 738\n",
      "2025-05-19 00:43:06.431347: Current learning rate: 0.003\n",
      "2025-05-19 00:44:55.669194: train_loss -0.9345\n",
      "2025-05-19 00:44:55.669311: val_loss -0.9269\n",
      "2025-05-19 00:44:55.669342: Pseudo dice [np.float32(0.9718)]\n",
      "2025-05-19 00:44:55.669376: Epoch time: 109.24 s\n",
      "2025-05-19 00:44:56.225585: \n",
      "2025-05-19 00:44:56.225839: Epoch 739\n",
      "2025-05-19 00:44:56.226048: Current learning rate: 0.00299\n",
      "2025-05-19 00:46:45.432365: train_loss -0.9476\n",
      "2025-05-19 00:46:45.432541: val_loss -0.934\n",
      "2025-05-19 00:46:45.432649: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-19 00:46:45.432704: Epoch time: 109.21 s\n",
      "2025-05-19 00:46:46.182855: \n",
      "2025-05-19 00:46:46.183028: Epoch 740\n",
      "2025-05-19 00:46:46.183197: Current learning rate: 0.00297\n",
      "2025-05-19 00:48:35.484326: train_loss -0.9485\n",
      "2025-05-19 00:48:35.484504: val_loss -0.9227\n",
      "2025-05-19 00:48:35.484536: Pseudo dice [np.float32(0.9696)]\n",
      "2025-05-19 00:48:35.484570: Epoch time: 109.3 s\n",
      "2025-05-19 00:48:36.044639: \n",
      "2025-05-19 00:48:36.044922: Epoch 741\n",
      "2025-05-19 00:48:36.045035: Current learning rate: 0.00296\n",
      "2025-05-19 00:50:25.289687: train_loss -0.9529\n",
      "2025-05-19 00:50:25.289816: val_loss -0.9245\n",
      "2025-05-19 00:50:25.289850: Pseudo dice [np.float32(0.9701)]\n",
      "2025-05-19 00:50:25.289884: Epoch time: 109.25 s\n",
      "2025-05-19 00:50:25.856076: \n",
      "2025-05-19 00:50:25.856602: Epoch 742\n",
      "2025-05-19 00:50:25.856729: Current learning rate: 0.00295\n",
      "2025-05-19 00:52:15.222811: train_loss -0.9608\n",
      "2025-05-19 00:52:15.222938: val_loss -0.935\n",
      "2025-05-19 00:52:15.222969: Pseudo dice [np.float32(0.9721)]\n",
      "2025-05-19 00:52:15.223002: Epoch time: 109.37 s\n",
      "2025-05-19 00:52:15.788768: \n",
      "2025-05-19 00:52:15.789463: Epoch 743\n",
      "2025-05-19 00:52:15.789595: Current learning rate: 0.00294\n",
      "2025-05-19 00:54:05.139438: train_loss -0.9612\n",
      "2025-05-19 00:54:05.139620: val_loss -0.9476\n",
      "2025-05-19 00:54:05.139653: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-19 00:54:05.139687: Epoch time: 109.35 s\n",
      "2025-05-19 00:54:05.706893: \n",
      "2025-05-19 00:54:05.707291: Epoch 744\n",
      "2025-05-19 00:54:05.707470: Current learning rate: 0.00293\n",
      "2025-05-19 00:55:54.955662: train_loss -0.9636\n",
      "2025-05-19 00:55:54.955915: val_loss -0.947\n",
      "2025-05-19 00:55:54.955964: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-19 00:55:54.956006: Epoch time: 109.25 s\n",
      "2025-05-19 00:55:55.519684: \n",
      "2025-05-19 00:55:55.519801: Epoch 745\n",
      "2025-05-19 00:55:55.519877: Current learning rate: 0.00292\n",
      "2025-05-19 00:57:44.829646: train_loss -0.9526\n",
      "2025-05-19 00:57:44.829875: val_loss -0.9175\n",
      "2025-05-19 00:57:44.829945: Pseudo dice [np.float32(0.9663)]\n",
      "2025-05-19 00:57:44.830085: Epoch time: 109.31 s\n",
      "2025-05-19 00:57:45.390695: \n",
      "2025-05-19 00:57:45.391026: Epoch 746\n",
      "2025-05-19 00:57:45.391110: Current learning rate: 0.00291\n",
      "2025-05-19 00:59:34.690604: train_loss -0.9139\n",
      "2025-05-19 00:59:34.690738: val_loss -0.937\n",
      "2025-05-19 00:59:34.690774: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-19 00:59:34.690810: Epoch time: 109.3 s\n",
      "2025-05-19 00:59:35.247092: \n",
      "2025-05-19 00:59:35.247266: Epoch 747\n",
      "2025-05-19 00:59:35.247343: Current learning rate: 0.0029\n",
      "2025-05-19 01:01:24.629003: train_loss -0.9382\n",
      "2025-05-19 01:01:24.629188: val_loss -0.9344\n",
      "2025-05-19 01:01:24.629225: Pseudo dice [np.float32(0.9728)]\n",
      "2025-05-19 01:01:24.629262: Epoch time: 109.38 s\n",
      "2025-05-19 01:01:25.207406: \n",
      "2025-05-19 01:01:25.207613: Epoch 748\n",
      "2025-05-19 01:01:25.207692: Current learning rate: 0.00289\n",
      "2025-05-19 01:03:14.429828: train_loss -0.9478\n",
      "2025-05-19 01:03:14.430017: val_loss -0.9349\n",
      "2025-05-19 01:03:14.430051: Pseudo dice [np.float32(0.9731)]\n",
      "2025-05-19 01:03:14.430085: Epoch time: 109.22 s\n",
      "2025-05-19 01:03:15.008922: \n",
      "2025-05-19 01:03:15.009090: Epoch 749\n",
      "2025-05-19 01:03:15.009202: Current learning rate: 0.00288\n",
      "2025-05-19 01:05:04.379004: train_loss -0.9443\n",
      "2025-05-19 01:05:04.379181: val_loss -0.937\n",
      "2025-05-19 01:05:04.379214: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-19 01:05:04.379246: Epoch time: 109.37 s\n",
      "2025-05-19 01:05:05.205092: \n",
      "2025-05-19 01:05:05.205410: Epoch 750\n",
      "2025-05-19 01:05:05.205491: Current learning rate: 0.00287\n",
      "2025-05-19 01:06:54.504117: train_loss -0.9545\n",
      "2025-05-19 01:06:54.504262: val_loss -0.9445\n",
      "2025-05-19 01:06:54.504297: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-19 01:06:54.504334: Epoch time: 109.3 s\n",
      "2025-05-19 01:06:55.088327: \n",
      "2025-05-19 01:06:55.088650: Epoch 751\n",
      "2025-05-19 01:06:55.088953: Current learning rate: 0.00286\n",
      "2025-05-19 01:08:44.404902: train_loss -0.9604\n",
      "2025-05-19 01:08:44.405098: val_loss -0.9477\n",
      "2025-05-19 01:08:44.405242: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-19 01:08:44.405307: Epoch time: 109.32 s\n",
      "2025-05-19 01:08:45.208594: \n",
      "2025-05-19 01:08:45.209144: Epoch 752\n",
      "2025-05-19 01:08:45.209280: Current learning rate: 0.00285\n",
      "2025-05-19 01:10:34.586869: train_loss -0.9608\n",
      "2025-05-19 01:10:34.587215: val_loss -0.9509\n",
      "2025-05-19 01:10:34.587293: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-19 01:10:34.587339: Epoch time: 109.38 s\n",
      "2025-05-19 01:10:35.177459: \n",
      "2025-05-19 01:10:35.177625: Epoch 753\n",
      "2025-05-19 01:10:35.177702: Current learning rate: 0.00284\n",
      "2025-05-19 01:12:24.569916: train_loss -0.9489\n",
      "2025-05-19 01:12:24.570088: val_loss -0.9374\n",
      "2025-05-19 01:12:24.570250: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-19 01:12:24.570404: Epoch time: 109.39 s\n",
      "2025-05-19 01:12:25.152251: \n",
      "2025-05-19 01:12:25.152715: Epoch 754\n",
      "2025-05-19 01:12:25.152798: Current learning rate: 0.00283\n",
      "2025-05-19 01:14:14.546384: train_loss -0.9511\n",
      "2025-05-19 01:14:14.546535: val_loss -0.9361\n",
      "2025-05-19 01:14:14.546586: Pseudo dice [np.float32(0.9741)]\n",
      "2025-05-19 01:14:14.546623: Epoch time: 109.39 s\n",
      "2025-05-19 01:14:15.182473: \n",
      "2025-05-19 01:14:15.182626: Epoch 755\n",
      "2025-05-19 01:14:15.182708: Current learning rate: 0.00282\n",
      "2025-05-19 01:16:04.619685: train_loss -0.956\n",
      "2025-05-19 01:16:04.619909: val_loss -0.935\n",
      "2025-05-19 01:16:04.619950: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-19 01:16:04.619989: Epoch time: 109.44 s\n",
      "2025-05-19 01:16:05.245130: \n",
      "2025-05-19 01:16:05.245739: Epoch 756\n",
      "2025-05-19 01:16:05.245965: Current learning rate: 0.00281\n",
      "2025-05-19 01:17:54.618870: train_loss -0.9589\n",
      "2025-05-19 01:17:54.619011: val_loss -0.9451\n",
      "2025-05-19 01:17:54.619046: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-19 01:17:54.619082: Epoch time: 109.37 s\n",
      "2025-05-19 01:17:55.260228: \n",
      "2025-05-19 01:17:55.260885: Epoch 757\n",
      "2025-05-19 01:17:55.261076: Current learning rate: 0.0028\n",
      "2025-05-19 01:19:44.722528: train_loss -0.9619\n",
      "2025-05-19 01:19:44.722776: val_loss -0.9483\n",
      "2025-05-19 01:19:44.722856: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-19 01:19:44.722915: Epoch time: 109.46 s\n",
      "2025-05-19 01:19:45.351464: \n",
      "2025-05-19 01:19:45.351818: Epoch 758\n",
      "2025-05-19 01:19:45.351972: Current learning rate: 0.00279\n",
      "2025-05-19 01:21:34.840492: train_loss -0.9605\n",
      "2025-05-19 01:21:34.840641: val_loss -0.9391\n",
      "2025-05-19 01:21:34.840680: Pseudo dice [np.float32(0.9758)]\n",
      "2025-05-19 01:21:34.840717: Epoch time: 109.49 s\n",
      "2025-05-19 01:21:35.456105: \n",
      "2025-05-19 01:21:35.456534: Epoch 759\n",
      "2025-05-19 01:21:35.456775: Current learning rate: 0.00278\n",
      "2025-05-19 01:23:24.875762: train_loss -0.9629\n",
      "2025-05-19 01:23:24.875897: val_loss -0.9423\n",
      "2025-05-19 01:23:24.875932: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-19 01:23:24.875969: Epoch time: 109.42 s\n",
      "2025-05-19 01:23:25.479707: \n",
      "2025-05-19 01:23:25.480145: Epoch 760\n",
      "2025-05-19 01:23:25.480232: Current learning rate: 0.00277\n",
      "2025-05-19 01:25:14.821848: train_loss -0.9637\n",
      "2025-05-19 01:25:14.822038: val_loss -0.9513\n",
      "2025-05-19 01:25:14.822072: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-19 01:25:14.822108: Epoch time: 109.34 s\n",
      "2025-05-19 01:25:15.419597: \n",
      "2025-05-19 01:25:15.419714: Epoch 761\n",
      "2025-05-19 01:25:15.419792: Current learning rate: 0.00276\n",
      "2025-05-19 01:27:04.733483: train_loss -0.9635\n",
      "2025-05-19 01:27:04.733610: val_loss -0.9433\n",
      "2025-05-19 01:27:04.733642: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-19 01:27:04.733676: Epoch time: 109.31 s\n",
      "2025-05-19 01:27:05.291645: \n",
      "2025-05-19 01:27:05.292084: Epoch 762\n",
      "2025-05-19 01:27:05.292415: Current learning rate: 0.00275\n",
      "2025-05-19 01:28:54.664900: train_loss -0.9628\n",
      "2025-05-19 01:28:54.665074: val_loss -0.9372\n",
      "2025-05-19 01:28:54.665164: Pseudo dice [np.float32(0.9742)]\n",
      "2025-05-19 01:28:54.665307: Epoch time: 109.37 s\n",
      "2025-05-19 01:28:55.226687: \n",
      "2025-05-19 01:28:55.226918: Epoch 763\n",
      "2025-05-19 01:28:55.227011: Current learning rate: 0.00274\n",
      "2025-05-19 01:30:44.908351: train_loss -0.9646\n",
      "2025-05-19 01:30:44.908473: val_loss -0.9517\n",
      "2025-05-19 01:30:44.908504: Pseudo dice [np.float32(0.9795)]\n",
      "2025-05-19 01:30:44.908544: Epoch time: 109.68 s\n",
      "2025-05-19 01:30:45.662200: \n",
      "2025-05-19 01:30:45.662590: Epoch 764\n",
      "2025-05-19 01:30:45.662685: Current learning rate: 0.00273\n",
      "2025-05-19 01:32:34.989723: train_loss -0.9649\n",
      "2025-05-19 01:32:34.989845: val_loss -0.9426\n",
      "2025-05-19 01:32:34.989877: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-19 01:32:34.989911: Epoch time: 109.33 s\n",
      "2025-05-19 01:32:35.553064: \n",
      "2025-05-19 01:32:35.553602: Epoch 765\n",
      "2025-05-19 01:32:35.553834: Current learning rate: 0.00272\n",
      "2025-05-19 01:34:24.888003: train_loss -0.9649\n",
      "2025-05-19 01:34:24.888181: val_loss -0.9408\n",
      "2025-05-19 01:34:24.888219: Pseudo dice [np.float32(0.9766)]\n",
      "2025-05-19 01:34:24.888446: Epoch time: 109.34 s\n",
      "2025-05-19 01:34:25.461620: \n",
      "2025-05-19 01:34:25.461792: Epoch 766\n",
      "2025-05-19 01:34:25.461866: Current learning rate: 0.00271\n",
      "2025-05-19 01:36:14.715339: train_loss -0.9657\n",
      "2025-05-19 01:36:14.715469: val_loss -0.9466\n",
      "2025-05-19 01:36:14.715502: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-19 01:36:14.715535: Epoch time: 109.25 s\n",
      "2025-05-19 01:36:15.284820: \n",
      "2025-05-19 01:36:15.285324: Epoch 767\n",
      "2025-05-19 01:36:15.285445: Current learning rate: 0.0027\n",
      "2025-05-19 01:38:04.596330: train_loss -0.9651\n",
      "2025-05-19 01:38:04.596457: val_loss -0.9476\n",
      "2025-05-19 01:38:04.596500: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-19 01:38:04.596540: Epoch time: 109.31 s\n",
      "2025-05-19 01:38:05.162038: \n",
      "2025-05-19 01:38:05.162189: Epoch 768\n",
      "2025-05-19 01:38:05.162254: Current learning rate: 0.00268\n",
      "2025-05-19 01:39:54.382373: train_loss -0.9655\n",
      "2025-05-19 01:39:54.382486: val_loss -0.9437\n",
      "2025-05-19 01:39:54.382518: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-19 01:39:54.382596: Epoch time: 109.22 s\n",
      "2025-05-19 01:39:54.949719: \n",
      "2025-05-19 01:39:54.949841: Epoch 769\n",
      "2025-05-19 01:39:54.949911: Current learning rate: 0.00267\n",
      "2025-05-19 01:41:44.264156: train_loss -0.966\n",
      "2025-05-19 01:41:44.264322: val_loss -0.9389\n",
      "2025-05-19 01:41:44.264355: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-19 01:41:44.264389: Epoch time: 109.31 s\n",
      "2025-05-19 01:41:44.824607: \n",
      "2025-05-19 01:41:44.824813: Epoch 770\n",
      "2025-05-19 01:41:44.824904: Current learning rate: 0.00266\n",
      "2025-05-19 01:43:34.137184: train_loss -0.9668\n",
      "2025-05-19 01:43:34.137329: val_loss -0.9463\n",
      "2025-05-19 01:43:34.137367: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-19 01:43:34.137402: Epoch time: 109.31 s\n",
      "2025-05-19 01:43:34.700689: \n",
      "2025-05-19 01:43:34.701029: Epoch 771\n",
      "2025-05-19 01:43:34.701115: Current learning rate: 0.00265\n",
      "2025-05-19 01:45:23.979396: train_loss -0.9645\n",
      "2025-05-19 01:45:23.979549: val_loss -0.9505\n",
      "2025-05-19 01:45:23.979601: Pseudo dice [np.float32(0.9797)]\n",
      "2025-05-19 01:45:23.979639: Epoch time: 109.28 s\n",
      "2025-05-19 01:45:24.542507: \n",
      "2025-05-19 01:45:24.542972: Epoch 772\n",
      "2025-05-19 01:45:24.543124: Current learning rate: 0.00264\n",
      "2025-05-19 01:47:13.906433: train_loss -0.9665\n",
      "2025-05-19 01:47:13.906606: val_loss -0.9431\n",
      "2025-05-19 01:47:13.906639: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-19 01:47:13.906673: Epoch time: 109.36 s\n",
      "2025-05-19 01:47:14.475415: \n",
      "2025-05-19 01:47:14.475557: Epoch 773\n",
      "2025-05-19 01:47:14.475759: Current learning rate: 0.00263\n",
      "2025-05-19 01:49:03.815103: train_loss -0.9665\n",
      "2025-05-19 01:49:03.815234: val_loss -0.9451\n",
      "2025-05-19 01:49:03.815269: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-19 01:49:03.815301: Epoch time: 109.34 s\n",
      "2025-05-19 01:49:04.380094: \n",
      "2025-05-19 01:49:04.380198: Epoch 774\n",
      "2025-05-19 01:49:04.380298: Current learning rate: 0.00262\n",
      "2025-05-19 01:50:53.737381: train_loss -0.9668\n",
      "2025-05-19 01:50:53.737512: val_loss -0.9472\n",
      "2025-05-19 01:50:53.737549: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-19 01:50:53.737582: Epoch time: 109.36 s\n",
      "2025-05-19 01:50:54.300593: \n",
      "2025-05-19 01:50:54.301308: Epoch 775\n",
      "2025-05-19 01:50:54.301604: Current learning rate: 0.00261\n",
      "2025-05-19 01:52:43.692401: train_loss -0.9669\n",
      "2025-05-19 01:52:43.692529: val_loss -0.9445\n",
      "2025-05-19 01:52:43.692564: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-19 01:52:43.692598: Epoch time: 109.39 s\n",
      "2025-05-19 01:52:44.461889: \n",
      "2025-05-19 01:52:44.462168: Epoch 776\n",
      "2025-05-19 01:52:44.462262: Current learning rate: 0.0026\n",
      "2025-05-19 01:54:33.864559: train_loss -0.9669\n",
      "2025-05-19 01:54:33.864709: val_loss -0.9354\n",
      "2025-05-19 01:54:33.864745: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-19 01:54:33.864780: Epoch time: 109.4 s\n",
      "2025-05-19 01:54:34.433054: \n",
      "2025-05-19 01:54:34.433294: Epoch 777\n",
      "2025-05-19 01:54:34.433401: Current learning rate: 0.00259\n",
      "2025-05-19 01:56:23.771826: train_loss -0.965\n",
      "2025-05-19 01:56:23.771988: val_loss -0.9372\n",
      "2025-05-19 01:56:23.772022: Pseudo dice [np.float32(0.9754)]\n",
      "2025-05-19 01:56:23.772057: Epoch time: 109.34 s\n",
      "2025-05-19 01:56:24.348941: \n",
      "2025-05-19 01:56:24.349308: Epoch 778\n",
      "2025-05-19 01:56:24.349393: Current learning rate: 0.00258\n",
      "2025-05-19 01:58:13.720624: train_loss -0.9669\n",
      "2025-05-19 01:58:13.720805: val_loss -0.9582\n",
      "2025-05-19 01:58:13.720840: Pseudo dice [np.float32(0.982)]\n",
      "2025-05-19 01:58:13.720878: Epoch time: 109.37 s\n",
      "2025-05-19 01:58:14.298090: \n",
      "2025-05-19 01:58:14.298794: Epoch 779\n",
      "2025-05-19 01:58:14.298970: Current learning rate: 0.00257\n",
      "2025-05-19 02:00:03.638557: train_loss -0.9682\n",
      "2025-05-19 02:00:03.638743: val_loss -0.9361\n",
      "2025-05-19 02:00:03.638791: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-19 02:00:03.638828: Epoch time: 109.34 s\n",
      "2025-05-19 02:00:04.213002: \n",
      "2025-05-19 02:00:04.213221: Epoch 780\n",
      "2025-05-19 02:00:04.213310: Current learning rate: 0.00256\n",
      "2025-05-19 02:01:53.623211: train_loss -0.9668\n",
      "2025-05-19 02:01:53.623346: val_loss -0.9459\n",
      "2025-05-19 02:01:53.623378: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-19 02:01:53.623412: Epoch time: 109.41 s\n",
      "2025-05-19 02:01:54.207278: \n",
      "2025-05-19 02:01:54.207605: Epoch 781\n",
      "2025-05-19 02:01:54.207725: Current learning rate: 0.00255\n",
      "2025-05-19 02:03:43.603458: train_loss -0.9662\n",
      "2025-05-19 02:03:43.603603: val_loss -0.9497\n",
      "2025-05-19 02:03:43.603642: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-19 02:03:43.603678: Epoch time: 109.4 s\n",
      "2025-05-19 02:03:44.184496: \n",
      "2025-05-19 02:03:44.184765: Epoch 782\n",
      "2025-05-19 02:03:44.184873: Current learning rate: 0.00254\n",
      "2025-05-19 02:05:33.572807: train_loss -0.9687\n",
      "2025-05-19 02:05:33.572949: val_loss -0.9385\n",
      "2025-05-19 02:05:33.572984: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-19 02:05:33.573018: Epoch time: 109.39 s\n",
      "2025-05-19 02:05:34.155296: \n",
      "2025-05-19 02:05:34.155393: Epoch 783\n",
      "2025-05-19 02:05:34.155462: Current learning rate: 0.00253\n",
      "2025-05-19 02:07:23.551423: train_loss -0.9695\n",
      "2025-05-19 02:07:23.551561: val_loss -0.9343\n",
      "2025-05-19 02:07:23.551595: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-19 02:07:23.551631: Epoch time: 109.4 s\n",
      "2025-05-19 02:07:24.130632: \n",
      "2025-05-19 02:07:24.131039: Epoch 784\n",
      "2025-05-19 02:07:24.131180: Current learning rate: 0.00252\n",
      "2025-05-19 02:09:13.527022: train_loss -0.9669\n",
      "2025-05-19 02:09:13.527215: val_loss -0.9383\n",
      "2025-05-19 02:09:13.527265: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-19 02:09:13.527301: Epoch time: 109.4 s\n",
      "2025-05-19 02:09:14.112503: \n",
      "2025-05-19 02:09:14.112783: Epoch 785\n",
      "2025-05-19 02:09:14.112895: Current learning rate: 0.00251\n",
      "2025-05-19 02:11:03.535666: train_loss -0.9659\n",
      "2025-05-19 02:11:03.535868: val_loss -0.9533\n",
      "2025-05-19 02:11:03.535904: Pseudo dice [np.float32(0.98)]\n",
      "2025-05-19 02:11:03.535939: Epoch time: 109.42 s\n",
      "2025-05-19 02:11:04.124745: \n",
      "2025-05-19 02:11:04.124949: Epoch 786\n",
      "2025-05-19 02:11:04.125033: Current learning rate: 0.0025\n",
      "2025-05-19 02:12:53.522246: train_loss -0.9686\n",
      "2025-05-19 02:12:53.522387: val_loss -0.9484\n",
      "2025-05-19 02:12:53.522422: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-19 02:12:53.522457: Epoch time: 109.4 s\n",
      "2025-05-19 02:12:54.115073: \n",
      "2025-05-19 02:12:54.115393: Epoch 787\n",
      "2025-05-19 02:12:54.115471: Current learning rate: 0.00249\n",
      "2025-05-19 02:14:43.459397: train_loss -0.9671\n",
      "2025-05-19 02:14:43.459579: val_loss -0.9495\n",
      "2025-05-19 02:14:43.459683: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-19 02:14:43.459778: Epoch time: 109.34 s\n",
      "2025-05-19 02:14:44.215950: \n",
      "2025-05-19 02:14:44.216160: Epoch 788\n",
      "2025-05-19 02:14:44.216262: Current learning rate: 0.00248\n",
      "2025-05-19 02:16:33.668190: train_loss -0.9678\n",
      "2025-05-19 02:16:33.668397: val_loss -0.9428\n",
      "2025-05-19 02:16:33.668433: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-19 02:16:33.668469: Epoch time: 109.45 s\n",
      "2025-05-19 02:16:34.247280: \n",
      "2025-05-19 02:16:34.247408: Epoch 789\n",
      "2025-05-19 02:16:34.247489: Current learning rate: 0.00247\n",
      "2025-05-19 02:18:23.667734: train_loss -0.9667\n",
      "2025-05-19 02:18:23.667866: val_loss -0.948\n",
      "2025-05-19 02:18:23.667900: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-19 02:18:23.667933: Epoch time: 109.42 s\n",
      "2025-05-19 02:18:24.257799: \n",
      "2025-05-19 02:18:24.258433: Epoch 790\n",
      "2025-05-19 02:18:24.258672: Current learning rate: 0.00245\n",
      "2025-05-19 02:20:13.673919: train_loss -0.9671\n",
      "2025-05-19 02:20:13.674072: val_loss -0.9403\n",
      "2025-05-19 02:20:13.674105: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-19 02:20:13.674140: Epoch time: 109.42 s\n",
      "2025-05-19 02:20:14.258756: \n",
      "2025-05-19 02:20:14.259091: Epoch 791\n",
      "2025-05-19 02:20:14.259184: Current learning rate: 0.00244\n",
      "2025-05-19 02:22:03.673038: train_loss -0.9677\n",
      "2025-05-19 02:22:03.673191: val_loss -0.9421\n",
      "2025-05-19 02:22:03.673229: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-19 02:22:03.673265: Epoch time: 109.41 s\n",
      "2025-05-19 02:22:04.260677: \n",
      "2025-05-19 02:22:04.261033: Epoch 792\n",
      "2025-05-19 02:22:04.261243: Current learning rate: 0.00243\n",
      "2025-05-19 02:23:53.672252: train_loss -0.968\n",
      "2025-05-19 02:23:53.672383: val_loss -0.9503\n",
      "2025-05-19 02:23:53.672417: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-19 02:23:53.672451: Epoch time: 109.41 s\n",
      "2025-05-19 02:23:54.258271: \n",
      "2025-05-19 02:23:54.258489: Epoch 793\n",
      "2025-05-19 02:23:54.258692: Current learning rate: 0.00242\n",
      "2025-05-19 02:25:43.645878: train_loss -0.9672\n",
      "2025-05-19 02:25:43.646032: val_loss -0.9399\n",
      "2025-05-19 02:25:43.646064: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-19 02:25:43.646098: Epoch time: 109.39 s\n",
      "2025-05-19 02:25:44.227778: \n",
      "2025-05-19 02:25:44.228131: Epoch 794\n",
      "2025-05-19 02:25:44.228243: Current learning rate: 0.00241\n",
      "2025-05-19 02:27:33.616836: train_loss -0.9675\n",
      "2025-05-19 02:27:33.616963: val_loss -0.9372\n",
      "2025-05-19 02:27:33.616995: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-19 02:27:33.617028: Epoch time: 109.39 s\n",
      "2025-05-19 02:27:34.185007: \n",
      "2025-05-19 02:27:34.185629: Epoch 795\n",
      "2025-05-19 02:27:34.185727: Current learning rate: 0.0024\n",
      "2025-05-19 02:29:23.573912: train_loss -0.9685\n",
      "2025-05-19 02:29:23.574039: val_loss -0.9384\n",
      "2025-05-19 02:29:23.574073: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-19 02:29:23.574105: Epoch time: 109.39 s\n",
      "2025-05-19 02:29:24.136500: \n",
      "2025-05-19 02:29:24.136924: Epoch 796\n",
      "2025-05-19 02:29:24.137061: Current learning rate: 0.00239\n",
      "2025-05-19 02:31:13.452185: train_loss -0.9678\n",
      "2025-05-19 02:31:13.452385: val_loss -0.9484\n",
      "2025-05-19 02:31:13.452435: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-19 02:31:13.452471: Epoch time: 109.32 s\n",
      "2025-05-19 02:31:14.015873: \n",
      "2025-05-19 02:31:14.016045: Epoch 797\n",
      "2025-05-19 02:31:14.016122: Current learning rate: 0.00238\n",
      "2025-05-19 02:33:03.328487: train_loss -0.9678\n",
      "2025-05-19 02:33:03.328618: val_loss -0.936\n",
      "2025-05-19 02:33:03.328656: Pseudo dice [np.float32(0.9735)]\n",
      "2025-05-19 02:33:03.328689: Epoch time: 109.31 s\n",
      "2025-05-19 02:33:03.890956: \n",
      "2025-05-19 02:33:03.891124: Epoch 798\n",
      "2025-05-19 02:33:03.891212: Current learning rate: 0.00237\n",
      "2025-05-19 02:34:53.243732: train_loss -0.9662\n",
      "2025-05-19 02:34:53.243918: val_loss -0.9475\n",
      "2025-05-19 02:34:53.243953: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-19 02:34:53.243985: Epoch time: 109.35 s\n",
      "2025-05-19 02:34:53.807195: \n",
      "2025-05-19 02:34:53.807509: Epoch 799\n",
      "2025-05-19 02:34:53.807925: Current learning rate: 0.00236\n",
      "2025-05-19 02:36:43.139630: train_loss -0.967\n",
      "2025-05-19 02:36:43.139771: val_loss -0.9389\n",
      "2025-05-19 02:36:43.139811: Pseudo dice [np.float32(0.9753)]\n",
      "2025-05-19 02:36:43.139844: Epoch time: 109.33 s\n",
      "2025-05-19 02:36:44.114204: \n",
      "2025-05-19 02:36:44.114541: Epoch 800\n",
      "2025-05-19 02:36:44.114631: Current learning rate: 0.00235\n",
      "2025-05-19 02:38:33.003177: train_loss -0.9685\n",
      "2025-05-19 02:38:33.003310: val_loss -0.948\n",
      "2025-05-19 02:38:33.003346: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-19 02:38:33.003377: Epoch time: 108.89 s\n",
      "2025-05-19 02:38:33.568910: \n",
      "2025-05-19 02:38:33.569349: Epoch 801\n",
      "2025-05-19 02:38:33.569579: Current learning rate: 0.00234\n",
      "2025-05-19 02:40:22.549318: train_loss -0.9684\n",
      "2025-05-19 02:40:22.549442: val_loss -0.9359\n",
      "2025-05-19 02:40:22.549475: Pseudo dice [np.float32(0.9746)]\n",
      "2025-05-19 02:40:22.549508: Epoch time: 108.98 s\n",
      "2025-05-19 02:40:23.120997: \n",
      "2025-05-19 02:40:23.121143: Epoch 802\n",
      "2025-05-19 02:40:23.121227: Current learning rate: 0.00233\n",
      "2025-05-19 02:42:12.117691: train_loss -0.9681\n",
      "2025-05-19 02:42:12.117848: val_loss -0.9386\n",
      "2025-05-19 02:42:12.117882: Pseudo dice [np.float32(0.974)]\n",
      "2025-05-19 02:42:12.117915: Epoch time: 109.0 s\n",
      "2025-05-19 02:42:12.677796: \n",
      "2025-05-19 02:42:12.678210: Epoch 803\n",
      "2025-05-19 02:42:12.678389: Current learning rate: 0.00232\n",
      "2025-05-19 02:44:01.730578: train_loss -0.9684\n",
      "2025-05-19 02:44:01.730699: val_loss -0.9296\n",
      "2025-05-19 02:44:01.730730: Pseudo dice [np.float32(0.9737)]\n",
      "2025-05-19 02:44:01.730778: Epoch time: 109.05 s\n",
      "2025-05-19 02:44:02.296258: \n",
      "2025-05-19 02:44:02.296518: Epoch 804\n",
      "2025-05-19 02:44:02.296662: Current learning rate: 0.00231\n",
      "2025-05-19 02:45:51.264111: train_loss -0.9685\n",
      "2025-05-19 02:45:51.264234: val_loss -0.935\n",
      "2025-05-19 02:45:51.264266: Pseudo dice [np.float32(0.975)]\n",
      "2025-05-19 02:45:51.264300: Epoch time: 108.97 s\n",
      "2025-05-19 02:45:51.831126: \n",
      "2025-05-19 02:45:51.831380: Epoch 805\n",
      "2025-05-19 02:45:51.831465: Current learning rate: 0.0023\n",
      "2025-05-19 02:47:40.661483: train_loss -0.9693\n",
      "2025-05-19 02:47:40.661618: val_loss -0.9413\n",
      "2025-05-19 02:47:40.661726: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-19 02:47:40.661767: Epoch time: 108.83 s\n",
      "2025-05-19 02:47:41.221648: \n",
      "2025-05-19 02:47:41.222037: Epoch 806\n",
      "2025-05-19 02:47:41.222167: Current learning rate: 0.00229\n",
      "2025-05-19 02:49:30.055489: train_loss -0.9675\n",
      "2025-05-19 02:49:30.055603: val_loss -0.9432\n",
      "2025-05-19 02:49:30.055635: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-19 02:49:30.055666: Epoch time: 108.83 s\n",
      "2025-05-19 02:49:30.619085: \n",
      "2025-05-19 02:49:30.619246: Epoch 807\n",
      "2025-05-19 02:49:30.619343: Current learning rate: 0.00228\n",
      "2025-05-19 02:51:19.392516: train_loss -0.9693\n",
      "2025-05-19 02:51:19.392643: val_loss -0.9455\n",
      "2025-05-19 02:51:19.392677: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-19 02:51:19.392710: Epoch time: 108.77 s\n",
      "2025-05-19 02:51:19.954211: \n",
      "2025-05-19 02:51:19.954540: Epoch 808\n",
      "2025-05-19 02:51:19.954663: Current learning rate: 0.00226\n",
      "2025-05-19 02:53:08.924694: train_loss -0.967\n",
      "2025-05-19 02:53:08.924876: val_loss -0.9501\n",
      "2025-05-19 02:53:08.925009: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-19 02:53:08.925069: Epoch time: 108.97 s\n",
      "2025-05-19 02:53:09.490552: \n",
      "2025-05-19 02:53:09.491159: Epoch 809\n",
      "2025-05-19 02:53:09.491361: Current learning rate: 0.00225\n",
      "2025-05-19 02:54:58.488678: train_loss -0.9691\n",
      "2025-05-19 02:54:58.488812: val_loss -0.9258\n",
      "2025-05-19 02:54:58.488851: Pseudo dice [np.float32(0.97)]\n",
      "2025-05-19 02:54:58.488884: Epoch time: 109.0 s\n",
      "2025-05-19 02:54:59.051869: \n",
      "2025-05-19 02:54:59.052379: Epoch 810\n",
      "2025-05-19 02:54:59.052489: Current learning rate: 0.00224\n",
      "2025-05-19 02:56:48.120431: train_loss -0.9686\n",
      "2025-05-19 02:56:48.120562: val_loss -0.9499\n",
      "2025-05-19 02:56:48.120596: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-19 02:56:48.120629: Epoch time: 109.07 s\n",
      "2025-05-19 02:56:48.883794: \n",
      "2025-05-19 02:56:48.884032: Epoch 811\n",
      "2025-05-19 02:56:48.884139: Current learning rate: 0.00223\n",
      "2025-05-19 02:58:38.001779: train_loss -0.9687\n",
      "2025-05-19 02:58:38.001908: val_loss -0.9405\n",
      "2025-05-19 02:58:38.001957: Pseudo dice [np.float32(0.9751)]\n",
      "2025-05-19 02:58:38.002084: Epoch time: 109.12 s\n",
      "2025-05-19 02:58:38.570216: \n",
      "2025-05-19 02:58:38.570574: Epoch 812\n",
      "2025-05-19 02:58:38.570778: Current learning rate: 0.00222\n",
      "2025-05-19 03:00:27.610530: train_loss -0.9693\n",
      "2025-05-19 03:00:27.610722: val_loss -0.9484\n",
      "2025-05-19 03:00:27.610758: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-19 03:00:27.610799: Epoch time: 109.04 s\n",
      "2025-05-19 03:00:28.180669: \n",
      "2025-05-19 03:00:28.181172: Epoch 813\n",
      "2025-05-19 03:00:28.181272: Current learning rate: 0.00221\n",
      "2025-05-19 03:02:17.227710: train_loss -0.9686\n",
      "2025-05-19 03:02:17.227899: val_loss -0.9421\n",
      "2025-05-19 03:02:17.227934: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-19 03:02:17.227968: Epoch time: 109.05 s\n",
      "2025-05-19 03:02:17.799330: \n",
      "2025-05-19 03:02:17.799914: Epoch 814\n",
      "2025-05-19 03:02:17.800041: Current learning rate: 0.0022\n",
      "2025-05-19 03:04:06.640874: train_loss -0.971\n",
      "2025-05-19 03:04:06.641114: val_loss -0.9437\n",
      "2025-05-19 03:04:06.641156: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-19 03:04:06.641192: Epoch time: 108.84 s\n",
      "2025-05-19 03:04:07.221667: \n",
      "2025-05-19 03:04:07.222161: Epoch 815\n",
      "2025-05-19 03:04:07.222255: Current learning rate: 0.00219\n",
      "2025-05-19 03:05:56.182736: train_loss -0.9685\n",
      "2025-05-19 03:05:56.182887: val_loss -0.95\n",
      "2025-05-19 03:05:56.182924: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-19 03:05:56.182958: Epoch time: 108.96 s\n",
      "2025-05-19 03:05:56.779040: \n",
      "2025-05-19 03:05:56.779157: Epoch 816\n",
      "2025-05-19 03:05:56.779236: Current learning rate: 0.00218\n",
      "2025-05-19 03:07:45.730857: train_loss -0.969\n",
      "2025-05-19 03:07:45.730992: val_loss -0.9415\n",
      "2025-05-19 03:07:45.731028: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-19 03:07:45.731060: Epoch time: 108.95 s\n",
      "2025-05-19 03:07:46.312644: \n",
      "2025-05-19 03:07:46.312984: Epoch 817\n",
      "2025-05-19 03:07:46.313116: Current learning rate: 0.00217\n",
      "2025-05-19 03:09:35.356418: train_loss -0.9698\n",
      "2025-05-19 03:09:35.356577: val_loss -0.9434\n",
      "2025-05-19 03:09:35.356614: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-19 03:09:35.356647: Epoch time: 109.04 s\n",
      "2025-05-19 03:09:35.946867: \n",
      "2025-05-19 03:09:35.947268: Epoch 818\n",
      "2025-05-19 03:09:35.947367: Current learning rate: 0.00216\n",
      "2025-05-19 03:11:24.949653: train_loss -0.9687\n",
      "2025-05-19 03:11:24.949803: val_loss -0.9476\n",
      "2025-05-19 03:11:24.949838: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-19 03:11:24.949871: Epoch time: 109.0 s\n",
      "2025-05-19 03:11:25.537824: \n",
      "2025-05-19 03:11:25.537942: Epoch 819\n",
      "2025-05-19 03:11:25.538013: Current learning rate: 0.00215\n",
      "2025-05-19 03:13:14.441318: train_loss -0.9677\n",
      "2025-05-19 03:13:14.441452: val_loss -0.9452\n",
      "2025-05-19 03:13:14.441486: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-19 03:13:14.441522: Epoch time: 108.9 s\n",
      "2025-05-19 03:13:15.008634: \n",
      "2025-05-19 03:13:15.008967: Epoch 820\n",
      "2025-05-19 03:13:15.009085: Current learning rate: 0.00214\n",
      "2025-05-19 03:15:04.029405: train_loss -0.9705\n",
      "2025-05-19 03:15:04.029544: val_loss -0.9388\n",
      "2025-05-19 03:15:04.029579: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-19 03:15:04.029620: Epoch time: 109.02 s\n",
      "2025-05-19 03:15:04.602052: \n",
      "2025-05-19 03:15:04.602378: Epoch 821\n",
      "2025-05-19 03:15:04.602542: Current learning rate: 0.00213\n",
      "2025-05-19 03:16:53.780636: train_loss -0.9692\n",
      "2025-05-19 03:16:53.780805: val_loss -0.938\n",
      "2025-05-19 03:16:53.780840: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-19 03:16:53.780876: Epoch time: 109.18 s\n",
      "2025-05-19 03:16:54.383548: \n",
      "2025-05-19 03:16:54.383962: Epoch 822\n",
      "2025-05-19 03:16:54.384054: Current learning rate: 0.00212\n",
      "2025-05-19 03:18:43.557899: train_loss -0.9685\n",
      "2025-05-19 03:18:43.558044: val_loss -0.9386\n",
      "2025-05-19 03:18:43.558192: Pseudo dice [np.float32(0.9768)]\n",
      "2025-05-19 03:18:43.558373: Epoch time: 109.17 s\n",
      "2025-05-19 03:18:44.447085: \n",
      "2025-05-19 03:18:44.447198: Epoch 823\n",
      "2025-05-19 03:18:44.447281: Current learning rate: 0.0021\n",
      "2025-05-19 03:20:33.621272: train_loss -0.969\n",
      "2025-05-19 03:20:33.621502: val_loss -0.9449\n",
      "2025-05-19 03:20:33.621908: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-19 03:20:33.622052: Epoch time: 109.17 s\n",
      "2025-05-19 03:20:34.223889: \n",
      "2025-05-19 03:20:34.224229: Epoch 824\n",
      "2025-05-19 03:20:34.224318: Current learning rate: 0.00209\n",
      "2025-05-19 03:22:23.335066: train_loss -0.9684\n",
      "2025-05-19 03:22:23.335423: val_loss -0.9458\n",
      "2025-05-19 03:22:23.335511: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-19 03:22:23.335558: Epoch time: 109.11 s\n",
      "2025-05-19 03:22:23.961601: \n",
      "2025-05-19 03:22:23.962030: Epoch 825\n",
      "2025-05-19 03:22:23.962133: Current learning rate: 0.00208\n",
      "2025-05-19 03:24:13.111454: train_loss -0.9704\n",
      "2025-05-19 03:24:13.111868: val_loss -0.948\n",
      "2025-05-19 03:24:13.111912: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-19 03:24:13.111951: Epoch time: 109.15 s\n",
      "2025-05-19 03:24:13.700678: \n",
      "2025-05-19 03:24:13.701187: Epoch 826\n",
      "2025-05-19 03:24:13.701465: Current learning rate: 0.00207\n",
      "2025-05-19 03:26:02.777079: train_loss -0.9683\n",
      "2025-05-19 03:26:02.777212: val_loss -0.9461\n",
      "2025-05-19 03:26:02.777248: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-19 03:26:02.777282: Epoch time: 109.08 s\n",
      "2025-05-19 03:26:03.365144: \n",
      "2025-05-19 03:26:03.365613: Epoch 827\n",
      "2025-05-19 03:26:03.365748: Current learning rate: 0.00206\n",
      "2025-05-19 03:27:52.496010: train_loss -0.9703\n",
      "2025-05-19 03:27:52.496137: val_loss -0.9435\n",
      "2025-05-19 03:27:52.496171: Pseudo dice [np.float32(0.9778)]\n",
      "2025-05-19 03:27:52.496204: Epoch time: 109.13 s\n",
      "2025-05-19 03:27:53.050658: \n",
      "2025-05-19 03:27:53.050806: Epoch 828\n",
      "2025-05-19 03:27:53.050882: Current learning rate: 0.00205\n",
      "2025-05-19 03:29:42.059558: train_loss -0.9703\n",
      "2025-05-19 03:29:42.059670: val_loss -0.9469\n",
      "2025-05-19 03:29:42.059701: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-19 03:29:42.059733: Epoch time: 109.01 s\n",
      "2025-05-19 03:29:42.597421: \n",
      "2025-05-19 03:29:42.597585: Epoch 829\n",
      "2025-05-19 03:29:42.597662: Current learning rate: 0.00204\n",
      "2025-05-19 03:31:31.537259: train_loss -0.9699\n",
      "2025-05-19 03:31:31.537384: val_loss -0.937\n",
      "2025-05-19 03:31:31.537418: Pseudo dice [np.float32(0.9736)]\n",
      "2025-05-19 03:31:31.537452: Epoch time: 108.94 s\n",
      "2025-05-19 03:31:32.080773: \n",
      "2025-05-19 03:31:32.080965: Epoch 830\n",
      "2025-05-19 03:31:32.081048: Current learning rate: 0.00203\n",
      "2025-05-19 03:33:21.093452: train_loss -0.9684\n",
      "2025-05-19 03:33:21.093669: val_loss -0.947\n",
      "2025-05-19 03:33:21.093753: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-19 03:33:21.093793: Epoch time: 109.01 s\n",
      "2025-05-19 03:33:21.639497: \n",
      "2025-05-19 03:33:21.639826: Epoch 831\n",
      "2025-05-19 03:33:21.639904: Current learning rate: 0.00202\n",
      "2025-05-19 03:35:10.712814: train_loss -0.9691\n",
      "2025-05-19 03:35:10.712969: val_loss -0.9482\n",
      "2025-05-19 03:35:10.713079: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-19 03:35:10.713248: Epoch time: 109.07 s\n",
      "2025-05-19 03:35:11.257368: \n",
      "2025-05-19 03:35:11.257701: Epoch 832\n",
      "2025-05-19 03:35:11.257797: Current learning rate: 0.00201\n",
      "2025-05-19 03:37:00.267271: train_loss -0.9691\n",
      "2025-05-19 03:37:00.267393: val_loss -0.9458\n",
      "2025-05-19 03:37:00.267427: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-19 03:37:00.267460: Epoch time: 109.01 s\n",
      "2025-05-19 03:37:00.806527: \n",
      "2025-05-19 03:37:00.806867: Epoch 833\n",
      "2025-05-19 03:37:00.806960: Current learning rate: 0.002\n",
      "2025-05-19 03:38:49.779097: train_loss -0.969\n",
      "2025-05-19 03:38:49.779215: val_loss -0.9335\n",
      "2025-05-19 03:38:49.779248: Pseudo dice [np.float32(0.9733)]\n",
      "2025-05-19 03:38:49.779281: Epoch time: 108.97 s\n",
      "2025-05-19 03:38:50.325443: \n",
      "2025-05-19 03:38:50.325622: Epoch 834\n",
      "2025-05-19 03:38:50.325706: Current learning rate: 0.00199\n",
      "2025-05-19 03:40:39.375240: train_loss -0.9686\n",
      "2025-05-19 03:40:39.375367: val_loss -0.939\n",
      "2025-05-19 03:40:39.375401: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-19 03:40:39.375433: Epoch time: 109.05 s\n",
      "2025-05-19 03:40:39.914835: \n",
      "2025-05-19 03:40:39.914923: Epoch 835\n",
      "2025-05-19 03:40:39.914986: Current learning rate: 0.00198\n",
      "2025-05-19 03:42:28.901567: train_loss -0.9708\n",
      "2025-05-19 03:42:28.901710: val_loss -0.9382\n",
      "2025-05-19 03:42:28.901745: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-19 03:42:28.901778: Epoch time: 108.99 s\n",
      "2025-05-19 03:42:29.748814: \n",
      "2025-05-19 03:42:29.749263: Epoch 836\n",
      "2025-05-19 03:42:29.749423: Current learning rate: 0.00196\n",
      "2025-05-19 03:44:18.729485: train_loss -0.97\n",
      "2025-05-19 03:44:18.730181: val_loss -0.9341\n",
      "2025-05-19 03:44:18.730224: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-19 03:44:18.730255: Epoch time: 108.98 s\n",
      "2025-05-19 03:44:19.277187: \n",
      "2025-05-19 03:44:19.277436: Epoch 837\n",
      "2025-05-19 03:44:19.277592: Current learning rate: 0.00195\n",
      "2025-05-19 03:46:08.313507: train_loss -0.97\n",
      "2025-05-19 03:46:08.313625: val_loss -0.9435\n",
      "2025-05-19 03:46:08.313658: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-19 03:46:08.313691: Epoch time: 109.04 s\n",
      "2025-05-19 03:46:08.852995: \n",
      "2025-05-19 03:46:08.853131: Epoch 838\n",
      "2025-05-19 03:46:08.853323: Current learning rate: 0.00194\n",
      "2025-05-19 03:47:57.899683: train_loss -0.9706\n",
      "2025-05-19 03:47:57.899875: val_loss -0.9481\n",
      "2025-05-19 03:47:57.899910: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-19 03:47:57.899944: Epoch time: 109.05 s\n",
      "2025-05-19 03:47:58.447421: \n",
      "2025-05-19 03:47:58.447767: Epoch 839\n",
      "2025-05-19 03:47:58.447863: Current learning rate: 0.00193\n",
      "2025-05-19 03:49:47.456986: train_loss -0.9692\n",
      "2025-05-19 03:49:47.457107: val_loss -0.9402\n",
      "2025-05-19 03:49:47.457141: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-19 03:49:47.457174: Epoch time: 109.01 s\n",
      "2025-05-19 03:49:48.005458: \n",
      "2025-05-19 03:49:48.005653: Epoch 840\n",
      "2025-05-19 03:49:48.005809: Current learning rate: 0.00192\n",
      "2025-05-19 03:51:37.027562: train_loss -0.9692\n",
      "2025-05-19 03:51:37.027691: val_loss -0.946\n",
      "2025-05-19 03:51:37.027731: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-19 03:51:37.027773: Epoch time: 109.02 s\n",
      "2025-05-19 03:51:37.574687: \n",
      "2025-05-19 03:51:37.575047: Epoch 841\n",
      "2025-05-19 03:51:37.575235: Current learning rate: 0.00191\n",
      "2025-05-19 03:53:26.635154: train_loss -0.9692\n",
      "2025-05-19 03:53:26.635272: val_loss -0.9375\n",
      "2025-05-19 03:53:26.635305: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-19 03:53:26.635338: Epoch time: 109.06 s\n",
      "2025-05-19 03:53:27.182195: \n",
      "2025-05-19 03:53:27.182461: Epoch 842\n",
      "2025-05-19 03:53:27.182535: Current learning rate: 0.0019\n",
      "2025-05-19 03:55:16.272103: train_loss -0.9714\n",
      "2025-05-19 03:55:16.272270: val_loss -0.9456\n",
      "2025-05-19 03:55:16.272302: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-19 03:55:16.272334: Epoch time: 109.09 s\n",
      "2025-05-19 03:55:16.813659: \n",
      "2025-05-19 03:55:16.813830: Epoch 843\n",
      "2025-05-19 03:55:16.813909: Current learning rate: 0.00189\n",
      "2025-05-19 03:57:05.895720: train_loss -0.9692\n",
      "2025-05-19 03:57:05.895847: val_loss -0.9393\n",
      "2025-05-19 03:57:05.895883: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-19 03:57:05.895916: Epoch time: 109.08 s\n",
      "2025-05-19 03:57:06.431608: \n",
      "2025-05-19 03:57:06.432043: Epoch 844\n",
      "2025-05-19 03:57:06.432127: Current learning rate: 0.00188\n",
      "2025-05-19 03:58:55.534919: train_loss -0.9683\n",
      "2025-05-19 03:58:55.535100: val_loss -0.9418\n",
      "2025-05-19 03:58:55.535142: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-19 03:58:55.535174: Epoch time: 109.1 s\n",
      "2025-05-19 03:58:56.085809: \n",
      "2025-05-19 03:58:56.086366: Epoch 845\n",
      "2025-05-19 03:58:56.086486: Current learning rate: 0.00187\n",
      "2025-05-19 04:00:45.142287: train_loss -0.9685\n",
      "2025-05-19 04:00:45.142415: val_loss -0.9408\n",
      "2025-05-19 04:00:45.142451: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-19 04:00:45.142485: Epoch time: 109.06 s\n",
      "2025-05-19 04:00:45.701261: \n",
      "2025-05-19 04:00:45.701431: Epoch 846\n",
      "2025-05-19 04:00:45.701515: Current learning rate: 0.00186\n",
      "2025-05-19 04:02:34.717979: train_loss -0.9702\n",
      "2025-05-19 04:02:34.718206: val_loss -0.9469\n",
      "2025-05-19 04:02:34.718253: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-19 04:02:34.718306: Epoch time: 109.02 s\n",
      "2025-05-19 04:02:35.271259: \n",
      "2025-05-19 04:02:35.271375: Epoch 847\n",
      "2025-05-19 04:02:35.271453: Current learning rate: 0.00185\n",
      "2025-05-19 04:04:24.338249: train_loss -0.9708\n",
      "2025-05-19 04:04:24.338474: val_loss -0.9389\n",
      "2025-05-19 04:04:24.338510: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-19 04:04:24.338544: Epoch time: 109.07 s\n",
      "2025-05-19 04:04:24.882430: \n",
      "2025-05-19 04:04:24.882835: Epoch 848\n",
      "2025-05-19 04:04:24.882941: Current learning rate: 0.00184\n",
      "2025-05-19 04:06:13.927827: train_loss -0.9694\n",
      "2025-05-19 04:06:13.927972: val_loss -0.9295\n",
      "2025-05-19 04:06:13.928010: Pseudo dice [np.float32(0.9712)]\n",
      "2025-05-19 04:06:13.928045: Epoch time: 109.05 s\n",
      "2025-05-19 04:06:14.705476: \n",
      "2025-05-19 04:06:14.705885: Epoch 849\n",
      "2025-05-19 04:06:14.706149: Current learning rate: 0.00182\n",
      "2025-05-19 04:08:03.745852: train_loss -0.97\n",
      "2025-05-19 04:08:03.745996: val_loss -0.9429\n",
      "2025-05-19 04:08:03.746048: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-19 04:08:03.746084: Epoch time: 109.04 s\n",
      "2025-05-19 04:08:04.551029: \n",
      "2025-05-19 04:08:04.551158: Epoch 850\n",
      "2025-05-19 04:08:04.551236: Current learning rate: 0.00181\n",
      "2025-05-19 04:09:53.606075: train_loss -0.97\n",
      "2025-05-19 04:09:53.606285: val_loss -0.9464\n",
      "2025-05-19 04:09:53.606321: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-19 04:09:53.606354: Epoch time: 109.06 s\n",
      "2025-05-19 04:09:54.169990: \n",
      "2025-05-19 04:09:54.170132: Epoch 851\n",
      "2025-05-19 04:09:54.170209: Current learning rate: 0.0018\n",
      "2025-05-19 04:11:43.244405: train_loss -0.9694\n",
      "2025-05-19 04:11:43.244579: val_loss -0.9525\n",
      "2025-05-19 04:11:43.244612: Pseudo dice [np.float32(0.9804)]\n",
      "2025-05-19 04:11:43.244645: Epoch time: 109.07 s\n",
      "2025-05-19 04:11:43.801357: \n",
      "2025-05-19 04:11:43.801763: Epoch 852\n",
      "2025-05-19 04:11:43.801939: Current learning rate: 0.00179\n",
      "2025-05-19 04:13:32.883306: train_loss -0.9695\n",
      "2025-05-19 04:13:32.883475: val_loss -0.9402\n",
      "2025-05-19 04:13:32.883510: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-19 04:13:32.883545: Epoch time: 109.08 s\n",
      "2025-05-19 04:13:33.440170: \n",
      "2025-05-19 04:13:33.440370: Epoch 853\n",
      "2025-05-19 04:13:33.440563: Current learning rate: 0.00178\n",
      "2025-05-19 04:15:22.482750: train_loss -0.9705\n",
      "2025-05-19 04:15:22.482944: val_loss -0.9514\n",
      "2025-05-19 04:15:22.482979: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-19 04:15:22.483014: Epoch time: 109.04 s\n",
      "2025-05-19 04:15:23.032932: \n",
      "2025-05-19 04:15:23.033332: Epoch 854\n",
      "2025-05-19 04:15:23.033514: Current learning rate: 0.00177\n",
      "2025-05-19 04:17:12.095802: train_loss -0.9691\n",
      "2025-05-19 04:17:12.095944: val_loss -0.9471\n",
      "2025-05-19 04:17:12.095982: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-19 04:17:12.096017: Epoch time: 109.06 s\n",
      "2025-05-19 04:17:12.651986: \n",
      "2025-05-19 04:17:12.652550: Epoch 855\n",
      "2025-05-19 04:17:12.652659: Current learning rate: 0.00176\n",
      "2025-05-19 04:19:01.723384: train_loss -0.9696\n",
      "2025-05-19 04:19:01.723735: val_loss -0.9418\n",
      "2025-05-19 04:19:01.723807: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-19 04:19:01.723852: Epoch time: 109.07 s\n",
      "2025-05-19 04:19:02.286038: \n",
      "2025-05-19 04:19:02.286150: Epoch 856\n",
      "2025-05-19 04:19:02.286220: Current learning rate: 0.00175\n",
      "2025-05-19 04:20:51.324189: train_loss -0.9705\n",
      "2025-05-19 04:20:51.324316: val_loss -0.9364\n",
      "2025-05-19 04:20:51.324349: Pseudo dice [np.float32(0.9752)]\n",
      "2025-05-19 04:20:51.324382: Epoch time: 109.04 s\n",
      "2025-05-19 04:20:51.877309: \n",
      "2025-05-19 04:20:51.877826: Epoch 857\n",
      "2025-05-19 04:20:51.877948: Current learning rate: 0.00174\n",
      "2025-05-19 04:22:40.887164: train_loss -0.9703\n",
      "2025-05-19 04:22:40.887321: val_loss -0.9466\n",
      "2025-05-19 04:22:40.887443: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-19 04:22:40.887520: Epoch time: 109.01 s\n",
      "2025-05-19 04:22:41.429490: \n",
      "2025-05-19 04:22:41.429644: Epoch 858\n",
      "2025-05-19 04:22:41.429718: Current learning rate: 0.00173\n",
      "2025-05-19 04:24:30.370782: train_loss -0.9703\n",
      "2025-05-19 04:24:30.370917: val_loss -0.9561\n",
      "2025-05-19 04:24:30.370951: Pseudo dice [np.float32(0.9818)]\n",
      "2025-05-19 04:24:30.370984: Epoch time: 108.94 s\n",
      "2025-05-19 04:24:30.929144: \n",
      "2025-05-19 04:24:30.929635: Epoch 859\n",
      "2025-05-19 04:24:30.929718: Current learning rate: 0.00172\n",
      "2025-05-19 04:26:19.974358: train_loss -0.9691\n",
      "2025-05-19 04:26:19.974557: val_loss -0.9384\n",
      "2025-05-19 04:26:19.974593: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-19 04:26:19.974627: Epoch time: 109.05 s\n",
      "2025-05-19 04:26:20.533838: \n",
      "2025-05-19 04:26:20.534113: Epoch 860\n",
      "2025-05-19 04:26:20.534331: Current learning rate: 0.0017\n",
      "2025-05-19 04:28:09.569575: train_loss -0.9705\n",
      "2025-05-19 04:28:09.569747: val_loss -0.9483\n",
      "2025-05-19 04:28:09.569779: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-19 04:28:09.569814: Epoch time: 109.04 s\n",
      "2025-05-19 04:28:10.119304: \n",
      "2025-05-19 04:28:10.119437: Epoch 861\n",
      "2025-05-19 04:28:10.119522: Current learning rate: 0.00169\n",
      "2025-05-19 04:29:59.163026: train_loss -0.9702\n",
      "2025-05-19 04:29:59.163174: val_loss -0.9433\n",
      "2025-05-19 04:29:59.163210: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-19 04:29:59.163254: Epoch time: 109.04 s\n",
      "2025-05-19 04:29:59.953353: \n",
      "2025-05-19 04:29:59.953654: Epoch 862\n",
      "2025-05-19 04:29:59.953884: Current learning rate: 0.00168\n",
      "2025-05-19 04:31:49.005565: train_loss -0.9703\n",
      "2025-05-19 04:31:49.005701: val_loss -0.9405\n",
      "2025-05-19 04:31:49.005739: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-19 04:31:49.005774: Epoch time: 109.05 s\n",
      "2025-05-19 04:31:49.560572: \n",
      "2025-05-19 04:31:49.560898: Epoch 863\n",
      "2025-05-19 04:31:49.561061: Current learning rate: 0.00167\n",
      "2025-05-19 04:33:38.529688: train_loss -0.9702\n",
      "2025-05-19 04:33:38.529848: val_loss -0.9484\n",
      "2025-05-19 04:33:38.529888: Pseudo dice [np.float32(0.9795)]\n",
      "2025-05-19 04:33:38.529937: Epoch time: 108.97 s\n",
      "2025-05-19 04:33:39.082746: \n",
      "2025-05-19 04:33:39.083015: Epoch 864\n",
      "2025-05-19 04:33:39.083145: Current learning rate: 0.00166\n",
      "2025-05-19 04:35:28.162799: train_loss -0.9701\n",
      "2025-05-19 04:35:28.162934: val_loss -0.9386\n",
      "2025-05-19 04:35:28.162968: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-19 04:35:28.163002: Epoch time: 109.08 s\n",
      "2025-05-19 04:35:28.729684: \n",
      "2025-05-19 04:35:28.729950: Epoch 865\n",
      "2025-05-19 04:35:28.730188: Current learning rate: 0.00165\n",
      "2025-05-19 04:37:17.673176: train_loss -0.969\n",
      "2025-05-19 04:37:17.673329: val_loss -0.9499\n",
      "2025-05-19 04:37:17.673365: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-19 04:37:17.673401: Epoch time: 108.94 s\n",
      "2025-05-19 04:37:18.226732: \n",
      "2025-05-19 04:37:18.227055: Epoch 866\n",
      "2025-05-19 04:37:18.227141: Current learning rate: 0.00164\n",
      "2025-05-19 04:39:07.278920: train_loss -0.97\n",
      "2025-05-19 04:39:07.279053: val_loss -0.947\n",
      "2025-05-19 04:39:07.279088: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-19 04:39:07.279120: Epoch time: 109.05 s\n",
      "2025-05-19 04:39:07.828601: \n",
      "2025-05-19 04:39:07.828708: Epoch 867\n",
      "2025-05-19 04:39:07.828780: Current learning rate: 0.00163\n",
      "2025-05-19 04:40:56.798948: train_loss -0.9705\n",
      "2025-05-19 04:40:56.799128: val_loss -0.9449\n",
      "2025-05-19 04:40:56.799191: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-19 04:40:56.799306: Epoch time: 108.97 s\n",
      "2025-05-19 04:40:56.799343: Yayy! New best EMA pseudo Dice: 0.9776999950408936\n",
      "2025-05-19 04:40:57.618799: \n",
      "2025-05-19 04:40:57.618905: Epoch 868\n",
      "2025-05-19 04:40:57.618975: Current learning rate: 0.00162\n",
      "2025-05-19 04:42:46.637912: train_loss -0.9717\n",
      "2025-05-19 04:42:46.638097: val_loss -0.9428\n",
      "2025-05-19 04:42:46.638134: Pseudo dice [np.float32(0.9761)]\n",
      "2025-05-19 04:42:46.638167: Epoch time: 109.02 s\n",
      "2025-05-19 04:42:47.185481: \n",
      "2025-05-19 04:42:47.185901: Epoch 869\n",
      "2025-05-19 04:42:47.186046: Current learning rate: 0.00161\n",
      "2025-05-19 04:44:36.199449: train_loss -0.9711\n",
      "2025-05-19 04:44:36.199565: val_loss -0.9481\n",
      "2025-05-19 04:44:36.199598: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-19 04:44:36.199638: Epoch time: 109.01 s\n",
      "2025-05-19 04:44:36.737762: \n",
      "2025-05-19 04:44:36.737945: Epoch 870\n",
      "2025-05-19 04:44:36.738027: Current learning rate: 0.00159\n",
      "2025-05-19 04:46:25.704035: train_loss -0.9705\n",
      "2025-05-19 04:46:25.704154: val_loss -0.947\n",
      "2025-05-19 04:46:25.704198: Pseudo dice [np.float32(0.9801)]\n",
      "2025-05-19 04:46:25.704231: Epoch time: 108.97 s\n",
      "2025-05-19 04:46:25.704257: Yayy! New best EMA pseudo Dice: 0.9779000282287598\n",
      "2025-05-19 04:46:26.461063: \n",
      "2025-05-19 04:46:26.461155: Epoch 871\n",
      "2025-05-19 04:46:26.461221: Current learning rate: 0.00158\n",
      "2025-05-19 04:48:15.384347: train_loss -0.9713\n",
      "2025-05-19 04:48:15.384534: val_loss -0.9467\n",
      "2025-05-19 04:48:15.384568: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-19 04:48:15.384602: Epoch time: 108.92 s\n",
      "2025-05-19 04:48:15.384623: Yayy! New best EMA pseudo Dice: 0.9779999852180481\n",
      "2025-05-19 04:48:16.151280: \n",
      "2025-05-19 04:48:16.151709: Epoch 872\n",
      "2025-05-19 04:48:16.151924: Current learning rate: 0.00157\n",
      "2025-05-19 04:50:05.172838: train_loss -0.9702\n",
      "2025-05-19 04:50:05.172981: val_loss -0.9463\n",
      "2025-05-19 04:50:05.173023: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-19 04:50:05.173107: Epoch time: 109.02 s\n",
      "2025-05-19 04:50:05.173228: Yayy! New best EMA pseudo Dice: 0.9781000018119812\n",
      "2025-05-19 04:50:05.942941: \n",
      "2025-05-19 04:50:05.943220: Epoch 873\n",
      "2025-05-19 04:50:05.943308: Current learning rate: 0.00156\n",
      "2025-05-19 04:51:54.852333: train_loss -0.9703\n",
      "2025-05-19 04:51:54.852467: val_loss -0.9472\n",
      "2025-05-19 04:51:54.852502: Pseudo dice [np.float32(0.9788)]\n",
      "2025-05-19 04:51:54.852534: Epoch time: 108.91 s\n",
      "2025-05-19 04:51:54.852553: Yayy! New best EMA pseudo Dice: 0.9782000184059143\n",
      "2025-05-19 04:51:55.618058: \n",
      "2025-05-19 04:51:55.618244: Epoch 874\n",
      "2025-05-19 04:51:55.618354: Current learning rate: 0.00155\n",
      "2025-05-19 04:53:44.713332: train_loss -0.9716\n",
      "2025-05-19 04:53:44.713467: val_loss -0.942\n",
      "2025-05-19 04:53:44.713501: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-19 04:53:44.713535: Epoch time: 109.1 s\n",
      "2025-05-19 04:53:45.445775: \n",
      "2025-05-19 04:53:45.445952: Epoch 875\n",
      "2025-05-19 04:53:45.446033: Current learning rate: 0.00154\n",
      "2025-05-19 04:55:34.452967: train_loss -0.9707\n",
      "2025-05-19 04:55:34.453107: val_loss -0.9401\n",
      "[np.float32(0.9763)]453147: Pseudo dice \n",
      "2025-05-19 04:55:34.453240: Epoch time: 109.01 s\n",
      "2025-05-19 04:55:34.995190: \n",
      "2025-05-19 04:55:34.995394: Epoch 876\n",
      "2025-05-19 04:55:34.995474: Current learning rate: 0.00153\n",
      "2025-05-19 04:57:24.083526: train_loss -0.971\n",
      "2025-05-19 04:57:24.083780: val_loss -0.9425\n",
      "2025-05-19 04:57:24.083828: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-19 04:57:24.083866: Epoch time: 109.09 s\n",
      "2025-05-19 04:57:24.631042: \n",
      "2025-05-19 04:57:24.631208: Epoch 877\n",
      "2025-05-19 04:57:24.631337: Current learning rate: 0.00152\n",
      "2025-05-19 04:59:13.703431: train_loss -0.971\n",
      "2025-05-19 04:59:13.703645: val_loss -0.9465\n",
      "2025-05-19 04:59:13.703680: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-19 04:59:13.703715: Epoch time: 109.07 s\n",
      "2025-05-19 04:59:14.236331: \n",
      "2025-05-19 04:59:14.236567: Epoch 878\n",
      "2025-05-19 04:59:14.236790: Current learning rate: 0.00151\n",
      "2025-05-19 05:01:03.317634: train_loss -0.9715\n",
      "2025-05-19 05:01:03.317764: val_loss -0.9471\n",
      "2025-05-19 05:01:03.317799: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-19 05:01:03.317833: Epoch time: 109.08 s\n",
      "2025-05-19 05:01:03.860824: \n",
      "2025-05-19 05:01:03.861055: Epoch 879\n",
      "2025-05-19 05:01:03.861158: Current learning rate: 0.00149\n",
      "2025-05-19 05:02:52.879609: train_loss -0.9704\n",
      "2025-05-19 05:02:52.879868: val_loss -0.9437\n",
      "2025-05-19 05:02:52.879966: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-19 05:02:52.880057: Epoch time: 109.02 s\n",
      "2025-05-19 05:02:53.429189: \n",
      "2025-05-19 05:02:53.429326: Epoch 880\n",
      "2025-05-19 05:02:53.429406: Current learning rate: 0.00148\n",
      "2025-05-19 05:04:42.512072: train_loss -0.9711\n",
      "2025-05-19 05:04:42.512218: val_loss -0.931\n",
      "2025-05-19 05:04:42.512254: Pseudo dice [np.float32(0.9745)]\n",
      "2025-05-19 05:04:42.512293: Epoch time: 109.08 s\n",
      "2025-05-19 05:04:43.056053: \n",
      "2025-05-19 05:04:43.056221: Epoch 881\n",
      "2025-05-19 05:04:43.056304: Current learning rate: 0.00147\n",
      "2025-05-19 05:06:32.123740: train_loss -0.9714\n",
      "2025-05-19 05:06:32.123887: val_loss -0.933\n",
      "2025-05-19 05:06:32.123922: Pseudo dice [np.float32(0.9739)]\n",
      "2025-05-19 05:06:32.123955: Epoch time: 109.07 s\n",
      "2025-05-19 05:06:32.673022: \n",
      "2025-05-19 05:06:32.673462: Epoch 882\n",
      "2025-05-19 05:06:32.673608: Current learning rate: 0.00146\n",
      "2025-05-19 05:08:21.702783: train_loss -0.9705\n",
      "2025-05-19 05:08:21.702911: val_loss -0.9317\n",
      "2025-05-19 05:08:21.702946: Pseudo dice [np.float32(0.9734)]\n",
      "2025-05-19 05:08:21.702981: Epoch time: 109.03 s\n",
      "2025-05-19 05:08:22.249741: \n",
      "2025-05-19 05:08:22.249916: Epoch 883\n",
      "2025-05-19 05:08:22.249988: Current learning rate: 0.00145\n",
      "2025-05-19 05:10:11.278331: train_loss -0.9687\n",
      "2025-05-19 05:10:11.278485: val_loss -0.9487\n",
      "2025-05-19 05:10:11.278522: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-19 05:10:11.278559: Epoch time: 109.03 s\n",
      "2025-05-19 05:10:11.836066: \n",
      "2025-05-19 05:10:11.836177: Epoch 884\n",
      "2025-05-19 05:10:11.836254: Current learning rate: 0.00144\n",
      "2025-05-19 05:12:00.821587: train_loss -0.9691\n",
      "2025-05-19 05:12:00.821794: val_loss -0.945\n",
      "2025-05-19 05:12:00.821833: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-19 05:12:00.821868: Epoch time: 108.99 s\n",
      "2025-05-19 05:12:01.388429: \n",
      "2025-05-19 05:12:01.388892: Epoch 885\n",
      "2025-05-19 05:12:01.388999: Current learning rate: 0.00143\n",
      "2025-05-19 05:13:50.417687: train_loss -0.9706\n",
      "2025-05-19 05:13:50.417814: val_loss -0.9552\n",
      "2025-05-19 05:13:50.417845: Pseudo dice [np.float32(0.9815)]\n",
      "2025-05-19 05:13:50.417879: Epoch time: 109.03 s\n",
      "2025-05-19 05:13:50.970590: \n",
      "2025-05-19 05:13:50.970968: Epoch 886\n",
      "2025-05-19 05:13:50.971061: Current learning rate: 0.00142\n",
      "2025-05-19 05:15:40.012549: train_loss -0.9719\n",
      "2025-05-19 05:15:40.012684: val_loss -0.9512\n",
      "2025-05-19 05:15:40.012719: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-19 05:15:40.012754: Epoch time: 109.04 s\n",
      "2025-05-19 05:15:40.590204: \n",
      "2025-05-19 05:15:40.590414: Epoch 887\n",
      "2025-05-19 05:15:40.590568: Current learning rate: 0.00141\n",
      "2025-05-19 05:17:29.628813: train_loss -0.971\n",
      "2025-05-19 05:17:29.628946: val_loss -0.9397\n",
      "2025-05-19 05:17:29.628980: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-19 05:17:29.629037: Epoch time: 109.04 s\n",
      "2025-05-19 05:17:30.423049: \n",
      "2025-05-19 05:17:30.423441: Epoch 888\n",
      "2025-05-19 05:17:30.423592: Current learning rate: 0.00139\n",
      "2025-05-19 05:19:19.326015: train_loss -0.9706\n",
      "2025-05-19 05:19:19.326154: val_loss -0.9425\n",
      "2025-05-19 05:19:19.326190: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-19 05:19:19.326224: Epoch time: 108.9 s\n",
      "2025-05-19 05:19:19.878632: \n",
      "2025-05-19 05:19:19.879053: Epoch 889\n",
      "2025-05-19 05:19:19.879136: Current learning rate: 0.00138\n",
      "2025-05-19 05:21:08.950771: train_loss -0.9716\n",
      "2025-05-19 05:21:08.950926: val_loss -0.945\n",
      "2025-05-19 05:21:08.950968: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-19 05:21:08.951001: Epoch time: 109.07 s\n",
      "2025-05-19 05:21:09.514911: \n",
      "2025-05-19 05:21:09.515245: Epoch 890\n",
      "2025-05-19 05:21:09.515329: Current learning rate: 0.00137\n",
      "2025-05-19 05:22:58.568712: train_loss -0.9713\n",
      "2025-05-19 05:22:58.568838: val_loss -0.9573\n",
      "2025-05-19 05:22:58.568871: Pseudo dice [np.float32(0.9819)]\n",
      "2025-05-19 05:22:58.569029: Epoch time: 109.05 s\n",
      "2025-05-19 05:22:59.117176: \n",
      "2025-05-19 05:22:59.117561: Epoch 891\n",
      "2025-05-19 05:22:59.117656: Current learning rate: 0.00136\n",
      "2025-05-19 05:24:48.071018: train_loss -0.9689\n",
      "2025-05-19 05:24:48.071303: val_loss -0.9453\n",
      "2025-05-19 05:24:48.071431: Pseudo dice [np.float32(0.9782)]\n",
      "2025-05-19 05:24:48.071474: Epoch time: 108.95 s\n",
      "2025-05-19 05:24:48.630362: \n",
      "2025-05-19 05:24:48.630774: Epoch 892\n",
      "2025-05-19 05:24:48.630875: Current learning rate: 0.00135\n",
      "2025-05-19 05:26:37.597133: train_loss -0.9711\n",
      "2025-05-19 05:26:37.597289: val_loss -0.9414\n",
      "2025-05-19 05:26:37.597326: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-19 05:26:37.597361: Epoch time: 108.97 s\n",
      "2025-05-19 05:26:38.165499: \n",
      "2025-05-19 05:26:38.165614: Epoch 893\n",
      "2025-05-19 05:26:38.165694: Current learning rate: 0.00134\n",
      "2025-05-19 05:28:27.239666: train_loss -0.9701\n",
      "2025-05-19 05:28:27.239810: val_loss -0.9488\n",
      "2025-05-19 05:28:27.239845: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-19 05:28:27.239880: Epoch time: 109.07 s\n",
      "2025-05-19 05:28:27.239902: Yayy! New best EMA pseudo Dice: 0.9782000184059143\n",
      "2025-05-19 05:28:28.033894: \n",
      "2025-05-19 05:28:28.034286: Epoch 894\n",
      "2025-05-19 05:28:28.034489: Current learning rate: 0.00133\n",
      "2025-05-19 05:30:17.079410: train_loss -0.9695\n",
      "2025-05-19 05:30:17.079598: val_loss -0.949\n",
      "2025-05-19 05:30:17.079745: Pseudo dice [np.float32(0.9795)]\n",
      "2025-05-19 05:30:17.079807: Epoch time: 109.05 s\n",
      "2025-05-19 05:30:17.079836: Yayy! New best EMA pseudo Dice: 0.9782999753952026\n",
      "2025-05-19 05:30:17.868736: \n",
      "2025-05-19 05:30:17.869081: Epoch 895\n",
      "2025-05-19 05:30:17.869158: Current learning rate: 0.00132\n",
      "2025-05-19 05:32:06.910911: train_loss -0.9715\n",
      "2025-05-19 05:32:06.911061: val_loss -0.955\n",
      "2025-05-19 05:32:06.911096: Pseudo dice [np.float32(0.9817)]\n",
      "2025-05-19 05:32:06.911132: Epoch time: 109.04 s\n",
      "2025-05-19 05:32:06.911285: Yayy! New best EMA pseudo Dice: 0.978600025177002\n",
      "2025-05-19 05:32:07.704572: \n",
      "2025-05-19 05:32:07.705052: Epoch 896\n",
      "2025-05-19 05:32:07.705161: Current learning rate: 0.0013\n",
      "2025-05-19 05:33:56.664504: train_loss -0.9702\n",
      "2025-05-19 05:33:56.664722: val_loss -0.9475\n",
      "2025-05-19 05:33:56.664791: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-19 05:33:56.664830: Epoch time: 108.96 s\n",
      "2025-05-19 05:33:57.217427: \n",
      "2025-05-19 05:33:57.217851: Epoch 897\n",
      "2025-05-19 05:33:57.217948: Current learning rate: 0.00129\n",
      "2025-05-19 05:35:46.210338: train_loss -0.9702\n",
      "2025-05-19 05:35:46.210481: val_loss -0.9531\n",
      "2025-05-19 05:35:46.210516: Pseudo dice [np.float32(0.9805)]\n",
      "2025-05-19 05:35:46.210552: Epoch time: 108.99 s\n",
      "2025-05-19 05:35:46.210573: Yayy! New best EMA pseudo Dice: 0.9787999987602234\n",
      "2025-05-19 05:35:47.002388: \n",
      "2025-05-19 05:35:47.002504: Epoch 898\n",
      "2025-05-19 05:35:47.002593: Current learning rate: 0.00128\n",
      "2025-05-19 05:37:36.060421: train_loss -0.9702\n",
      "2025-05-19 05:37:36.060565: val_loss -0.9478\n",
      "2025-05-19 05:37:36.060717: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-19 05:37:36.060787: Epoch time: 109.06 s\n",
      "2025-05-19 05:37:36.609948: \n",
      "2025-05-19 05:37:36.610174: Epoch 899\n",
      "2025-05-19 05:37:36.610510: Current learning rate: 0.00127\n",
      "2025-05-19 05:39:25.673780: train_loss -0.9715\n",
      "2025-05-19 05:39:25.673920: val_loss -0.9391\n",
      "[np.float32(0.9749)]673956: Pseudo dice \n",
      "2025-05-19 05:39:25.674049: Epoch time: 109.06 s\n",
      "2025-05-19 05:39:26.459878: \n",
      "2025-05-19 05:39:26.459979: Epoch 900\n",
      "2025-05-19 05:39:26.460060: Current learning rate: 0.00126\n",
      "2025-05-19 05:41:15.546495: train_loss -0.9709\n",
      "2025-05-19 05:41:15.546809: val_loss -0.9464\n",
      "2025-05-19 05:41:15.546876: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-19 05:41:15.546928: Epoch time: 109.09 s\n",
      "2025-05-19 05:41:16.332759: \n",
      "2025-05-19 05:41:16.333007: Epoch 901\n",
      "2025-05-19 05:41:16.333171: Current learning rate: 0.00125\n",
      "2025-05-19 05:43:05.327331: train_loss -0.9708\n",
      "2025-05-19 05:43:05.327451: val_loss -0.9473\n",
      "2025-05-19 05:43:05.327496: Pseudo dice [np.float32(0.9785)]\n",
      "2025-05-19 05:43:05.327529: Epoch time: 109.0 s\n",
      "2025-05-19 05:43:05.874273: \n",
      "2025-05-19 05:43:05.874686: Epoch 902\n",
      "2025-05-19 05:43:05.874768: Current learning rate: 0.00124\n",
      "2025-05-19 05:44:54.809196: train_loss -0.9689\n",
      "2025-05-19 05:44:54.809378: val_loss -0.9445\n",
      "2025-05-19 05:44:54.809412: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-19 05:44:54.809448: Epoch time: 108.94 s\n",
      "2025-05-19 05:44:55.348107: \n",
      "2025-05-19 05:44:55.348643: Epoch 903\n",
      "2025-05-19 05:44:55.348964: Current learning rate: 0.00122\n",
      "2025-05-19 05:46:44.367220: train_loss -0.9719\n",
      "2025-05-19 05:46:44.367388: val_loss -0.939\n",
      "2025-05-19 05:46:44.367430: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-19 05:46:44.367469: Epoch time: 109.02 s\n",
      "2025-05-19 05:46:44.901744: \n",
      "2025-05-19 05:46:44.902117: Epoch 904\n",
      "2025-05-19 05:46:44.902283: Current learning rate: 0.00121\n",
      "2025-05-19 05:48:33.918876: train_loss -0.9712\n",
      "2025-05-19 05:48:33.919003: val_loss -0.9414\n",
      "2025-05-19 05:48:33.919035: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-19 05:48:33.919068: Epoch time: 109.02 s\n",
      "2025-05-19 05:48:34.459350: \n",
      "2025-05-19 05:48:34.459580: Epoch 905\n",
      "2025-05-19 05:48:34.459679: Current learning rate: 0.0012\n",
      "2025-05-19 05:50:23.473272: train_loss -0.9717\n",
      "2025-05-19 05:50:23.473390: val_loss -0.9492\n",
      "2025-05-19 05:50:23.473420: Pseudo dice [np.float32(0.9793)]\n",
      "2025-05-19 05:50:23.473453: Epoch time: 109.01 s\n",
      "2025-05-19 05:50:24.010360: \n",
      "2025-05-19 05:50:24.010627: Epoch 906\n",
      "2025-05-19 05:50:24.010742: Current learning rate: 0.00119\n",
      "2025-05-19 05:52:12.990201: train_loss -0.9726\n",
      "2025-05-19 05:52:12.990324: val_loss -0.9507\n",
      "2025-05-19 05:52:12.990358: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-19 05:52:12.990418: Epoch time: 108.98 s\n",
      "2025-05-19 05:52:13.528367: \n",
      "2025-05-19 05:52:13.528761: Epoch 907\n",
      "2025-05-19 05:52:13.528838: Current learning rate: 0.00118\n",
      "2025-05-19 05:54:02.614394: train_loss -0.9718\n",
      "2025-05-19 05:54:02.614517: val_loss -0.9437\n",
      "2025-05-19 05:54:02.614551: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-19 05:54:02.614582: Epoch time: 109.09 s\n",
      "2025-05-19 05:54:03.153316: \n",
      "2025-05-19 05:54:03.153468: Epoch 908\n",
      "2025-05-19 05:54:03.153540: Current learning rate: 0.00117\n",
      "2025-05-19 05:55:52.173595: train_loss -0.9711\n",
      "2025-05-19 05:55:52.173728: val_loss -0.9506\n",
      "2025-05-19 05:55:52.173774: Pseudo dice [np.float32(0.9805)]\n",
      "2025-05-19 05:55:52.173809: Epoch time: 109.02 s\n",
      "2025-05-19 05:55:52.709962: \n",
      "2025-05-19 05:55:52.710181: Epoch 909\n",
      "2025-05-19 05:55:52.710361: Current learning rate: 0.00116\n",
      "2025-05-19 05:57:41.786334: train_loss -0.9719\n",
      "2025-05-19 05:57:41.786457: val_loss -0.9403\n",
      "2025-05-19 05:57:41.786491: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-19 05:57:41.786524: Epoch time: 109.08 s\n",
      "2025-05-19 05:57:42.312006: \n",
      "2025-05-19 05:57:42.312117: Epoch 910\n",
      "2025-05-19 05:57:42.312196: Current learning rate: 0.00115\n",
      "2025-05-19 05:59:31.335782: train_loss -0.9717\n",
      "2025-05-19 05:59:31.335924: val_loss -0.9381\n",
      "2025-05-19 05:59:31.335958: Pseudo dice [np.float32(0.9755)]\n",
      "2025-05-19 05:59:31.335993: Epoch time: 109.02 s\n",
      "2025-05-19 05:59:31.871296: \n",
      "2025-05-19 05:59:31.871610: Epoch 911\n",
      "2025-05-19 05:59:31.871683: Current learning rate: 0.00113\n",
      "2025-05-19 06:01:20.926700: train_loss -0.9717\n",
      "2025-05-19 06:01:20.926888: val_loss -0.947\n",
      "2025-05-19 06:01:20.926947: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-19 06:01:20.926986: Epoch time: 109.06 s\n",
      "2025-05-19 06:01:21.472778: \n",
      "2025-05-19 06:01:21.472882: Epoch 912\n",
      "2025-05-19 06:01:21.472953: Current learning rate: 0.00112\n",
      "2025-05-19 06:03:10.555555: train_loss -0.9704\n",
      "2025-05-19 06:03:10.555688: val_loss -0.9512\n",
      "2025-05-19 06:03:10.555723: Pseudo dice [np.float32(0.981)]\n",
      "2025-05-19 06:03:10.555756: Epoch time: 109.08 s\n",
      "2025-05-19 06:03:11.100439: \n",
      "2025-05-19 06:03:11.100840: Epoch 913\n",
      "2025-05-19 06:03:11.100986: Current learning rate: 0.00111\n",
      "2025-05-19 06:05:00.191236: train_loss -0.9706\n",
      "2025-05-19 06:05:00.191428: val_loss -0.9458\n",
      "2025-05-19 06:05:00.191462: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-19 06:05:00.191497: Epoch time: 109.09 s\n",
      "2025-05-19 06:05:00.943885: \n",
      "2025-05-19 06:05:00.944254: Epoch 914\n",
      "2025-05-19 06:05:00.944425: Current learning rate: 0.0011\n",
      "2025-05-19 06:06:49.998792: train_loss -0.9708\n",
      "2025-05-19 06:06:49.998921: val_loss -0.9437\n",
      "2025-05-19 06:06:49.998956: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-19 06:06:49.998991: Epoch time: 109.06 s\n",
      "2025-05-19 06:06:50.545973: \n",
      "2025-05-19 06:06:50.546394: Epoch 915\n",
      "2025-05-19 06:06:50.546484: Current learning rate: 0.00109\n",
      "2025-05-19 06:08:39.620756: train_loss -0.9724\n",
      "2025-05-19 06:08:39.621038: val_loss -0.9415\n",
      "2025-05-19 06:08:39.621332: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-19 06:08:39.621382: Epoch time: 109.08 s\n",
      "2025-05-19 06:08:40.174811: \n",
      "2025-05-19 06:08:40.175308: Epoch 916\n",
      "2025-05-19 06:08:40.175400: Current learning rate: 0.00108\n",
      "2025-05-19 06:10:29.241952: train_loss -0.9701\n",
      "2025-05-19 06:10:29.242218: val_loss -0.9418\n",
      "2025-05-19 06:10:29.242297: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-19 06:10:29.242338: Epoch time: 109.07 s\n",
      "2025-05-19 06:10:29.795503: \n",
      "2025-05-19 06:10:29.795923: Epoch 917\n",
      "2025-05-19 06:10:29.796079: Current learning rate: 0.00106\n",
      "2025-05-19 06:12:18.871267: train_loss -0.9718\n",
      "2025-05-19 06:12:18.871414: val_loss -0.9464\n",
      "2025-05-19 06:12:18.871450: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-19 06:12:18.871485: Epoch time: 109.08 s\n",
      "2025-05-19 06:12:19.421145: \n",
      "2025-05-19 06:12:19.421248: Epoch 918\n",
      "2025-05-19 06:12:19.421326: Current learning rate: 0.00105\n",
      "2025-05-19 06:14:08.427734: train_loss -0.9714\n",
      "2025-05-19 06:14:08.427957: val_loss -0.9429\n",
      "2025-05-19 06:14:08.427996: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-19 06:14:08.428032: Epoch time: 109.01 s\n",
      "2025-05-19 06:14:08.983345: \n",
      "2025-05-19 06:14:08.983899: Epoch 919\n",
      "2025-05-19 06:14:08.984015: Current learning rate: 0.00104\n",
      "2025-05-19 06:15:58.048331: train_loss -0.9714\n",
      "2025-05-19 06:15:58.048468: val_loss -0.9458\n",
      "2025-05-19 06:15:58.048501: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-19 06:15:58.048537: Epoch time: 109.07 s\n",
      "2025-05-19 06:15:58.600689: \n",
      "2025-05-19 06:15:58.600814: Epoch 920\n",
      "2025-05-19 06:15:58.600890: Current learning rate: 0.00103\n",
      "2025-05-19 06:17:47.627964: train_loss -0.9703\n",
      "2025-05-19 06:17:47.628102: val_loss -0.9367\n",
      "2025-05-19 06:17:47.628136: Pseudo dice [np.float32(0.9765)]\n",
      "2025-05-19 06:17:47.628180: Epoch time: 109.03 s\n",
      "2025-05-19 06:17:48.184603: \n",
      "2025-05-19 06:17:48.184711: Epoch 921\n",
      "2025-05-19 06:17:48.184779: Current learning rate: 0.00102\n",
      "2025-05-19 06:19:37.239169: train_loss -0.9708\n",
      "2025-05-19 06:19:37.239318: val_loss -0.948\n",
      "2025-05-19 06:19:37.239352: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-19 06:19:37.239387: Epoch time: 109.06 s\n",
      "2025-05-19 06:19:37.797937: \n",
      "2025-05-19 06:19:37.798430: Epoch 922\n",
      "2025-05-19 06:19:37.798507: Current learning rate: 0.00101\n",
      "2025-05-19 06:21:26.848089: train_loss -0.9707\n",
      "2025-05-19 06:21:26.848309: val_loss -0.9476\n",
      "2025-05-19 06:21:26.848351: Pseudo dice [np.float32(0.9806)]\n",
      "2025-05-19 06:21:26.848398: Epoch time: 109.05 s\n",
      "2025-05-19 06:21:27.402776: \n",
      "2025-05-19 06:21:27.402948: Epoch 923\n",
      "2025-05-19 06:21:27.403082: Current learning rate: 0.001\n",
      "2025-05-19 06:23:16.450057: train_loss -0.9724\n",
      "2025-05-19 06:23:16.450188: val_loss -0.9463\n",
      "2025-05-19 06:23:16.450221: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-19 06:23:16.450253: Epoch time: 109.05 s\n",
      "2025-05-19 06:23:16.989130: \n",
      "2025-05-19 06:23:16.989282: Epoch 924\n",
      "2025-05-19 06:23:16.989373: Current learning rate: 0.00098\n",
      "2025-05-19 06:25:06.028340: train_loss -0.972\n",
      "2025-05-19 06:25:06.028543: val_loss -0.9439\n",
      "[np.float32(0.9785)]028586: Pseudo dice \n",
      "2025-05-19 06:25:06.028697: Epoch time: 109.04 s\n",
      "2025-05-19 06:25:06.580927: \n",
      "2025-05-19 06:25:06.581045: Epoch 925\n",
      "2025-05-19 06:25:06.581128: Current learning rate: 0.00097\n",
      "2025-05-19 06:26:55.637961: train_loss -0.9715\n",
      "2025-05-19 06:26:55.638102: val_loss -0.9433\n",
      "2025-05-19 06:26:55.638139: Pseudo dice [np.float32(0.9774)]\n",
      "2025-05-19 06:26:55.638176: Epoch time: 109.06 s\n",
      "2025-05-19 06:26:56.198188: \n",
      "2025-05-19 06:26:56.198620: Epoch 926\n",
      "2025-05-19 06:26:56.198713: Current learning rate: 0.00096\n",
      "2025-05-19 06:28:45.242005: train_loss -0.9696\n",
      "2025-05-19 06:28:45.242192: val_loss -0.9445\n",
      "2025-05-19 06:28:45.242225: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-19 06:28:45.242259: Epoch time: 109.04 s\n",
      "2025-05-19 06:28:45.795223: \n",
      "2025-05-19 06:28:45.795404: Epoch 927\n",
      "2025-05-19 06:28:45.795489: Current learning rate: 0.00095\n",
      "2025-05-19 06:30:34.635991: train_loss -0.9708\n",
      "2025-05-19 06:30:34.636142: val_loss -0.948\n",
      "2025-05-19 06:30:34.636178: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-19 06:30:34.636213: Epoch time: 108.84 s\n",
      "2025-05-19 06:30:35.445004: \n",
      "2025-05-19 06:30:35.445486: Epoch 928\n",
      "2025-05-19 06:30:35.445569: Current learning rate: 0.00094\n",
      "2025-05-19 06:32:24.435669: train_loss -0.971\n",
      "2025-05-19 06:32:24.435812: val_loss -0.9343\n",
      "2025-05-19 06:32:24.435848: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-19 06:32:24.435883: Epoch time: 108.99 s\n",
      "2025-05-19 06:32:24.980919: \n",
      "2025-05-19 06:32:24.981189: Epoch 929\n",
      "2025-05-19 06:32:24.981282: Current learning rate: 0.00092\n",
      "2025-05-19 06:34:13.968642: train_loss -0.9724\n",
      "2025-05-19 06:34:13.968773: val_loss -0.9507\n",
      "2025-05-19 06:34:13.968807: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-19 06:34:13.968839: Epoch time: 108.99 s\n",
      "2025-05-19 06:34:14.524428: \n",
      "2025-05-19 06:34:14.524904: Epoch 930\n",
      "2025-05-19 06:34:14.525007: Current learning rate: 0.00091\n",
      "2025-05-19 06:36:03.609750: train_loss -0.972\n",
      "2025-05-19 06:36:03.609885: val_loss -0.9367\n",
      "2025-05-19 06:36:03.609916: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-19 06:36:03.609951: Epoch time: 109.09 s\n",
      "2025-05-19 06:36:04.180404: \n",
      "2025-05-19 06:36:04.180743: Epoch 931\n",
      "2025-05-19 06:36:04.180830: Current learning rate: 0.0009\n",
      "2025-05-19 06:37:53.221966: train_loss -0.972\n",
      "2025-05-19 06:37:53.222119: val_loss -0.9396\n",
      "2025-05-19 06:37:53.222156: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-19 06:37:53.222193: Epoch time: 109.04 s\n",
      "2025-05-19 06:37:53.767659: \n",
      "2025-05-19 06:37:53.768078: Epoch 932\n",
      "2025-05-19 06:37:53.768173: Current learning rate: 0.00089\n",
      "2025-05-19 06:39:42.740421: train_loss -0.9719\n",
      "2025-05-19 06:39:42.740666: val_loss -0.9475\n",
      "2025-05-19 06:39:42.740714: Pseudo dice [np.float32(0.9799)]\n",
      "2025-05-19 06:39:42.740757: Epoch time: 108.97 s\n",
      "2025-05-19 06:39:43.286063: \n",
      "2025-05-19 06:39:43.286332: Epoch 933\n",
      "2025-05-19 06:39:43.286443: Current learning rate: 0.00088\n",
      "2025-05-19 06:41:32.279584: train_loss -0.9728\n",
      "2025-05-19 06:41:32.279726: val_loss -0.945\n",
      "2025-05-19 06:41:32.279762: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-19 06:41:32.279797: Epoch time: 108.99 s\n",
      "2025-05-19 06:41:32.825638: \n",
      "2025-05-19 06:41:32.826054: Epoch 934\n",
      "2025-05-19 06:41:32.826175: Current learning rate: 0.00087\n",
      "2025-05-19 06:43:21.881878: train_loss -0.9724\n",
      "2025-05-19 06:43:21.881997: val_loss -0.9512\n",
      "2025-05-19 06:43:21.882029: Pseudo dice [np.float32(0.9804)]\n",
      "2025-05-19 06:43:21.882063: Epoch time: 109.06 s\n",
      "2025-05-19 06:43:22.427063: \n",
      "2025-05-19 06:43:22.427346: Epoch 935\n",
      "2025-05-19 06:43:22.427518: Current learning rate: 0.00085\n",
      "2025-05-19 06:45:11.414164: train_loss -0.9727\n",
      "2025-05-19 06:45:11.414285: val_loss -0.9438\n",
      "2025-05-19 06:45:11.414318: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-19 06:45:11.414351: Epoch time: 108.99 s\n",
      "2025-05-19 06:45:11.959493: \n",
      "2025-05-19 06:45:11.959696: Epoch 936\n",
      "2025-05-19 06:45:11.959782: Current learning rate: 0.00084\n",
      "2025-05-19 06:47:00.816031: train_loss -0.9711\n",
      "2025-05-19 06:47:00.816202: val_loss -0.948\n",
      "2025-05-19 06:47:00.816237: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-19 06:47:00.816269: Epoch time: 108.86 s\n",
      "2025-05-19 06:47:01.358546: \n",
      "2025-05-19 06:47:01.359026: Epoch 937\n",
      "2025-05-19 06:47:01.359238: Current learning rate: 0.00083\n",
      "2025-05-19 06:48:50.394364: train_loss -0.9713\n",
      "2025-05-19 06:48:50.394566: val_loss -0.9445\n",
      "2025-05-19 06:48:50.394599: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-19 06:48:50.394631: Epoch time: 109.04 s\n",
      "2025-05-19 06:48:50.945731: \n",
      "2025-05-19 06:48:50.946223: Epoch 938\n",
      "2025-05-19 06:48:50.946372: Current learning rate: 0.00082\n",
      "2025-05-19 06:50:39.866224: train_loss -0.9712\n",
      "2025-05-19 06:50:39.866382: val_loss -0.9415\n",
      "2025-05-19 06:50:39.866415: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-19 06:50:39.866448: Epoch time: 108.92 s\n",
      "2025-05-19 06:50:40.404351: \n",
      "2025-05-19 06:50:40.404499: Epoch 939\n",
      "2025-05-19 06:50:40.404583: Current learning rate: 0.00081\n",
      "2025-05-19 06:52:29.315690: train_loss -0.9726\n",
      "2025-05-19 06:52:29.315897: val_loss -0.9437\n",
      "2025-05-19 06:52:29.315938: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-19 06:52:29.315973: Epoch time: 108.91 s\n",
      "2025-05-19 06:52:29.847828: \n",
      "2025-05-19 06:52:29.848148: Epoch 940\n",
      "2025-05-19 06:52:29.848252: Current learning rate: 0.00079\n",
      "2025-05-19 06:54:18.796862: train_loss -0.9721\n",
      "2025-05-19 06:54:18.796990: val_loss -0.939\n",
      "2025-05-19 06:54:18.797024: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-19 06:54:18.797060: Epoch time: 108.95 s\n",
      "2025-05-19 06:54:19.526468: \n",
      "2025-05-19 06:54:19.526685: Epoch 941\n",
      "2025-05-19 06:54:19.526797: Current learning rate: 0.00078\n",
      "2025-05-19 06:56:08.615243: train_loss -0.9729\n",
      "2025-05-19 06:56:08.615449: val_loss -0.9444\n",
      "2025-05-19 06:56:08.615486: Pseudo dice [np.float32(0.9784)]\n",
      "2025-05-19 06:56:08.615521: Epoch time: 109.09 s\n",
      "2025-05-19 06:56:09.154474: \n",
      "2025-05-19 06:56:09.154850: Epoch 942\n",
      "2025-05-19 06:56:09.154937: Current learning rate: 0.00077\n",
      "2025-05-19 06:57:58.198078: train_loss -0.9721\n",
      "2025-05-19 06:57:58.198207: val_loss -0.9558\n",
      "2025-05-19 06:57:58.198244: Pseudo dice [np.float32(0.9825)]\n",
      "2025-05-19 06:57:58.198292: Epoch time: 109.04 s\n",
      "2025-05-19 06:57:58.738586: \n",
      "2025-05-19 06:57:58.738698: Epoch 943\n",
      "2025-05-19 06:57:58.738765: Current learning rate: 0.00076\n",
      "2025-05-19 06:59:47.739225: train_loss -0.9708\n",
      "2025-05-19 06:59:47.739357: val_loss -0.938\n",
      "2025-05-19 06:59:47.739490: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-19 06:59:47.739636: Epoch time: 109.0 s\n",
      "2025-05-19 06:59:48.279292: \n",
      "2025-05-19 06:59:48.279414: Epoch 944\n",
      "2025-05-19 06:59:48.279494: Current learning rate: 0.00075\n",
      "2025-05-19 07:01:37.339634: train_loss -0.9723\n",
      "2025-05-19 07:01:37.339866: val_loss -0.944\n",
      "2025-05-19 07:01:37.339931: Pseudo dice [np.float32(0.977)]\n",
      "2025-05-19 07:01:37.340082: Epoch time: 109.06 s\n",
      "2025-05-19 07:01:37.892065: \n",
      "2025-05-19 07:01:37.892308: Epoch 945\n",
      "2025-05-19 07:01:37.892381: Current learning rate: 0.00074\n",
      "2025-05-19 07:03:26.899658: train_loss -0.9717\n",
      "2025-05-19 07:03:26.899956: val_loss -0.9482\n",
      "2025-05-19 07:03:26.900103: Pseudo dice [np.float32(0.9792)]\n",
      "2025-05-19 07:03:26.900154: Epoch time: 109.01 s\n",
      "2025-05-19 07:03:27.445250: \n",
      "2025-05-19 07:03:27.445710: Epoch 946\n",
      "2025-05-19 07:03:27.445825: Current learning rate: 0.00072\n",
      "2025-05-19 07:05:16.527766: train_loss -0.9715\n",
      "2025-05-19 07:05:16.527898: val_loss -0.9519\n",
      "2025-05-19 07:05:16.527932: Pseudo dice [np.float32(0.9803)]\n",
      "2025-05-19 07:05:16.527966: Epoch time: 109.08 s\n",
      "2025-05-19 07:05:17.071337: \n",
      "2025-05-19 07:05:17.071648: Epoch 947\n",
      "2025-05-19 07:05:17.071736: Current learning rate: 0.00071\n",
      "2025-05-19 07:07:05.936252: train_loss -0.9723\n",
      "2025-05-19 07:07:05.936384: val_loss -0.9337\n",
      "2025-05-19 07:07:05.936418: Pseudo dice [np.float32(0.9749)]\n",
      "2025-05-19 07:07:05.936451: Epoch time: 108.87 s\n",
      "2025-05-19 07:07:06.490602: \n",
      "2025-05-19 07:07:06.490800: Epoch 948\n",
      "2025-05-19 07:07:06.490874: Current learning rate: 0.0007\n",
      "2025-05-19 07:08:55.567699: train_loss -0.9724\n",
      "2025-05-19 07:08:55.567878: val_loss -0.9507\n",
      "2025-05-19 07:08:55.567910: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-19 07:08:55.567944: Epoch time: 109.08 s\n",
      "2025-05-19 07:08:56.117802: \n",
      "2025-05-19 07:08:56.118176: Epoch 949\n",
      "2025-05-19 07:08:56.118364: Current learning rate: 0.00069\n",
      "2025-05-19 07:10:45.209075: train_loss -0.972\n",
      "2025-05-19 07:10:45.209272: val_loss -0.9308\n",
      "[np.float32(0.9735)]209318: Pseudo dice \n",
      "2025-05-19 07:10:45.209419: Epoch time: 109.09 s\n",
      "2025-05-19 07:10:46.034001: \n",
      "2025-05-19 07:10:46.034226: Epoch 950\n",
      "2025-05-19 07:10:46.034310: Current learning rate: 0.00067\n",
      "2025-05-19 07:12:35.026420: train_loss -0.9723\n",
      "2025-05-19 07:12:35.026845: val_loss -0.9441\n",
      "2025-05-19 07:12:35.026890: Pseudo dice [np.float32(0.9787)]\n",
      "2025-05-19 07:12:35.026925: Epoch time: 108.99 s\n",
      "2025-05-19 07:12:35.591680: \n",
      "2025-05-19 07:12:35.591804: Epoch 951\n",
      "2025-05-19 07:12:35.591920: Current learning rate: 0.00066\n",
      "2025-05-19 07:14:24.641443: train_loss -0.9717\n",
      "2025-05-19 07:14:24.641599: val_loss -0.9443\n",
      "2025-05-19 07:14:24.641637: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-19 07:14:24.641673: Epoch time: 109.05 s\n",
      "2025-05-19 07:14:25.203261: \n",
      "2025-05-19 07:14:25.203745: Epoch 952\n",
      "2025-05-19 07:14:25.203836: Current learning rate: 0.00065\n",
      "2025-05-19 07:16:14.248397: train_loss -0.9727\n",
      "2025-05-19 07:16:14.248537: val_loss -0.9433\n",
      "2025-05-19 07:16:14.248569: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-19 07:16:14.248604: Epoch time: 109.05 s\n",
      "2025-05-19 07:16:14.810254: \n",
      "2025-05-19 07:16:14.810352: Epoch 953\n",
      "2025-05-19 07:16:14.810419: Current learning rate: 0.00064\n",
      "2025-05-19 07:18:03.858015: train_loss -0.9726\n",
      "2025-05-19 07:18:03.858485: val_loss -0.9506\n",
      "2025-05-19 07:18:03.858551: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-19 07:18:03.858594: Epoch time: 109.05 s\n",
      "2025-05-19 07:18:04.423125: \n",
      "2025-05-19 07:18:04.423366: Epoch 954\n",
      "2025-05-19 07:18:04.423458: Current learning rate: 0.00063\n",
      "2025-05-19 07:19:53.404446: train_loss -0.972\n",
      "2025-05-19 07:19:53.404583: val_loss -0.9403\n",
      "2025-05-19 07:19:53.404617: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-19 07:19:53.404650: Epoch time: 108.98 s\n",
      "2025-05-19 07:19:54.214408: \n",
      "2025-05-19 07:19:54.214535: Epoch 955\n",
      "2025-05-19 07:19:54.214609: Current learning rate: 0.00061\n",
      "2025-05-19 07:21:43.251905: train_loss -0.973\n",
      "2025-05-19 07:21:43.252103: val_loss -0.939\n",
      "2025-05-19 07:21:43.252139: Pseudo dice [np.float32(0.9747)]\n",
      "2025-05-19 07:21:43.252174: Epoch time: 109.04 s\n",
      "2025-05-19 07:21:43.825799: \n",
      "2025-05-19 07:21:43.825918: Epoch 956\n",
      "2025-05-19 07:21:43.826228: Current learning rate: 0.0006\n",
      "2025-05-19 07:23:32.878882: train_loss -0.9726\n",
      "2025-05-19 07:23:32.879006: val_loss -0.9445\n",
      "2025-05-19 07:23:32.879040: Pseudo dice [np.float32(0.9797)]\n",
      "2025-05-19 07:23:32.879074: Epoch time: 109.05 s\n",
      "2025-05-19 07:23:33.425946: \n",
      "2025-05-19 07:23:33.426438: Epoch 957\n",
      "2025-05-19 07:23:33.426519: Current learning rate: 0.00059\n",
      "2025-05-19 07:25:22.444434: train_loss -0.9712\n",
      "2025-05-19 07:25:22.444557: val_loss -0.9474\n",
      "2025-05-19 07:25:22.444593: Pseudo dice [np.float32(0.9795)]\n",
      "2025-05-19 07:25:22.444628: Epoch time: 109.02 s\n",
      "2025-05-19 07:25:23.012214: \n",
      "2025-05-19 07:25:23.012502: Epoch 958\n",
      "2025-05-19 07:25:23.012629: Current learning rate: 0.00058\n",
      "2025-05-19 07:27:12.022801: train_loss -0.9727\n",
      "2025-05-19 07:27:12.022920: val_loss -0.9338\n",
      "2025-05-19 07:27:12.022948: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-19 07:27:12.022981: Epoch time: 109.01 s\n",
      "2025-05-19 07:27:12.598332: \n",
      "2025-05-19 07:27:12.598473: Epoch 959\n",
      "2025-05-19 07:27:12.598552: Current learning rate: 0.00056\n",
      "2025-05-19 07:29:01.636064: train_loss -0.9716\n",
      "2025-05-19 07:29:01.636250: val_loss -0.9422\n",
      "2025-05-19 07:29:01.636284: Pseudo dice [np.float32(0.9769)]\n",
      "2025-05-19 07:29:01.636316: Epoch time: 109.04 s\n",
      "2025-05-19 07:29:02.193068: \n",
      "2025-05-19 07:29:02.193365: Epoch 960\n",
      "2025-05-19 07:29:02.193449: Current learning rate: 0.00055\n",
      "2025-05-19 07:30:51.188449: train_loss -0.9723\n",
      "2025-05-19 07:30:51.188613: val_loss -0.9363\n",
      "2025-05-19 07:30:51.188651: Pseudo dice [np.float32(0.9748)]\n",
      "2025-05-19 07:30:51.188691: Epoch time: 109.0 s\n",
      "2025-05-19 07:30:51.750993: \n",
      "2025-05-19 07:30:51.751467: Epoch 961\n",
      "2025-05-19 07:30:51.751598: Current learning rate: 0.00054\n",
      "2025-05-19 07:32:40.796741: train_loss -0.9717\n",
      "2025-05-19 07:32:40.796886: val_loss -0.9487\n",
      "2025-05-19 07:32:40.796937: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-19 07:32:40.797004: Epoch time: 109.05 s\n",
      "2025-05-19 07:32:41.353798: \n",
      "2025-05-19 07:32:41.354041: Epoch 962\n",
      "2025-05-19 07:32:41.354220: Current learning rate: 0.00053\n",
      "2025-05-19 07:34:30.338880: train_loss -0.9731\n",
      "2025-05-19 07:34:30.339041: val_loss -0.9477\n",
      "2025-05-19 07:34:30.339076: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-19 07:34:30.339110: Epoch time: 108.99 s\n",
      "2025-05-19 07:34:30.894392: \n",
      "2025-05-19 07:34:30.894496: Epoch 963\n",
      "2025-05-19 07:34:30.894565: Current learning rate: 0.00051\n",
      "2025-05-19 07:36:19.970269: train_loss -0.973\n",
      "2025-05-19 07:36:19.970629: val_loss -0.9414\n",
      "2025-05-19 07:36:19.970730: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-19 07:36:19.970772: Epoch time: 109.08 s\n",
      "2025-05-19 07:36:20.526368: \n",
      "2025-05-19 07:36:20.526695: Epoch 964\n",
      "2025-05-19 07:36:20.526781: Current learning rate: 0.0005\n",
      "2025-05-19 07:38:09.594195: train_loss -0.9737\n",
      "2025-05-19 07:38:09.594365: val_loss -0.9368\n",
      "2025-05-19 07:38:09.594408: Pseudo dice [np.float32(0.9762)]\n",
      "2025-05-19 07:38:09.594441: Epoch time: 109.07 s\n",
      "2025-05-19 07:38:10.151434: \n",
      "2025-05-19 07:38:10.151610: Epoch 965\n",
      "2025-05-19 07:38:10.151694: Current learning rate: 0.00049\n",
      "2025-05-19 07:39:59.205784: train_loss -0.972\n",
      "2025-05-19 07:39:59.205918: val_loss -0.945\n",
      "2025-05-19 07:39:59.205951: Pseudo dice [np.float32(0.9786)]\n",
      "2025-05-19 07:39:59.205983: Epoch time: 109.05 s\n",
      "2025-05-19 07:39:59.764836: \n",
      "2025-05-19 07:39:59.765130: Epoch 966\n",
      "2025-05-19 07:39:59.765211: Current learning rate: 0.00048\n",
      "2025-05-19 07:41:48.804058: train_loss -0.972\n",
      "2025-05-19 07:41:48.804197: val_loss -0.9416\n",
      "2025-05-19 07:41:48.804233: Pseudo dice [np.float32(0.9781)]\n",
      "2025-05-19 07:41:48.804267: Epoch time: 109.04 s\n",
      "2025-05-19 07:41:49.363115: \n",
      "2025-05-19 07:41:49.363299: Epoch 967\n",
      "2025-05-19 07:41:49.363483: Current learning rate: 0.00046\n",
      "2025-05-19 07:43:38.363588: train_loss -0.9729\n",
      "2025-05-19 07:43:38.363706: val_loss -0.942\n",
      "2025-05-19 07:43:38.363739: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-19 07:43:38.363771: Epoch time: 109.0 s\n",
      "2025-05-19 07:43:39.084569: \n",
      "2025-05-19 07:43:39.084765: Epoch 968\n",
      "2025-05-19 07:43:39.084840: Current learning rate: 0.00045\n",
      "2025-05-19 07:45:28.053169: train_loss -0.9727\n",
      "2025-05-19 07:45:28.053288: val_loss -0.9407\n",
      "2025-05-19 07:45:28.053331: Pseudo dice [np.float32(0.9764)]\n",
      "2025-05-19 07:45:28.053370: Epoch time: 108.97 s\n",
      "2025-05-19 07:45:28.606217: \n",
      "2025-05-19 07:45:28.606556: Epoch 969\n",
      "2025-05-19 07:45:28.606649: Current learning rate: 0.00044\n",
      "2025-05-19 07:47:17.647733: train_loss -0.9724\n",
      "2025-05-19 07:47:17.647882: val_loss -0.9494\n",
      "2025-05-19 07:47:17.647918: Pseudo dice [np.float32(0.9796)]\n",
      "2025-05-19 07:47:17.647953: Epoch time: 109.04 s\n",
      "2025-05-19 07:47:18.191631: \n",
      "2025-05-19 07:47:18.192205: Epoch 970\n",
      "2025-05-19 07:47:18.192365: Current learning rate: 0.00043\n",
      "2025-05-19 07:49:07.195677: train_loss -0.9726\n",
      "2025-05-19 07:49:07.195889: val_loss -0.945\n",
      "2025-05-19 07:49:07.195956: Pseudo dice [np.float32(0.9789)]\n",
      "2025-05-19 07:49:07.195994: Epoch time: 109.0 s\n",
      "2025-05-19 07:49:07.742409: \n",
      "2025-05-19 07:49:07.742657: Epoch 971\n",
      "2025-05-19 07:49:07.742816: Current learning rate: 0.00041\n",
      "2025-05-19 07:50:56.723179: train_loss -0.972\n",
      "2025-05-19 07:50:56.723361: val_loss -0.945\n",
      "2025-05-19 07:50:56.723501: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-19 07:50:56.723567: Epoch time: 108.98 s\n",
      "2025-05-19 07:50:57.263336: \n",
      "2025-05-19 07:50:57.263727: Epoch 972\n",
      "2025-05-19 07:50:57.263828: Current learning rate: 0.0004\n",
      "2025-05-19 07:52:46.270592: train_loss -0.9746\n",
      "2025-05-19 07:52:46.270826: val_loss -0.9295\n",
      "2025-05-19 07:52:46.270926: Pseudo dice [np.float32(0.9732)]\n",
      "2025-05-19 07:52:46.270980: Epoch time: 109.01 s\n",
      "2025-05-19 07:52:46.815853: \n",
      "2025-05-19 07:52:46.815958: Epoch 973\n",
      "2025-05-19 07:52:46.816033: Current learning rate: 0.00039\n",
      "2025-05-19 07:54:35.818782: train_loss -0.967\n",
      "2025-05-19 07:54:35.818961: val_loss -0.9426\n",
      "2025-05-19 07:54:35.818996: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-19 07:54:35.819031: Epoch time: 109.0 s\n",
      "2025-05-19 07:54:36.358830: \n",
      "2025-05-19 07:54:36.359035: Epoch 974\n",
      "2025-05-19 07:54:36.359125: Current learning rate: 0.00037\n",
      "2025-05-19 07:56:25.415024: train_loss -0.9731\n",
      "2025-05-19 07:56:25.415148: val_loss -0.9407\n",
      "[np.float32(0.9771)]415184: Pseudo dice \n",
      "2025-05-19 07:56:25.415279: Epoch time: 109.06 s\n",
      "2025-05-19 07:56:25.964818: \n",
      "2025-05-19 07:56:25.965295: Epoch 975\n",
      "2025-05-19 07:56:25.965394: Current learning rate: 0.00036\n",
      "2025-05-19 07:58:15.064306: train_loss -0.973\n",
      "2025-05-19 07:58:15.064430: val_loss -0.945\n",
      "2025-05-19 07:58:15.064464: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-19 07:58:15.064497: Epoch time: 109.1 s\n",
      "2025-05-19 07:58:15.616306: \n",
      "2025-05-19 07:58:15.616771: Epoch 976\n",
      "2025-05-19 07:58:15.617012: Current learning rate: 0.00035\n",
      "2025-05-19 08:00:04.638982: train_loss -0.9719\n",
      "2025-05-19 08:00:04.639109: val_loss -0.9471\n",
      "2025-05-19 08:00:04.639142: Pseudo dice [np.float32(0.9794)]\n",
      "2025-05-19 08:00:04.639175: Epoch time: 109.02 s\n",
      "2025-05-19 08:00:05.197001: \n",
      "2025-05-19 08:00:05.197397: Epoch 977\n",
      "2025-05-19 08:00:05.197553: Current learning rate: 0.00034\n",
      "2025-05-19 08:01:54.253050: train_loss -0.9724\n",
      "2025-05-19 08:01:54.253187: val_loss -0.9349\n",
      "2025-05-19 08:01:54.253223: Pseudo dice [np.float32(0.9756)]\n",
      "2025-05-19 08:01:54.253258: Epoch time: 109.06 s\n",
      "2025-05-19 08:01:54.802514: \n",
      "2025-05-19 08:01:54.802770: Epoch 978\n",
      "2025-05-19 08:01:54.802902: Current learning rate: 0.00032\n",
      "2025-05-19 08:03:43.777433: train_loss -0.9743\n",
      "2025-05-19 08:03:43.777561: val_loss -0.9433\n",
      "2025-05-19 08:03:43.777596: Pseudo dice [np.float32(0.9763)]\n",
      "2025-05-19 08:03:43.777630: Epoch time: 108.98 s\n",
      "2025-05-19 08:03:44.334308: \n",
      "2025-05-19 08:03:44.334435: Epoch 979\n",
      "2025-05-19 08:03:44.334515: Current learning rate: 0.00031\n",
      "2025-05-19 08:05:33.385365: train_loss -0.9728\n",
      "2025-05-19 08:05:33.385493: val_loss -0.9431\n",
      "2025-05-19 08:05:33.385527: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-19 08:05:33.385561: Epoch time: 109.05 s\n",
      "2025-05-19 08:05:34.137263: \n",
      "2025-05-19 08:05:34.137647: Epoch 980\n",
      "2025-05-19 08:05:34.137944: Current learning rate: 0.0003\n",
      "2025-05-19 08:07:23.095154: train_loss -0.9727\n",
      "2025-05-19 08:07:23.095366: val_loss -0.9459\n",
      "2025-05-19 08:07:23.095403: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-19 08:07:23.095444: Epoch time: 108.96 s\n",
      "2025-05-19 08:07:23.651673: \n",
      "2025-05-19 08:07:23.652041: Epoch 981\n",
      "2025-05-19 08:07:23.652173: Current learning rate: 0.00028\n",
      "2025-05-19 08:09:12.480529: train_loss -0.9734\n",
      "2025-05-19 08:09:12.480683: val_loss -0.9421\n",
      "2025-05-19 08:09:12.480744: Pseudo dice [np.float32(0.9775)]\n",
      "2025-05-19 08:09:12.480786: Epoch time: 108.83 s\n",
      "2025-05-19 08:09:13.046782: \n",
      "2025-05-19 08:09:13.047269: Epoch 982\n",
      "2025-05-19 08:09:13.047366: Current learning rate: 0.00027\n",
      "2025-05-19 08:11:02.120380: train_loss -0.9729\n",
      "2025-05-19 08:11:02.120527: val_loss -0.9384\n",
      "2025-05-19 08:11:02.120564: Pseudo dice [np.float32(0.9771)]\n",
      "2025-05-19 08:11:02.120600: Epoch time: 109.07 s\n",
      "2025-05-19 08:11:02.690925: \n",
      "2025-05-19 08:11:02.691248: Epoch 983\n",
      "2025-05-19 08:11:02.691404: Current learning rate: 0.00026\n",
      "2025-05-19 08:12:51.747681: train_loss -0.9744\n",
      "2025-05-19 08:12:51.748194: val_loss -0.9516\n",
      "2025-05-19 08:12:51.748381: Pseudo dice [np.float32(0.9807)]\n",
      "2025-05-19 08:12:51.748458: Epoch time: 109.06 s\n",
      "2025-05-19 08:12:52.320147: \n",
      "2025-05-19 08:12:52.320815: Epoch 984\n",
      "2025-05-19 08:12:52.321101: Current learning rate: 0.00024\n",
      "2025-05-19 08:14:41.378356: train_loss -0.973\n",
      "2025-05-19 08:14:41.378512: val_loss -0.9563\n",
      "2025-05-19 08:14:41.378546: Pseudo dice [np.float32(0.9823)]\n",
      "2025-05-19 08:14:41.378580: Epoch time: 109.06 s\n",
      "2025-05-19 08:14:41.944387: \n",
      "2025-05-19 08:14:41.944619: Epoch 985\n",
      "2025-05-19 08:14:41.944704: Current learning rate: 0.00023\n",
      "2025-05-19 08:16:30.980028: train_loss -0.9721\n",
      "2025-05-19 08:16:30.980247: val_loss -0.9411\n",
      "2025-05-19 08:16:30.980283: Pseudo dice [np.float32(0.9772)]\n",
      "2025-05-19 08:16:30.980318: Epoch time: 109.04 s\n",
      "2025-05-19 08:16:31.550661: \n",
      "2025-05-19 08:16:31.550784: Epoch 986\n",
      "2025-05-19 08:16:31.550858: Current learning rate: 0.00021\n",
      "2025-05-19 08:18:20.613746: train_loss -0.973\n",
      "2025-05-19 08:18:20.613884: val_loss -0.9435\n",
      "2025-05-19 08:18:20.613918: Pseudo dice [np.float32(0.9777)]\n",
      "2025-05-19 08:18:20.613953: Epoch time: 109.06 s\n",
      "2025-05-19 08:18:21.184311: \n",
      "2025-05-19 08:18:21.184854: Epoch 987\n",
      "2025-05-19 08:18:21.184962: Current learning rate: 0.0002\n",
      "2025-05-19 08:20:10.245620: train_loss -0.9721\n",
      "2025-05-19 08:20:10.245874: val_loss -0.9381\n",
      "2025-05-19 08:20:10.245911: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-19 08:20:10.245947: Epoch time: 109.06 s\n",
      "2025-05-19 08:20:10.813685: \n",
      "2025-05-19 08:20:10.814105: Epoch 988\n",
      "2025-05-19 08:20:10.814201: Current learning rate: 0.00019\n",
      "2025-05-19 08:21:59.871207: train_loss -0.9733\n",
      "2025-05-19 08:21:59.871352: val_loss -0.9373\n",
      "2025-05-19 08:21:59.871388: Pseudo dice [np.float32(0.9759)]\n",
      "2025-05-19 08:21:59.871423: Epoch time: 109.06 s\n",
      "2025-05-19 08:22:00.460439: \n",
      "2025-05-19 08:22:00.460561: Epoch 989\n",
      "2025-05-19 08:22:00.460634: Current learning rate: 0.00017\n",
      "2025-05-19 08:23:49.377995: train_loss -0.9727\n",
      "2025-05-19 08:23:49.378135: val_loss -0.9434\n",
      "2025-05-19 08:23:49.378169: Pseudo dice [np.float32(0.9767)]\n",
      "2025-05-19 08:23:49.378203: Epoch time: 108.92 s\n",
      "2025-05-19 08:23:49.947650: \n",
      "2025-05-19 08:23:49.947781: Epoch 990\n",
      "2025-05-19 08:23:49.947860: Current learning rate: 0.00016\n",
      "2025-05-19 08:25:38.965252: train_loss -0.9725\n",
      "2025-05-19 08:25:38.965379: val_loss -0.9532\n",
      "2025-05-19 08:25:38.965410: Pseudo dice [np.float32(0.9802)]\n",
      "2025-05-19 08:25:38.965492: Epoch time: 109.02 s\n",
      "2025-05-19 08:25:39.514471: \n",
      "2025-05-19 08:25:39.514616: Epoch 991\n",
      "2025-05-19 08:25:39.514684: Current learning rate: 0.00014\n",
      "2025-05-19 08:27:28.559292: train_loss -0.9723\n",
      "2025-05-19 08:27:28.559607: val_loss -0.9413\n",
      "2025-05-19 08:27:28.559644: Pseudo dice [np.float32(0.9773)]\n",
      "2025-05-19 08:27:28.559679: Epoch time: 109.05 s\n",
      "2025-05-19 08:27:29.125577: \n",
      "2025-05-19 08:27:29.125850: Epoch 992\n",
      "2025-05-19 08:27:29.126123: Current learning rate: 0.00013\n",
      "2025-05-19 08:29:18.190293: train_loss -0.9728\n",
      "2025-05-19 08:29:18.190424: val_loss -0.9465\n",
      "2025-05-19 08:29:18.190458: Pseudo dice [np.float32(0.979)]\n",
      "2025-05-19 08:29:18.190492: Epoch time: 109.07 s\n",
      "2025-05-19 08:29:19.000225: \n",
      "2025-05-19 08:29:19.000344: Epoch 993\n",
      "2025-05-19 08:29:19.000423: Current learning rate: 0.00011\n",
      "2025-05-19 08:31:07.960477: train_loss -0.9724\n",
      "2025-05-19 08:31:07.960618: val_loss -0.9531\n",
      "2025-05-19 08:31:07.960654: Pseudo dice [np.float32(0.9808)]\n",
      "2025-05-19 08:31:07.960688: Epoch time: 108.96 s\n",
      "2025-05-19 08:31:08.514285: \n",
      "2025-05-19 08:31:08.514416: Epoch 994\n",
      "2025-05-19 08:31:08.514496: Current learning rate: 0.0001\n",
      "2025-05-19 08:32:57.477285: train_loss -0.973\n",
      "2025-05-19 08:32:57.477421: val_loss -0.9435\n",
      "2025-05-19 08:32:57.477453: Pseudo dice [np.float32(0.978)]\n",
      "2025-05-19 08:32:57.477488: Epoch time: 108.96 s\n",
      "2025-05-19 08:32:58.040532: \n",
      "2025-05-19 08:32:58.040827: Epoch 995\n",
      "2025-05-19 08:32:58.041043: Current learning rate: 8e-05\n",
      "2025-05-19 08:34:46.984132: train_loss -0.9723\n",
      "2025-05-19 08:34:46.984351: val_loss -0.9457\n",
      "2025-05-19 08:34:46.984426: Pseudo dice [np.float32(0.9783)]\n",
      "2025-05-19 08:34:46.984468: Epoch time: 108.94 s\n",
      "2025-05-19 08:34:47.550401: \n",
      "2025-05-19 08:34:47.550659: Epoch 996\n",
      "2025-05-19 08:34:47.550819: Current learning rate: 7e-05\n",
      "2025-05-19 08:36:36.384844: train_loss -0.9729\n",
      "2025-05-19 08:36:36.384982: val_loss -0.9378\n",
      "2025-05-19 08:36:36.385018: Pseudo dice [np.float32(0.9776)]\n",
      "2025-05-19 08:36:36.385052: Epoch time: 108.83 s\n",
      "2025-05-19 08:36:36.950087: \n",
      "2025-05-19 08:36:36.950519: Epoch 997\n",
      "2025-05-19 08:36:36.950600: Current learning rate: 5e-05\n",
      "2025-05-19 08:38:25.943222: train_loss -0.9729\n",
      "2025-05-19 08:38:25.943360: val_loss -0.9411\n",
      "2025-05-19 08:38:25.943393: Pseudo dice [np.float32(0.9779)]\n",
      "2025-05-19 08:38:25.943425: Epoch time: 108.99 s\n",
      "2025-05-19 08:38:26.505312: \n",
      "2025-05-19 08:38:26.505534: Epoch 998\n",
      "2025-05-19 08:38:26.505636: Current learning rate: 4e-05\n",
      "2025-05-19 08:40:15.332638: train_loss -0.973\n",
      "2025-05-19 08:40:15.332789: val_loss -0.9425\n",
      "2025-05-19 08:40:15.332825: Pseudo dice [np.float32(0.9791)]\n",
      "2025-05-19 08:40:15.332860: Epoch time: 108.83 s\n",
      "2025-05-19 08:40:15.895296: \n",
      "2025-05-19 08:40:15.895568: Epoch 999\n",
      "2025-05-19 08:40:15.895710: Current learning rate: 2e-05\n",
      "2025-05-19 08:42:04.872254: train_loss -0.9735\n",
      "2025-05-19 08:42:04.872419: val_loss -0.9467\n",
      "[np.float32(0.9788)]872549: Pseudo dice \n",
      "2025-05-19 08:42:04.872713: Epoch time: 108.98 s\n",
      "2025-05-19 08:42:05.727832: Training done.\n",
      "2025-05-19 08:42:05.738263: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/splits_final.json\n",
      "2025-05-19 08:42:05.738803: The split file contains 5 splits.\n",
      "2025-05-19 08:42:05.738918: Desired fold for training: 4\n",
      "2025-05-19 08:42:05.738982: This split has 29 training and 7 validation cases.\n",
      "2025-05-19 08:42:05.739147: predicting LCTSC-Train-S1-002\n",
      "2025-05-19 08:42:05.741301: LCTSC-Train-S1-002, shape torch.Size([1, 193, 512, 512]), rank 0\n",
      "2025-05-19 08:43:24.944841: predicting LCTSC-Train-S2-006\n",
      "2025-05-19 08:43:24.947552: LCTSC-Train-S2-006, shape torch.Size([1, 144, 512, 512]), rank 0\n",
      "2025-05-19 08:44:25.619304: predicting LCTSC-Train-S2-011\n",
      "2025-05-19 08:44:25.621924: LCTSC-Train-S2-011, shape torch.Size([1, 160, 512, 512]), rank 0\n",
      "2025-05-19 08:45:26.298408: predicting LCTSC-Train-S3-006\n",
      "2025-05-19 08:45:26.300508: LCTSC-Train-S3-006, shape torch.Size([1, 178, 614, 614]), rank 0\n",
      "2025-05-19 08:47:11.204547: predicting LCTSC-Train-S3-010\n",
      "2025-05-19 08:47:11.207433: LCTSC-Train-S3-010, shape torch.Size([1, 180, 614, 614]), rank 0\n",
      "2025-05-19 08:48:56.101194: predicting LCTSC-Train-S3-011\n",
      "2025-05-19 08:48:56.104528: LCTSC-Train-S3-011, shape torch.Size([1, 160, 614, 614]), rank 0\n",
      "2025-05-19 08:50:23.542632: predicting LCTSC-Train-S3-012\n",
      "2025-05-19 08:50:23.545393: LCTSC-Train-S3-012, shape torch.Size([1, 107, 512, 512]), rank 0\n",
      "2025-05-19 08:51:12.371802: Validation complete\n",
      "2025-05-19 08:51:12.371867: Mean Validation Dice:  0.976975300030342\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 3\n",
    "config = \"3d_fullres\"\n",
    "\n",
    "for fold in range(5):\n",
    "    command = f'nnUNetv2_train {dataset_id} {config} {fold} --c'\n",
    "    print(f\"🔁 Running Fold {fold}: {command}\")\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluate All 5 Training Folds (Cross-Validation Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running test set prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/inference/predict_from_raw_data.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(join(model_training_output_dir, f'fold_{f}', checkpoint_name),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 24 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 24 cases that I would like to predict\n",
      "\n",
      "Predicting LCTSC-Test-S1-101:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:15<00:00,  1.66it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.71it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.71it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S1-101\n",
      "\n",
      "Predicting LCTSC-Test-S1-102:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.71it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S1-102\n",
      "\n",
      "Predicting LCTSC-Test-S1-103:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.71it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S1-103\n",
      "\n",
      "Predicting LCTSC-Test-S1-104:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:12<00:00,  1.71it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.71it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.71it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.71it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S1-104\n",
      "\n",
      "Predicting LCTSC-Test-S1-201:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S1-201\n",
      "\n",
      "Predicting LCTSC-Test-S1-202:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S1-202\n",
      "\n",
      "Predicting LCTSC-Test-S1-203:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S1-203\n",
      "\n",
      "Predicting LCTSC-Test-S1-204:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S1-204\n",
      "\n",
      "Predicting LCTSC-Test-S2-101:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S2-101\n",
      "\n",
      "Predicting LCTSC-Test-S2-102:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S2-102\n",
      "\n",
      "Predicting LCTSC-Test-S2-103:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S2-103\n",
      "\n",
      "Predicting LCTSC-Test-S2-104:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:13<00:00,  1.71it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S2-104\n",
      "\n",
      "Predicting LCTSC-Test-S2-201:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:57<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S2-201\n",
      "\n",
      "Predicting LCTSC-Test-S2-202:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S2-202\n",
      "\n",
      "Predicting LCTSC-Test-S2-203:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S2-203\n",
      "\n",
      "Predicting LCTSC-Test-S2-204:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n",
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S2-204\n",
      "\n",
      "Predicting LCTSC-Test-S3-101:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.72it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S3-101\n",
      "\n",
      "Predicting LCTSC-Test-S3-102:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:41<00:00,  1.73it/s]\n",
      "100%|██████████| 175/175 [01:41<00:00,  1.73it/s]\n",
      "100%|██████████| 175/175 [01:41<00:00,  1.73it/s]\n",
      "100%|██████████| 175/175 [01:41<00:00,  1.72it/s]\n",
      "100%|██████████| 175/175 [01:41<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S3-102\n",
      "\n",
      "Predicting LCTSC-Test-S3-103:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S3-103\n",
      "\n",
      "Predicting LCTSC-Test-S3-104:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:12<00:00,  1.73it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.73it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.73it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.73it/s]\n",
      "100%|██████████| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S3-104\n",
      "\n",
      "Predicting LCTSC-Test-S3-201:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [01:44<00:00,  1.72it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.72it/s]\n",
      "100%|██████████| 180/180 [01:44<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S3-201\n",
      "\n",
      "Predicting LCTSC-Test-S3-202:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n",
      "100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n",
      "100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n",
      "100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n",
      "100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S3-202\n",
      "\n",
      "Predicting LCTSC-Test-S3-203:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:41<00:00,  1.73it/s]\n",
      "100%|██████████| 175/175 [01:41<00:00,  1.73it/s]\n",
      "100%|██████████| 175/175 [01:41<00:00,  1.73it/s]\n",
      "100%|██████████| 175/175 [01:41<00:00,  1.73it/s]\n",
      "100%|██████████| 175/175 [01:41<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S3-203\n",
      "\n",
      "Predicting LCTSC-Test-S3-204:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:26<00:00,  1.73it/s]\n",
      "100%|██████████| 150/150 [01:26<00:00,  1.73it/s]\n",
      "100%|██████████| 150/150 [01:26<00:00,  1.73it/s]\n",
      "100%|██████████| 150/150 [01:26<00:00,  1.73it/s]\n",
      "100%|██████████| 150/150 [01:26<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with LCTSC-Test-S3-204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths\n",
    "dataset_id = 3\n",
    "config = \"3d_fullres\"\n",
    "trainer = \"nnUNetTrainer\"\n",
    "plans = \"nnUNetPlans\"\n",
    "imagesTs = \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset003_Lung_only/imagesTs\"\n",
    "pred_folder = \"/home/doodledaron/FYP/nnUnet/FYP-file/predictions\"\n",
    "\n",
    "# Ensemble across all 5 folds\n",
    "predict_command = (\n",
    "    f\"nnUNetv2_predict \"\n",
    "    f\"-d {dataset_id} \"\n",
    "    f\"-c {config} \"\n",
    "    f\"-tr {trainer} \"\n",
    "    f\"-p {plans} \"\n",
    "    f\"-i {imagesTs} \"\n",
    "    f\"-o {pred_folder}\"\n",
    ")\n",
    "\n",
    "print(\"🚀 Running test set prediction...\")\n",
    "os.system(predict_command)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluating predictions...\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "ground_truth = \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset003_Lung_only/labelsTs\"\n",
    "dataset_json = \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset003_Lung_only/dataset.json\"\n",
    "plans_file = \"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset003_Lung_only/nnUNetPlans.json\"\n",
    "output_file = \"/home/doodledaron/FYP/nnUnet/FYP-file/test_evaluation.json\"\n",
    "\n",
    "# Evaluation command\n",
    "eval_command = (\n",
    "    f\"nnUNetv2_evaluate_folder {ground_truth} {pred_folder} \"\n",
    "    f\"-djfile {dataset_json} \"\n",
    "    f\"-pfile {plans_file} \"\n",
    "    f\"-o {output_file}\"\n",
    ")\n",
    "\n",
    "print(\"📊 Evaluating predictions...\")\n",
    "os.system(eval_command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ✅ Replace this with one of your test case names\n",
    "case_id = \"LCTSC-Test-S1-101\"\n",
    "\n",
    "# Paths\n",
    "pred_path = f\"/home/doodledaron/FYP/nnUnet/FYP-file/predictions/{case_id}.nii.gz\"\n",
    "ct_path   = f\"/home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset003_Lung_only/imagesTs/{case_id}_0000.nii.gz\"\n",
    "# /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_raw/Dataset003_Lung_only/imagesTs/LCTSC-Test-S1-101_0000.nii.gz\n",
    "# Load NIfTI volumes\n",
    "ct = nib.load(ct_path).get_fdata()\n",
    "pred = nib.load(pred_path).get_fdata()\n",
    "\n",
    "# Pick middle slice\n",
    "z = ct.shape[0] // 2\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(ct[z], cmap='gray')\n",
    "plt.title(f\"{case_id} - CT (Slice {z})\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ct[z], cmap='gray')\n",
    "plt.imshow(pred[z], cmap='Reds', alpha=0.4)\n",
    "plt.title(f\"{case_id} - Predicted Lung Mask\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running inference on single file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/inference/predict_from_raw_data.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(join(model_training_output_dir, f'fold_{f}', checkpoint_name),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting lung_001:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:22<00:00,  1.63it/s]\n",
      "100%|██████████| 36/36 [00:20<00:00,  1.74it/s]\n",
      "100%|██████████| 36/36 [00:20<00:00,  1.73it/s]\n",
      "100%|██████████| 36/36 [00:20<00:00,  1.73it/s]\n",
      "100%|██████████| 36/36 [00:20<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with lung_001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# \"\\\\wsl.localhost\\Ubuntu-22.04\\home\\doodledaron\\FYP\\nnUnet\\FYP-file\\nnUNet_raw\\Dataset002_Lung_split\\imagesTs\\lung_001_0000.nii.gz\"\n",
    "# === Your single input and output ===\n",
    "input_image = \"/home/doodledaron/FYP/nnUnet/FYP-file/imagesTs_temp/lung_001_0000.nii.gz\"\n",
    "output_dir  = \"/home/doodledaron/FYP/nnUnet/FYP-file/inference_output\"\n",
    "\n",
    "# === Inference config ===\n",
    "dataset_id = 3\n",
    "config = \"3d_fullres\"\n",
    "trainer = \"nnUNetTrainer\"\n",
    "plans = \"nnUNetPlans\"\n",
    "\n",
    "# === Run prediction ===\n",
    "command = (\n",
    "    f\"nnUNetv2_predict \"\n",
    "    f\"-d {dataset_id} \"\n",
    "    f\"-c {config} \"\n",
    "    f\"-tr {trainer} \"\n",
    "    f\"-p {plans} \"\n",
    "    f\"-i {os.path.dirname(input_image)} \"\n",
    "    f\"-o {output_dir} \"\n",
    ")\n",
    "\n",
    "print(\"🚀 Running inference on single file...\")\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 3d_fullres\n",
      "Exporting fold_0\n",
      "Exporting fold_1\n",
      "Exporting fold_2\n",
      "Exporting fold_3\n",
      "Exporting fold_4\n",
      "No ensemble directory found for task 3\n"
     ]
    }
   ],
   "source": [
    "# Exporting the model\n",
    "!nnUNetv2_export_model_to_zip -d 3 -c 3d_fullres -f 0 1 2 3 4 -o lung_segmentation_only_model.zip"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMS5+MfL2vX9zQV3XuIXqH+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FYP-file-w_hePvOQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
